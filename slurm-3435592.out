Running "module reset". Resetting modules to system default. The following $MODULEPATH directories have been removed: None
Starting script...
Output: Requirement already satisfied: torch in /u/choprahetarth/.conda/envs/hetarth_py10/lib/python3.10/site-packages (2.2.2)
Requirement already satisfied: torchvision in /u/choprahetarth/.conda/envs/hetarth_py10/lib/python3.10/site-packages (0.17.2)
Requirement already satisfied: torchaudio in /u/choprahetarth/.conda/envs/hetarth_py10/lib/python3.10/site-packages (2.2.2)
Requirement already satisfied: filelock in /u/choprahetarth/.conda/envs/hetarth_py10/lib/python3.10/site-packages (from torch) (3.13.4)
Requirement already satisfied: typing-extensions>=4.8.0 in /u/choprahetarth/.conda/envs/hetarth_py10/lib/python3.10/site-packages (from torch) (4.11.0)
Requirement already satisfied: sympy in /u/choprahetarth/.conda/envs/hetarth_py10/lib/python3.10/site-packages (from torch) (1.12)
Requirement already satisfied: networkx in /u/choprahetarth/.conda/envs/hetarth_py10/lib/python3.10/site-packages (from torch) (3.3)
Requirement already satisfied: jinja2 in /u/choprahetarth/.conda/envs/hetarth_py10/lib/python3.10/site-packages (from torch) (3.1.3)
Requirement already satisfied: fsspec in /u/choprahetarth/.conda/envs/hetarth_py10/lib/python3.10/site-packages (from torch) (2024.2.0)
Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /u/choprahetarth/.conda/envs/hetarth_py10/lib/python3.10/site-packages (from torch) (12.1.105)
Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /u/choprahetarth/.conda/envs/hetarth_py10/lib/python3.10/site-packages (from torch) (12.1.105)
Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /u/choprahetarth/.conda/envs/hetarth_py10/lib/python3.10/site-packages (from torch) (12.1.105)
Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /u/choprahetarth/.conda/envs/hetarth_py10/lib/python3.10/site-packages (from torch) (8.9.2.26)
Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /u/choprahetarth/.conda/envs/hetarth_py10/lib/python3.10/site-packages (from torch) (12.1.3.1)
Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /u/choprahetarth/.conda/envs/hetarth_py10/lib/python3.10/site-packages (from torch) (11.0.2.54)
Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /u/choprahetarth/.conda/envs/hetarth_py10/lib/python3.10/site-packages (from torch) (10.3.2.106)
Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /u/choprahetarth/.conda/envs/hetarth_py10/lib/python3.10/site-packages (from torch) (11.4.5.107)
Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /u/choprahetarth/.conda/envs/hetarth_py10/lib/python3.10/site-packages (from torch) (12.1.0.106)
Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /u/choprahetarth/.conda/envs/hetarth_py10/lib/python3.10/site-packages (from torch) (2.19.3)
Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /u/choprahetarth/.conda/envs/hetarth_py10/lib/python3.10/site-packages (from torch) (12.1.105)
Requirement already satisfied: triton==2.2.0 in /u/choprahetarth/.conda/envs/hetarth_py10/lib/python3.10/site-packages (from torch) (2.2.0)
Requirement already satisfied: nvidia-nvjitlink-cu12 in /u/choprahetarth/.conda/envs/hetarth_py10/lib/python3.10/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch) (12.4.127)
Requirement already satisfied: numpy in /u/choprahetarth/.conda/envs/hetarth_py10/lib/python3.10/site-packages (from torchvision) (1.26.4)
Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /u/choprahetarth/.conda/envs/hetarth_py10/lib/python3.10/site-packages (from torchvision) (10.3.0)
Requirement already satisfied: MarkupSafe>=2.0 in /u/choprahetarth/.conda/envs/hetarth_py10/lib/python3.10/site-packages (from jinja2->torch) (2.1.5)
Requirement already satisfied: mpmath>=0.19 in /u/choprahetarth/.conda/envs/hetarth_py10/lib/python3.10/site-packages (from sympy->torch) (1.3.0)

Output: Requirement already satisfied: transformers in /u/choprahetarth/.conda/envs/hetarth_py10/lib/python3.10/site-packages (4.40.0.dev0)
Requirement already satisfied: filelock in /u/choprahetarth/.conda/envs/hetarth_py10/lib/python3.10/site-packages (from transformers) (3.13.4)
Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /u/choprahetarth/.conda/envs/hetarth_py10/lib/python3.10/site-packages (from transformers) (0.22.2)
Requirement already satisfied: numpy>=1.17 in /u/choprahetarth/.conda/envs/hetarth_py10/lib/python3.10/site-packages (from transformers) (1.26.4)
Requirement already satisfied: packaging>=20.0 in /u/choprahetarth/.conda/envs/hetarth_py10/lib/python3.10/site-packages (from transformers) (24.0)
Requirement already satisfied: pyyaml>=5.1 in /u/choprahetarth/.conda/envs/hetarth_py10/lib/python3.10/site-packages (from transformers) (6.0.1)
Requirement already satisfied: regex!=2019.12.17 in /u/choprahetarth/.conda/envs/hetarth_py10/lib/python3.10/site-packages (from transformers) (2023.12.25)
Requirement already satisfied: requests in /u/choprahetarth/.conda/envs/hetarth_py10/lib/python3.10/site-packages (from transformers) (2.31.0)
Requirement already satisfied: tokenizers<0.19,>=0.14 in /u/choprahetarth/.conda/envs/hetarth_py10/lib/python3.10/site-packages (from transformers) (0.15.2)
Requirement already satisfied: safetensors>=0.4.1 in /u/choprahetarth/.conda/envs/hetarth_py10/lib/python3.10/site-packages (from transformers) (0.4.2)
Requirement already satisfied: tqdm>=4.27 in /u/choprahetarth/.conda/envs/hetarth_py10/lib/python3.10/site-packages (from transformers) (4.66.2)
Requirement already satisfied: fsspec>=2023.5.0 in /u/choprahetarth/.conda/envs/hetarth_py10/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (2024.2.0)
Requirement already satisfied: typing-extensions>=3.7.4.3 in /u/choprahetarth/.conda/envs/hetarth_py10/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (4.11.0)
Requirement already satisfied: charset-normalizer<4,>=2 in /u/choprahetarth/.conda/envs/hetarth_py10/lib/python3.10/site-packages (from requests->transformers) (3.3.2)
Requirement already satisfied: idna<4,>=2.5 in /u/choprahetarth/.conda/envs/hetarth_py10/lib/python3.10/site-packages (from requests->transformers) (3.7)
Requirement already satisfied: urllib3<3,>=1.21.1 in /u/choprahetarth/.conda/envs/hetarth_py10/lib/python3.10/site-packages (from requests->transformers) (2.2.1)
Requirement already satisfied: certifi>=2017.4.17 in /u/choprahetarth/.conda/envs/hetarth_py10/lib/python3.10/site-packages (from requests->transformers) (2024.2.2)

Output: Found existing installation: peft 0.10.1.dev0
Uninstalling peft-0.10.1.dev0:
  Would remove:
    /u/choprahetarth/.conda/envs/hetarth_py10/lib/python3.10/site-packages/peft-0.10.1.dev0.dist-info/*
    /u/choprahetarth/.conda/envs/hetarth_py10/lib/python3.10/site-packages/peft/*
Proceed (Y/n)?   Successfully uninstalled peft-0.10.1.dev0

Output: Collecting git+https://github.com/huggingface/peft.git
  Cloning https://github.com/huggingface/peft.git to /tmp/pip-req-build-bqs_a5ty
  Resolved https://github.com/huggingface/peft.git to commit ed865e2812bd4bf3946292430525d0825b0fab7e
  Installing build dependencies: started
  Installing build dependencies: finished with status 'done'
  Getting requirements to build wheel: started
  Getting requirements to build wheel: finished with status 'done'
  Preparing metadata (pyproject.toml): started
  Preparing metadata (pyproject.toml): finished with status 'done'
Requirement already satisfied: numpy>=1.17 in /u/choprahetarth/.conda/envs/hetarth_py10/lib/python3.10/site-packages (from peft==0.10.1.dev0) (1.26.4)
Requirement already satisfied: packaging>=20.0 in /u/choprahetarth/.conda/envs/hetarth_py10/lib/python3.10/site-packages (from peft==0.10.1.dev0) (24.0)
Requirement already satisfied: psutil in /u/choprahetarth/.conda/envs/hetarth_py10/lib/python3.10/site-packages (from peft==0.10.1.dev0) (5.9.8)
Requirement already satisfied: pyyaml in /u/choprahetarth/.conda/envs/hetarth_py10/lib/python3.10/site-packages (from peft==0.10.1.dev0) (6.0.1)
Requirement already satisfied: torch>=1.13.0 in /u/choprahetarth/.conda/envs/hetarth_py10/lib/python3.10/site-packages (from peft==0.10.1.dev0) (2.2.2)
Requirement already satisfied: transformers in /u/choprahetarth/.conda/envs/hetarth_py10/lib/python3.10/site-packages (from peft==0.10.1.dev0) (4.40.0.dev0)
Requirement already satisfied: tqdm in /u/choprahetarth/.conda/envs/hetarth_py10/lib/python3.10/site-packages (from peft==0.10.1.dev0) (4.66.2)
Requirement already satisfied: accelerate>=0.21.0 in /u/choprahetarth/.conda/envs/hetarth_py10/lib/python3.10/site-packages (from peft==0.10.1.dev0) (0.27.2)
Requirement already satisfied: safetensors in /u/choprahetarth/.conda/envs/hetarth_py10/lib/python3.10/site-packages (from peft==0.10.1.dev0) (0.4.2)
Requirement already satisfied: huggingface-hub>=0.17.0 in /u/choprahetarth/.conda/envs/hetarth_py10/lib/python3.10/site-packages (from peft==0.10.1.dev0) (0.22.2)
Requirement already satisfied: filelock in /u/choprahetarth/.conda/envs/hetarth_py10/lib/python3.10/site-packages (from huggingface-hub>=0.17.0->peft==0.10.1.dev0) (3.13.4)
Requirement already satisfied: fsspec>=2023.5.0 in /u/choprahetarth/.conda/envs/hetarth_py10/lib/python3.10/site-packages (from huggingface-hub>=0.17.0->peft==0.10.1.dev0) (2024.2.0)
Requirement already satisfied: requests in /u/choprahetarth/.conda/envs/hetarth_py10/lib/python3.10/site-packages (from huggingface-hub>=0.17.0->peft==0.10.1.dev0) (2.31.0)
Requirement already satisfied: typing-extensions>=3.7.4.3 in /u/choprahetarth/.conda/envs/hetarth_py10/lib/python3.10/site-packages (from huggingface-hub>=0.17.0->peft==0.10.1.dev0) (4.11.0)
Requirement already satisfied: sympy in /u/choprahetarth/.conda/envs/hetarth_py10/lib/python3.10/site-packages (from torch>=1.13.0->peft==0.10.1.dev0) (1.12)
Requirement already satisfied: networkx in /u/choprahetarth/.conda/envs/hetarth_py10/lib/python3.10/site-packages (from torch>=1.13.0->peft==0.10.1.dev0) (3.3)
Requirement already satisfied: jinja2 in /u/choprahetarth/.conda/envs/hetarth_py10/lib/python3.10/site-packages (from torch>=1.13.0->peft==0.10.1.dev0) (3.1.3)
Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /u/choprahetarth/.conda/envs/hetarth_py10/lib/python3.10/site-packages (from torch>=1.13.0->peft==0.10.1.dev0) (12.1.105)
Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /u/choprahetarth/.conda/envs/hetarth_py10/lib/python3.10/site-packages (from torch>=1.13.0->peft==0.10.1.dev0) (12.1.105)
Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /u/choprahetarth/.conda/envs/hetarth_py10/lib/python3.10/site-packages (from torch>=1.13.0->peft==0.10.1.dev0) (12.1.105)
Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /u/choprahetarth/.conda/envs/hetarth_py10/lib/python3.10/site-packages (from torch>=1.13.0->peft==0.10.1.dev0) (8.9.2.26)
Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /u/choprahetarth/.conda/envs/hetarth_py10/lib/python3.10/site-packages (from torch>=1.13.0->peft==0.10.1.dev0) (12.1.3.1)
Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /u/choprahetarth/.conda/envs/hetarth_py10/lib/python3.10/site-packages (from torch>=1.13.0->peft==0.10.1.dev0) (11.0.2.54)
Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /u/choprahetarth/.conda/envs/hetarth_py10/lib/python3.10/site-packages (from torch>=1.13.0->peft==0.10.1.dev0) (10.3.2.106)
Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /u/choprahetarth/.conda/envs/hetarth_py10/lib/python3.10/site-packages (from torch>=1.13.0->peft==0.10.1.dev0) (11.4.5.107)
Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /u/choprahetarth/.conda/envs/hetarth_py10/lib/python3.10/site-packages (from torch>=1.13.0->peft==0.10.1.dev0) (12.1.0.106)
Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /u/choprahetarth/.conda/envs/hetarth_py10/lib/python3.10/site-packages (from torch>=1.13.0->peft==0.10.1.dev0) (2.19.3)
Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /u/choprahetarth/.conda/envs/hetarth_py10/lib/python3.10/site-packages (from torch>=1.13.0->peft==0.10.1.dev0) (12.1.105)
Requirement already satisfied: triton==2.2.0 in /u/choprahetarth/.conda/envs/hetarth_py10/lib/python3.10/site-packages (from torch>=1.13.0->peft==0.10.1.dev0) (2.2.0)
Requirement already satisfied: nvidia-nvjitlink-cu12 in /u/choprahetarth/.conda/envs/hetarth_py10/lib/python3.10/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.13.0->peft==0.10.1.dev0) (12.4.127)
Requirement already satisfied: regex!=2019.12.17 in /u/choprahetarth/.conda/envs/hetarth_py10/lib/python3.10/site-packages (from transformers->peft==0.10.1.dev0) (2023.12.25)
Requirement already satisfied: tokenizers<0.19,>=0.14 in /u/choprahetarth/.conda/envs/hetarth_py10/lib/python3.10/site-packages (from transformers->peft==0.10.1.dev0) (0.15.2)
Requirement already satisfied: MarkupSafe>=2.0 in /u/choprahetarth/.conda/envs/hetarth_py10/lib/python3.10/site-packages (from jinja2->torch>=1.13.0->peft==0.10.1.dev0) (2.1.5)
Requirement already satisfied: charset-normalizer<4,>=2 in /u/choprahetarth/.conda/envs/hetarth_py10/lib/python3.10/site-packages (from requests->huggingface-hub>=0.17.0->peft==0.10.1.dev0) (3.3.2)
Requirement already satisfied: idna<4,>=2.5 in /u/choprahetarth/.conda/envs/hetarth_py10/lib/python3.10/site-packages (from requests->huggingface-hub>=0.17.0->peft==0.10.1.dev0) (3.7)
Requirement already satisfied: urllib3<3,>=1.21.1 in /u/choprahetarth/.conda/envs/hetarth_py10/lib/python3.10/site-packages (from requests->huggingface-hub>=0.17.0->peft==0.10.1.dev0) (2.2.1)
Requirement already satisfied: certifi>=2017.4.17 in /u/choprahetarth/.conda/envs/hetarth_py10/lib/python3.10/site-packages (from requests->huggingface-hub>=0.17.0->peft==0.10.1.dev0) (2024.2.2)
Requirement already satisfied: mpmath>=0.19 in /u/choprahetarth/.conda/envs/hetarth_py10/lib/python3.10/site-packages (from sympy->torch>=1.13.0->peft==0.10.1.dev0) (1.3.0)
Building wheels for collected packages: peft
  Building wheel for peft (pyproject.toml): started
  Building wheel for peft (pyproject.toml): finished with status 'done'
  Created wheel for peft: filename=peft-0.10.1.dev0-py3-none-any.whl size=218560 sha256=bd88de0af17693af07a3945808c9225b0fdcf12600304b2f12febdb7499bd4c6
  Stored in directory: /tmp/pip-ephem-wheel-cache-hvmj_m79/wheels/d7/c7/de/1368fac8590e1b103ddc2ec2a28ad51d83aded1a3830e8a087
Successfully built peft
Installing collected packages: peft
Successfully installed peft-0.10.1.dev0

Output: Collecting git+https://github.com/huggingface/transformers
  Cloning https://github.com/huggingface/transformers to /tmp/pip-req-build-dow8klrt
  Resolved https://github.com/huggingface/transformers to commit ec92f983af5295fc92414a37b988d8384785988a
  Installing build dependencies: started
  Installing build dependencies: finished with status 'done'
  Getting requirements to build wheel: started
  Getting requirements to build wheel: finished with status 'done'
  Preparing metadata (pyproject.toml): started
  Preparing metadata (pyproject.toml): finished with status 'done'
Requirement already satisfied: filelock in /u/choprahetarth/.conda/envs/hetarth_py10/lib/python3.10/site-packages (from transformers==4.40.0.dev0) (3.13.4)
Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /u/choprahetarth/.conda/envs/hetarth_py10/lib/python3.10/site-packages (from transformers==4.40.0.dev0) (0.22.2)
Requirement already satisfied: numpy>=1.17 in /u/choprahetarth/.conda/envs/hetarth_py10/lib/python3.10/site-packages (from transformers==4.40.0.dev0) (1.26.4)
Requirement already satisfied: packaging>=20.0 in /u/choprahetarth/.conda/envs/hetarth_py10/lib/python3.10/site-packages (from transformers==4.40.0.dev0) (24.0)
Requirement already satisfied: pyyaml>=5.1 in /u/choprahetarth/.conda/envs/hetarth_py10/lib/python3.10/site-packages (from transformers==4.40.0.dev0) (6.0.1)
Requirement already satisfied: regex!=2019.12.17 in /u/choprahetarth/.conda/envs/hetarth_py10/lib/python3.10/site-packages (from transformers==4.40.0.dev0) (2023.12.25)
Requirement already satisfied: requests in /u/choprahetarth/.conda/envs/hetarth_py10/lib/python3.10/site-packages (from transformers==4.40.0.dev0) (2.31.0)
Collecting tokenizers<0.20,>=0.19 (from transformers==4.40.0.dev0)
  Downloading tokenizers-0.19.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)
Requirement already satisfied: safetensors>=0.4.1 in /u/choprahetarth/.conda/envs/hetarth_py10/lib/python3.10/site-packages (from transformers==4.40.0.dev0) (0.4.2)
Requirement already satisfied: tqdm>=4.27 in /u/choprahetarth/.conda/envs/hetarth_py10/lib/python3.10/site-packages (from transformers==4.40.0.dev0) (4.66.2)
Requirement already satisfied: fsspec>=2023.5.0 in /u/choprahetarth/.conda/envs/hetarth_py10/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers==4.40.0.dev0) (2024.2.0)
Requirement already satisfied: typing-extensions>=3.7.4.3 in /u/choprahetarth/.conda/envs/hetarth_py10/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers==4.40.0.dev0) (4.11.0)
Requirement already satisfied: charset-normalizer<4,>=2 in /u/choprahetarth/.conda/envs/hetarth_py10/lib/python3.10/site-packages (from requests->transformers==4.40.0.dev0) (3.3.2)
Requirement already satisfied: idna<4,>=2.5 in /u/choprahetarth/.conda/envs/hetarth_py10/lib/python3.10/site-packages (from requests->transformers==4.40.0.dev0) (3.7)
Requirement already satisfied: urllib3<3,>=1.21.1 in /u/choprahetarth/.conda/envs/hetarth_py10/lib/python3.10/site-packages (from requests->transformers==4.40.0.dev0) (2.2.1)
Requirement already satisfied: certifi>=2017.4.17 in /u/choprahetarth/.conda/envs/hetarth_py10/lib/python3.10/site-packages (from requests->transformers==4.40.0.dev0) (2024.2.2)
Downloading tokenizers-0.19.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)
   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 3.6/3.6 MB 32.1 MB/s eta 0:00:00
Installing collected packages: tokenizers
  Attempting uninstall: tokenizers
    Found existing installation: tokenizers 0.15.2
    Uninstalling tokenizers-0.15.2:
      Successfully uninstalled tokenizers-0.15.2
Successfully installed tokenizers-0.19.1

Output: Requirement already satisfied: datasets in /u/choprahetarth/.conda/envs/hetarth_py10/lib/python3.10/site-packages (2.18.0)
Requirement already satisfied: filelock in /u/choprahetarth/.conda/envs/hetarth_py10/lib/python3.10/site-packages (from datasets) (3.13.4)
Requirement already satisfied: numpy>=1.17 in /u/choprahetarth/.conda/envs/hetarth_py10/lib/python3.10/site-packages (from datasets) (1.26.4)
Requirement already satisfied: pyarrow>=12.0.0 in /u/choprahetarth/.conda/envs/hetarth_py10/lib/python3.10/site-packages (from datasets) (15.0.2)
Requirement already satisfied: pyarrow-hotfix in /u/choprahetarth/.conda/envs/hetarth_py10/lib/python3.10/site-packages (from datasets) (0.6)
Requirement already satisfied: dill<0.3.9,>=0.3.0 in /u/choprahetarth/.conda/envs/hetarth_py10/lib/python3.10/site-packages (from datasets) (0.3.8)
Requirement already satisfied: pandas in /u/choprahetarth/.conda/envs/hetarth_py10/lib/python3.10/site-packages (from datasets) (2.2.2)
Requirement already satisfied: requests>=2.19.0 in /u/choprahetarth/.conda/envs/hetarth_py10/lib/python3.10/site-packages (from datasets) (2.31.0)
Requirement already satisfied: tqdm>=4.62.1 in /u/choprahetarth/.conda/envs/hetarth_py10/lib/python3.10/site-packages (from datasets) (4.66.2)
Requirement already satisfied: xxhash in /u/choprahetarth/.conda/envs/hetarth_py10/lib/python3.10/site-packages (from datasets) (3.4.1)
Requirement already satisfied: multiprocess in /u/choprahetarth/.conda/envs/hetarth_py10/lib/python3.10/site-packages (from datasets) (0.70.16)
Requirement already satisfied: fsspec<=2024.2.0,>=2023.1.0 in /u/choprahetarth/.conda/envs/hetarth_py10/lib/python3.10/site-packages (from fsspec[http]<=2024.2.0,>=2023.1.0->datasets) (2024.2.0)
Requirement already satisfied: aiohttp in /u/choprahetarth/.conda/envs/hetarth_py10/lib/python3.10/site-packages (from datasets) (3.9.3)
Requirement already satisfied: huggingface-hub>=0.19.4 in /u/choprahetarth/.conda/envs/hetarth_py10/lib/python3.10/site-packages (from datasets) (0.22.2)
Requirement already satisfied: packaging in /u/choprahetarth/.conda/envs/hetarth_py10/lib/python3.10/site-packages (from datasets) (24.0)
Requirement already satisfied: pyyaml>=5.1 in /u/choprahetarth/.conda/envs/hetarth_py10/lib/python3.10/site-packages (from datasets) (6.0.1)
Requirement already satisfied: aiosignal>=1.1.2 in /u/choprahetarth/.conda/envs/hetarth_py10/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.1)
Requirement already satisfied: attrs>=17.3.0 in /u/choprahetarth/.conda/envs/hetarth_py10/lib/python3.10/site-packages (from aiohttp->datasets) (23.2.0)
Requirement already satisfied: frozenlist>=1.1.1 in /u/choprahetarth/.conda/envs/hetarth_py10/lib/python3.10/site-packages (from aiohttp->datasets) (1.4.1)
Requirement already satisfied: multidict<7.0,>=4.5 in /u/choprahetarth/.conda/envs/hetarth_py10/lib/python3.10/site-packages (from aiohttp->datasets) (6.0.5)
Requirement already satisfied: yarl<2.0,>=1.0 in /u/choprahetarth/.conda/envs/hetarth_py10/lib/python3.10/site-packages (from aiohttp->datasets) (1.9.4)
Requirement already satisfied: async-timeout<5.0,>=4.0 in /u/choprahetarth/.conda/envs/hetarth_py10/lib/python3.10/site-packages (from aiohttp->datasets) (4.0.3)
Requirement already satisfied: typing-extensions>=3.7.4.3 in /u/choprahetarth/.conda/envs/hetarth_py10/lib/python3.10/site-packages (from huggingface-hub>=0.19.4->datasets) (4.11.0)
Requirement already satisfied: charset-normalizer<4,>=2 in /u/choprahetarth/.conda/envs/hetarth_py10/lib/python3.10/site-packages (from requests>=2.19.0->datasets) (3.3.2)
Requirement already satisfied: idna<4,>=2.5 in /u/choprahetarth/.conda/envs/hetarth_py10/lib/python3.10/site-packages (from requests>=2.19.0->datasets) (3.7)
Requirement already satisfied: urllib3<3,>=1.21.1 in /u/choprahetarth/.conda/envs/hetarth_py10/lib/python3.10/site-packages (from requests>=2.19.0->datasets) (2.2.1)
Requirement already satisfied: certifi>=2017.4.17 in /u/choprahetarth/.conda/envs/hetarth_py10/lib/python3.10/site-packages (from requests>=2.19.0->datasets) (2024.2.2)
Requirement already satisfied: python-dateutil>=2.8.2 in /u/choprahetarth/.conda/envs/hetarth_py10/lib/python3.10/site-packages (from pandas->datasets) (2.9.0.post0)
Requirement already satisfied: pytz>=2020.1 in /u/choprahetarth/.conda/envs/hetarth_py10/lib/python3.10/site-packages (from pandas->datasets) (2024.1)
Requirement already satisfied: tzdata>=2022.7 in /u/choprahetarth/.conda/envs/hetarth_py10/lib/python3.10/site-packages (from pandas->datasets) (2024.1)
Requirement already satisfied: six>=1.5 in /u/choprahetarth/.conda/envs/hetarth_py10/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)

Output: Requirement already satisfied: accelerate in /u/choprahetarth/.conda/envs/hetarth_py10/lib/python3.10/site-packages (0.27.2)
Requirement already satisfied: numpy>=1.17 in /u/choprahetarth/.conda/envs/hetarth_py10/lib/python3.10/site-packages (from accelerate) (1.26.4)
Requirement already satisfied: packaging>=20.0 in /u/choprahetarth/.conda/envs/hetarth_py10/lib/python3.10/site-packages (from accelerate) (24.0)
Requirement already satisfied: psutil in /u/choprahetarth/.conda/envs/hetarth_py10/lib/python3.10/site-packages (from accelerate) (5.9.8)
Requirement already satisfied: pyyaml in /u/choprahetarth/.conda/envs/hetarth_py10/lib/python3.10/site-packages (from accelerate) (6.0.1)
Requirement already satisfied: torch>=1.10.0 in /u/choprahetarth/.conda/envs/hetarth_py10/lib/python3.10/site-packages (from accelerate) (2.2.2)
Requirement already satisfied: huggingface-hub in /u/choprahetarth/.conda/envs/hetarth_py10/lib/python3.10/site-packages (from accelerate) (0.22.2)
Requirement already satisfied: safetensors>=0.3.1 in /u/choprahetarth/.conda/envs/hetarth_py10/lib/python3.10/site-packages (from accelerate) (0.4.2)
Requirement already satisfied: filelock in /u/choprahetarth/.conda/envs/hetarth_py10/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.13.4)
Requirement already satisfied: typing-extensions>=4.8.0 in /u/choprahetarth/.conda/envs/hetarth_py10/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (4.11.0)
Requirement already satisfied: sympy in /u/choprahetarth/.conda/envs/hetarth_py10/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (1.12)
Requirement already satisfied: networkx in /u/choprahetarth/.conda/envs/hetarth_py10/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.3)
Requirement already satisfied: jinja2 in /u/choprahetarth/.conda/envs/hetarth_py10/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.1.3)
Requirement already satisfied: fsspec in /u/choprahetarth/.conda/envs/hetarth_py10/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (2024.2.0)
Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /u/choprahetarth/.conda/envs/hetarth_py10/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (12.1.105)
Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /u/choprahetarth/.conda/envs/hetarth_py10/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (12.1.105)
Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /u/choprahetarth/.conda/envs/hetarth_py10/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (12.1.105)
Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /u/choprahetarth/.conda/envs/hetarth_py10/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (8.9.2.26)
Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /u/choprahetarth/.conda/envs/hetarth_py10/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (12.1.3.1)
Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /u/choprahetarth/.conda/envs/hetarth_py10/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (11.0.2.54)
Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /u/choprahetarth/.conda/envs/hetarth_py10/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (10.3.2.106)
Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /u/choprahetarth/.conda/envs/hetarth_py10/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (11.4.5.107)
Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /u/choprahetarth/.conda/envs/hetarth_py10/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (12.1.0.106)
Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /u/choprahetarth/.conda/envs/hetarth_py10/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (2.19.3)
Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /u/choprahetarth/.conda/envs/hetarth_py10/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (12.1.105)
Requirement already satisfied: triton==2.2.0 in /u/choprahetarth/.conda/envs/hetarth_py10/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (2.2.0)
Requirement already satisfied: nvidia-nvjitlink-cu12 in /u/choprahetarth/.conda/envs/hetarth_py10/lib/python3.10/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.10.0->accelerate) (12.4.127)
Requirement already satisfied: requests in /u/choprahetarth/.conda/envs/hetarth_py10/lib/python3.10/site-packages (from huggingface-hub->accelerate) (2.31.0)
Requirement already satisfied: tqdm>=4.42.1 in /u/choprahetarth/.conda/envs/hetarth_py10/lib/python3.10/site-packages (from huggingface-hub->accelerate) (4.66.2)
Requirement already satisfied: MarkupSafe>=2.0 in /u/choprahetarth/.conda/envs/hetarth_py10/lib/python3.10/site-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.5)
Requirement already satisfied: charset-normalizer<4,>=2 in /u/choprahetarth/.conda/envs/hetarth_py10/lib/python3.10/site-packages (from requests->huggingface-hub->accelerate) (3.3.2)
Requirement already satisfied: idna<4,>=2.5 in /u/choprahetarth/.conda/envs/hetarth_py10/lib/python3.10/site-packages (from requests->huggingface-hub->accelerate) (3.7)
Requirement already satisfied: urllib3<3,>=1.21.1 in /u/choprahetarth/.conda/envs/hetarth_py10/lib/python3.10/site-packages (from requests->huggingface-hub->accelerate) (2.2.1)
Requirement already satisfied: certifi>=2017.4.17 in /u/choprahetarth/.conda/envs/hetarth_py10/lib/python3.10/site-packages (from requests->huggingface-hub->accelerate) (2024.2.2)
Requirement already satisfied: mpmath>=0.19 in /u/choprahetarth/.conda/envs/hetarth_py10/lib/python3.10/site-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)

Output: Requirement already satisfied: huggingface_hub in /u/choprahetarth/.conda/envs/hetarth_py10/lib/python3.10/site-packages (0.22.2)
Requirement already satisfied: filelock in /u/choprahetarth/.conda/envs/hetarth_py10/lib/python3.10/site-packages (from huggingface_hub) (3.13.4)
Requirement already satisfied: fsspec>=2023.5.0 in /u/choprahetarth/.conda/envs/hetarth_py10/lib/python3.10/site-packages (from huggingface_hub) (2024.2.0)
Requirement already satisfied: packaging>=20.9 in /u/choprahetarth/.conda/envs/hetarth_py10/lib/python3.10/site-packages (from huggingface_hub) (24.0)
Requirement already satisfied: pyyaml>=5.1 in /u/choprahetarth/.conda/envs/hetarth_py10/lib/python3.10/site-packages (from huggingface_hub) (6.0.1)
Requirement already satisfied: requests in /u/choprahetarth/.conda/envs/hetarth_py10/lib/python3.10/site-packages (from huggingface_hub) (2.31.0)
Requirement already satisfied: tqdm>=4.42.1 in /u/choprahetarth/.conda/envs/hetarth_py10/lib/python3.10/site-packages (from huggingface_hub) (4.66.2)
Requirement already satisfied: typing-extensions>=3.7.4.3 in /u/choprahetarth/.conda/envs/hetarth_py10/lib/python3.10/site-packages (from huggingface_hub) (4.11.0)
Requirement already satisfied: charset-normalizer<4,>=2 in /u/choprahetarth/.conda/envs/hetarth_py10/lib/python3.10/site-packages (from requests->huggingface_hub) (3.3.2)
Requirement already satisfied: idna<4,>=2.5 in /u/choprahetarth/.conda/envs/hetarth_py10/lib/python3.10/site-packages (from requests->huggingface_hub) (3.7)
Requirement already satisfied: urllib3<3,>=1.21.1 in /u/choprahetarth/.conda/envs/hetarth_py10/lib/python3.10/site-packages (from requests->huggingface_hub) (2.2.1)
Requirement already satisfied: certifi>=2017.4.17 in /u/choprahetarth/.conda/envs/hetarth_py10/lib/python3.10/site-packages (from requests->huggingface_hub) (2024.2.2)

Output: Requirement already satisfied: bitsandbytes in /u/choprahetarth/.conda/envs/hetarth_py10/lib/python3.10/site-packages (0.43.0)
Requirement already satisfied: torch in /u/choprahetarth/.conda/envs/hetarth_py10/lib/python3.10/site-packages (from bitsandbytes) (2.2.2)
Requirement already satisfied: numpy in /u/choprahetarth/.conda/envs/hetarth_py10/lib/python3.10/site-packages (from bitsandbytes) (1.26.4)
Requirement already satisfied: filelock in /u/choprahetarth/.conda/envs/hetarth_py10/lib/python3.10/site-packages (from torch->bitsandbytes) (3.13.4)
Requirement already satisfied: typing-extensions>=4.8.0 in /u/choprahetarth/.conda/envs/hetarth_py10/lib/python3.10/site-packages (from torch->bitsandbytes) (4.11.0)
Requirement already satisfied: sympy in /u/choprahetarth/.conda/envs/hetarth_py10/lib/python3.10/site-packages (from torch->bitsandbytes) (1.12)
Requirement already satisfied: networkx in /u/choprahetarth/.conda/envs/hetarth_py10/lib/python3.10/site-packages (from torch->bitsandbytes) (3.3)
Requirement already satisfied: jinja2 in /u/choprahetarth/.conda/envs/hetarth_py10/lib/python3.10/site-packages (from torch->bitsandbytes) (3.1.3)
Requirement already satisfied: fsspec in /u/choprahetarth/.conda/envs/hetarth_py10/lib/python3.10/site-packages (from torch->bitsandbytes) (2024.2.0)
Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /u/choprahetarth/.conda/envs/hetarth_py10/lib/python3.10/site-packages (from torch->bitsandbytes) (12.1.105)
Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /u/choprahetarth/.conda/envs/hetarth_py10/lib/python3.10/site-packages (from torch->bitsandbytes) (12.1.105)
Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /u/choprahetarth/.conda/envs/hetarth_py10/lib/python3.10/site-packages (from torch->bitsandbytes) (12.1.105)
Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /u/choprahetarth/.conda/envs/hetarth_py10/lib/python3.10/site-packages (from torch->bitsandbytes) (8.9.2.26)
Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /u/choprahetarth/.conda/envs/hetarth_py10/lib/python3.10/site-packages (from torch->bitsandbytes) (12.1.3.1)
Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /u/choprahetarth/.conda/envs/hetarth_py10/lib/python3.10/site-packages (from torch->bitsandbytes) (11.0.2.54)
Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /u/choprahetarth/.conda/envs/hetarth_py10/lib/python3.10/site-packages (from torch->bitsandbytes) (10.3.2.106)
Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /u/choprahetarth/.conda/envs/hetarth_py10/lib/python3.10/site-packages (from torch->bitsandbytes) (11.4.5.107)
Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /u/choprahetarth/.conda/envs/hetarth_py10/lib/python3.10/site-packages (from torch->bitsandbytes) (12.1.0.106)
Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /u/choprahetarth/.conda/envs/hetarth_py10/lib/python3.10/site-packages (from torch->bitsandbytes) (2.19.3)
Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /u/choprahetarth/.conda/envs/hetarth_py10/lib/python3.10/site-packages (from torch->bitsandbytes) (12.1.105)
Requirement already satisfied: triton==2.2.0 in /u/choprahetarth/.conda/envs/hetarth_py10/lib/python3.10/site-packages (from torch->bitsandbytes) (2.2.0)
Requirement already satisfied: nvidia-nvjitlink-cu12 in /u/choprahetarth/.conda/envs/hetarth_py10/lib/python3.10/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch->bitsandbytes) (12.4.127)
Requirement already satisfied: MarkupSafe>=2.0 in /u/choprahetarth/.conda/envs/hetarth_py10/lib/python3.10/site-packages (from jinja2->torch->bitsandbytes) (2.1.5)
Requirement already satisfied: mpmath>=0.19 in /u/choprahetarth/.conda/envs/hetarth_py10/lib/python3.10/site-packages (from sympy->torch->bitsandbytes) (1.3.0)

Output: Requirement already satisfied: wandb in /u/choprahetarth/.conda/envs/hetarth_py10/lib/python3.10/site-packages (0.16.6)
Requirement already satisfied: Click!=8.0.0,>=7.1 in /u/choprahetarth/.conda/envs/hetarth_py10/lib/python3.10/site-packages (from wandb) (8.1.7)
Requirement already satisfied: GitPython!=3.1.29,>=1.0.0 in /u/choprahetarth/.conda/envs/hetarth_py10/lib/python3.10/site-packages (from wandb) (3.1.43)
Requirement already satisfied: requests<3,>=2.0.0 in /u/choprahetarth/.conda/envs/hetarth_py10/lib/python3.10/site-packages (from wandb) (2.31.0)
Requirement already satisfied: psutil>=5.0.0 in /u/choprahetarth/.conda/envs/hetarth_py10/lib/python3.10/site-packages (from wandb) (5.9.8)
Requirement already satisfied: sentry-sdk>=1.0.0 in /u/choprahetarth/.conda/envs/hetarth_py10/lib/python3.10/site-packages (from wandb) (1.45.0)
Requirement already satisfied: docker-pycreds>=0.4.0 in /u/choprahetarth/.conda/envs/hetarth_py10/lib/python3.10/site-packages (from wandb) (0.4.0)
Requirement already satisfied: PyYAML in /u/choprahetarth/.conda/envs/hetarth_py10/lib/python3.10/site-packages (from wandb) (6.0.1)
Requirement already satisfied: setproctitle in /u/choprahetarth/.conda/envs/hetarth_py10/lib/python3.10/site-packages (from wandb) (1.3.3)
Requirement already satisfied: setuptools in /u/choprahetarth/.conda/envs/hetarth_py10/lib/python3.10/site-packages (from wandb) (68.2.2)
Requirement already satisfied: appdirs>=1.4.3 in /u/choprahetarth/.conda/envs/hetarth_py10/lib/python3.10/site-packages (from wandb) (1.4.4)
Requirement already satisfied: protobuf!=4.21.0,<5,>=3.19.0 in /u/choprahetarth/.conda/envs/hetarth_py10/lib/python3.10/site-packages (from wandb) (4.25.3)
Requirement already satisfied: six>=1.4.0 in /u/choprahetarth/.conda/envs/hetarth_py10/lib/python3.10/site-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)
Requirement already satisfied: gitdb<5,>=4.0.1 in /u/choprahetarth/.conda/envs/hetarth_py10/lib/python3.10/site-packages (from GitPython!=3.1.29,>=1.0.0->wandb) (4.0.11)
Requirement already satisfied: charset-normalizer<4,>=2 in /u/choprahetarth/.conda/envs/hetarth_py10/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (3.3.2)
Requirement already satisfied: idna<4,>=2.5 in /u/choprahetarth/.conda/envs/hetarth_py10/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (3.7)
Requirement already satisfied: urllib3<3,>=1.21.1 in /u/choprahetarth/.conda/envs/hetarth_py10/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (2.2.1)
Requirement already satisfied: certifi>=2017.4.17 in /u/choprahetarth/.conda/envs/hetarth_py10/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (2024.2.2)
Requirement already satisfied: smmap<6,>=3.0.1 in /u/choprahetarth/.conda/envs/hetarth_py10/lib/python3.10/site-packages (from gitdb<5,>=4.0.1->GitPython!=3.1.29,>=1.0.0->wandb) (5.0.1)

Output: Requirement already satisfied: scikit-learn in /u/choprahetarth/.conda/envs/hetarth_py10/lib/python3.10/site-packages (1.4.2)
Requirement already satisfied: numpy>=1.19.5 in /u/choprahetarth/.conda/envs/hetarth_py10/lib/python3.10/site-packages (from scikit-learn) (1.26.4)
Requirement already satisfied: scipy>=1.6.0 in /u/choprahetarth/.conda/envs/hetarth_py10/lib/python3.10/site-packages (from scikit-learn) (1.13.0)
Requirement already satisfied: joblib>=1.2.0 in /u/choprahetarth/.conda/envs/hetarth_py10/lib/python3.10/site-packages (from scikit-learn) (1.4.0)
Requirement already satisfied: threadpoolctl>=2.0.0 in /u/choprahetarth/.conda/envs/hetarth_py10/lib/python3.10/site-packages (from scikit-learn) (3.4.0)

Output: Requirement already satisfied: code_bert_score in /u/choprahetarth/.conda/envs/hetarth_py10/lib/python3.10/site-packages (0.4.1)
Requirement already satisfied: torch>=1.0.0 in /u/choprahetarth/.conda/envs/hetarth_py10/lib/python3.10/site-packages (from code_bert_score) (2.2.2)
Requirement already satisfied: numpy in /u/choprahetarth/.conda/envs/hetarth_py10/lib/python3.10/site-packages (from code_bert_score) (1.26.4)
Requirement already satisfied: pandas>=1.0.1 in /u/choprahetarth/.conda/envs/hetarth_py10/lib/python3.10/site-packages (from code_bert_score) (2.2.2)
Requirement already satisfied: requests in /u/choprahetarth/.conda/envs/hetarth_py10/lib/python3.10/site-packages (from code_bert_score) (2.31.0)
Requirement already satisfied: tqdm>=4.31.1 in /u/choprahetarth/.conda/envs/hetarth_py10/lib/python3.10/site-packages (from code_bert_score) (4.66.2)
Requirement already satisfied: matplotlib in /u/choprahetarth/.conda/envs/hetarth_py10/lib/python3.10/site-packages (from code_bert_score) (3.8.4)
Requirement already satisfied: transformers>=3.0.0 in /u/choprahetarth/.conda/envs/hetarth_py10/lib/python3.10/site-packages (from code_bert_score) (4.40.0.dev0)
Requirement already satisfied: python-dateutil>=2.8.2 in /u/choprahetarth/.conda/envs/hetarth_py10/lib/python3.10/site-packages (from pandas>=1.0.1->code_bert_score) (2.9.0.post0)
Requirement already satisfied: pytz>=2020.1 in /u/choprahetarth/.conda/envs/hetarth_py10/lib/python3.10/site-packages (from pandas>=1.0.1->code_bert_score) (2024.1)
Requirement already satisfied: tzdata>=2022.7 in /u/choprahetarth/.conda/envs/hetarth_py10/lib/python3.10/site-packages (from pandas>=1.0.1->code_bert_score) (2024.1)
Requirement already satisfied: filelock in /u/choprahetarth/.conda/envs/hetarth_py10/lib/python3.10/site-packages (from torch>=1.0.0->code_bert_score) (3.13.4)
Requirement already satisfied: typing-extensions>=4.8.0 in /u/choprahetarth/.conda/envs/hetarth_py10/lib/python3.10/site-packages (from torch>=1.0.0->code_bert_score) (4.11.0)
Requirement already satisfied: sympy in /u/choprahetarth/.conda/envs/hetarth_py10/lib/python3.10/site-packages (from torch>=1.0.0->code_bert_score) (1.12)
Requirement already satisfied: networkx in /u/choprahetarth/.conda/envs/hetarth_py10/lib/python3.10/site-packages (from torch>=1.0.0->code_bert_score) (3.3)
Requirement already satisfied: jinja2 in /u/choprahetarth/.conda/envs/hetarth_py10/lib/python3.10/site-packages (from torch>=1.0.0->code_bert_score) (3.1.3)
Requirement already satisfied: fsspec in /u/choprahetarth/.conda/envs/hetarth_py10/lib/python3.10/site-packages (from torch>=1.0.0->code_bert_score) (2024.2.0)
Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /u/choprahetarth/.conda/envs/hetarth_py10/lib/python3.10/site-packages (from torch>=1.0.0->code_bert_score) (12.1.105)
Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /u/choprahetarth/.conda/envs/hetarth_py10/lib/python3.10/site-packages (from torch>=1.0.0->code_bert_score) (12.1.105)
Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /u/choprahetarth/.conda/envs/hetarth_py10/lib/python3.10/site-packages (from torch>=1.0.0->code_bert_score) (12.1.105)
Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /u/choprahetarth/.conda/envs/hetarth_py10/lib/python3.10/site-packages (from torch>=1.0.0->code_bert_score) (8.9.2.26)
Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /u/choprahetarth/.conda/envs/hetarth_py10/lib/python3.10/site-packages (from torch>=1.0.0->code_bert_score) (12.1.3.1)
Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /u/choprahetarth/.conda/envs/hetarth_py10/lib/python3.10/site-packages (from torch>=1.0.0->code_bert_score) (11.0.2.54)
Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /u/choprahetarth/.conda/envs/hetarth_py10/lib/python3.10/site-packages (from torch>=1.0.0->code_bert_score) (10.3.2.106)
Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /u/choprahetarth/.conda/envs/hetarth_py10/lib/python3.10/site-packages (from torch>=1.0.0->code_bert_score) (11.4.5.107)
Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /u/choprahetarth/.conda/envs/hetarth_py10/lib/python3.10/site-packages (from torch>=1.0.0->code_bert_score) (12.1.0.106)
Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /u/choprahetarth/.conda/envs/hetarth_py10/lib/python3.10/site-packages (from torch>=1.0.0->code_bert_score) (2.19.3)
Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /u/choprahetarth/.conda/envs/hetarth_py10/lib/python3.10/site-packages (from torch>=1.0.0->code_bert_score) (12.1.105)
Requirement already satisfied: triton==2.2.0 in /u/choprahetarth/.conda/envs/hetarth_py10/lib/python3.10/site-packages (from torch>=1.0.0->code_bert_score) (2.2.0)
Requirement already satisfied: nvidia-nvjitlink-cu12 in /u/choprahetarth/.conda/envs/hetarth_py10/lib/python3.10/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.0.0->code_bert_score) (12.4.127)
Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /u/choprahetarth/.conda/envs/hetarth_py10/lib/python3.10/site-packages (from transformers>=3.0.0->code_bert_score) (0.22.2)
Requirement already satisfied: packaging>=20.0 in /u/choprahetarth/.conda/envs/hetarth_py10/lib/python3.10/site-packages (from transformers>=3.0.0->code_bert_score) (24.0)
Requirement already satisfied: pyyaml>=5.1 in /u/choprahetarth/.conda/envs/hetarth_py10/lib/python3.10/site-packages (from transformers>=3.0.0->code_bert_score) (6.0.1)
Requirement already satisfied: regex!=2019.12.17 in /u/choprahetarth/.conda/envs/hetarth_py10/lib/python3.10/site-packages (from transformers>=3.0.0->code_bert_score) (2023.12.25)
Collecting tokenizers<0.19,>=0.14 (from transformers>=3.0.0->code_bert_score)
  Using cached tokenizers-0.15.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)
Requirement already satisfied: safetensors>=0.4.1 in /u/choprahetarth/.conda/envs/hetarth_py10/lib/python3.10/site-packages (from transformers>=3.0.0->code_bert_score) (0.4.2)
Requirement already satisfied: contourpy>=1.0.1 in /u/choprahetarth/.conda/envs/hetarth_py10/lib/python3.10/site-packages (from matplotlib->code_bert_score) (1.2.1)
Requirement already satisfied: cycler>=0.10 in /u/choprahetarth/.conda/envs/hetarth_py10/lib/python3.10/site-packages (from matplotlib->code_bert_score) (0.12.1)
Requirement already satisfied: fonttools>=4.22.0 in /u/choprahetarth/.conda/envs/hetarth_py10/lib/python3.10/site-packages (from matplotlib->code_bert_score) (4.51.0)
Requirement already satisfied: kiwisolver>=1.3.1 in /u/choprahetarth/.conda/envs/hetarth_py10/lib/python3.10/site-packages (from matplotlib->code_bert_score) (1.4.5)
Requirement already satisfied: pillow>=8 in /u/choprahetarth/.conda/envs/hetarth_py10/lib/python3.10/site-packages (from matplotlib->code_bert_score) (10.3.0)
Requirement already satisfied: pyparsing>=2.3.1 in /u/choprahetarth/.conda/envs/hetarth_py10/lib/python3.10/site-packages (from matplotlib->code_bert_score) (3.1.2)
Requirement already satisfied: charset-normalizer<4,>=2 in /u/choprahetarth/.conda/envs/hetarth_py10/lib/python3.10/site-packages (from requests->code_bert_score) (3.3.2)
Requirement already satisfied: idna<4,>=2.5 in /u/choprahetarth/.conda/envs/hetarth_py10/lib/python3.10/site-packages (from requests->code_bert_score) (3.7)
Requirement already satisfied: urllib3<3,>=1.21.1 in /u/choprahetarth/.conda/envs/hetarth_py10/lib/python3.10/site-packages (from requests->code_bert_score) (2.2.1)
Requirement already satisfied: certifi>=2017.4.17 in /u/choprahetarth/.conda/envs/hetarth_py10/lib/python3.10/site-packages (from requests->code_bert_score) (2024.2.2)
Requirement already satisfied: six>=1.5 in /u/choprahetarth/.conda/envs/hetarth_py10/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas>=1.0.1->code_bert_score) (1.16.0)
Requirement already satisfied: MarkupSafe>=2.0 in /u/choprahetarth/.conda/envs/hetarth_py10/lib/python3.10/site-packages (from jinja2->torch>=1.0.0->code_bert_score) (2.1.5)
Requirement already satisfied: mpmath>=0.19 in /u/choprahetarth/.conda/envs/hetarth_py10/lib/python3.10/site-packages (from sympy->torch>=1.0.0->code_bert_score) (1.3.0)
Using cached tokenizers-0.15.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)
Installing collected packages: tokenizers
  Attempting uninstall: tokenizers
    Found existing installation: tokenizers 0.19.1
    Uninstalling tokenizers-0.19.1:
      Successfully uninstalled tokenizers-0.19.1
Successfully installed tokenizers-0.15.2

Output: Requirement already satisfied: nltk in /u/choprahetarth/.conda/envs/hetarth_py10/lib/python3.10/site-packages (3.8.1)
Requirement already satisfied: click in /u/choprahetarth/.conda/envs/hetarth_py10/lib/python3.10/site-packages (from nltk) (8.1.7)
Requirement already satisfied: joblib in /u/choprahetarth/.conda/envs/hetarth_py10/lib/python3.10/site-packages (from nltk) (1.4.0)
Requirement already satisfied: regex>=2021.8.3 in /u/choprahetarth/.conda/envs/hetarth_py10/lib/python3.10/site-packages (from nltk) (2023.12.25)
Requirement already satisfied: tqdm in /u/choprahetarth/.conda/envs/hetarth_py10/lib/python3.10/site-packages (from nltk) (4.66.2)

WARNING:__main__:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
/u/choprahetarth/.conda/envs/hetarth_py10/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py:747: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.
  warnings.warn(
/u/choprahetarth/.conda/envs/hetarth_py10/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py:747: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.
  warnings.warn(
/u/choprahetarth/.conda/envs/hetarth_py10/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py:747: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.
  warnings.warn(
/u/choprahetarth/.conda/envs/hetarth_py10/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py:747: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.
  warnings.warn(
loading file vocab.json from cache at /projects/bbvz/choprahetarth/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/vocab.json
loading file merges.txt from cache at /projects/bbvz/choprahetarth/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/merges.txt
loading file tokenizer.json from cache at /projects/bbvz/choprahetarth/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/tokenizer.json
loading file added_tokens.json from cache at None
loading file special_tokens_map.json from cache at None
loading file tokenizer_config.json from cache at /projects/bbvz/choprahetarth/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/tokenizer_config.json
loading file vocab.json from cache at /projects/bbvz/choprahetarth/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/vocab.json
loading file merges.txt from cache at /projects/bbvz/choprahetarth/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/merges.txt
loading file tokenizer.json from cache at /projects/bbvz/choprahetarth/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/tokenizer.json
loading file added_tokens.json from cache at None
loading file special_tokens_map.json from cache at None
loading file tokenizer_config.json from cache at /projects/bbvz/choprahetarth/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/tokenizer_config.json
loading file vocab.json from cache at /projects/bbvz/choprahetarth/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/vocab.json
loading file vocab.json from cache at /projects/bbvz/choprahetarth/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/vocab.json
loading file merges.txt from cache at /projects/bbvz/choprahetarth/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/merges.txt
loading file tokenizer.json from cache at /projects/bbvz/choprahetarth/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/tokenizer.json
loading file added_tokens.json from cache at None
loading file merges.txt from cache at /projects/bbvz/choprahetarth/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/merges.txt
loading file special_tokens_map.json from cache at None
loading file tokenizer_config.json from cache at /projects/bbvz/choprahetarth/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/tokenizer_config.json
loading file tokenizer.json from cache at /projects/bbvz/choprahetarth/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/tokenizer.json
loading file added_tokens.json from cache at None
loading file special_tokens_map.json from cache at None
loading file tokenizer_config.json from cache at /projects/bbvz/choprahetarth/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/tokenizer_config.json
/u/choprahetarth/.conda/envs/hetarth_py10/lib/python3.10/site-packages/datasets/load.py:2516: FutureWarning: 'use_auth_token' was deprecated in favor of 'token' in version 2.14.0 and will be removed in 3.0.0.
You can remove this warning by passing 'token=<use_auth_token>' instead.
  warnings.warn(
/u/choprahetarth/.conda/envs/hetarth_py10/lib/python3.10/site-packages/datasets/load.py:2516: FutureWarning: 'use_auth_token' was deprecated in favor of 'token' in version 2.14.0 and will be removed in 3.0.0.
You can remove this warning by passing 'token=<use_auth_token>' instead.
  warnings.warn(
/u/choprahetarth/.conda/envs/hetarth_py10/lib/python3.10/site-packages/datasets/load.py:2516: FutureWarning: 'use_auth_token' was deprecated in favor of 'token' in version 2.14.0 and will be removed in 3.0.0.
You can remove this warning by passing 'token=<use_auth_token>' instead.
  warnings.warn(
/u/choprahetarth/.conda/envs/hetarth_py10/lib/python3.10/site-packages/datasets/load.py:2516: FutureWarning: 'use_auth_token' was deprecated in favor of 'token' in version 2.14.0 and will be removed in 3.0.0.
You can remove this warning by passing 'token=<use_auth_token>' instead.
  warnings.warn(
Loading the dataset
Loading the dataset in streaming mode
Loading the dataset
Loading the dataset in streaming mode
Loading the dataset
Loading the dataset in streaming mode
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<FOR DEBUGGING PURPOSES ONLY!>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<FOR DEBUGGING PURPOSES ONLY!>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>{'input': 'name: SCORED | 18.2.3 | PATCH | L1 Ensure Enable Local Admin Password Management is', 'repo_name': 'windows_2016_cis', 'download_link': 'https://old-galaxy.ansible.com/mindpointgroup/windows_2016_cis', 'path': 'data/repos/mindpointgroup/windows_2016_cis/tasks/section18.yml', 'download_count': '744', 'output': '  set to Enabled MS only\ncommand: echo true\nwhen:\n- is_implemented\n- rule_18_2_3\n- not ansible_windows_domain_role == "Primary domain controller"\ntags:\n- level1\n- level2\n- rule_18.2.3\n- patch\n', 'org_name': 'mindpointgroup', 'license': 'MIT'}

{'input': 'name: SCORED | 18.2.3 | PATCH | L1 Ensure Enable Local Admin Password Management is', 'repo_name': 'windows_2016_cis', 'download_link': 'https://old-galaxy.ansible.com/mindpointgroup/windows_2016_cis', 'path': 'data/repos/mindpointgroup/windows_2016_cis/tasks/section18.yml', 'download_count': '744', 'output': '  set to Enabled MS only\ncommand: echo true\nwhen:\n- is_implemented\n- rule_18_2_3\n- not ansible_windows_domain_role == "Primary domain controller"\ntags:\n- level1\n- level2\n- rule_18.2.3\n- patch\n', 'org_name': 'mindpointgroup', 'license': 'MIT'}
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<FOR DEBUGGING PURPOSES ONLY!>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
{'input': 'name: SCORED | 18.2.3 | PATCH | L1 Ensure Enable Local Admin Password Management is', 'repo_name': 'windows_2016_cis', 'download_link': 'https://old-galaxy.ansible.com/mindpointgroup/windows_2016_cis', 'path': 'data/repos/mindpointgroup/windows_2016_cis/tasks/section18.yml', 'download_count': '744', 'output': '  set to Enabled MS only\ncommand: echo true\nwhen:\n- is_implemented\n- rule_18_2_3\n- not ansible_windows_domain_role == "Primary domain controller"\ntags:\n- level1\n- level2\n- rule_18.2.3\n- patch\n', 'org_name': 'mindpointgroup', 'license': 'MIT'}
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<FOR DEBUGGING PURPOSES ONLY!>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
{'input': 'name: add a comment to an existing txt record', 'repo_name': 'nios_modules', 'download_link': 'https://old-galaxy.ansible.com/infoblox/nios_modules', 'path': 'data/repos/infoblox/nios_modules/integration/targets/nios_txt_record/tasks/nios_txt_record_idempotence.yml', 'download_count': '456033', 'output': "nios_txt_record:\n  name: txt.ansible.com\n  text: mytext\n  state: present\n  comment: mycomment\n  provider: ''\nregister: txt_update1\n", 'org_name': 'infoblox', 'license': ''}
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<FOR DEBUGGING PURPOSES ONLY!>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
{'input': 'name: add a comment to an existing txt record', 'repo_name': 'nios_modules', 'download_link': 'https://old-galaxy.ansible.com/infoblox/nios_modules', 'path': 'data/repos/infoblox/nios_modules/integration/targets/nios_txt_record/tasks/nios_txt_record_idempotence.yml', 'download_count': '456033', 'output': "nios_txt_record:\n  name: txt.ansible.com\n  text: mytext\n  state: present\n  comment: mycomment\n  provider: ''\nregister: txt_update1\n", 'org_name': 'infoblox', 'license': ''}
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<FOR DEBUGGING PURPOSES ONLY!>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
{'input': 'name: add a comment to an existing txt record', 'repo_name': 'nios_modules', 'download_link': 'https://old-galaxy.ansible.com/infoblox/nios_modules', 'path': 'data/repos/infoblox/nios_modules/integration/targets/nios_txt_record/tasks/nios_txt_record_idempotence.yml', 'download_count': '456033', 'output': "nios_txt_record:\n  name: txt.ansible.com\n  text: mytext\n  state: present\n  comment: mycomment\n  provider: ''\nregister: txt_update1\n", 'org_name': 'infoblox', 'license': ''}
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<FOR DEBUGGING PURPOSES ONLY!>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
{'input': 'name: CIS Docker Community Edition Benchmark prelim tasks', 'repo_name': 'dockerce-cis', 'download_link': 'https://old-galaxy.ansible.com/florianutz/dockerce-cis', 'path': 'data/repos/florianutz/dockerce-cis/tasks/main.yml', 'download_count': '1122', 'output': 'include_tasks: prelim.yml\n', 'org_name': 'florianutz', 'license': 'MIT'}
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<FOR DEBUGGING PURPOSES ONLY!>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
{'input': 'name: CIS Docker Community Edition Benchmark prelim tasks', 'repo_name': 'dockerce-cis', 'download_link': 'https://old-galaxy.ansible.com/florianutz/dockerce-cis', 'path': 'data/repos/florianutz/dockerce-cis/tasks/main.yml', 'download_count': '1122', 'output': 'include_tasks: prelim.yml\n', 'org_name': 'florianutz', 'license': 'MIT'}
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<FOR DEBUGGING PURPOSES ONLY!>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
{'input': 'name: CIS Docker Community Edition Benchmark prelim tasks', 'repo_name': 'dockerce-cis', 'download_link': 'https://old-galaxy.ansible.com/florianutz/dockerce-cis', 'path': 'data/repos/florianutz/dockerce-cis/tasks/main.yml', 'download_count': '1122', 'output': 'include_tasks: prelim.yml\n', 'org_name': 'florianutz', 'license': 'MIT'}
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<FOR DEBUGGING PURPOSES ONLY!>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
{'input': 'name: Import assert.yml', 'repo_name': 'hashicorp', 'download_link': 'https://old-galaxy.ansible.com/robertdebock/hashicorp', 'path': 'data/repos/robertdebock/hashicorp/tasks/main.yml', 'download_count': '148117', 'output': 'ansible.builtin.import_tasks:\n  file: assert.yml\nrun_once: true\ndelegate_to: localhost\n', 'org_name': 'robertdebock', 'license': 'Apache-2.0-Short,Apache-2.0'}
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<FOR DEBUGGING PURPOSES ONLY!>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
{'input': 'name: Import assert.yml', 'repo_name': 'hashicorp', 'download_link': 'https://old-galaxy.ansible.com/robertdebock/hashicorp', 'path': 'data/repos/robertdebock/hashicorp/tasks/main.yml', 'download_count': '148117', 'output': 'ansible.builtin.import_tasks:\n  file: assert.yml\nrun_once: true\ndelegate_to: localhost\n', 'org_name': 'robertdebock', 'license': 'Apache-2.0-Short,Apache-2.0'}
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<FOR DEBUGGING PURPOSES ONLY!>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
{'input': 'name: Import assert.yml', 'repo_name': 'hashicorp', 'download_link': 'https://old-galaxy.ansible.com/robertdebock/hashicorp', 'path': 'data/repos/robertdebock/hashicorp/tasks/main.yml', 'download_count': '148117', 'output': 'ansible.builtin.import_tasks:\n  file: assert.yml\nrun_once: true\ndelegate_to: localhost\n', 'org_name': 'robertdebock', 'license': 'Apache-2.0-Short,Apache-2.0'}
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<FOR DEBUGGING PURPOSES ONLY!>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
{'input': 'name: Uncompress Ansible Tower', 'repo_name': 'ansibletower', 'download_link': 'https://old-galaxy.ansible.com/aplyca/ansibletower', 'path': 'data/repos/aplyca/ansibletower/tasks/main.yml', 'download_count': '2746', 'output': 'unarchive:\n  src: /tmp/ansible-tower-setup-3.2.2.tar.gz\n  dest: /tmp\n  copy: false\n  creates: /tmp/ansible-tower-setup-3.2.2/setup.sh\n', 'org_name': 'aplyca', 'license': 'MIT'}
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<FOR DEBUGGING PURPOSES ONLY!>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
{'input': 'name: Uncompress Ansible Tower', 'repo_name': 'ansibletower', 'download_link': 'https://old-galaxy.ansible.com/aplyca/ansibletower', 'path': 'data/repos/aplyca/ansibletower/tasks/main.yml', 'download_count': '2746', 'output': 'unarchive:\n  src: /tmp/ansible-tower-setup-3.2.2.tar.gz\n  dest: /tmp\n  copy: false\n  creates: /tmp/ansible-tower-setup-3.2.2/setup.sh\n', 'org_name': 'aplyca', 'license': 'MIT'}
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<FOR DEBUGGING PURPOSES ONLY!>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
{'input': 'name: Uncompress Ansible Tower', 'repo_name': 'ansibletower', 'download_link': 'https://old-galaxy.ansible.com/aplyca/ansibletower', 'path': 'data/repos/aplyca/ansibletower/tasks/main.yml', 'download_count': '2746', 'output': 'unarchive:\n  src: /tmp/ansible-tower-setup-3.2.2.tar.gz\n  dest: /tmp\n  copy: false\n  creates: /tmp/ansible-tower-setup-3.2.2/setup.sh\n', 'org_name': 'aplyca', 'license': 'MIT'}
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<FOR DEBUGGING PURPOSES ONLY!>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
{'input': 'name: RVM | Clone/Update', 'repo_name': 'ruby', 'download_link': 'https://old-galaxy.ansible.com/fubarhouse/ruby', 'path': 'data/repos/fubarhouse/ruby/tasks/rvm.yml', 'download_count': '1360', 'output': "become: true\nbecome_user: ''\ngit:\n  repo: https://github.com/rvm/rvm.git\n  dest: ''\n  clone: true\n  update: false\n  force: false\n  version: master\n  recursive: false\n", 'org_name': 'fubarhouse', 'license': 'MIT'}
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<FOR DEBUGGING PURPOSES ONLY!>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
{'input': 'name: RVM | Clone/Update', 'repo_name': 'ruby', 'download_link': 'https://old-galaxy.ansible.com/fubarhouse/ruby', 'path': 'data/repos/fubarhouse/ruby/tasks/rvm.yml', 'download_count': '1360', 'output': "become: true\nbecome_user: ''\ngit:\n  repo: https://github.com/rvm/rvm.git\n  dest: ''\n  clone: true\n  update: false\n  force: false\n  version: master\n  recursive: false\n", 'org_name': 'fubarhouse', 'license': 'MIT'}
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<FOR DEBUGGING PURPOSES ONLY!>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<FOR DEBUGGING PURPOSES ONLY!>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
{'input': 'name: Import install_el.yml if OS family is EL', 'repo_name': 'kubeadm', 'download_link': 'https://old-galaxy.ansible.com/darkwizard242/kubeadm', 'path': 'data/repos/darkwizard242/kubeadm/tasks/main.yml', 'download_count': '1463', 'output': 'ansible.builtin.import_tasks: install_el.yml\nwhen: ansible_os_family == "RedHat"\n', 'org_name': 'darkwizard242', 'license': 'MIT'}
{'input': 'name: RVM | Clone/Update', 'repo_name': 'ruby', 'download_link': 'https://old-galaxy.ansible.com/fubarhouse/ruby', 'path': 'data/repos/fubarhouse/ruby/tasks/rvm.yml', 'download_count': '1360', 'output': "become: true\nbecome_user: ''\ngit:\n  repo: https://github.com/rvm/rvm.git\n  dest: ''\n  clone: true\n  update: false\n  force: false\n  version: master\n  recursive: false\n", 'org_name': 'fubarhouse', 'license': 'MIT'}
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<FOR DEBUGGING PURPOSES ONLY!>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
{'input': 'name: Import install_el.yml if OS family is EL', 'repo_name': 'kubeadm', 'download_link': 'https://old-galaxy.ansible.com/darkwizard242/kubeadm', 'path': 'data/repos/darkwizard242/kubeadm/tasks/main.yml', 'download_count': '1463', 'output': 'ansible.builtin.import_tasks: install_el.yml\nwhen: ansible_os_family == "RedHat"\n', 'org_name': 'darkwizard242', 'license': 'MIT'}
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<FOR DEBUGGING PURPOSES ONLY!>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
{'input': 'name: update apt cache', 'repo_name': 'thumbor', 'download_link': 'https://old-galaxy.ansible.com/hugomrdias/thumbor', 'path': 'data/repos/hugomrdias/thumbor/tasks/main.yml', 'download_count': '954', 'output': 'apt: update_cache=yes cache_valid_time=3600\n', 'org_name': 'hugomrdias', 'license': 'MIT'}
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<FOR DEBUGGING PURPOSES ONLY!>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
{'input': 'name: Import install_el.yml if OS family is EL', 'repo_name': 'kubeadm', 'download_link': 'https://old-galaxy.ansible.com/darkwizard242/kubeadm', 'path': 'data/repos/darkwizard242/kubeadm/tasks/main.yml', 'download_count': '1463', 'output': 'ansible.builtin.import_tasks: install_el.yml\nwhen: ansible_os_family == "RedHat"\n', 'org_name': 'darkwizard242', 'license': 'MIT'}
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<FOR DEBUGGING PURPOSES ONLY!>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
{'input': 'name: update apt cache', 'repo_name': 'thumbor', 'download_link': 'https://old-galaxy.ansible.com/hugomrdias/thumbor', 'path': 'data/repos/hugomrdias/thumbor/tasks/main.yml', 'download_count': '954', 'output': 'apt: update_cache=yes cache_valid_time=3600\n', 'org_name': 'hugomrdias', 'license': 'MIT'}
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<FOR DEBUGGING PURPOSES ONLY!>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
{'input': 'name: EL Family | Installing serverspec', 'repo_name': 'serverspec', 'download_link': 'https://old-galaxy.ansible.com/darkwizard242/serverspec', 'path': 'data/repos/darkwizard242/serverspec/tasks/install_el.yml', 'download_count': '3229', 'output': "ansible.builtin.gem:\n  name: serverspec\n  state: present\n  user_install: 'false'\n  include_dependencies: 'true'\n", 'org_name': 'darkwizard242', 'license': 'MIT'}
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<FOR DEBUGGING PURPOSES ONLY!>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
{'input': 'name: update apt cache', 'repo_name': 'thumbor', 'download_link': 'https://old-galaxy.ansible.com/hugomrdias/thumbor', 'path': 'data/repos/hugomrdias/thumbor/tasks/main.yml', 'download_count': '954', 'output': 'apt: update_cache=yes cache_valid_time=3600\n', 'org_name': 'hugomrdias', 'license': 'MIT'}
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<FOR DEBUGGING PURPOSES ONLY!>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
{'input': 'name: EL Family | Installing serverspec', 'repo_name': 'serverspec', 'download_link': 'https://old-galaxy.ansible.com/darkwizard242/serverspec', 'path': 'data/repos/darkwizard242/serverspec/tasks/install_el.yml', 'download_count': '3229', 'output': "ansible.builtin.gem:\n  name: serverspec\n  state: present\n  user_install: 'false'\n  include_dependencies: 'true'\n", 'org_name': 'darkwizard242', 'license': 'MIT'}
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<FOR DEBUGGING PURPOSES ONLY!>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
{'input': 'name: assert | Test if haproxy_backend_default_balance is set correctly', 'repo_name': 'haproxy', 'download_link': 'https://old-galaxy.ansible.com/buluma/haproxy', 'path': 'data/repos/buluma/haproxy/tasks/assert.yml', 'download_count': '15511', 'output': 'ansible.builtin.assert:\n  that:\n  - haproxy_backend_default_balance is defined\n  - haproxy_backend_default_balance is string\n  - haproxy_backend_default_balance in [ "roundrobin", "static-rr", "leastconn", "first",\n    "source", "uri", "url_param", "hdr", "rdp-cookie" ]\n  quiet: true\n', 'org_name': 'buluma', 'license': 'Apache-2.0-Short,Apache-2.0'}
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<FOR DEBUGGING PURPOSES ONLY!>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
{'input': 'name: EL Family | Installing serverspec', 'repo_name': 'serverspec', 'download_link': 'https://old-galaxy.ansible.com/darkwizard242/serverspec', 'path': 'data/repos/darkwizard242/serverspec/tasks/install_el.yml', 'download_count': '3229', 'output': "ansible.builtin.gem:\n  name: serverspec\n  state: present\n  user_install: 'false'\n  include_dependencies: 'true'\n", 'org_name': 'darkwizard242', 'license': 'MIT'}
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<FOR DEBUGGING PURPOSES ONLY!>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
{'input': 'name: assert | Test if haproxy_backend_default_balance is set correctly', 'repo_name': 'haproxy', 'download_link': 'https://old-galaxy.ansible.com/buluma/haproxy', 'path': 'data/repos/buluma/haproxy/tasks/assert.yml', 'download_count': '15511', 'output': 'ansible.builtin.assert:\n  that:\n  - haproxy_backend_default_balance is defined\n  - haproxy_backend_default_balance is string\n  - haproxy_backend_default_balance in [ "roundrobin", "static-rr", "leastconn", "first",\n    "source", "uri", "url_param", "hdr", "rdp-cookie" ]\n  quiet: true\n', 'org_name': 'buluma', 'license': 'Apache-2.0-Short,Apache-2.0'}
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<FOR DEBUGGING PURPOSES ONLY!>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
{'input': 'name: Enable firewalld service.', 'repo_name': 'webmin', 'download_link': 'https://old-galaxy.ansible.com/semuadmin/webmin', 'path': 'data/repos/semuadmin/webmin/tasks/webmin.yml', 'download_count': '671', 'output': 'ansible.posix.firewalld:\n  zone: public\n  service: webmin\n  permanent: true\n  state: enabled\n  immediate: true\nwhen: enable_firewalld\n', 'org_name': 'semuadmin', 'license': ''}
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<FOR DEBUGGING PURPOSES ONLY!>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
{'input': 'name: assert | Test if haproxy_backend_default_balance is set correctly', 'repo_name': 'haproxy', 'download_link': 'https://old-galaxy.ansible.com/buluma/haproxy', 'path': 'data/repos/buluma/haproxy/tasks/assert.yml', 'download_count': '15511', 'output': 'ansible.builtin.assert:\n  that:\n  - haproxy_backend_default_balance is defined\n  - haproxy_backend_default_balance is string\n  - haproxy_backend_default_balance in [ "roundrobin", "static-rr", "leastconn", "first",\n    "source", "uri", "url_param", "hdr", "rdp-cookie" ]\n  quiet: true\n', 'org_name': 'buluma', 'license': 'Apache-2.0-Short,Apache-2.0'}
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<FOR DEBUGGING PURPOSES ONLY!>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
{'input': 'name: Enable firewalld service.', 'repo_name': 'webmin', 'download_link': 'https://old-galaxy.ansible.com/semuadmin/webmin', 'path': 'data/repos/semuadmin/webmin/tasks/webmin.yml', 'download_count': '671', 'output': 'ansible.posix.firewalld:\n  zone: public\n  service: webmin\n  permanent: true\n  state: enabled\n  immediate: true\nwhen: enable_firewalld\n', 'org_name': 'semuadmin', 'license': ''}
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<FOR DEBUGGING PURPOSES ONLY!>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
{'input': 'name: Enable firewalld service.', 'repo_name': 'webmin', 'download_link': 'https://old-galaxy.ansible.com/semuadmin/webmin', 'path': 'data/repos/semuadmin/webmin/tasks/webmin.yml', 'download_count': '671', 'output': 'ansible.posix.firewalld:\n  zone: public\n  service: webmin\n  permanent: true\n  state: enabled\n  immediate: true\nwhen: enable_firewalld\n', 'org_name': 'semuadmin', 'license': ''}
Loading the dataset
Loading the dataset in streaming mode
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<FOR DEBUGGING PURPOSES ONLY!>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
{'input': 'name: SCORED | 18.2.3 | PATCH | L1 Ensure Enable Local Admin Password Management is', 'repo_name': 'windows_2016_cis', 'download_link': 'https://old-galaxy.ansible.com/mindpointgroup/windows_2016_cis', 'path': 'data/repos/mindpointgroup/windows_2016_cis/tasks/section18.yml', 'download_count': '744', 'output': '  set to Enabled MS only\ncommand: echo true\nwhen:\n- is_implemented\n- rule_18_2_3\n- not ansible_windows_domain_role == "Primary domain controller"\ntags:\n- level1\n- level2\n- rule_18.2.3\n- patch\n', 'org_name': 'mindpointgroup', 'license': 'MIT'}
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<FOR DEBUGGING PURPOSES ONLY!>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
{'input': 'name: add a comment to an existing txt record', 'repo_name': 'nios_modules', 'download_link': 'https://old-galaxy.ansible.com/infoblox/nios_modules', 'path': 'data/repos/infoblox/nios_modules/integration/targets/nios_txt_record/tasks/nios_txt_record_idempotence.yml', 'download_count': '456033', 'output': "nios_txt_record:\n  name: txt.ansible.com\n  text: mytext\n  state: present\n  comment: mycomment\n  provider: ''\nregister: txt_update1\n", 'org_name': 'infoblox', 'license': ''}
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<FOR DEBUGGING PURPOSES ONLY!>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
{'input': 'name: CIS Docker Community Edition Benchmark prelim tasks', 'repo_name': 'dockerce-cis', 'download_link': 'https://old-galaxy.ansible.com/florianutz/dockerce-cis', 'path': 'data/repos/florianutz/dockerce-cis/tasks/main.yml', 'download_count': '1122', 'output': 'include_tasks: prelim.yml\n', 'org_name': 'florianutz', 'license': 'MIT'}
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<FOR DEBUGGING PURPOSES ONLY!>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
{'input': 'name: Import assert.yml', 'repo_name': 'hashicorp', 'download_link': 'https://old-galaxy.ansible.com/robertdebock/hashicorp', 'path': 'data/repos/robertdebock/hashicorp/tasks/main.yml', 'download_count': '148117', 'output': 'ansible.builtin.import_tasks:\n  file: assert.yml\nrun_once: true\ndelegate_to: localhost\n', 'org_name': 'robertdebock', 'license': 'Apache-2.0-Short,Apache-2.0'}
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<FOR DEBUGGING PURPOSES ONLY!>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
{'input': 'name: Uncompress Ansible Tower', 'repo_name': 'ansibletower', 'download_link': 'https://old-galaxy.ansible.com/aplyca/ansibletower', 'path': 'data/repos/aplyca/ansibletower/tasks/main.yml', 'download_count': '2746', 'output': 'unarchive:\n  src: /tmp/ansible-tower-setup-3.2.2.tar.gz\n  dest: /tmp\n  copy: false\n  creates: /tmp/ansible-tower-setup-3.2.2/setup.sh\n', 'org_name': 'aplyca', 'license': 'MIT'}
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<FOR DEBUGGING PURPOSES ONLY!>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
{'input': 'name: RVM | Clone/Update', 'repo_name': 'ruby', 'download_link': 'https://old-galaxy.ansible.com/fubarhouse/ruby', 'path': 'data/repos/fubarhouse/ruby/tasks/rvm.yml', 'download_count': '1360', 'output': "become: true\nbecome_user: ''\ngit:\n  repo: https://github.com/rvm/rvm.git\n  dest: ''\n  clone: true\n  update: false\n  force: false\n  version: master\n  recursive: false\n", 'org_name': 'fubarhouse', 'license': 'MIT'}
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<FOR DEBUGGING PURPOSES ONLY!>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
{'input': 'name: Import install_el.yml if OS family is EL', 'repo_name': 'kubeadm', 'download_link': 'https://old-galaxy.ansible.com/darkwizard242/kubeadm', 'path': 'data/repos/darkwizard242/kubeadm/tasks/main.yml', 'download_count': '1463', 'output': 'ansible.builtin.import_tasks: install_el.yml\nwhen: ansible_os_family == "RedHat"\n', 'org_name': 'darkwizard242', 'license': 'MIT'}
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<FOR DEBUGGING PURPOSES ONLY!>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
{'input': 'name: update apt cache', 'repo_name': 'thumbor', 'download_link': 'https://old-galaxy.ansible.com/hugomrdias/thumbor', 'path': 'data/repos/hugomrdias/thumbor/tasks/main.yml', 'download_count': '954', 'output': 'apt: update_cache=yes cache_valid_time=3600\n', 'org_name': 'hugomrdias', 'license': 'MIT'}
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<FOR DEBUGGING PURPOSES ONLY!>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
{'input': 'name: EL Family | Installing serverspec', 'repo_name': 'serverspec', 'download_link': 'https://old-galaxy.ansible.com/darkwizard242/serverspec', 'path': 'data/repos/darkwizard242/serverspec/tasks/install_el.yml', 'download_count': '3229', 'output': "ansible.builtin.gem:\n  name: serverspec\n  state: present\n  user_install: 'false'\n  include_dependencies: 'true'\n", 'org_name': 'darkwizard242', 'license': 'MIT'}
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<FOR DEBUGGING PURPOSES ONLY!>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
{'input': 'name: assert | Test if haproxy_backend_default_balance is set correctly', 'repo_name': 'haproxy', 'download_link': 'https://old-galaxy.ansible.com/buluma/haproxy', 'path': 'data/repos/buluma/haproxy/tasks/assert.yml', 'download_count': '15511', 'output': 'ansible.builtin.assert:\n  that:\n  - haproxy_backend_default_balance is defined\n  - haproxy_backend_default_balance is string\n  - haproxy_backend_default_balance in [ "roundrobin", "static-rr", "leastconn", "first",\n    "source", "uri", "url_param", "hdr", "rdp-cookie" ]\n  quiet: true\n', 'org_name': 'buluma', 'license': 'Apache-2.0-Short,Apache-2.0'}
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<FOR DEBUGGING PURPOSES ONLY!>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
{'input': 'name: Enable firewalld service.', 'repo_name': 'webmin', 'download_link': 'https://old-galaxy.ansible.com/semuadmin/webmin', 'path': 'data/repos/semuadmin/webmin/tasks/webmin.yml', 'download_count': '671', 'output': 'ansible.posix.firewalld:\n  zone: public\n  service: webmin\n  permanent: true\n  state: enabled\n  immediate: true\nwhen: enable_firewalld\n', 'org_name': 'semuadmin', 'license': ''}

  0%|          | 0/400 [00:00<?, ?it/s]
  0%|          | 0/400 [00:00<?, ?it/s]
  0%|          | 0/400 [00:00<?, ?it/s]
  0%|          | 0/400 [00:00<?, ?it/s]
  0%|          | 1/400 [00:00<01:17,  5.16it/s]
  0%|          | 1/400 [00:00<02:20,  2.84it/s]
  0%|          | 1/400 [00:00<02:20,  2.84it/s]
  0%|          | 1/400 [00:00<02:20,  2.84it/s]
 69%|██████▉   | 275/400 [00:00<00:00, 1164.72it/s]
 69%|██████▉   | 276/400 [00:00<00:00, 794.98it/s]
 68%|██████▊   | 271/400 [00:00<00:00, 780.33it/s]
 68%|██████▊   | 272/400 [00:00<00:00, 783.60it/s]
100%|██████████| 400/400 [00:00<00:00, 800.71it/s]

100%|██████████| 400/400 [00:00<00:00, 1169.97it/s]

100%|██████████| 400/400 [00:00<00:00, 796.83it/s]

100%|██████████| 400/400 [00:00<00:00, 795.74it/s]
The character to token ratio of the dataset is: 3.18
Loading the model
The character to token ratio of the dataset is: 3.18
Loading the model
The character to token ratio of the dataset is: 3.18
Loading the model
Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
The character to token ratio of the dataset is: 3.18
Loading the model
/u/choprahetarth/.conda/envs/hetarth_py10/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py:468: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.
  warnings.warn(
/u/choprahetarth/.conda/envs/hetarth_py10/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py:468: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.
  warnings.warn(
/u/choprahetarth/.conda/envs/hetarth_py10/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py:468: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.
  warnings.warn(
/u/choprahetarth/.conda/envs/hetarth_py10/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py:468: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.
  warnings.warn(
loading configuration file config.json from cache at /projects/bbvz/choprahetarth/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "bigcode/starcoderbase-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.40.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

loading configuration file config.json from cache at /projects/bbvz/choprahetarth/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "bigcode/starcoderbase-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.40.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

loading configuration file config.json from cache at /projects/bbvz/choprahetarth/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "bigcode/starcoderbase-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.40.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

loading configuration file config.json from cache at /projects/bbvz/choprahetarth/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "bigcode/starcoderbase-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.40.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
Overriding torch_dtype=None with `torch_dtype=torch.float16` due to requirements of `bitsandbytes` to enable model loading in 8-bit or 4-bit. Pass your own torch_dtype to specify the dtype of the remaining non-linear layers or pass torch_dtype=torch.float16 to remove this warning.
Overriding torch_dtype=None with `torch_dtype=torch.float16` due to requirements of `bitsandbytes` to enable model loading in 8-bit or 4-bit. Pass your own torch_dtype to specify the dtype of the remaining non-linear layers or pass torch_dtype=torch.float16 to remove this warning.
Overriding torch_dtype=None with `torch_dtype=torch.float16` due to requirements of `bitsandbytes` to enable model loading in 8-bit or 4-bit. Pass your own torch_dtype to specify the dtype of the remaining non-linear layers or pass torch_dtype=torch.float16 to remove this warning.
Overriding torch_dtype=None with `torch_dtype=torch.float16` due to requirements of `bitsandbytes` to enable model loading in 8-bit or 4-bit. Pass your own torch_dtype to specify the dtype of the remaining non-linear layers or pass torch_dtype=torch.float16 to remove this warning.
loading weights file pytorch_model.bin from cache at /projects/bbvz/choprahetarth/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/pytorch_model.bin.index.json
loading weights file pytorch_model.bin from cache at /projects/bbvz/choprahetarth/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/pytorch_model.bin.index.json
loading weights file pytorch_model.bin from cache at /projects/bbvz/choprahetarth/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/pytorch_model.bin.index.json
loading weights file pytorch_model.bin from cache at /projects/bbvz/choprahetarth/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/pytorch_model.bin.index.json
Instantiating GPTBigCodeForCausalLM model under default dtype torch.float16.
Instantiating GPTBigCodeForCausalLM model under default dtype torch.float16.
Instantiating GPTBigCodeForCausalLM model under default dtype torch.float16.
Instantiating GPTBigCodeForCausalLM model under default dtype torch.float16.
Generate config GenerationConfig {
  "bos_token_id": 0,
  "eos_token_id": 0
}

Generate config GenerationConfig {
  "bos_token_id": 0,
  "eos_token_id": 0
}

Generate config GenerationConfig {
  "bos_token_id": 0,
  "eos_token_id": 0
}

Generate config GenerationConfig {
  "bos_token_id": 0,
  "eos_token_id": 0
}


Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]
Loading checkpoint shards:  25%|██▌       | 1/4 [28:45<1:26:15, 1725.28s/it]
Loading checkpoint shards:  25%|██▌       | 1/4 [28:45<1:26:17, 1725.94s/it]
Loading checkpoint shards:  25%|██▌       | 1/4 [28:45<1:26:17, 1725.94s/it]
Loading checkpoint shards:  25%|██▌       | 1/4 [28:45<1:26:17, 1725.94s/it]
Loading checkpoint shards:  50%|█████     | 2/4 [55:03<54:37, 1638.87s/it]  
Loading checkpoint shards:  50%|█████     | 2/4 [55:04<54:38, 1639.21s/it]  
Loading checkpoint shards:  50%|█████     | 2/4 [55:04<54:38, 1639.21s/it]  
Loading checkpoint shards:  50%|█████     | 2/4 [55:04<54:38, 1639.24s/it]  
Loading checkpoint shards:  75%|███████▌  | 3/4 [1:18:42<25:38, 1538.50s/it]
Loading checkpoint shards:  75%|███████▌  | 3/4 [1:18:43<25:38, 1538.46s/it]
Loading checkpoint shards:  75%|███████▌  | 3/4 [1:18:43<25:38, 1538.46s/it]
Loading checkpoint shards:  75%|███████▌  | 3/4 [1:18:43<25:38, 1538.46s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [1:21:23<00:00, 994.39s/it] 
Loading checkpoint shards: 100%|██████████| 4/4 [1:21:23<00:00, 1220.83s/it]
All model checkpoint weights were used when initializing GPTBigCodeForCausalLM.

All the weights of GPTBigCodeForCausalLM were initialized from the model checkpoint at bigcode/starcoderbase-7b.
If your task is similar to the task the model of the checkpoint was trained on, you can already use GPTBigCodeForCausalLM for predictions without further training.

Loading checkpoint shards: 100%|██████████| 4/4 [1:21:23<00:00, 994.53s/it] 
Loading checkpoint shards: 100%|██████████| 4/4 [1:21:23<00:00, 1220.83s/it]
All model checkpoint weights were used when initializing GPTBigCodeForCausalLM.

All the weights of GPTBigCodeForCausalLM were initialized from the model checkpoint at bigcode/starcoderbase-7b.
If your task is similar to the task the model of the checkpoint was trained on, you can already use GPTBigCodeForCausalLM for predictions without further training.

Loading checkpoint shards: 100%|██████████| 4/4 [1:21:23<00:00, 994.39s/it] 
Loading checkpoint shards: 100%|██████████| 4/4 [1:21:23<00:00, 1220.84s/it]
All model checkpoint weights were used when initializing GPTBigCodeForCausalLM.

All the weights of GPTBigCodeForCausalLM were initialized from the model checkpoint at bigcode/starcoderbase-7b.
If your task is similar to the task the model of the checkpoint was trained on, you can already use GPTBigCodeForCausalLM for predictions without further training.

Loading checkpoint shards: 100%|██████████| 4/4 [1:21:23<00:00, 994.39s/it] 
Loading checkpoint shards: 100%|██████████| 4/4 [1:21:23<00:00, 1220.84s/it]
All model checkpoint weights were used when initializing GPTBigCodeForCausalLM.

All the weights of GPTBigCodeForCausalLM were initialized from the model checkpoint at bigcode/starcoderbase-7b.
If your task is similar to the task the model of the checkpoint was trained on, you can already use GPTBigCodeForCausalLM for predictions without further training.
loading configuration file generation_config.json from cache at /projects/bbvz/choprahetarth/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/generation_config.json
loading configuration file generation_config.json from cache at /projects/bbvz/choprahetarth/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/generation_config.json
loading configuration file generation_config.json from cache at /projects/bbvz/choprahetarth/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/generation_config.json
Generate config GenerationConfig {
  "bos_token_id": 0,
  "eos_token_id": 0
}

Generate config GenerationConfig {
  "bos_token_id": 0,
  "eos_token_id": 0
}

Generate config GenerationConfig {
  "bos_token_id": 0,
  "eos_token_id": 0
}

loading configuration file generation_config.json from cache at /projects/bbvz/choprahetarth/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/generation_config.json
Generate config GenerationConfig {
  "bos_token_id": 0,
  "eos_token_id": 0
}

Started the causalLM Thing
Started the causalLM Thing
Started the causalLM Thing
Started the causalLM Thing
Prepared model for kbit training
lora config done
Prepared model for kbit training
lora config done
Prepared model for kbit training
lora config done
Prepared model for kbit training
lora config done
peft model prepared
trainable params: 24944640 || all params: 7352207872 || trainable%: 0.33928094028732053
model printed
Starting main loop
PyTorch: setting up devices
peft model prepared
peft model prepared
trainable params: 24944640 || all params: 7352207872 || trainable%: 0.33928094028732053
model printed
Starting main loop
PyTorch: setting up devices
trainable params: 24944640 || all params: 7352207872 || trainable%: 0.33928094028732053
model printed
Starting main loop
PyTorch: setting up devices
peft model prepared
trainable params: 24944640 || all params: 7352207872 || trainable%: 0.33928094028732053
model printed
Starting main loop
PyTorch: setting up devices
Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
max_steps is given, it will override any value given in num_train_epochs
max_steps is given, it will override any value given in num_train_epochs
Training...
Training...
max_steps is given, it will override any value given in num_train_epochs
Training...
max_steps is given, it will override any value given in num_train_epochs
Using auto half precision backend
Training...
Currently training with a batch size of: 16
***** Running training *****
  Num examples = 27,392
  Num Epochs = 9,223,372,036,854,775,807
  Instantaneous batch size per device = 16
  Total train batch size (w. parallel, distributed & accumulation) = 128
  Gradient Accumulation steps = 2
  Total optimization steps = 214
  Number of trainable parameters = 24,944,640
Automatic Weights & Biases logging enabled, to disable set os.environ["WANDB_DISABLED"] = "true"
wandb: Currently logged in as: hetarthvader. Use `wandb login --relogin` to force relogin
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /projects/bbvz/choprahetarth/wandb/run-20240418_010311-45iynwaw
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run FinalRuns-/projects/bbvz/choprahetarth/new_experiments/experiment_2
wandb: ⭐️ View project at https://wandb.ai/hetarthvader/huggingface
wandb: 🚀 View run at https://wandb.ai/hetarthvader/huggingface/runs/45iynwaw

  0%|          | 0/214 [00:00<?, ?it/s]/u/choprahetarth/.conda/envs/hetarth_py10/lib/python3.10/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/choprahetarth/.conda/envs/hetarth_py10/lib/python3.10/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/choprahetarth/.conda/envs/hetarth_py10/lib/python3.10/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/choprahetarth/.conda/envs/hetarth_py10/lib/python3.10/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/choprahetarth/.conda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/choprahetarth/.conda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/choprahetarth/.conda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/choprahetarth/.conda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/choprahetarth/.conda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/choprahetarth/.conda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/choprahetarth/.conda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/choprahetarth/.conda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 1.8966, 'grad_norm': 0.8307441473007202, 'learning_rate': 5e-05, 'epoch': 0.0}
{'loss': 2.179, 'grad_norm': 0.8084935545921326, 'learning_rate': 0.0001, 'epoch': 0.01}
{'loss': 1.9542, 'grad_norm': 0.8453488945960999, 'learning_rate': 9.999451015497595e-05, 'epoch': 0.01}
{'loss': 1.9519, 'grad_norm': 0.8809479475021362, 'learning_rate': 9.997804182543973e-05, 'epoch': 0.02}
{'loss': 1.7947, 'grad_norm': 0.6770243048667908, 'learning_rate': 9.99505986277344e-05, 'epoch': 0.02}
{'loss': 1.6712, 'grad_norm': 0.4821780323982239, 'learning_rate': 9.991218658821608e-05, 'epoch': 0.03}

  0%|          | 1/214 [00:30<1:48:48, 30.65s/it]
                                                 

  0%|          | 1/214 [00:30<1:48:48, 30.65s/it]
  1%|          | 2/214 [00:48<1:22:01, 23.22s/it]
                                                 

  1%|          | 2/214 [00:48<1:22:01, 23.22s/it]
  1%|▏         | 3/214 [01:06<1:12:50, 20.71s/it]
                                                 

  1%|▏         | 3/214 [01:06<1:12:50, 20.71s/it]
  2%|▏         | 4/214 [01:24<1:08:21, 19.53s/it]
                                                 

  2%|▏         | 4/214 [01:24<1:08:21, 19.53s/it]
  2%|▏         | 5/214 [01:41<1:05:46, 18.88s/it]
                                                 

  2%|▏         | 5/214 [01:41<1:05:46, 18.88s/it]
  3%|▎         | 6/214 [01:59<1:04:05, 18.49s/it]
                                                 

  3%|▎         | 6/214 [01:59<1:04:05, 18.49s/it]
  3%|▎         | 7/214 [02:17<1:02:55, 18.24s/it]
                                                 {'loss': 1.6495, 'grad_norm': 0.37738463282585144, 'learning_rate': 9.986281414193051e-05, 'epoch': 0.03}
{'loss': 1.6305, 'grad_norm': 0.34457454085350037, 'learning_rate': 9.980249213076084e-05, 'epoch': 0.04}
{'loss': 1.5984, 'grad_norm': 0.33673542737960815, 'learning_rate': 9.973123380104681e-05, 'epoch': 0.04}
{'loss': 1.5288, 'grad_norm': 0.33353930711746216, 'learning_rate': 9.964905480067586e-05, 'epoch': 0.05}


  3%|▎         | 7/214 [02:17<1:02:55, 18.24s/it]
  4%|▎         | 8/214 [02:36<1:04:06, 18.67s/it]
                                                 

  4%|▎         | 8/214 [02:36<1:04:06, 18.67s/it]
  4%|▍         | 9/214 [02:54<1:02:47, 18.38s/it]
                                                 

  4%|▍         | 9/214 [02:54<1:02:47, 18.38s/it]
  5%|▍         | 10/214 [03:12<1:01:48, 18.18s/it]
                                                  

  5%|▍         | 10/214 [03:12<1:01:48, 18.18s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 16
{'eval_loss': 1.4660632610321045, 'eval_runtime': 4.2105, 'eval_samples_per_second': 11.875, 'eval_steps_per_second': 0.238, 'epoch': 0.05}

                                                  

  5%|▍         | 10/214 [03:16<1:01:48, 18.18s/it]Saving model checkpoint to /projects/bbvz/choprahetarth/new_experiments/experiment_2/checkpoint-10
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/choprahetarth/new_experiments/experiment_2/checkpoint-10I AM HERE AND I AM SAVING THE MODEL

the checkpoint model will be saved in  /projects/bbvz/choprahetarth/new_experiments/experiment_2/checkpoint-10
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/choprahetarth/new_experiments/experiment_2/checkpoint-10
loading configuration file config.json from cache at /projects/bbvz/choprahetarth/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.40.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/choprahetarth/new_experiments/experiment_2/checkpoint-10
loading configuration file config.json from cache at /projects/bbvz/choprahetarth/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.40.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

the pytorch model path is /projects/bbvz/choprahetarth/new_experiments/experiment_2/checkpoint-10/pytorch_model.bin
the pytorch model path is /projects/bbvz/choprahetarth/new_experiments/experiment_2/checkpoint-10/pytorch_model.bin
the pytorch model path is /projects/bbvz/choprahetarth/new_experiments/experiment_2/checkpoint-10/pytorch_model.bin
the pytorch model path is /projects/bbvz/choprahetarth/new_experiments/experiment_2/checkpoint-10/pytorch_model.bin
/u/choprahetarth/.conda/envs/hetarth_py10/lib/python3.10/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/choprahetarth/.conda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/choprahetarth/.conda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 1.4789, 'grad_norm': 0.3460691273212433, 'learning_rate': 9.955597317564705e-05, 'epoch': 0.05}
{'loss': 1.4454, 'grad_norm': 0.37949609756469727, 'learning_rate': 9.94520093661082e-05, 'epoch': 0.06}
{'loss': 1.3895, 'grad_norm': 0.40085703134536743, 'learning_rate': 9.933718620186744e-05, 'epoch': 0.06}
{'loss': 1.3683, 'grad_norm': 0.35936257243156433, 'learning_rate': 9.921152889737984e-05, 'epoch': 0.07}
{'loss': 1.2995, 'grad_norm': 0.32470467686653137, 'learning_rate': 9.907506504621052e-05, 'epoch': 0.07}
{'loss': 1.5621, 'grad_norm': 0.30973532795906067, 'learning_rate': 9.89278246149752e-05, 'epoch': 0.07}

  5%|▌         | 11/214 [36:34<35:16:05, 625.45s/it]
                                                    

  5%|▌         | 11/214 [36:34<35:16:05, 625.45s/it]
  6%|▌         | 12/214 [36:52<24:43:14, 440.57s/it]
                                                    

  6%|▌         | 12/214 [36:52<24:43:14, 440.57s/it]
  6%|▌         | 13/214 [37:10<17:26:46, 312.47s/it]
                                                    

  6%|▌         | 13/214 [37:10<17:26:46, 312.47s/it]
  7%|▋         | 14/214 [37:27<12:24:48, 223.44s/it]
                                                    

  7%|▋         | 14/214 [37:27<12:24:48, 223.44s/it]
  7%|▋         | 15/214 [37:45<8:55:24, 161.43s/it] 
                                                   

  7%|▋         | 15/214 [37:45<8:55:24, 161.43s/it]
  7%|▋         | 16/214 [38:04<6:31:33, 118.65s/it]
                                                   

  7%|▋         | 16/214 [38:04<6:31:33, 118.65s/it]
  8%|▊         | 17/214 [38:22<4:50:09, 88{'loss': 1.3096, 'grad_norm': 0.31071993708610535, 'learning_rate': 9.876983993675989e-05, 'epoch': 0.08}
{'loss': 1.3122, 'grad_norm': 0.3118046522140503, 'learning_rate': 9.860114570402054e-05, 'epoch': 0.08}
{'loss': 1.2766, 'grad_norm': 0.25334563851356506, 'learning_rate': 9.842177896096494e-05, 'epoch': 0.09}
{'loss': 1.2117, 'grad_norm': 0.21931657195091248, 'learning_rate': 9.823177909541794e-05, 'epoch': 0.09}
.37s/it] 
                                                  

  8%|▊         | 17/214 [38:22<4:50:09, 88.37s/it]
  8%|▊         | 18/214 [38:40<3:39:19, 67.14s/it]
                                                  

  8%|▊         | 18/214 [38:40<3:39:19, 67.14s/it]
  9%|▉         | 19/214 [38:58<2:49:57, 52.30s/it]
                                                  

  9%|▉         | 19/214 [38:58<2:49:57, 52.30s/it]
  9%|▉         | 20/214 [39:16<2:15:31, 41.91s/it]
                                                  

  9%|▉         | 20/214 [39:16<2:15:31, 41.91s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 16
{'eval_loss': 1.2221102714538574, 'eval_runtime': 4.0456, 'eval_samples_per_second': 12.359, 'eval_steps_per_second': 0.247, 'epoch': 0.09}

                                                  

  9%|▉         | 20/214 [39:20<2:15:31, 41.91s/it]Saving model checkpoint to /projects/bbvz/choprahetarth/new_experiments/experiment_2/checkpoint-20
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/choprahetarth/new_experiments/experiment_2/checkpoint-20
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/choprahetarth/new_experiments/experiment_2/checkpoint-20
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/choprahetarth/new_experiments/experiment_2/checkpoint-20
loading configuration file config.json from cache at /projects/bbvz/choprahetarth/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.40.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

the pytorch model path is /projects/bbvz/choprahetarth/new_experiments/experiment_2/checkpoint-20/pytorch_model.bin
the pytorch model path is /projects/bbvz/choprahetarth/new_experiments/experiment_2/checkpoint-20/pytorch_model.bin
the pytorch model path is /projects/bbvz/choprahetarth/new_experiments/experiment_2/checkpoint-20/pytorch_model.bin
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/choprahetarth/new_experiments/experiment_2/checkpoint-20
loading configuration file config.json from cache at /projects/bbvz/choprahetarth/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.40.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

the pytorch model path is /projects/bbvz/choprahetarth/new_experiments/experiment_2/checkpoint-20/pytorch_model.bin
/u/choprahetarth/.conda/envs/hetarth_py10/lib/python3.10/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/choprahetarth/.conda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/choprahetarth/.conda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 1.2215, 'grad_norm': 0.2084965705871582, 'learning_rate': 9.803118783017222e-05, 'epoch': 0.1}
{'loss': 1.2374, 'grad_norm': 0.19649411737918854, 'learning_rate': 9.782004921382612e-05, 'epoch': 0.1}
{'loss': 1.2232, 'grad_norm': 0.18625319004058838, 'learning_rate': 9.759840961111098e-05, 'epoch': 0.11}
{'loss': 1.218, 'grad_norm': 0.16666385531425476, 'learning_rate': 9.736631769270957e-05, 'epoch': 0.11}
{'loss': 1.1908, 'grad_norm': 0.15715917944908142, 'learning_rate': 9.712382442456845e-05, 'epoch': 0.12}
{'loss': 1.2074, 'grad_norm': 0.15500059723854065, 'learning_rate': 9.687098305670605e-05, 'epoch': 0.12}

 10%|▉         | 21/214 [47:22<9:24:31, 175.50s/it]
                                                   

 10%|▉         | 21/214 [47:22<9:24:31, 175.50s/it]
 10%|█         | 22/214 [47:40<6:50:04, 128.15s/it]
                                                   

 10%|█         | 22/214 [47:40<6:50:04, 128.15s/it]
 11%|█         | 23/214 [47:58<5:02:26, 95.01s/it] 
                                                  

 11%|█         | 23/214 [47:58<5:02:26, 95.01s/it]
 11%|█         | 24/214 [48:16<3:47:25, 71.82s/it]
                                                  

 11%|█         | 24/214 [48:16<3:47:25, 71.82s/it]
 12%|█▏        | 25/214 [48:36<2:57:38, 56.39s/it]
                                                  

 12%|█▏        | 25/214 [48:36<2:57:38, 56.39s/it]
 12%|█▏        | 26/214 [48:54<2:20:20, 44.79s/it]
                                                  

 12%|█▏        | 26/214 [48:54<2:20:20, 44.79s/it]
 13%|█▎        | 27/214 [49:11<1:54:17, 36.67s/it]
     {'loss': 1.1885, 'grad_norm': 0.15928809344768524, 'learning_rate': 9.66078491115194e-05, 'epoch': 0.13}
{'loss': 1.1982, 'grad_norm': 0.1503317654132843, 'learning_rate': 9.633448037159167e-05, 'epoch': 0.13}
{'loss': 1.157, 'grad_norm': 0.13895481824874878, 'learning_rate': 9.605093686700355e-05, 'epoch': 0.14}
{'loss': 1.4262, 'grad_norm': 0.17617692053318024, 'learning_rate': 9.575728086215092e-05, 'epoch': 0.14}
                                             

 13%|█▎        | 27/214 [49:11<1:54:17, 36.67s/it]
 13%|█▎        | 28/214 [49:29<1:36:03, 30.99s/it]
                                                  

 13%|█▎        | 28/214 [49:29<1:36:03, 30.99s/it]
 14%|█▎        | 29/214 [49:47<1:23:16, 27.01s/it]
                                                  

 14%|█▎        | 29/214 [49:47<1:23:16, 27.01s/it]
 14%|█▍        | 30/214 [50:05<1:14:17, 24.22s/it]
                                                  

 14%|█▍        | 30/214 [50:05<1:14:17, 24.22s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 16
{'eval_loss': 1.1548813581466675, 'eval_runtime': 4.0821, 'eval_samples_per_second': 12.249, 'eval_steps_per_second': 0.245, 'epoch': 0.14}

                                                  

 14%|█▍        | 30/214 [50:09<1:14:17, 24.22s/it]Saving model checkpoint to /projects/bbvz/choprahetarth/new_experiments/experiment_2/checkpoint-30
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/choprahetarth/new_experiments/experiment_2/checkpoint-30
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/choprahetarth/new_experiments/experiment_2/checkpoint-30
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/choprahetarth/new_experiments/experiment_2/checkpoint-30
Repo card metadata block was not found. Setting CardData to empty.
loading configuration file config.json from cache at /projects/bbvz/choprahetarth/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.40.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

the pytorch model path is /projects/bbvz/choprahetarth/new_experiments/experiment_2/checkpoint-30/pytorch_model.bin
the pytorch model path is /projects/bbvz/choprahetarth/new_experiments/experiment_2/checkpoint-30/pytorch_model.bin
the pytorch model path is /projects/bbvz/choprahetarth/new_experiments/experiment_2/checkpoint-30/pytorch_model.bin
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/choprahetarth/new_experiments/experiment_2/checkpoint-30
loading configuration file config.json from cache at /projects/bbvz/choprahetarth/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.40.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

the pytorch model path is /projects/bbvz/choprahetarth/new_experiments/experiment_2/checkpoint-30/pytorch_model.bin
/u/choprahetarth/.conda/envs/hetarth_py10/lib/python3.10/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/choprahetarth/.conda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/choprahetarth/.conda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 1.1716, 'grad_norm': 0.13970047235488892, 'learning_rate': 9.54535768420721e-05, 'epoch': 0.14}
{'loss': 1.1921, 'grad_norm': 0.14380894601345062, 'learning_rate': 9.513989149828718e-05, 'epoch': 0.15}
{'loss': 1.1686, 'grad_norm': 0.1375349760055542, 'learning_rate': 9.481629371415313e-05, 'epoch': 0.15}
{'loss': 1.1123, 'grad_norm': 0.12300974875688553, 'learning_rate': 9.448285454973738e-05, 'epoch': 0.16}
{'loss': 1.1443, 'grad_norm': 0.12664471566677094, 'learning_rate': 9.413964722621338e-05, 'epoch': 0.16}
{'loss': 1.1381, 'grad_norm': 0.12259920686483383, 'learning_rate': 9.378674710978185e-05, 'epoch': 0.17}

 14%|█▍        | 31/214 [54:05<4:31:45, 89.10s/it]
                                                  

 14%|█▍        | 31/214 [54:05<4:31:45, 89.10s/it]
 15%|█▍        | 32/214 [54:23<3:25:18, 67.68s/it]
                                                  

 15%|█▍        | 32/214 [54:23<3:25:18, 67.68s/it]
 15%|█▌        | 33/214 [54:43<2:40:56, 53.35s/it]
                                                  

 15%|█▌        | 33/214 [54:43<2:40:56, 53.35s/it]
 16%|█▌        | 34/214 [55:00<2:07:59, 42.66s/it]
                                                  

 16%|█▌        | 34/214 [55:01<2:07:59, 42.66s/it]
 16%|█▋        | 35/214 [55:18<1:44:58, 35.18s/it]
                                                  

 16%|█▋        | 35/214 [55:18<1:44:58, 35.18s/it]
 17%|█▋        | 36/214 [55:36<1:28:50, 29.94s/it]
                                                  

 17%|█▋        | 36/214 [55:36<1:28:50, 29.94s/it]
 17%|█▋        | 37/214 [55:54<1:17:31, 26.28s/{'loss': 1.1546, 'grad_norm': 0.13021889328956604, 'learning_rate': 9.342423169512071e-05, 'epoch': 0.17}
{'loss': 1.1343, 'grad_norm': 0.12777602672576904, 'learning_rate': 9.305218058836778e-05, 'epoch': 0.18}
{'loss': 1.1064, 'grad_norm': 0.11504070460796356, 'learning_rate': 9.267067548963975e-05, 'epoch': 0.18}
{'loss': 1.1429, 'grad_norm': 0.11664102971553802, 'learning_rate': 9.22798001750913e-05, 'epoch': 0.19}
it]
                                                  

 17%|█▋        | 37/214 [55:54<1:17:31, 26.28s/it]
 18%|█▊        | 38/214 [56:11<1:09:33, 23.71s/it]
                                                  

 18%|█▊        | 38/214 [56:11<1:09:33, 23.71s/it]
 18%|█▊        | 39/214 [56:29<1:03:54, 21.91s/it]
                                                  

 18%|█▊        | 39/214 [56:29<1:03:54, 21.91s/it]
 19%|█▊        | 40/214 [56:47<59:54, 20.66s/it]  
                                                

 19%|█▊        | 40/214 [56:47<59:54, 20.66s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 16
{'eval_loss': 1.111093282699585, 'eval_runtime': 4.0528, 'eval_samples_per_second': 12.337, 'eval_steps_per_second': 0.247, 'epoch': 0.19}

                                                

 19%|█▊        | 40/214 [56:51<59:54, 20.66s/it]Saving model checkpoint to /projects/bbvz/choprahetarth/new_experiments/experiment_2/checkpoint-40
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/choprahetarth/new_experiments/experiment_2/checkpoint-40
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/choprahetarth/new_experiments/experiment_2/checkpoint-40
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/choprahetarth/new_experiments/experiment_2/checkpoint-40
loading configuration file config.json from cache at /projects/bbvz/choprahetarth/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.40.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

the pytorch model path is /projects/bbvz/choprahetarth/new_experiments/experiment_2/checkpoint-40/pytorch_model.bin
the pytorch model path is /projects/bbvz/choprahetarth/new_experiments/experiment_2/checkpoint-40/pytorch_model.bin
the pytorch model path is /projects/bbvz/choprahetarth/new_experiments/experiment_2/checkpoint-40/pytorch_model.bin
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/choprahetarth/new_experiments/experiment_2/checkpoint-40
loading configuration file config.json from cache at /projects/bbvz/choprahetarth/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.40.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

the pytorch model path is /projects/bbvz/choprahetarth/new_experiments/experiment_2/checkpoint-40/pytorch_model.bin
/u/choprahetarth/.conda/envs/hetarth_py10/lib/python3.10/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/choprahetarth/.conda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/choprahetarth/.conda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 1.1067, 'grad_norm': 0.1136847585439682, 'learning_rate': 9.187964047851852e-05, 'epoch': 0.19}
{'loss': 1.1299, 'grad_norm': 0.11311269551515579, 'learning_rate': 9.14702842725101e-05, 'epoch': 0.2}
{'loss': 1.1025, 'grad_norm': 0.10964952409267426, 'learning_rate': 9.10518214491513e-05, 'epoch': 0.2}
{'loss': 1.3604, 'grad_norm': 0.11493506282567978, 'learning_rate': 9.062434390028407e-05, 'epoch': 0.21}
{'loss': 1.1072, 'grad_norm': 0.11087322980165482, 'learning_rate': 9.018794549732819e-05, 'epoch': 0.21}
{'loss': 1.1279, 'grad_norm': 0.11052602529525757, 'learning_rate': 8.974272207066767e-05, 'epoch': 0.21}

 19%|█▉        | 41/214 [1:09:02<11:17:42, 235.05s/it]
                                                      

 19%|█▉        | 41/214 [1:09:02<11:17:42, 235.05s/it]
 20%|█▉        | 42/214 [1:09:20<8:06:57, 169.87s/it] 
                                                     

 20%|█▉        | 42/214 [1:09:20<8:06:57, 169.87s/it]
 20%|██        | 43/214 [1:09:38<5:54:02, 124.23s/it]
                                                     

 20%|██        | 43/214 [1:09:38<5:54:02, 124.23s/it]
 21%|██        | 44/214 [1:09:55<4:21:26, 92.27s/it] 
                                                    

 21%|██        | 44/214 [1:09:55<4:21:26, 92.27s/it]
 21%|██        | 45/214 [1:10:13<3:16:54, 69.91s/it]
                                                    

 21%|██        | 45/214 [1:10:13<3:16:54, 69.91s/it]
 21%|██▏       | 46/214 [1:10:31<2:31:57, 54.27s/it]
                                                    

 21%|██▏       | 46/214 [1:10:31<2:31:57, 54.27s/i{'loss': 1.1043, 'grad_norm': 0.10792374610900879, 'learning_rate': 8.928877138860707e-05, 'epoch': 0.22}
{'loss': 1.0562, 'grad_norm': 0.10887493193149567, 'learning_rate': 8.882619313590212e-05, 'epoch': 0.22}
{'loss': 1.0913, 'grad_norm': 0.11603451520204544, 'learning_rate': 8.835508889186956e-05, 'epoch': 0.23}
{'loss': 1.0834, 'grad_norm': 0.11095144599676132, 'learning_rate': 8.787556210808101e-05, 'epoch': 0.23}
t]
 22%|██▏       | 47/214 [1:10:49<2:00:31, 43.31s/it]
                                                    

 22%|██▏       | 47/214 [1:10:49<2:00:31, 43.31s/it]
 22%|██▏       | 48/214 [1:11:06<1:38:34, 35.63s/it]
                                                    

 22%|██▏       | 48/214 [1:11:06<1:38:34, 35.63s/it]
 23%|██▎       | 49/214 [1:11:26<1:24:37, 30.77s/it]
                                                    

 23%|██▎       | 49/214 [1:11:26<1:24:37, 30.77s/it]
 23%|██▎       | 50/214 [1:11:43<1:13:24, 26.86s/it]
                                                    

 23%|██▎       | 50/214 [1:11:43<1:13:24, 26.86s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 16
{'eval_loss': 1.078117847442627, 'eval_runtime': 4.1776, 'eval_samples_per_second': 11.969, 'eval_steps_per_second': 0.239, 'epoch': 0.23}

                                                    

 23%|██▎       | 50/214 [1:11:48<1:13:24, 26.86s/it]Saving model checkpoint to /projects/bbvz/choprahetarth/new_experiments/experiment_2/checkpoint-50
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/choprahetarth/new_experiments/experiment_2/checkpoint-50
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/choprahetarth/new_experiments/experiment_2/checkpoint-50
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/choprahetarth/new_experiments/experiment_2/checkpoint-50
Repo card metadata block was not found. Setting CardData to empty.
loading configuration file config.json from cache at /projects/bbvz/choprahetarth/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.40.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/choprahetarth/new_experiments/experiment_2/checkpoint-50
loading configuration file config.json from cache at /projects/bbvz/choprahetarth/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.40.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

the pytorch model path is /projects/bbvz/choprahetarth/new_experiments/experiment_2/checkpoint-50/pytorch_model.bin
the pytorch model path is /projects/bbvz/choprahetarth/new_experiments/experiment_2/checkpoint-50/pytorch_model.bin
the pytorch model path is /projects/bbvz/choprahetarth/new_experiments/experiment_2/checkpoint-50/pytorch_model.bin
the pytorch model path is /projects/bbvz/choprahetarth/new_experiments/experiment_2/checkpoint-50/pytorch_model.bin
/u/choprahetarth/.conda/envs/hetarth_py10/lib/python3.10/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/choprahetarth/.conda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/choprahetarth/.conda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 1.0969, 'grad_norm': 0.11017727851867676, 'learning_rate': 8.738771808564555e-05, 'epoch': 0.24}
{'loss': 1.0831, 'grad_norm': 0.10553857684135437, 'learning_rate': 8.689166395208636e-05, 'epoch': 0.24}
{'loss': 1.0657, 'grad_norm': 0.11368779093027115, 'learning_rate': 8.638750863781612e-05, 'epoch': 0.25}
{'loss': 1.0875, 'grad_norm': 0.11203917860984802, 'learning_rate': 8.587536285221656e-05, 'epoch': 0.25}
{'loss': 1.0602, 'grad_norm': 0.11217638850212097, 'learning_rate': 8.535533905932738e-05, 'epoch': 0.26}
{'loss': 1.0851, 'grad_norm': 0.11689835041761398, 'learning_rate': 8.482755145314986e-05, 'epoch': 0.26}

 24%|██▍       | 51/214 [1:16:38<4:51:22, 107.26s/it]
                                                     

 24%|██▍       | 51/214 [1:16:38<4:51:22, 107.26s/it]
 24%|██▍       | 52/214 [1:16:56<3:37:03, 80.39s/it] 
                                                    

 24%|██▍       | 52/214 [1:16:56<3:37:03, 80.39s/it]
 25%|██▍       | 53/214 [1:17:14<2:45:16, 61.59s/it]
                                                    

 25%|██▍       | 53/214 [1:17:14<2:45:16, 61.59s/it]
 25%|██▌       | 54/214 [1:17:31<2:09:08, 48.43s/it]
                                                    

 25%|██▌       | 54/214 [1:17:31<2:09:08, 48.43s/it]
 26%|██▌       | 55/214 [1:17:49<1:43:55, 39.22s/it]
                                                    

 26%|██▌       | 55/214 [1:17:49<1:43:55, 39.22s/it]
 26%|██▌       | 56/214 [1:18:07<1:26:17, 32.77s/it]
                                                    

 26%|██▌       | 56/214 [1:18:07<1:26:17{'loss': 1.0682, 'grad_norm': 0.11922690272331238, 'learning_rate': 8.429211593257054e-05, 'epoch': 0.27}
{'loss': 1.3202, 'grad_norm': 0.12533552944660187, 'learning_rate': 8.374915007591053e-05, 'epoch': 0.27}
{'loss': 1.0649, 'grad_norm': 0.11606404185295105, 'learning_rate': 8.319877311510613e-05, 'epoch': 0.28}
{'loss': 1.0863, 'grad_norm': 0.10862066596746445, 'learning_rate': 8.264110590952609e-05, 'epoch': 0.28}
, 32.77s/it]
 27%|██▋       | 57/214 [1:18:26<1:15:16, 28.77s/it]
                                                    

 27%|██▋       | 57/214 [1:18:26<1:15:16, 28.77s/it]
 27%|██▋       | 58/214 [1:18:44<1:06:10, 25.45s/it]
                                                    

 27%|██▋       | 58/214 [1:18:44<1:06:10, 25.45s/it]
 28%|██▊       | 59/214 [1:19:02<59:45, 23.13s/it]  
                                                  

 28%|██▊       | 59/214 [1:19:02<59:45, 23.13s/it]
 28%|██▊       | 60/214 [1:19:20<55:12, 21.51s/it]
                                                  

 28%|██▊       | 60/214 [1:19:20<55:12, 21.51s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 16
{'eval_loss': 1.0540207624435425, 'eval_runtime': 4.0398, 'eval_samples_per_second': 12.377, 'eval_steps_per_second': 0.248, 'epoch': 0.28}

                                                  

 28%|██▊       | 60/214 [1:19:24<55:12, 21.51s/it]Saving model checkpoint to /projects/bbvz/choprahetarth/new_experiments/experiment_2/checkpoint-60
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/choprahetarth/new_experiments/experiment_2/checkpoint-60
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/choprahetarth/new_experiments/experiment_2/checkpoint-60
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/choprahetarth/new_experiments/experiment_2/checkpoint-60
Repo card metadata block was not found. Setting CardData to empty.
loading configuration file config.json from cache at /projects/bbvz/choprahetarth/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.40.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

the pytorch model path is /projects/bbvz/choprahetarth/new_experiments/experiment_2/checkpoint-60/pytorch_model.bin
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/choprahetarth/new_experiments/experiment_2/checkpoint-60
loading configuration file config.json from cache at /projects/bbvz/choprahetarth/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.40.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

the pytorch model path is /projects/bbvz/choprahetarth/new_experiments/experiment_2/checkpoint-60/pytorch_model.bin
the pytorch model path is /projects/bbvz/choprahetarth/new_experiments/experiment_2/checkpoint-60/pytorch_model.bin
the pytorch model path is /projects/bbvz/choprahetarth/new_experiments/experiment_2/checkpoint-60/pytorch_model.bin
/u/choprahetarth/.conda/envs/hetarth_py10/lib/python3.10/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/choprahetarth/.conda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/choprahetarth/.conda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 1.07, 'grad_norm': 0.1149602010846138, 'learning_rate': 8.207627091943178e-05, 'epoch': 0.29}
{'loss': 1.0215, 'grad_norm': 0.1158127710223198, 'learning_rate': 8.150439217908556e-05, 'epoch': 0.29}
{'loss': 1.0538, 'grad_norm': 0.11508529633283615, 'learning_rate': 8.092559526951374e-05, 'epoch': 0.29}
{'loss': 1.0473, 'grad_norm': 0.11544465273618698, 'learning_rate': 8.034000729092968e-05, 'epoch': 0.3}
{'loss': 1.0626, 'grad_norm': 0.11484788358211517, 'learning_rate': 7.974775683482338e-05, 'epoch': 0.3}
{'loss': 1.051, 'grad_norm': 0.10891903936862946, 'learning_rate': 7.91489739557236e-05, 'epoch': 0.31}

 29%|██▊       | 61/214 [1:32:03<10:22:16, 244.03s/it]
                                                      

 29%|██▊       | 61/214 [1:32:03<10:22:16, 244.03s/it]
 29%|██▉       | 62/214 [1:32:20<7:26:12, 176.14s/it] 
                                                     

 29%|██▉       | 62/214 [1:32:20<7:26:12, 176.14s/it]
 29%|██▉       | 63/214 [1:32:38<5:23:40, 128.61s/it]
                                                     

 29%|██▉       | 63/214 [1:32:38<5:23:40, 128.61s/it]
 30%|██▉       | 64/214 [1:32:56<3:58:21, 95.34s/it] 
                                                    

 30%|██▉       | 64/214 [1:32:56<3:58:21, 95.34s/it]
 30%|███       | 65/214 [1:33:14<2:58:56, 72.05s/it]
                                                    

 30%|███       | 65/214 [1:33:14<2:58:56, 72.05s/it]
 31%|███       | 66/214 [1:33:33<2:18:48, 56.27s/it]
                                                    

 31%|███       | 66/214 [1:33:{'loss': 1.0305, 'grad_norm': 0.11419760435819626, 'learning_rate': 7.854379014263876e-05, 'epoch': 0.31}
{'loss': 1.0528, 'grad_norm': 0.1150856614112854, 'learning_rate': 7.793233829018262e-05, 'epoch': 0.32}
{'loss': 1.0218, 'grad_norm': 0.1149163544178009, 'learning_rate': 7.731475266939159e-05, 'epoch': 0.32}
{'loss': 1.0519, 'grad_norm': 0.11349857598543167, 'learning_rate': 7.669116889823955e-05, 'epoch': 0.33}
33<2:18:48, 56.27s/it]
 31%|███▏      | 67/214 [1:33:51<1:49:32, 44.71s/it]
                                                    

 31%|███▏      | 67/214 [1:33:51<1:49:32, 44.71s/it]
 32%|███▏      | 68/214 [1:34:09<1:29:05, 36.61s/it]
                                                    

 32%|███▏      | 68/214 [1:34:09<1:29:05, 36.61s/it]
 32%|███▏      | 69/214 [1:34:26<1:14:46, 30.94s/it]
                                                    

 32%|███▏      | 69/214 [1:34:26<1:14:46, 30.94s/it]
 33%|███▎      | 70/214 [1:34:44<1:04:44, 26.98s/it]
                                                    

 33%|███▎      | 70/214 [1:34:44<1:04:44, 26.98s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 16
{'eval_loss': 1.035506010055542, 'eval_runtime': 4.0387, 'eval_samples_per_second': 12.38, 'eval_steps_per_second': 0.248, 'epoch': 0.33}

                                                    

 33%|███▎      | 70/214 [1:34:48<1:04:44, 26.98s/it]Saving model checkpoint to /projects/bbvz/choprahetarth/new_experiments/experiment_2/checkpoint-70
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/choprahetarth/new_experiments/experiment_2/checkpoint-70
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/choprahetarth/new_experiments/experiment_2/checkpoint-70
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/choprahetarth/new_experiments/experiment_2/checkpoint-70
loading configuration file config.json from cache at /projects/bbvz/choprahetarth/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.40.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

the pytorch model path is /projects/bbvz/choprahetarth/new_experiments/experiment_2/checkpoint-70/pytorch_model.bin
the pytorch model path is /projects/bbvz/choprahetarth/new_experiments/experiment_2/checkpoint-70/pytorch_model.bin
the pytorch model path is /projects/bbvz/choprahetarth/new_experiments/experiment_2/checkpoint-70/pytorch_model.bin
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/choprahetarth/new_experiments/experiment_2/checkpoint-70
loading configuration file config.json from cache at /projects/bbvz/choprahetarth/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.40.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

the pytorch model path is /projects/bbvz/choprahetarth/new_experiments/experiment_2/checkpoint-70/pytorch_model.bin
/u/choprahetarth/.conda/envs/hetarth_py10/lib/python3.10/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/choprahetarth/.conda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/choprahetarth/.conda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 1.0345, 'grad_norm': 0.11479628831148148, 'learning_rate': 7.6061723911857e-05, 'epoch': 0.33}
{'loss': 1.2987, 'grad_norm': 0.14950105547904968, 'learning_rate': 7.542655593246103e-05, 'epoch': 0.34}
{'loss': 1.0418, 'grad_norm': 0.11532565951347351, 'learning_rate': 7.478580443900247e-05, 'epoch': 0.34}
{'loss': 1.0614, 'grad_norm': 0.11416167765855789, 'learning_rate': 7.413961013653726e-05, 'epoch': 0.35}
{'loss': 1.0318, 'grad_norm': 0.11985605955123901, 'learning_rate': 7.34881149253284e-05, 'epoch': 0.35}
{'loss': 0.9969, 'grad_norm': 0.11743152886629105, 'learning_rate': 7.283146186968565e-05, 'epoch': 0.36}

 33%|███▎      | 71/214 [1:53:52<14:26:07, 363.41s/it]
                                                      

 33%|███▎      | 71/214 [1:53:53<14:26:07, 363.41s/it]
 34%|███▎      | 72/214 [1:54:11<10:14:59, 259.86s/it]
                                                      

 34%|███▎      | 72/214 [1:54:11<10:14:59, 259.86s/it]
 34%|███▍      | 73/214 [1:54:28<7:19:57, 187.21s/it] 
                                                     

 34%|███▍      | 73/214 [1:54:28<7:19:57, 187.21s/it]
 35%|███▍      | 74/214 [1:54:49<5:19:59, 137.14s/it]
                                                     

 35%|███▍      | 74/214 [1:54:49<5:19:59, 137.14s/it]
 35%|███▌      | 75/214 [1:55:06<3:54:42, 101.31s/it]
                                                     

 35%|███▌      | 75/214 [1:55:06<3:54:42, 101.31s/it]
 36%|███▌      | 76/214 [1:55:24<2:55:20, 76.23s/it] 
                                                    

 36%|{'loss': 1.0357, 'grad_norm': 0.11724266409873962, 'learning_rate': 7.216979516654943e-05, 'epoch': 0.36}
{'loss': 1.0257, 'grad_norm': 0.12192590534687042, 'learning_rate': 7.150326011382604e-05, 'epoch': 0.36}
{'loss': 1.036, 'grad_norm': 0.11782144010066986, 'learning_rate': 7.083200307848116e-05, 'epoch': 0.37}
{'loss': 1.0288, 'grad_norm': 0.11585895717144012, 'learning_rate': 7.015617146439863e-05, 'epoch': 0.37}
███▌      | 76/214 [1:55:24<2:55:20, 76.23s/it]
 36%|███▌      | 77/214 [1:55:42<2:13:58, 58.68s/it]
                                                    

 36%|███▌      | 77/214 [1:55:42<2:13:58, 58.68s/it]
 36%|███▋      | 78/214 [1:55:59<1:45:09, 46.39s/it]
                                                    

 36%|███▋      | 78/214 [1:55:59<1:45:09, 46.39s/it]
 37%|███▋      | 79/214 [1:56:17<1:25:01, 37.79s/it]
                                                    

 37%|███▋      | 79/214 [1:56:17<1:25:01, 37.79s/it]
 37%|███▋      | 80/214 [1:56:35<1:10:57, 31.77s/it]
                                                    

 37%|███▋      | 80/214 [1:56:35<1:10:57, 31.77s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 16
{'eval_loss': 1.0208970308303833, 'eval_runtime': 4.2114, 'eval_samples_per_second': 11.872, 'eval_steps_per_second': 0.237, 'epoch': 0.37}

                                                    

 37%|███▋      | 80/214 [1:56:39<1:10:57, 31.77s/it]Saving model checkpoint to /projects/bbvz/choprahetarth/new_experiments/experiment_2/checkpoint-80
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/choprahetarth/new_experiments/experiment_2/checkpoint-80
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/choprahetarth/new_experiments/experiment_2/checkpoint-80
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/choprahetarth/new_experiments/experiment_2/checkpoint-80
loading configuration file config.json from cache at /projects/bbvz/choprahetarth/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.40.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/choprahetarth/new_experiments/experiment_2/checkpoint-80
loading configuration file config.json from cache at /projects/bbvz/choprahetarth/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.40.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

the pytorch model path is /projects/bbvz/choprahetarth/new_experiments/experiment_2/checkpoint-80/pytorch_model.bin
the pytorch model path is /projects/bbvz/choprahetarth/new_experiments/experiment_2/checkpoint-80/pytorch_model.bin
the pytorch model path is /projects/bbvz/choprahetarth/new_experiments/experiment_2/checkpoint-80/pytorch_model.bin
the pytorch model path is /projects/bbvz/choprahetarth/new_experiments/experiment_2/checkpoint-80/pytorch_model.bin
/u/choprahetarth/.conda/envs/hetarth_py10/lib/python3.10/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/choprahetarth/.conda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/choprahetarth/.conda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 1.0081, 'grad_norm': 0.13058196008205414, 'learning_rate': 6.947591368001138e-05, 'epoch': 0.38}
{'loss': 1.026, 'grad_norm': 0.12673647701740265, 'learning_rate': 6.879137910571191e-05, 'epoch': 0.38}
{'loss': 1.0057, 'grad_norm': 0.11862807720899582, 'learning_rate': 6.810271806104931e-05, 'epoch': 0.39}
{'loss': 1.0333, 'grad_norm': 0.12588496506214142, 'learning_rate': 6.741008177171995e-05, 'epoch': 0.39}
{'loss': 1.0144, 'grad_norm': 0.12551864981651306, 'learning_rate': 6.671362233635925e-05, 'epoch': 0.4}
{'loss': 1.2783, 'grad_norm': 0.12115170061588287, 'learning_rate': 6.601349269314188e-05, 'epoch': 0.4}

 38%|███▊      | 81/214 [2:24:25<19:20:06, 523.36s/it]
                                                      

 38%|███▊      | 81/214 [2:24:25<19:20:06, 523.36s/it]
 38%|███▊      | 82/214 [2:24:46<13:39:29, 372.49s/it]
                                                      

 38%|███▊      | 82/214 [2:24:46<13:39:29, 372.49s/it]
 39%|███▉      | 83/214 [2:25:04<9:40:54, 266.06s/it] 
                                                     

 39%|███▉      | 83/214 [2:25:04<9:40:54, 266.06s/it]
 39%|███▉      | 84/214 [2:25:21<6:55:02, 191.56s/it]
                                                     

 39%|███▉      | 84/214 [2:25:21<6:55:02, 191.56s/it]
 40%|███▉      | 85/214 [2:25:39<4:59:43, 139.41s/it]
                                                     

 40%|███▉      | 85/214 [2:25:39<4:59:43, 139.41s/it]
 40%|████      | 86/214 [2:25:57<3:39:31, 102.90s/it]
                                                     

 40%{'loss': 1.0125, 'grad_norm': 0.12365873903036118, 'learning_rate': 6.530984658619734e-05, 'epoch': 0.41}
{'loss': 1.0489, 'grad_norm': 0.12274517118930817, 'learning_rate': 6.460283853184879e-05, 'epoch': 0.41}
{'loss': 1.0181, 'grad_norm': 0.12794986367225647, 'learning_rate': 6.38926237846822e-05, 'epoch': 0.42}
{'loss': 0.9801, 'grad_norm': 0.13598494231700897, 'learning_rate': 6.317935830345338e-05, 'epoch': 0.42}
|████      | 86/214 [2:25:57<3:39:31, 102.90s/it]
 41%|████      | 87/214 [2:26:14<2:43:43, 77.35s/it] 
                                                    

 41%|████      | 87/214 [2:26:14<2:43:43, 77.35s/it]
 41%|████      | 88/214 [2:26:32<2:04:52, 59.46s/it]
                                                    

 41%|████      | 88/214 [2:26:32<2:04:52, 59.46s/it]
 42%|████▏     | 89/214 [2:26:50<1:37:47, 46.94s/it]
                                                    

 42%|████▏     | 89/214 [2:26:50<1:37:47, 46.94s/it]
 42%|████▏     | 90/214 [2:27:09<1:19:56, 38.68s/it]
                                                    

 42%|████▏     | 90/214 [2:27:09<1:19:56, 38.68s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 16
{'eval_loss': 1.0085139274597168, 'eval_runtime': 4.3314, 'eval_samples_per_second': 11.544, 'eval_steps_per_second': 0.231, 'epoch': 0.42}

                                                    

 42%|████▏     | 90/214 [2:27:14<1:19:56, 38.68s/it]Saving model checkpoint to /projects/bbvz/choprahetarth/new_experiments/experiment_2/checkpoint-90
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in I AM HERE AND I AM SAVING THE MODEL /projects/bbvz/choprahetarth/new_experiments/experiment_2/checkpoint-90

I AM HERE AND I AM SAVING THE MODELthe checkpoint model will be saved in 
 /projects/bbvz/choprahetarth/new_experiments/experiment_2/checkpoint-90
the checkpoint model will be saved in  /projects/bbvz/choprahetarth/new_experiments/experiment_2/checkpoint-90
loading configuration file config.json from cache at /projects/bbvz/choprahetarth/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.40.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

the pytorch model path is /projects/bbvz/choprahetarth/new_experiments/experiment_2/checkpoint-90/pytorch_model.bin
the pytorch model path is /projects/bbvz/choprahetarth/new_experiments/experiment_2/checkpoint-90/pytorch_model.bin
the pytorch model path is /projects/bbvz/choprahetarth/new_experiments/experiment_2/checkpoint-90/pytorch_model.bin
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/choprahetarth/new_experiments/experiment_2/checkpoint-90
loading configuration file config.json from cache at /projects/bbvz/choprahetarth/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.40.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

the pytorch model path is /projects/bbvz/choprahetarth/new_experiments/experiment_2/checkpoint-90/pytorch_model.bin
/u/choprahetarth/.conda/envs/hetarth_py10/lib/python3.10/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/choprahetarth/.conda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/choprahetarth/.conda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 1.0079, 'grad_norm': 0.13175417482852936, 'learning_rate': 6.246319871684048e-05, 'epoch': 0.43}
{'loss': 1.0067, 'grad_norm': 0.1291847974061966, 'learning_rate': 6.174430228904919e-05, 'epoch': 0.43}
{'loss': 1.0124, 'grad_norm': 0.13311560451984406, 'learning_rate': 6.102282688527859e-05, 'epoch': 0.43}
{'loss': 1.0059, 'grad_norm': 0.12350684404373169, 'learning_rate': 6.029893093705492e-05, 'epoch': 0.44}
{'loss': 0.9943, 'grad_norm': 0.1345192790031433, 'learning_rate': 5.957277340744094e-05, 'epoch': 0.44}

 43%|████▎     | 91/214 [2:33:18<4:42:11, 137.65s/it]
                                                     

 43%|████▎     | 91/214 [2:33:18<4:42:11, 137.65s/it]
 43%|████▎     | 92/214 [2:33:36<3:26:44, 101.67s/it]
                                                     

 43%|████▎     | 92/214 [2:33:36<3:26:44, 101.67s/it]
 43%|████▎     | 93/214 [2:33:53<2:34:14, 76.49s/it] 
                                                    

 43%|████▎     | 93/214 [2:33:53<2:34:14, 76.49s/it]
 44%|████▍     | 94/214 [2:34:11<1:57:42, 58.85s/it]
                                                    

 44%|████▍     | 94/214 [2:34:11<1:57:42, 58.85s/it]
 44%|████▍     | 95/214 [2:34:29<1:32:15, 46.52s/it]
                                                    

 44%|████▍     | 95/214 [2:34:29<1:32:15, 46.52s/it]
 45%|████▍     | 96/214 [2:34:46<1:14:29, 37.87s/it]
                                                    
{'loss': 1.0066, 'grad_norm': 0.1309228539466858, 'learning_rate': 5.884451375612865e-05, 'epoch': 0.45}
{'loss': 0.9927, 'grad_norm': 0.1266697645187378, 'learning_rate': 5.8114311904423004e-05, 'epoch': 0.45}
{'loss': 1.0111, 'grad_norm': 0.142589271068573, 'learning_rate': 5.738232820012407e-05, 'epoch': 0.46}
{'loss': 0.9974, 'grad_norm': 0.13142581284046173, 'learning_rate': 5.6648723382315715e-05, 'epoch': 0.46}
{'loss': 1.2669, 'grad_norm': 0.1300058811903, 'learning_rate': 5.5913658546068295e-05, 'epoch': 0.47}

 45%|████▍     | 96/214 [2:34:46<1:14:29, 37.87s/it]
 45%|████▌     | 97/214 [2:35:04<1:02:03, 31.83s/it]
                                                    

 45%|████▌     | 97/214 [2:35:04<1:02:03, 31.83s/it]
 46%|████▌     | 98/214 [2:35:24<54:23, 28.13s/it]  
                                                  

 46%|████▌     | 98/214 [2:35:24<54:23, 28.13s/it]
 46%|████▋     | 99/214 [2:35:41<47:56, 25.01s/it]
                                                  

 46%|████▋     | 99/214 [2:35:41<47:56, 25.01s/it]
 47%|████▋     | 100/214 [2:35:59<43:21, 22.82s/it]
                                                   

 47%|████▋     | 100/214 [2:35:59<43:21, 22.82s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 16
{'eval_loss': 0.9994807243347168, 'eval_runtime': 4.04, 'eval_samples_per_second': 12.376, 'eval_steps_per_second': 0.248, 'epoch': 0.47}

                                                   

 47%|████▋     | 100/214 [2:36:03<43:21, 22.82s/it]Saving model checkpoint to /projects/bbvz/choprahetarth/new_experiments/experiment_2/checkpoint-100
I AM HERE AND I AM SAVING THE MODELI AM HERE AND I AM SAVING THE MODEL

the checkpoint model will be saved in the checkpoint model will be saved in  /projects/bbvz/choprahetarth/new_experiments/experiment_2/checkpoint-100
 /projects/bbvz/choprahetarth/new_experiments/experiment_2/checkpoint-100
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/choprahetarth/new_experiments/experiment_2/checkpoint-100
Repo card metadata block was not found. Setting CardData to empty.
loading configuration file config.json from cache at /projects/bbvz/choprahetarth/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.40.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/choprahetarth/new_experiments/experiment_2/checkpoint-100
loading configuration file config.json from cache at /projects/bbvz/choprahetarth/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.40.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

the pytorch model path is /projects/bbvz/choprahetarth/new_experiments/experiment_2/checkpoint-100/pytorch_model.bin
the pytorch model path is /projects/bbvz/choprahetarth/new_experiments/experiment_2/checkpoint-100/pytorch_model.bin
the pytorch model path is /projects/bbvz/choprahetarth/new_experiments/experiment_2/checkpoint-100/pytorch_model.bin
the pytorch model path is /projects/bbvz/choprahetarth/new_experiments/experiment_2/checkpoint-100/pytorch_model.bin
/u/choprahetarth/.conda/envs/hetarth_py10/lib/python3.10/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/choprahetarth/.conda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/choprahetarth/.conda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.9917, 'grad_norm': 0.126450315117836, 'learning_rate': 5.517729510706315e-05, 'epoch': 0.47}
{'loss': 1.0292, 'grad_norm': 0.14007100462913513, 'learning_rate': 5.4439794766146746e-05, 'epoch': 0.48}
{'loss': 1.0084, 'grad_norm': 0.13584217429161072, 'learning_rate': 5.3701319473822146e-05, 'epoch': 0.48}
{'loss': 0.9619, 'grad_norm': 0.13186539709568024, 'learning_rate': 5.296203139468572e-05, 'epoch': 0.49}
{'loss': 0.994, 'grad_norm': 0.1386231929063797, 'learning_rate': 5.2222092871816766e-05, 'epoch': 0.49}

 47%|████▋     | 101/214 [2:56:37<12:09:43, 387.46s/it]
                                                       

 47%|████▋     | 101/214 [2:56:37<12:09:43, 387.46s/it]
 48%|████▊     | 102/214 [2:56:55<8:36:12, 276.54s/it] 
                                                      

 48%|████▊     | 102/214 [2:56:55<8:36:12, 276.54s/it]
 48%|████▊     | 103/214 [2:57:13<6:07:56, 198.89s/it]
                                                      

 48%|████▊     | 103/214 [2:57:13<6:07:56, 198.89s/it]
 49%|████▊     | 104/214 [2:57:31<4:24:59, 144.54s/it]
                                                      

 49%|████▊     | 104/214 [2:57:31<4:24:59, 144.54s/it]
 49%|████▉     | 105/214 [2:57:48<3:13:27, 106.49s/it]
                                                      

 49%|████▉     | 105/214 [2:57:48<3:13:27, 106.49s/it]
 50%|████▉     | 106/214 [2:58:08<2:24:39, 80.36s/it] 
                        {'loss': 0.9877, 'grad_norm': 0.13463540375232697, 'learning_rate': 5.148166639112799e-05, 'epoch': 0.5}
{'loss': 0.9972, 'grad_norm': 0.13877807557582855, 'learning_rate': 5.074091454568464e-05, 'epoch': 0.5}
{'loss': 1.0016, 'grad_norm': 0.13256323337554932, 'learning_rate': 5e-05, 'epoch': 0.5}
{'loss': 0.9763, 'grad_norm': 0.14611303806304932, 'learning_rate': 4.925908545431537e-05, 'epoch': 0.51}
{'loss': 0.9867, 'grad_norm': 0.1399519443511963, 'learning_rate': 4.851833360887201e-05, 'epoch': 0.51}
                             

 50%|████▉     | 106/214 [2:58:08<2:24:39, 80.36s/it]
 50%|█████     | 107/214 [2:58:25<1:49:47, 61.57s/it]
                                                     

 50%|█████     | 107/214 [2:58:25<1:49:47, 61.57s/it]
 50%|█████     | 108/214 [2:58:43<1:25:31, 48.41s/it]
                                                     

 50%|█████     | 108/214 [2:58:43<1:25:31, 48.41s/it]
 51%|█████     | 109/214 [2:59:01<1:08:36, 39.21s/it]
                                                     

 51%|█████     | 109/214 [2:59:01<1:08:36, 39.21s/it]
 51%|█████▏    | 110/214 [2:59:19<56:46, 32.76s/it]  
                                                   

 51%|█████▏    | 110/214 [2:59:19<56:46, 32.76s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 16
{'eval_loss': 0.9914006590843201, 'eval_runtime': 4.0364, 'eval_samples_per_second': 12.387, 'eval_steps_per_second': 0.248, 'epoch': 0.51}

                                                   

 51%|█████▏    | 110/214 [2:59:23<56:46, 32.76s/it]Saving model checkpoint to /projects/bbvz/choprahetarth/new_experiments/experiment_2/checkpoint-110
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/choprahetarth/new_experiments/experiment_2/checkpoint-110
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/choprahetarth/new_experiments/experiment_2/checkpoint-110
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/choprahetarth/new_experiments/experiment_2/checkpoint-110
Repo card metadata block was not found. Setting CardData to empty.
loading configuration file config.json from cache at /projects/bbvz/choprahetarth/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.40.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

the pytorch model path is /projects/bbvz/choprahetarth/new_experiments/experiment_2/checkpoint-110/pytorch_model.bin
the pytorch model path is /projects/bbvz/choprahetarth/new_experiments/experiment_2/checkpoint-110/pytorch_model.bin
the pytorch model path is /projects/bbvz/choprahetarth/new_experiments/experiment_2/checkpoint-110/pytorch_model.bin
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/choprahetarth/new_experiments/experiment_2/checkpoint-110
loading configuration file config.json from cache at /projects/bbvz/choprahetarth/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.40.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

the pytorch model path is /projects/bbvz/choprahetarth/new_experiments/experiment_2/checkpoint-110/pytorch_model.bin
/u/choprahetarth/.conda/envs/hetarth_py10/lib/python3.10/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/choprahetarth/.conda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/choprahetarth/.conda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.9821, 'grad_norm': 0.14422376453876495, 'learning_rate': 4.777790712818324e-05, 'epoch': 0.52}
{'loss': 1.0047, 'grad_norm': 0.15523187816143036, 'learning_rate': 4.703796860531429e-05, 'epoch': 0.52}
{'loss': 0.9923, 'grad_norm': 0.1462201029062271, 'learning_rate': 4.629868052617786e-05, 'epoch': 0.53}
{'loss': 1.2558, 'grad_norm': 0.1403019279241562, 'learning_rate': 4.5560205233853266e-05, 'epoch': 0.53}
{'loss': 0.9816, 'grad_norm': 0.14524070918560028, 'learning_rate': 4.482270489293685e-05, 'epoch': 0.54}

 52%|█████▏    | 111/214 [3:01:53<1:58:55, 69.27s/it]
                                                     

 52%|█████▏    | 111/214 [3:01:53<1:58:55, 69.27s/it]
 52%|█████▏    | 112/214 [3:02:11<1:31:28, 53.80s/it]
                                                     

 52%|█████▏    | 112/214 [3:02:11<1:31:28, 53.80s/it]
 53%|█████▎    | 113/214 [3:02:28<1:12:20, 42.98s/it]
                                                     

 53%|█████▎    | 113/214 [3:02:28<1:12:20, 42.98s/it]
 53%|█████▎    | 114/214 [3:02:46<59:00, 35.40s/it]  
                                                   

 53%|█████▎    | 114/214 [3:02:46<59:00, 35.40s/it]
 54%|█████▎    | 115/214 [3:03:06<50:30, 30.61s/it]
                                                   

 54%|█████▎    | 115/214 [3:03:06<50:30, 30.61s/it]
 54%|█████▍    | 116/214 [3:03:23<43:40, 26.74s/it]
                                  {'loss': 1.0208, 'grad_norm': 0.15229341387748718, 'learning_rate': 4.4086341453931716e-05, 'epoch': 0.54}
{'loss': 0.9931, 'grad_norm': 0.14400909841060638, 'learning_rate': 4.335127661768429e-05, 'epoch': 0.55}
{'loss': 0.948, 'grad_norm': 0.14872393012046814, 'learning_rate': 4.2617671799875944e-05, 'epoch': 0.55}
{'loss': 0.9824, 'grad_norm': 0.14015260338783264, 'learning_rate': 4.1885688095577e-05, 'epoch': 0.56}
{'loss': 0.9765, 'grad_norm': 0.14239796996116638, 'learning_rate': 4.115548624387137e-05, 'epoch': 0.56}
                 

 54%|█████▍    | 116/214 [3:03:23<43:40, 26.74s/it]
 55%|█████▍    | 117/214 [3:03:41<38:51, 24.03s/it]
                                                   

 55%|█████▍    | 117/214 [3:03:41<38:51, 24.03s/it]
 55%|█████▌    | 118/214 [3:03:59<35:25, 22.14s/it]
                                                   

 55%|█████▌    | 118/214 [3:03:59<35:25, 22.14s/it]
 56%|█████▌    | 119/214 [3:04:16<32:57, 20.81s/it]
                                                   

 56%|█████▌    | 119/214 [3:04:16<32:57, 20.81s/it]
 56%|█████▌    | 120/214 [3:04:34<31:09, 19.88s/it]
                                                   

 56%|█████▌    | 120/214 [3:04:34<31:09, 19.88s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 16
{'eval_loss': 0.9848700165748596, 'eval_runtime': 4.0407, 'eval_samples_per_second': 12.374, 'eval_steps_per_second': 0.247, 'epoch': 0.56}

                                                   

 56%|█████▌    | 120/214 [3:04:38<31:09, 19.88s/it]Saving model checkpoint to /projects/bbvz/choprahetarth/new_experiments/experiment_2/checkpoint-120
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/choprahetarth/new_experiments/experiment_2/checkpoint-120
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/choprahetarth/new_experiments/experiment_2/checkpoint-120
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/choprahetarth/new_experiments/experiment_2/checkpoint-120
Repo card metadata block was not found. Setting CardData to empty.
Repo card metadata block was not found. Setting CardData to empty.
Repo card metadata block was not found. Setting CardData to empty.
loading configuration file config.json from cache at /projects/bbvz/choprahetarth/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.40.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

the pytorch model path is /projects/bbvz/choprahetarth/new_experiments/experiment_2/checkpoint-120/pytorch_model.bin
the pytorch model path is /projects/bbvz/choprahetarth/new_experiments/experiment_2/checkpoint-120/pytorch_model.bin
the pytorch model path is /projects/bbvz/choprahetarth/new_experiments/experiment_2/checkpoint-120/pytorch_model.bin
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/choprahetarth/new_experiments/experiment_2/checkpoint-120
loading configuration file config.json from cache at /projects/bbvz/choprahetarth/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.40.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

the pytorch model path is /projects/bbvz/choprahetarth/new_experiments/experiment_2/checkpoint-120/pytorch_model.bin
/u/choprahetarth/.conda/envs/hetarth_py10/lib/python3.10/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/choprahetarth/.conda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/choprahetarth/.conda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.9886, 'grad_norm': 0.14707064628601074, 'learning_rate': 4.0427226592559065e-05, 'epoch': 0.57}
{'loss': 0.9864, 'grad_norm': 0.1380932331085205, 'learning_rate': 3.970106906294509e-05, 'epoch': 0.57}
{'loss': 0.9656, 'grad_norm': 0.15262672305107117, 'learning_rate': 3.897717311472141e-05, 'epoch': 0.57}
{'loss': 0.9812, 'grad_norm': 0.15214112401008606, 'learning_rate': 3.825569771095082e-05, 'epoch': 0.58}
{'loss': 0.9605, 'grad_norm': 0.14395280182361603, 'learning_rate': 3.753680128315952e-05, 'epoch': 0.58}

 57%|█████▋    | 121/214 [3:12:31<4:03:19, 156.98s/it]
                                                      

 57%|█████▋    | 121/214 [3:12:31<4:03:19, 156.98s/it]
 57%|█████▋    | 122/214 [3:12:49<2:56:38, 115.21s/it]
                                                      

 57%|█████▋    | 122/214 [3:12:49<2:56:38, 115.21s/it]
 57%|█████▋    | 123/214 [3:13:09<2:11:29, 86.69s/it] 
                                                     

 57%|█████▋    | 123/214 [3:13:09<2:11:29, 86.69s/it]
 58%|█████▊    | 124/214 [3:13:27<1:38:59, 66.00s/it]
                                                     

 58%|█████▊    | 124/214 [3:13:27<1:38:59, 66.00s/it]
 58%|█████▊    | 125/214 [3:13:44<1:16:24, 51.51s/it]
                                                     

 58%|█████▊    | 125/214 [3:13:44<1:16:24, 51.51s/it]
 59%|█████▉    | 126/214 [3:14:02<1:00:41, 41.38s/it]
               {'loss': 0.9973, 'grad_norm': 0.1483408510684967, 'learning_rate': 3.682064169654663e-05, 'epoch': 0.59}
{'loss': 0.9759, 'grad_norm': 0.1472802609205246, 'learning_rate': 3.6107376215317815e-05, 'epoch': 0.59}
{'loss': 1.2491, 'grad_norm': 0.144279345870018, 'learning_rate': 3.539716146815122e-05, 'epoch': 0.6}
{'loss': 0.9696, 'grad_norm': 0.1463136523962021, 'learning_rate': 3.469015341380266e-05, 'epoch': 0.6}
{'loss': 1.0048, 'grad_norm': 0.1442924439907074, 'learning_rate': 3.3986507306858125e-05, 'epoch': 0.61}
                                      

 59%|█████▉    | 126/214 [3:14:02<1:00:41, 41.38s/it]
 59%|█████▉    | 127/214 [3:14:20<49:42, 34.28s/it]  
                                                   

 59%|█████▉    | 127/214 [3:14:20<49:42, 34.28s/it]
 60%|█████▉    | 128/214 [3:14:38<42:00, 29.31s/it]
                                                   

 60%|█████▉    | 128/214 [3:14:38<42:00, 29.31s/it]
 60%|██████    | 129/214 [3:14:55<36:35, 25.83s/it]
                                                   

 60%|██████    | 129/214 [3:14:55<36:35, 25.83s/it]
 61%|██████    | 130/214 [3:15:13<32:58, 23.55s/it]
                                                   

 61%|██████    | 130/214 [3:15:13<32:58, 23.55s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 16
{'eval_loss': 0.9795377254486084, 'eval_runtime': 5.6999, 'eval_samples_per_second': 8.772, 'eval_steps_per_second': 0.175, 'epoch': 0.61}

                                                   

 61%|██████    | 130/214 [3:15:19<32:58, 23.55s/it]Saving model checkpoint to /projects/bbvz/choprahetarth/new_experiments/experiment_2/checkpoint-130
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/choprahetarth/new_experiments/experiment_2/checkpoint-130
I AM HERE AND I AM SAVING THE MODEL
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  the checkpoint model will be saved in /projects/bbvz/choprahetarth/new_experiments/experiment_2/checkpoint-130
 /projects/bbvz/choprahetarth/new_experiments/experiment_2/checkpoint-130
loading configuration file config.json from cache at /projects/bbvz/choprahetarth/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.40.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

the pytorch model path is /projects/bbvz/choprahetarth/new_experiments/experiment_2/checkpoint-130/pytorch_model.bin
the pytorch model path is /projects/bbvz/choprahetarth/new_experiments/experiment_2/checkpoint-130/pytorch_model.bin
the pytorch model path is /projects/bbvz/choprahetarth/new_experiments/experiment_2/checkpoint-130/pytorch_model.bin
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/choprahetarth/new_experiments/experiment_2/checkpoint-130
loading configuration file config.json from cache at /projects/bbvz/choprahetarth/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.40.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

the pytorch model path is /projects/bbvz/choprahetarth/new_experiments/experiment_2/checkpoint-130/pytorch_model.bin
/u/choprahetarth/.conda/envs/hetarth_py10/lib/python3.10/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/choprahetarth/.conda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/choprahetarth/.conda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.9805, 'grad_norm': 0.14685414731502533, 'learning_rate': 3.328637766364075e-05, 'epoch': 0.61}
{'loss': 0.9416, 'grad_norm': 0.15473124384880066, 'learning_rate': 3.258991822828007e-05, 'epoch': 0.62}
{'loss': 0.9723, 'grad_norm': 0.1488180309534073, 'learning_rate': 3.189728193895069e-05, 'epoch': 0.62}
{'loss': 0.9714, 'grad_norm': 0.14975646138191223, 'learning_rate': 3.12086208942881e-05, 'epoch': 0.63}
{'loss': 0.9791, 'grad_norm': 0.14987801015377045, 'learning_rate': 3.0524086319988634e-05, 'epoch': 0.63}

 61%|██████    | 131/214 [3:16:31<54:51, 39.65s/it]
                                                   

 61%|██████    | 131/214 [3:16:31<54:51, 39.65s/it]
 62%|██████▏   | 132/214 [3:16:48<45:12, 33.07s/it]
                                                   

 62%|██████▏   | 132/214 [3:16:48<45:12, 33.07s/it]
 62%|██████▏   | 133/214 [3:17:06<38:25, 28.47s/it]
                                                   

 62%|██████▏   | 133/214 [3:17:06<38:25, 28.47s/it]
 63%|██████▎   | 134/214 [3:17:24<33:39, 25.24s/it]
                                                   

 63%|██████▎   | 134/214 [3:17:24<33:39, 25.24s/it]
 63%|██████▎   | 135/214 [3:17:42<30:16, 22.99s/it]
                                                   

 63%|██████▎   | 135/214 [3:17:42<30:16, 22.99s/it]
 64%|██████▎   | 136/214 [3:17:59<27:49, 21.41s/it]
                                    {'loss': 0.9845, 'grad_norm': 0.141366645693779, 'learning_rate': 2.98438285356014e-05, 'epoch': 0.64}
{'loss': 0.9534, 'grad_norm': 0.15253610908985138, 'learning_rate': 2.9167996921518848e-05, 'epoch': 0.64}
{'loss': 0.9735, 'grad_norm': 0.15127556025981903, 'learning_rate': 2.8496739886173995e-05, 'epoch': 0.64}
{'loss': 0.9438, 'grad_norm': 0.1507689207792282, 'learning_rate': 2.7830204833450575e-05, 'epoch': 0.65}
{'loss': 0.9996, 'grad_norm': 0.16570667922496796, 'learning_rate': 2.716853813031435e-05, 'epoch': 0.65}
               

 64%|██████▎   | 136/214 [3:17:59<27:49, 21.41s/it]
 64%|██████▍   | 137/214 [3:18:17<26:03, 20.30s/it]
                                                   

 64%|██████▍   | 137/214 [3:18:17<26:03, 20.30s/it]
 64%|██████▍   | 138/214 [3:18:35<24:44, 19.53s/it]
                                                   

 64%|██████▍   | 138/214 [3:18:35<24:44, 19.53s/it]
 65%|██████▍   | 139/214 [3:18:55<24:32, 19.64s/it]
                                                   

 65%|██████▍   | 139/214 [3:18:55<24:32, 19.64s/it]
 65%|██████▌   | 140/214 [3:19:12<23:31, 19.08s/it]
                                                   

 65%|██████▌   | 140/214 [3:19:12<23:31, 19.08s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 16
{'eval_loss': 0.9760768413543701, 'eval_runtime': 4.0394, 'eval_samples_per_second': 12.378, 'eval_steps_per_second': 0.248, 'epoch': 0.65}

                                                   

 65%|██████▌   | 140/214 [3:19:16<23:31, 19.08s/it]Saving model checkpoint to /projects/bbvz/choprahetarth/new_experiments/experiment_2/checkpoint-140
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/choprahetarth/new_experiments/experiment_2/checkpoint-140
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/choprahetarth/new_experiments/experiment_2/checkpoint-140
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/choprahetarth/new_experiments/experiment_2/checkpoint-140
Repo card metadata block was not found. Setting CardData to empty.
loading configuration file config.json from cache at /projects/bbvz/choprahetarth/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.40.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

the pytorch model path is /projects/bbvz/choprahetarth/new_experiments/experiment_2/checkpoint-140/pytorch_model.bin
the pytorch model path is /projects/bbvz/choprahetarth/new_experiments/experiment_2/checkpoint-140/pytorch_model.bin
the pytorch model path is /projects/bbvz/choprahetarth/new_experiments/experiment_2/checkpoint-140/pytorch_model.bin
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/choprahetarth/new_experiments/experiment_2/checkpoint-140
loading configuration file config.json from cache at /projects/bbvz/choprahetarth/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.40.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

the pytorch model path is /projects/bbvz/choprahetarth/new_experiments/experiment_2/checkpoint-140/pytorch_model.bin
/u/choprahetarth/.conda/envs/hetarth_py10/lib/python3.10/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/choprahetarth/.conda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/choprahetarth/.conda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.9763, 'grad_norm': 0.15744861960411072, 'learning_rate': 2.651188507467161e-05, 'epoch': 0.66}
{'loss': 1.2443, 'grad_norm': 0.16390904784202576, 'learning_rate': 2.5860389863462765e-05, 'epoch': 0.66}
{'loss': 0.9691, 'grad_norm': 0.14487306773662567, 'learning_rate': 2.5214195560997544e-05, 'epoch': 0.67}
{'loss': 1.0025, 'grad_norm': 0.15036721527576447, 'learning_rate': 2.4573444067538986e-05, 'epoch': 0.67}
{'loss': 0.971, 'grad_norm': 0.14995980262756348, 'learning_rate': 2.3938276088143003e-05, 'epoch': 0.68}

 66%|██████▌   | 141/214 [3:22:04<1:18:53, 64.85s/it]
                                                     

 66%|██████▌   | 141/214 [3:22:04<1:18:53, 64.85s/it]
 66%|██████▋   | 142/214 [3:22:22<1:00:50, 50.71s/it]
                                                     

 66%|██████▋   | 142/214 [3:22:22<1:00:50, 50.71s/it]
 67%|██████▋   | 143/214 [3:22:39<48:17, 40.81s/it]  
                                                   

 67%|██████▋   | 143/214 [3:22:39<48:17, 40.81s/it]
 67%|██████▋   | 144/214 [3:22:57<39:31, 33.89s/it]
                                                   

 67%|██████▋   | 144/214 [3:22:57<39:31, 33.89s/it]
 68%|██████▊   | 145/214 [3:23:15<33:23, 29.04s/it]
                                                   

 68%|██████▊   | 145/214 [3:23:15<33:23, 29.04s/it]
 68%|██████▊   | 146/214 [3:23:33<29:03, 25.64s/it]
                  {'loss': 0.9314, 'grad_norm': 0.1580999791622162, 'learning_rate': 2.3308831101760486e-05, 'epoch': 0.68}
{'loss': 0.956, 'grad_norm': 0.14861363172531128, 'learning_rate': 2.2685247330608417e-05, 'epoch': 0.69}
{'loss': 0.9747, 'grad_norm': 0.15075959265232086, 'learning_rate': 2.2067661709817383e-05, 'epoch': 0.69}
{'loss': 0.9781, 'grad_norm': 0.15576927363872528, 'learning_rate': 2.1456209857361248e-05, 'epoch': 0.7}
{'loss': 0.9764, 'grad_norm': 0.15475288033485413, 'learning_rate': 2.0851026044276406e-05, 'epoch': 0.7}
                                 

 68%|██████▊   | 146/214 [3:23:33<29:03, 25.64s/it]
 69%|██████▊   | 147/214 [3:23:52<26:36, 23.83s/it]
                                                   

 69%|██████▊   | 147/214 [3:23:52<26:36, 23.83s/it]
 69%|██████▉   | 148/214 [3:24:10<24:11, 22.00s/it]
                                                   

 69%|██████▉   | 148/214 [3:24:10<24:11, 22.00s/it]
 70%|██████▉   | 149/214 [3:24:28<22:26, 20.72s/it]
                                                   

 70%|██████▉   | 149/214 [3:24:28<22:26, 20.72s/it]
 70%|███████   | 150/214 [3:24:45<21:08, 19.82s/it]
                                                   

 70%|███████   | 150/214 [3:24:45<21:08, 19.82s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 16
{'eval_loss': 0.9722837805747986, 'eval_runtime': 4.0362, 'eval_samples_per_second': 12.388, 'eval_steps_per_second': 0.248, 'epoch': 0.7}

                                                   

 70%|███████   | 150/214 [3:24:49<21:08, 19.82s/it]Saving model checkpoint to /projects/bbvz/choprahetarth/new_experiments/experiment_2/checkpoint-150
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/choprahetarth/new_experiments/experiment_2/checkpoint-150
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/choprahetarth/new_experiments/experiment_2/checkpoint-150
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/choprahetarth/new_experiments/experiment_2/checkpoint-150
Repo card metadata block was not found. Setting CardData to empty.
loading configuration file config.json from cache at /projects/bbvz/choprahetarth/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.40.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

the pytorch model path is /projects/bbvz/choprahetarth/new_experiments/experiment_2/checkpoint-150/pytorch_model.bin
the pytorch model path is /projects/bbvz/choprahetarth/new_experiments/experiment_2/checkpoint-150/pytorch_model.bin
the pytorch model path is /projects/bbvz/choprahetarth/new_experiments/experiment_2/checkpoint-150/pytorch_model.bin
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/choprahetarth/new_experiments/experiment_2/checkpoint-150
loading configuration file config.json from cache at /projects/bbvz/choprahetarth/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.40.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

the pytorch model path is /projects/bbvz/choprahetarth/new_experiments/experiment_2/checkpoint-150/pytorch_model.bin
/u/choprahetarth/.conda/envs/hetarth_py10/lib/python3.10/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/choprahetarth/.conda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/choprahetarth/.conda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.9542, 'grad_norm': 0.15402913093566895, 'learning_rate': 2.0252243165176632e-05, 'epoch': 0.71}
{'loss': 0.9645, 'grad_norm': 0.1478397697210312, 'learning_rate': 1.9659992709070345e-05, 'epoch': 0.71}
{'loss': 0.9469, 'grad_norm': 0.14708036184310913, 'learning_rate': 1.907440473048626e-05, 'epoch': 0.71}
{'loss': 0.9924, 'grad_norm': 0.15508189797401428, 'learning_rate': 1.849560782091445e-05, 'epoch': 0.72}
{'loss': 0.9659, 'grad_norm': 0.15471215546131134, 'learning_rate': 1.792372908056824e-05, 'epoch': 0.72}

 71%|███████   | 151/214 [3:26:30<47:22, 45.11s/it]
                                                   

 71%|███████   | 151/214 [3:26:30<47:22, 45.11s/it]
 71%|███████   | 152/214 [3:26:47<38:07, 36.89s/it]
                                                   

 71%|███████   | 152/214 [3:26:47<38:07, 36.89s/it]
 71%|███████▏  | 153/214 [3:27:05<31:39, 31.14s/it]
                                                   

 71%|███████▏  | 153/214 [3:27:05<31:39, 31.14s/it]
 72%|███████▏  | 154/214 [3:27:23<27:06, 27.11s/it]
                                                   

 72%|███████▏  | 154/214 [3:27:23<27:06, 27.11s/it]
 72%|███████▏  | 155/214 [3:27:40<23:53, 24.29s/it]
                                                   

 72%|███████▏  | 155/214 [3:27:40<23:53, 24.29s/it]
 73%|███████▎  | 156/214 [3:28:00<22:01, 22.79s/it]
                  {'loss': 1.2404, 'grad_norm': 0.16715644299983978, 'learning_rate': 1.7358894090473925e-05, 'epoch': 0.73}
{'loss': 0.963, 'grad_norm': 0.14712338149547577, 'learning_rate': 1.6801226884893896e-05, 'epoch': 0.73}
{'loss': 1.0008, 'grad_norm': 0.15467023849487305, 'learning_rate': 1.6250849924089484e-05, 'epoch': 0.74}
{'loss': 0.9538, 'grad_norm': 0.1489088237285614, 'learning_rate': 1.5707884067429473e-05, 'epoch': 0.74}
{'loss': 0.9289, 'grad_norm': 0.15166635811328888, 'learning_rate': 1.5172448546850165e-05, 'epoch': 0.75}
                                 

 73%|███████▎  | 156/214 [3:28:00<22:01, 22.79s/it]
 73%|███████▎  | 157/214 [3:28:17<20:12, 21.27s/it]
                                                   

 73%|███████▎  | 157/214 [3:28:17<20:12, 21.27s/it]
 74%|███████▍  | 158/214 [3:28:35<18:51, 20.20s/it]
                                                   

 74%|███████▍  | 158/214 [3:28:35<18:51, 20.20s/it]
 74%|███████▍  | 159/214 [3:28:53<17:50, 19.46s/it]
                                                   

 74%|███████▍  | 159/214 [3:28:53<17:50, 19.46s/it]
 75%|███████▍  | 160/214 [3:29:11<17:02, 18.94s/it]
                                                   

 75%|███████▍  | 160/214 [3:29:11<17:02, 18.94s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 16
{'eval_loss': 0.970098078250885, 'eval_runtime': 4.0427, 'eval_samples_per_second': 12.368, 'eval_steps_per_second': 0.247, 'epoch': 0.75}

                                                   

 75%|███████▍  | 160/214 [3:29:15<17:02, 18.94s/it]Saving model checkpoint to /projects/bbvz/choprahetarth/new_experiments/experiment_2/checkpoint-160
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/choprahetarth/new_experiments/experiment_2/checkpoint-160
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/choprahetarth/new_experiments/experiment_2/checkpoint-160
Repo card metadata block was not found. Setting CardData to empty.
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/choprahetarth/new_experiments/experiment_2/checkpoint-160
Repo card metadata block was not found. Setting CardData to empty.
Repo card metadata block was not found. Setting CardData to empty.
loading configuration file config.json from cache at /projects/bbvz/choprahetarth/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.40.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/choprahetarth/new_experiments/experiment_2/checkpoint-160
loading configuration file config.json from cache at /projects/bbvz/choprahetarth/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.40.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

the pytorch model path is /projects/bbvz/choprahetarth/new_experiments/experiment_2/checkpoint-160/pytorch_model.bin
the pytorch model path is /projects/bbvz/choprahetarth/new_experiments/experiment_2/checkpoint-160/pytorch_model.bin
the pytorch model path is /projects/bbvz/choprahetarth/new_experiments/experiment_2/checkpoint-160/pytorch_model.bin
the pytorch model path is /projects/bbvz/choprahetarth/new_experiments/experiment_2/checkpoint-160/pytorch_model.bin
/u/choprahetarth/.conda/envs/hetarth_py10/lib/python3.10/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/choprahetarth/.conda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/choprahetarth/.conda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.9528, 'grad_norm': 0.14566005766391754, 'learning_rate': 1.4644660940672627e-05, 'epoch': 0.75}
{'loss': 0.9784, 'grad_norm': 0.15262135863304138, 'learning_rate': 1.4124637147783432e-05, 'epoch': 0.76}
{'loss': 0.9661, 'grad_norm': 0.1510566622018814, 'learning_rate': 1.3612491362183888e-05, 'epoch': 0.76}
{'loss': 0.9708, 'grad_norm': 0.16122789680957794, 'learning_rate': 1.3108336047913633e-05, 'epoch': 0.77}
{'loss': 0.9359, 'grad_norm': 0.16722121834754944, 'learning_rate': 1.2612281914354451e-05, 'epoch': 0.77}

 75%|███████▌  | 161/214 [3:31:50<53:57, 61.08s/it]
                                                   

 75%|███████▌  | 161/214 [3:31:50<53:57, 61.08s/it]
 76%|███████▌  | 162/214 [3:32:08<41:39, 48.07s/it]
                                                   

 76%|███████▌  | 162/214 [3:32:08<41:39, 48.07s/it]
 76%|███████▌  | 163/214 [3:32:25<33:07, 38.97s/it]
                                                   

 76%|███████▌  | 163/214 [3:32:25<33:07, 38.97s/it]
 77%|███████▋  | 164/214 [3:32:45<27:39, 33.19s/it]
                                                   

 77%|███████▋  | 164/214 [3:32:45<27:39, 33.19s/it]
 77%|███████▋  | 165/214 [3:33:03<23:18, 28.55s/it]
                                                   

 77%|███████▋  | 165/214 [3:33:03<23:18, 28.55s/it]
 78%|███████▊  | 166/214 [3:33:21<20:14, 25.30s/it]
          {'loss': 0.9687, 'grad_norm': 0.16555459797382355, 'learning_rate': 1.2124437891918993e-05, 'epoch': 0.78}
{'loss': 0.9415, 'grad_norm': 0.15206311643123627, 'learning_rate': 1.1644911108130435e-05, 'epoch': 0.78}
{'loss': 0.9877, 'grad_norm': 0.1520889401435852, 'learning_rate': 1.1173806864097886e-05, 'epoch': 0.79}
{'loss': 0.9652, 'grad_norm': 0.1505090743303299, 'learning_rate': 1.0711228611392937e-05, 'epoch': 0.79}
{'loss': 1.2323, 'grad_norm': 0.14440159499645233, 'learning_rate': 1.0257277929332332e-05, 'epoch': 0.79}
                                         

 78%|███████▊  | 166/214 [3:33:21<20:14, 25.30s/it]
 78%|███████▊  | 167/214 [3:33:38<18:02, 23.03s/it]
                                                   

 78%|███████▊  | 167/214 [3:33:38<18:02, 23.03s/it]
 79%|███████▊  | 168/214 [3:33:56<16:25, 21.43s/it]
                                                   

 79%|███████▊  | 168/214 [3:33:56<16:25, 21.43s/it]
 79%|███████▉  | 169/214 [3:34:14<15:14, 20.32s/it]
                                                   

 79%|███████▉  | 169/214 [3:34:14<15:14, 20.32s/it]
 79%|███████▉  | 170/214 [3:34:31<14:19, 19.54s/it]
                                                   

 79%|███████▉  | 170/214 [3:34:31<14:19, 19.54s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 16
{'eval_loss': 0.9686208963394165, 'eval_runtime': 4.033, 'eval_samples_per_second': 12.398, 'eval_steps_per_second': 0.248, 'epoch': 0.79}

                                                   

 79%|███████▉  | 170/214 [3:34:36<14:19, 19.54s/it]Saving model checkpoint to /projects/bbvz/choprahetarth/new_experiments/experiment_2/checkpoint-170
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/choprahetarth/new_experiments/experiment_2/checkpoint-170
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/choprahetarth/new_experiments/experiment_2/checkpoint-170
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/choprahetarth/new_experiments/experiment_2/checkpoint-170
Repo card metadata block was not found. Setting CardData to empty.
Repo card metadata block was not found. Setting CardData to empty.
Repo card metadata block was not found. Setting CardData to empty.
loading configuration file config.json from cache at /projects/bbvz/choprahetarth/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.40.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

the pytorch model path is /projects/bbvz/choprahetarth/new_experiments/experiment_2/checkpoint-170/pytorch_model.bin
the pytorch model path is /projects/bbvz/choprahetarth/new_experiments/experiment_2/checkpoint-170/pytorch_model.bin
the pytorch model path is /projects/bbvz/choprahetarth/new_experiments/experiment_2/checkpoint-170/pytorch_model.bin
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/choprahetarth/new_experiments/experiment_2/checkpoint-170
loading configuration file config.json from cache at /projects/bbvz/choprahetarth/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.40.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

the pytorch model path is /projects/bbvz/choprahetarth/new_experiments/experiment_2/checkpoint-170/pytorch_model.bin
/u/choprahetarth/.conda/envs/hetarth_py10/lib/python3.10/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/choprahetarth/.conda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/choprahetarth/.conda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.9592, 'grad_norm': 0.2923051714897156, 'learning_rate': 9.812054502671836e-06, 'epoch': 0.8}
{'loss': 0.9951, 'grad_norm': 0.15078304708003998, 'learning_rate': 9.375656099715934e-06, 'epoch': 0.8}
{'loss': 0.942, 'grad_norm': 0.15295399725437164, 'learning_rate': 8.948178550848702e-06, 'epoch': 0.81}
{'loss': 0.9422, 'grad_norm': 0.14678789675235748, 'learning_rate': 8.529715727489912e-06, 'epoch': 0.81}
{'loss': 0.9637, 'grad_norm': 0.15098391473293304, 'learning_rate': 8.120359521481501e-06, 'epoch': 0.82}

 80%|███████▉  | 171/214 [3:51:08<3:43:59, 312.56s/it]
                                                      

 80%|███████▉  | 171/214 [3:51:08<3:43:59, 312.56s/it]
 80%|████████  | 172/214 [3:51:29<2:37:35, 225.12s/it]
                                                      

 80%|████████  | 172/214 [3:51:29<2:37:35, 225.12s/it]
 81%|████████  | 173/214 [3:51:47<1:51:18, 162.90s/it]
                                                      

 81%|████████  | 173/214 [3:51:47<1:51:18, 162.90s/it]
 81%|████████▏ | 174/214 [3:52:04<1:19:33, 119.35s/it]
                                                      

 81%|████████▏ | 174/214 [3:52:04<1:19:33, 119.35s/it]
 82%|████████▏ | 175/214 [3:52:22<57:45, 88.86s/it]   
                                                   

 82%|████████▏ | 175/214 [3:52:22<57:45, 88.86s/it]
 82%|████████�{'loss': 0.9689, 'grad_norm': 0.1546877920627594, 'learning_rate': 7.720199824908692e-06, 'epoch': 0.82}
{'loss': 0.9677, 'grad_norm': 0.1482013314962387, 'learning_rate': 7.329324510360269e-06, 'epoch': 0.83}
{'loss': 0.9725, 'grad_norm': 0.1444137990474701, 'learning_rate': 6.947819411632223e-06, 'epoch': 0.83}
{'loss': 0.9413, 'grad_norm': 0.15525376796722412, 'learning_rate': 6.575768304879293e-06, 'epoch': 0.84}
{'loss': 0.9572, 'grad_norm': 0.15349820256233215, 'learning_rate': 6.213252890218163e-06, 'epoch': 0.84}
� | 176/214 [3:52:41<42:55, 67.77s/it]
                                                   

 82%|████████▏ | 176/214 [3:52:41<42:55, 67.77s/it]
 83%|████████▎ | 177/214 [3:52:58<32:32, 52.76s/it]
                                                   

 83%|████████▎ | 177/214 [3:52:58<32:32, 52.76s/it]
 83%|████████▎ | 178/214 [3:53:16<25:21, 42.25s/it]
                                                   

 83%|████████▎ | 178/214 [3:53:16<25:21, 42.25s/it]
 84%|████████▎ | 179/214 [3:53:34<20:21, 34.89s/it]
                                                   

 84%|████████▎ | 179/214 [3:53:34<20:21, 34.89s/it]
 84%|████████▍ | 180/214 [3:53:54<17:17, 30.51s/it]
                                                   

 84%|████████▍ | 180/214 [3:53:54<17:17, 30.51s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 16
{'eval_loss': 0.967710554599762, 'eval_runtime': 4.3225, 'eval_samples_per_second': 11.567, 'eval_steps_per_second': 0.231, 'epoch': 0.84}

                                                   

 84%|████████▍ | 180/214 [3:53:58<17:17, 30.51s/it]Saving model checkpoint to /projects/bbvz/choprahetarth/new_experiments/experiment_2/checkpoint-180
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/choprahetarth/new_experiments/experiment_2/checkpoint-180
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/choprahetarth/new_experiments/experiment_2/checkpoint-180
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/choprahetarth/new_experiments/experiment_2/checkpoint-180
loading configuration file config.json from cache at /projects/bbvz/choprahetarth/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.40.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/choprahetarth/new_experiments/experiment_2/checkpoint-180
loading configuration file config.json from cache at /projects/bbvz/choprahetarth/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.40.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

the pytorch model path is /projects/bbvz/choprahetarth/new_experiments/experiment_2/checkpoint-180/pytorch_model.bin
the pytorch model path is /projects/bbvz/choprahetarth/new_experiments/experiment_2/checkpoint-180/pytorch_model.bin
the pytorch model path is /projects/bbvz/choprahetarth/new_experiments/experiment_2/checkpoint-180/pytorch_model.bin
the pytorch model path is /projects/bbvz/choprahetarth/new_experiments/experiment_2/checkpoint-180/pytorch_model.bin
/u/choprahetarth/.conda/envs/hetarth_py10/lib/python3.10/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/choprahetarth/.conda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/choprahetarth/.conda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.9738, 'grad_norm': 0.796787440776825, 'learning_rate': 5.860352773786631e-06, 'epoch': 0.85}
{'loss': 0.9782, 'grad_norm': 0.14889249205589294, 'learning_rate': 5.51714545026264e-06, 'epoch': 0.85}
{'loss': 0.9623, 'grad_norm': 0.14635467529296875, 'learning_rate': 5.1837062858468736e-06, 'epoch': 0.86}
{'loss': 1.2235, 'grad_norm': 0.14532171189785004, 'learning_rate': 4.860108501712824e-06, 'epoch': 0.86}
{'loss': 0.9651, 'grad_norm': 0.14649860560894012, 'learning_rate': 4.546423157927921e-06, 'epoch': 0.86}

 85%|████████▍ | 181/214 [4:00:09<1:13:34, 133.77s/it]
                                                      

 85%|████████▍ | 181/214 [4:00:09<1:13:34, 133.77s/it]
 85%|████████▌ | 182/214 [4:00:26<52:46, 98.95s/it]   
                                                   

 85%|████████▌ | 182/214 [4:00:26<52:46, 98.95s/it]
 86%|████████▌ | 183/214 [4:00:44<38:32, 74.58s/it]
                                                   

 86%|████████▌ | 183/214 [4:00:44<38:32, 74.58s/it]
 86%|████████▌ | 184/214 [4:01:02<28:45, 57.52s/it]
                                                   

 86%|████████▌ | 184/214 [4:01:02<28:45, 57.52s/it]
 86%|████████▋ | 185/214 [4:01:20<22:01, 45.58s/it]
                                                   

 86%|████████▋ | 185/214 [4:01:20<22:01, 45.58s/it]
 87%|████████▋ | 186/214 [4:{'loss': 0.9926, 'grad_norm': 0.14978177845478058, 'learning_rate': 4.242719137849077e-06, 'epoch': 0.87}
{'loss': 0.9324, 'grad_norm': 0.14525051414966583, 'learning_rate': 3.949063132996455e-06, 'epoch': 0.87}
{'loss': 0.9461, 'grad_norm': 0.15093907713890076, 'learning_rate': 3.6655196284083317e-06, 'epoch': 0.88}
{'loss': 0.9504, 'grad_norm': 0.14666195213794708, 'learning_rate': 3.3921508884806108e-06, 'epoch': 0.88}
{'loss': 0.9668, 'grad_norm': 0.15601655840873718, 'learning_rate': 3.1290169432939553e-06, 'epoch': 0.89}
01:37<17:22, 37.22s/it]
                                                   

 87%|████████▋ | 186/214 [4:01:37<17:22, 37.22s/it]
 87%|████████▋ | 187/214 [4:01:55<14:06, 31.37s/it]
                                                   

 87%|████████▋ | 187/214 [4:01:55<14:06, 31.37s/it]
 88%|████████▊ | 188/214 [4:02:15<12:03, 27.83s/it]
                                                   

 88%|████████▊ | 188/214 [4:02:15<12:03, 27.83s/it]
 88%|████████▊ | 189/214 [4:02:32<10:20, 24.80s/it]
                                                   

 88%|████████▊ | 189/214 [4:02:32<10:20, 24.80s/it]
 89%|████████▉ | 190/214 [4:02:50<09:04, 22.68s/it]
                                                   

 89%|████████▉ | 190/214 [4:02:50<09:04, 22.68s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 16
{'eval_loss': 0.9670031666755676, 'eval_runtime': 4.0419, 'eval_samples_per_second': 12.371, 'eval_steps_per_second': 0.247, 'epoch': 0.89}

                                                   

 89%|████████▉ | 190/214 [4:02:54<09:04, 22.68s/it]Saving model checkpoint to /projects/bbvz/choprahetarth/new_experiments/experiment_2/checkpoint-190
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/choprahetarth/new_experiments/experiment_2/checkpoint-190
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/choprahetarth/new_experiments/experiment_2/checkpoint-190
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/choprahetarth/new_experiments/experiment_2/checkpoint-190
Repo card metadata block was not found. Setting CardData to empty.
Repo card metadata block was not found. Setting CardData to empty.
loading configuration file config.json from cache at /projects/bbvz/choprahetarth/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.40.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

the pytorch model path is /projects/bbvz/choprahetarth/new_experiments/experiment_2/checkpoint-190/pytorch_model.bin
the pytorch model path is /projects/bbvz/choprahetarth/new_experiments/experiment_2/checkpoint-190/pytorch_model.bin
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/choprahetarth/new_experiments/experiment_2/checkpoint-190
loading configuration file config.json from cache at /projects/bbvz/choprahetarth/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.40.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

the pytorch model path is /projects/bbvz/choprahetarth/new_experiments/experiment_2/checkpoint-190/pytorch_model.bin
the pytorch model path is /projects/bbvz/choprahetarth/new_experiments/experiment_2/checkpoint-190/pytorch_model.bin
/u/choprahetarth/.conda/envs/hetarth_py10/lib/python3.10/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/choprahetarth/.conda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/choprahetarth/.conda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.9661, 'grad_norm': 0.1450536996126175, 'learning_rate': 2.8761755754315668e-06, 'epoch': 0.89}
{'loss': 0.9588, 'grad_norm': 0.14394181966781616, 'learning_rate': 2.6336823072904304e-06, 'epoch': 0.9}
{'loss': 0.9435, 'grad_norm': 0.14982306957244873, 'learning_rate': 2.4015903888890245e-06, 'epoch': 0.9}
{'loss': 0.9687, 'grad_norm': 0.14681801199913025, 'learning_rate': 2.179950786173879e-06, 'epoch': 0.91}
{'loss': 0.9387, 'grad_norm': 0.14960774779319763, 'learning_rate': 1.9688121698277995e-06, 'epoch': 0.91}

 89%|████████▉ | 191/214 [4:04:54<20:19, 53.03s/it]
                                                   

 89%|████████▉ | 191/214 [4:04:54<20:19, 53.03s/it]
 90%|████████▉ | 192/214 [4:05:12<15:33, 42.44s/it]
                                                   

 90%|████████▉ | 192/214 [4:05:12<15:33, 42.44s/it]
 90%|█████████ | 193/214 [4:05:29<12:15, 35.02s/it]
                                                   

 90%|█████████ | 193/214 [4:05:29<12:15, 35.02s/it]
 91%|█████████ | 194/214 [4:05:47<09:56, 29.83s/it]
                                                   

 91%|█████████ | 194/214 [4:05:47<09:56, 29.83s/it]
 91%|█████████ | 195/214 [4:06:05<08:17, 26.20s/it]
                                                   

 91%|█████████ | 195/214 [4:06:05<08:17, 26.20s/it]
 92%|█████████▏| 196/214 [4:06:24<07:1{'loss': 0.9661, 'grad_norm': 0.14830948412418365, 'learning_rate': 1.7682209045820686e-06, 'epoch': 0.92}
{'loss': 0.9637, 'grad_norm': 0.1466965675354004, 'learning_rate': 1.5782210390350716e-06, 'epoch': 0.92}
{'loss': 1.2254, 'grad_norm': 0.15265795588493347, 'learning_rate': 1.3988542959794627e-06, 'epoch': 0.93}
{'loss': 0.9571, 'grad_norm': 0.14535048604011536, 'learning_rate': 1.230160063240121e-06, 'epoch': 0.93}
{'loss': 0.9961, 'grad_norm': 0.14980144798755646, 'learning_rate': 1.0721753850247984e-06, 'epoch': 0.93}
5, 24.19s/it]
                                                   

 92%|█████████▏| 196/214 [4:06:24<07:15, 24.19s/it]
 92%|█████████▏| 197/214 [4:06:42<06:18, 22.25s/it]
                                                   

 92%|█████████▏| 197/214 [4:06:42<06:18, 22.25s/it]
 93%|█████████▎| 198/214 [4:07:00<05:34, 20.89s/it]
                                                   

 93%|█████████▎| 198/214 [4:07:00<05:34, 20.89s/it]
 93%|█████████▎| 199/214 [4:07:17<04:59, 19.94s/it]
                                                   

 93%|█████████▎| 199/214 [4:07:17<04:59, 19.94s/it]
 93%|█████████▎| 200/214 [4:07:35<04:29, 19.27s/it]
                                                   

 93%|█████████▎| 200/214 [4:07:35<04:29, 19.27s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 16
{'eval_loss': 0.9666509032249451, 'eval_runtime': 4.0416, 'eval_samples_per_second': 12.371, 'eval_steps_per_second': 0.247, 'epoch': 0.93}

                                                   

 93%|█████████▎| 200/214 [4:07:39<04:29, 19.27s/it]Saving model checkpoint to /projects/bbvz/choprahetarth/new_experiments/experiment_2/checkpoint-200
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/choprahetarth/new_experiments/experiment_2/checkpoint-200
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/choprahetarth/new_experiments/experiment_2/checkpoint-200
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/choprahetarth/new_experiments/experiment_2/checkpoint-200
loading configuration file config.json from cache at /projects/bbvz/choprahetarth/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.40.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

the pytorch model path is /projects/bbvz/choprahetarth/new_experiments/experiment_2/checkpoint-200/pytorch_model.bin
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/choprahetarth/new_experiments/experiment_2/checkpoint-200
loading configuration file config.json from cache at /projects/bbvz/choprahetarth/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.40.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

the pytorch model path is /projects/bbvz/choprahetarth/new_experiments/experiment_2/checkpoint-200/pytorch_model.bin
the pytorch model path is /projects/bbvz/choprahetarth/new_experiments/experiment_2/checkpoint-200/pytorch_model.bin
the pytorch model path is /projects/bbvz/choprahetarth/new_experiments/experiment_2/checkpoint-200/pytorch_model.bin
/u/choprahetarth/.conda/envs/hetarth_py10/lib/python3.10/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/choprahetarth/.conda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/choprahetarth/.conda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.9275, 'grad_norm': 0.14438915252685547, 'learning_rate': 9.249349537894969e-07, 'epoch': 0.94}
{'loss': 0.9528, 'grad_norm': 0.14817307889461517, 'learning_rate': 7.884711026201585e-07, 'epoch': 0.94}
{'loss': 0.9355, 'grad_norm': 0.14400894939899445, 'learning_rate': 6.628137981325611e-07, 'epoch': 0.95}
{'loss': 0.9675, 'grad_norm': 0.15179896354675293, 'learning_rate': 5.479906338917984e-07, 'epoch': 0.95}
{'loss': 0.9714, 'grad_norm': 0.15093949437141418, 'learning_rate': 4.4402682435296663e-07, 'epoch': 0.96}

 94%|█████████▍| 201/214 [4:09:45<11:21, 52.43s/it]
                                                   

 94%|█████████▍| 201/214 [4:09:45<11:21, 52.43s/it]
 94%|█████████▍| 202/214 [4:10:03<08:24, 42.01s/it]
                                                   

 94%|█████████▍| 202/214 [4:10:03<08:24, 42.01s/it]
 95%|█████████▍| 203/214 [4:10:20<06:21, 34.72s/it]
                                                   

 95%|█████████▍| 203/214 [4:10:20<06:21, 34.72s/it]
 95%|█████████▌| 204/214 [4:10:38<04:56, 29.62s/it]
                                                   

 95%|█████████▌| 204/214 [4:10:38<04:56, 29.62s/it]
 96%|█████████▌| 205/214 [4:10:58<03:59, 26.56s/it]
                                                   

 96%|█████████▌| 205/214 [4:10:58<03:59, 26.56s/it]
 96%|█████████▋| 2{'loss': 0.9675, 'grad_norm': 0.1484813541173935, 'learning_rate': 3.5094519932415417e-07, 'epoch': 0.96}
{'loss': 0.9434, 'grad_norm': 0.14970503747463226, 'learning_rate': 2.687661989531964e-07, 'epoch': 0.97}
{'loss': 0.9623, 'grad_norm': 0.15130604803562164, 'learning_rate': 1.975078692391552e-07, 'epoch': 0.97}
{'loss': 0.9379, 'grad_norm': 0.14832593500614166, 'learning_rate': 1.3718585806949403e-07, 'epoch': 0.98}
{'loss': 0.9662, 'grad_norm': 0.14619120955467224, 'learning_rate': 8.781341178393244e-08, 'epoch': 0.98}
06/214 [4:11:15<03:11, 23.91s/it]
                                                   

 96%|█████████▋| 206/214 [4:11:15<03:11, 23.91s/it]
 97%|█████████▋| 207/214 [4:11:33<02:34, 22.05s/it]
                                                   

 97%|█████████▋| 207/214 [4:11:33<02:34, 22.05s/it]
 97%|█████████▋| 208/214 [4:11:51<02:04, 20.75s/it]
                                                   

 97%|█████████▋| 208/214 [4:11:51<02:04, 20.75s/it]
 98%|█████████▊| 209/214 [4:12:08<01:39, 19.84s/it]
                                                   

 98%|█████████▊| 209/214 [4:12:08<01:39, 19.84s/it]
 98%|█████████▊| 210/214 [4:12:26<01:16, 19.20s/it]
                                                   

 98%|█████████▊| 210/214 [4:12:26<01:16, 19.20s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 16
{'eval_loss': 0.9667007327079773, 'eval_runtime': 4.0396, 'eval_samples_per_second': 12.378, 'eval_steps_per_second': 0.248, 'epoch': 0.98}

                                                   

 98%|█████████▊| 210/214 [4:12:30<01:16, 19.20s/it]Saving model checkpoint to /projects/bbvz/choprahetarth/new_experiments/experiment_2/checkpoint-210
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/choprahetarth/new_experiments/experiment_2/checkpoint-210
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/choprahetarth/new_experiments/experiment_2/checkpoint-210
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/choprahetarth/new_experiments/experiment_2/checkpoint-210
loading configuration file config.json from cache at /projects/bbvz/choprahetarth/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.40.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

the pytorch model path is /projects/bbvz/choprahetarth/new_experiments/experiment_2/checkpoint-210/pytorch_model.bin
the pytorch model path is /projects/bbvz/choprahetarth/new_experiments/experiment_2/checkpoint-210/pytorch_model.bin
the pytorch model path is /projects/bbvz/choprahetarth/new_experiments/experiment_2/checkpoint-210/pytorch_model.bin
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/choprahetarth/new_experiments/experiment_2/checkpoint-210
loading configuration file config.json from cache at /projects/bbvz/choprahetarth/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.40.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

the pytorch model path is /projects/bbvz/choprahetarth/new_experiments/experiment_2/checkpoint-210/pytorch_model.bin
/u/choprahetarth/.conda/envs/hetarth_py10/lib/python3.10/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/choprahetarth/.conda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/choprahetarth/.conda/envs/hetarth_py10/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.9644, 'grad_norm': 0.14525416493415833, 'learning_rate': 4.940137226560615e-08, 'epoch': 0.99}
{'loss': 1.2301, 'grad_norm': 0.15005721151828766, 'learning_rate': 2.1958174560282595e-08, 'epoch': 0.99}
{'loss': 0.9629, 'grad_norm': 0.15128999948501587, 'learning_rate': 5.489845024053697e-09, 'epoch': 1.0}
{'loss': 0.9964, 'grad_norm': 0.14981479942798615, 'learning_rate': 0.0, 'epoch': 1.0}

 99%|█████████▊| 211/214 [4:20:07<07:34, 151.65s/it]
                                                    

 99%|█████████▊| 211/214 [4:20:07<07:34, 151.65s/it]
 99%|█████████▉| 212/214 [4:20:25<03:42, 111.48s/it]
                                                    

 99%|█████████▉| 212/214 [4:20:25<03:42, 111.48s/it]
100%|█████████▉| 213/214 [4:20:45<01:24, 84.03s/it] 
                                                   

100%|█████████▉| 213/214 [4:20:45<01:24, 84.03s/it]
100%|██████████| 214/214 [4:21:02<00:00, 64.13s/it]
                                                   

100%|██████████| 214/214 [4:21:02<00:00, 64.13s/it]

Training completed. Do not forget to share your model on huggingface.co/models =)


{'train_runtime': 15676.2152, 'train_samples_per_second': 1.747, 'train_steps_per_second': 0.014, 'train_loss': 1.0842225972180055, 'epoch': 1.0}
Saving last checkpoint of the model
Saving last checkpoint of the modelSaving last checkpoint of the model


                                                   

100%|██████████| 214/214 [4:21:02<00:00, 64.13s/it]
100%|██████████| 214/214 [4:21:02<00:00, 73.19s/it]
Saving last checkpoint of the model
loading configuration file config.json from cache at /projects/bbvz/choprahetarth/hub/models--bigcode--starcoderbase-7b/snapshots/4ab631381edb607557cbb04b6e9a225bad16807c/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "/fsx/bigcode/experiments/pretraining/conversions/starcoder-7b",
  "activation_function": "gelu",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 4096,
  "n_head": 32,
  "n_inner": 16384,
  "n_layer": 42,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.40.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

wandb: - 0.008 MB of 0.008 MB uploaded
wandb: \ 0.008 MB of 0.008 MB uploaded
wandb: | 0.008 MB of 0.008 MB uploaded
wandb: / 0.036 MB of 0.155 MB uploaded
wandb: - 0.154 MB of 0.157 MB uploaded
wandb: 
wandb: Run history:
wandb:               eval/loss █▅▄▃▃▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁
wandb:            eval/runtime ▂▁▁▁▂▁▁▂▂▁▁▁█▁▁▁▁▂▁▁▁
wandb: eval/samples_per_second ▇███▇██▇▆███▁████▆███
wandb:   eval/steps_per_second ▇███▇██▇▆███▁████▆███
wandb:             train/epoch ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇████
wandb:       train/global_step ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇████
wandb:         train/grad_norm █▅▄▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂▁▁▁▂▁▁▁▁▁▁▁▁▁
wandb:     train/learning_rate ███████▇▇▇▇▇▇▆▆▆▆▅▅▅▄▄▄▄▃▃▃▃▂▂▂▂▂▁▁▁▁▁▁▁
wandb:              train/loss █▅▄▃▃▂▂▂▃▂▂▂▂▃▁▁▂▁▁▁▁▁▁▁▁▁▃▁▁▁▁▃▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:                eval/loss 0.9667
wandb:             eval/runtime 4.0396
wandb:  eval/samples_per_second 12.378
wandb:    eval/steps_per_second 0.248
wandb:               total_flos 5.989104282480148e+17
wandb:              train/epoch 1.0
wandb:        train/global_step 214
wandb:          train/grad_norm 0.14981
wandb:      train/learning_rate 0.0
wandb:               train/loss 0.9964
wandb:               train_loss 1.08422
wandb:            train_runtime 15676.2152
wandb: train_samples_per_second 1.747
wandb:   train_steps_per_second 0.014
wandb: 
wandb: 🚀 View run FinalRuns-/projects/bbvz/choprahetarth/new_experiments/experiment_2 at: https://wandb.ai/hetarthvader/huggingface/runs/45iynwaw
wandb: ⭐️ View project at: https://wandb.ai/hetarthvader/huggingface
wandb: Synced 6 W&B file(s), 0 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: /projects/bbvz/choprahetarth/wandb/run-20240418_010311-45iynwaw/logs


srun --account=bbvz-delta-gpu \
python3 -m torch.distributed.run \
--nproc_per_node=4 \
finetune/custom_fine_tune.py \
--model_path="bigcode/starcoderbase-7b" \
--subset="data/finetune" \
--split="train" \
--streaming \
--data_path="/u/choprahetarth/all_files/data/train_ftdata-new-small.json" \
--size_valid_set 1525 \
--seq_length 512 \
--max_steps 214 \
--batch_size 16 \
--input_column_name="input" \
--output_column_name="output" \
--gradient_accumulation_steps 2 \
--learning_rate 1e-4 \
--lr_scheduler_type="cosine" \
--num_warmup_steps 2 \
--weight_decay 0.05 \
--output_dir="/projects/bbvz/choprahetarth/new_experiments/experiment_2" \
--seed 1234 \
--save_freq 10