Running "module reset". Resetting modules to system default. The following $MODULEPATH directories have been removed: None

Currently Loaded Modules:
  1) cue-login-env/1.0   8) libmd/1.0.4       15) openssl/3.1.3
  2) gcc/11.2.0          9) libbsd/0.11.7     16) util-linux-uuid/2.38.1
  3) ucx/1.11.2         10) expat/2.5.0       17) xz/5.2.4
  4) openmpi/4.1.2      11) gettext/0.19.8.1  18) python/3.11.6
  5) cuda/11.6.1        12) libffi/3.4.4      19) cudnn/8.9.0.131
  6) modtree/gpu        13) libxcrypt/4.4.35  20) anaconda3_gpu/23.9.0
  7) default            14) zlib-ng/2.1.3

 

job is starting on gpub080.delta.ncsa.illinois.edu
WARNING: A conda environment already exists at '/u/bzd2/.conda/envs/hetarth_py10'
Remove existing environment (y/[n])? 

CondaSystemExit: Exiting.

usage: conda [-h] [-v] [--no-plugins] [-V] COMMAND ...
conda: error: argument COMMAND: invalid choice: 'activate' (choose from 'clean', 'compare', 'config', 'create', 'info', 'init', 'install', 'list', 'notices', 'package', 'remove', 'uninstall', 'rename', 'run', 'search', 'update', 'upgrade', 'build', 'content-trust', 'convert', 'debug', 'develop', 'doctor', 'index', 'inspect', 'metapackage', 'render', 'repoquery', 'skeleton', 'server', 'repo', 'env', 'verify', 'token')
Starting script...
Output: Defaulting to user installation because normal site-packages is not writeable
Requirement already satisfied: torch in /sw/external/python/anaconda3/lib/python3.9/site-packages (2.2.0.dev20231003+cu121)
Requirement already satisfied: torchvision in /sw/external/python/anaconda3/lib/python3.9/site-packages (0.17.0.dev20231003+cu121)
Requirement already satisfied: torchaudio in /sw/external/python/anaconda3/lib/python3.9/site-packages (2.2.0.dev20231002)
Requirement already satisfied: filelock in /sw/external/python/anaconda3/lib/python3.9/site-packages (from torch) (3.9.0)
Requirement already satisfied: typing-extensions in /sw/external/python/anaconda3/lib/python3.9/site-packages (from torch) (4.7.1)
Requirement already satisfied: sympy in /sw/external/python/anaconda3/lib/python3.9/site-packages (from torch) (1.11.1)
Requirement already satisfied: networkx in /sw/external/python/anaconda3/lib/python3.9/site-packages (from torch) (3.1)
Requirement already satisfied: jinja2 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from torch) (3.1.2)
Requirement already satisfied: fsspec in /u/bzd2/.local/lib/python3.9/site-packages (from torch) (2023.6.0)
Requirement already satisfied: pytorch-triton==2.1.0+6e4932cda8 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from torch) (2.1.0+6e4932cda8)
Requirement already satisfied: numpy in /sw/external/python/anaconda3/lib/python3.9/site-packages (from torchvision) (1.24.3)
Requirement already satisfied: requests in /sw/external/python/anaconda3/lib/python3.9/site-packages (from torchvision) (2.31.0)
Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from torchvision) (9.4.0)
Requirement already satisfied: MarkupSafe>=2.0 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from jinja2->torch) (2.1.1)
Requirement already satisfied: charset-normalizer<4,>=2 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from requests->torchvision) (2.0.4)
Requirement already satisfied: idna<4,>=2.5 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from requests->torchvision) (3.4)
Requirement already satisfied: urllib3<3,>=1.21.1 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from requests->torchvision) (1.26.16)
Requirement already satisfied: certifi>=2017.4.17 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from requests->torchvision) (2023.7.22)
Requirement already satisfied: mpmath>=0.19 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from sympy->torch) (1.3.0)

Output: Defaulting to user installation because normal site-packages is not writeable
Requirement already satisfied: transformers in /u/bzd2/.local/lib/python3.9/site-packages (4.37.0.dev0)
Requirement already satisfied: filelock in /sw/external/python/anaconda3/lib/python3.9/site-packages (from transformers) (3.9.0)
Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /u/bzd2/.local/lib/python3.9/site-packages (from transformers) (0.19.4)
Requirement already satisfied: numpy>=1.17 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from transformers) (1.24.3)
Requirement already satisfied: packaging>=20.0 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from transformers) (23.1)
Requirement already satisfied: pyyaml>=5.1 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from transformers) (5.4.1)
Requirement already satisfied: regex!=2019.12.17 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from transformers) (2022.7.9)
Requirement already satisfied: requests in /sw/external/python/anaconda3/lib/python3.9/site-packages (from transformers) (2.31.0)
Requirement already satisfied: tokenizers<0.19,>=0.14 in /u/bzd2/.local/lib/python3.9/site-packages (from transformers) (0.15.0)
Requirement already satisfied: safetensors>=0.3.1 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from transformers) (0.3.2)
Requirement already satisfied: tqdm>=4.27 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from transformers) (4.65.0)
Requirement already satisfied: fsspec>=2023.5.0 in /u/bzd2/.local/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (2023.6.0)
Requirement already satisfied: typing-extensions>=3.7.4.3 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (4.7.1)
Requirement already satisfied: charset-normalizer<4,>=2 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from requests->transformers) (2.0.4)
Requirement already satisfied: idna<4,>=2.5 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from requests->transformers) (3.4)
Requirement already satisfied: urllib3<3,>=1.21.1 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from requests->transformers) (1.26.16)
Requirement already satisfied: certifi>=2017.4.17 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from requests->transformers) (2023.7.22)

Output: Defaulting to user installation because normal site-packages is not writeable
Collecting git+https://github.com/huggingface/peft.git
  Cloning https://github.com/huggingface/peft.git to /tmp/pip-req-build-a9fqic0b
  Resolved https://github.com/huggingface/peft.git to commit 0f1e9091cc975eb5458cc163bf1843a34fb42b76
  Installing build dependencies: started
  Installing build dependencies: finished with status 'done'
  Getting requirements to build wheel: started
  Getting requirements to build wheel: finished with status 'done'
  Preparing metadata (pyproject.toml): started
  Preparing metadata (pyproject.toml): finished with status 'done'
Requirement already satisfied: numpy>=1.17 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from peft==0.7.2.dev0) (1.24.3)
Requirement already satisfied: packaging>=20.0 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from peft==0.7.2.dev0) (23.1)
Requirement already satisfied: psutil in /sw/external/python/anaconda3/lib/python3.9/site-packages (from peft==0.7.2.dev0) (5.9.0)
Requirement already satisfied: pyyaml in /sw/external/python/anaconda3/lib/python3.9/site-packages (from peft==0.7.2.dev0) (5.4.1)
Requirement already satisfied: torch>=1.13.0 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from peft==0.7.2.dev0) (2.2.0.dev20231003+cu121)
Requirement already satisfied: transformers in /u/bzd2/.local/lib/python3.9/site-packages (from peft==0.7.2.dev0) (4.37.0.dev0)
Requirement already satisfied: tqdm in /sw/external/python/anaconda3/lib/python3.9/site-packages (from peft==0.7.2.dev0) (4.65.0)
Requirement already satisfied: accelerate>=0.21.0 in /u/bzd2/.local/lib/python3.9/site-packages (from peft==0.7.2.dev0) (0.25.0)
Requirement already satisfied: safetensors in /sw/external/python/anaconda3/lib/python3.9/site-packages (from peft==0.7.2.dev0) (0.3.2)
Requirement already satisfied: huggingface-hub>=0.17.0 in /u/bzd2/.local/lib/python3.9/site-packages (from peft==0.7.2.dev0) (0.19.4)
Requirement already satisfied: filelock in /sw/external/python/anaconda3/lib/python3.9/site-packages (from huggingface-hub>=0.17.0->peft==0.7.2.dev0) (3.9.0)
Requirement already satisfied: fsspec>=2023.5.0 in /u/bzd2/.local/lib/python3.9/site-packages (from huggingface-hub>=0.17.0->peft==0.7.2.dev0) (2023.6.0)
Requirement already satisfied: requests in /sw/external/python/anaconda3/lib/python3.9/site-packages (from huggingface-hub>=0.17.0->peft==0.7.2.dev0) (2.31.0)
Requirement already satisfied: typing-extensions>=3.7.4.3 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from huggingface-hub>=0.17.0->peft==0.7.2.dev0) (4.7.1)
Requirement already satisfied: sympy in /sw/external/python/anaconda3/lib/python3.9/site-packages (from torch>=1.13.0->peft==0.7.2.dev0) (1.11.1)
Requirement already satisfied: networkx in /sw/external/python/anaconda3/lib/python3.9/site-packages (from torch>=1.13.0->peft==0.7.2.dev0) (3.1)
Requirement already satisfied: jinja2 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from torch>=1.13.0->peft==0.7.2.dev0) (3.1.2)
Requirement already satisfied: pytorch-triton==2.1.0+6e4932cda8 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from torch>=1.13.0->peft==0.7.2.dev0) (2.1.0+6e4932cda8)
Requirement already satisfied: regex!=2019.12.17 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from transformers->peft==0.7.2.dev0) (2022.7.9)
Requirement already satisfied: tokenizers<0.19,>=0.14 in /u/bzd2/.local/lib/python3.9/site-packages (from transformers->peft==0.7.2.dev0) (0.15.0)
Requirement already satisfied: MarkupSafe>=2.0 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from jinja2->torch>=1.13.0->peft==0.7.2.dev0) (2.1.1)
Requirement already satisfied: charset-normalizer<4,>=2 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from requests->huggingface-hub>=0.17.0->peft==0.7.2.dev0) (2.0.4)
Requirement already satisfied: idna<4,>=2.5 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from requests->huggingface-hub>=0.17.0->peft==0.7.2.dev0) (3.4)
Requirement already satisfied: urllib3<3,>=1.21.1 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from requests->huggingface-hub>=0.17.0->peft==0.7.2.dev0) (1.26.16)
Requirement already satisfied: certifi>=2017.4.17 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from requests->huggingface-hub>=0.17.0->peft==0.7.2.dev0) (2023.7.22)
Requirement already satisfied: mpmath>=0.19 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from sympy->torch>=1.13.0->peft==0.7.2.dev0) (1.3.0)

Output: Defaulting to user installation because normal site-packages is not writeable
Collecting git+https://github.com/huggingface/transformers
  Cloning https://github.com/huggingface/transformers to /tmp/pip-req-build-hojwaqzn
  Resolved https://github.com/huggingface/transformers to commit 17506d1256c1780efc9e2a5898a828c10ad4ea69
  Installing build dependencies: started
  Installing build dependencies: finished with status 'done'
  Getting requirements to build wheel: started
  Getting requirements to build wheel: finished with status 'done'
  Preparing metadata (pyproject.toml): started
  Preparing metadata (pyproject.toml): finished with status 'done'
Requirement already satisfied: filelock in /sw/external/python/anaconda3/lib/python3.9/site-packages (from transformers==4.37.0.dev0) (3.9.0)
Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /u/bzd2/.local/lib/python3.9/site-packages (from transformers==4.37.0.dev0) (0.19.4)
Requirement already satisfied: numpy>=1.17 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from transformers==4.37.0.dev0) (1.24.3)
Requirement already satisfied: packaging>=20.0 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from transformers==4.37.0.dev0) (23.1)
Requirement already satisfied: pyyaml>=5.1 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from transformers==4.37.0.dev0) (5.4.1)
Requirement already satisfied: regex!=2019.12.17 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from transformers==4.37.0.dev0) (2022.7.9)
Requirement already satisfied: requests in /sw/external/python/anaconda3/lib/python3.9/site-packages (from transformers==4.37.0.dev0) (2.31.0)
Requirement already satisfied: tokenizers<0.19,>=0.14 in /u/bzd2/.local/lib/python3.9/site-packages (from transformers==4.37.0.dev0) (0.15.0)
Requirement already satisfied: safetensors>=0.3.1 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from transformers==4.37.0.dev0) (0.3.2)
Requirement already satisfied: tqdm>=4.27 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from transformers==4.37.0.dev0) (4.65.0)
Requirement already satisfied: fsspec>=2023.5.0 in /u/bzd2/.local/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers==4.37.0.dev0) (2023.6.0)
Requirement already satisfied: typing-extensions>=3.7.4.3 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers==4.37.0.dev0) (4.7.1)
Requirement already satisfied: charset-normalizer<4,>=2 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from requests->transformers==4.37.0.dev0) (2.0.4)
Requirement already satisfied: idna<4,>=2.5 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from requests->transformers==4.37.0.dev0) (3.4)
Requirement already satisfied: urllib3<3,>=1.21.1 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from requests->transformers==4.37.0.dev0) (1.26.16)
Requirement already satisfied: certifi>=2017.4.17 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from requests->transformers==4.37.0.dev0) (2023.7.22)

Output: Defaulting to user installation because normal site-packages is not writeable
Requirement already satisfied: datasets in /sw/external/python/anaconda3/lib/python3.9/site-packages (2.14.5)
Requirement already satisfied: numpy>=1.17 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from datasets) (1.24.3)
Requirement already satisfied: pyarrow>=8.0.0 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from datasets) (11.0.0)
Requirement already satisfied: dill<0.3.8,>=0.3.0 in /u/bzd2/.local/lib/python3.9/site-packages (from datasets) (0.3.7)
Requirement already satisfied: pandas in /sw/external/python/anaconda3/lib/python3.9/site-packages (from datasets) (2.0.3)
Requirement already satisfied: requests>=2.19.0 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from datasets) (2.31.0)
Requirement already satisfied: tqdm>=4.62.1 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from datasets) (4.65.0)
Requirement already satisfied: xxhash in /sw/external/python/anaconda3/lib/python3.9/site-packages (from datasets) (2.0.2)
Requirement already satisfied: multiprocess in /sw/external/python/anaconda3/lib/python3.9/site-packages (from datasets) (0.70.15)
Requirement already satisfied: fsspec[http]<2023.9.0,>=2023.1.0 in /u/bzd2/.local/lib/python3.9/site-packages (from datasets) (2023.6.0)
Requirement already satisfied: aiohttp in /sw/external/python/anaconda3/lib/python3.9/site-packages (from datasets) (3.8.5)
Requirement already satisfied: huggingface-hub<1.0.0,>=0.14.0 in /u/bzd2/.local/lib/python3.9/site-packages (from datasets) (0.19.4)
Requirement already satisfied: packaging in /sw/external/python/anaconda3/lib/python3.9/site-packages (from datasets) (23.1)
Requirement already satisfied: pyyaml>=5.1 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from datasets) (5.4.1)
Requirement already satisfied: attrs>=17.3.0 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from aiohttp->datasets) (23.1.0)
Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from aiohttp->datasets) (2.0.4)
Requirement already satisfied: multidict<7.0,>=4.5 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from aiohttp->datasets) (6.0.2)
Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from aiohttp->datasets) (4.0.2)
Requirement already satisfied: yarl<2.0,>=1.0 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from aiohttp->datasets) (1.8.1)
Requirement already satisfied: frozenlist>=1.1.1 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from aiohttp->datasets) (1.3.3)
Requirement already satisfied: aiosignal>=1.1.2 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from aiohttp->datasets) (1.2.0)
Requirement already satisfied: filelock in /sw/external/python/anaconda3/lib/python3.9/site-packages (from huggingface-hub<1.0.0,>=0.14.0->datasets) (3.9.0)
Requirement already satisfied: typing-extensions>=3.7.4.3 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from huggingface-hub<1.0.0,>=0.14.0->datasets) (4.7.1)
Requirement already satisfied: idna<4,>=2.5 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from requests>=2.19.0->datasets) (3.4)
Requirement already satisfied: urllib3<3,>=1.21.1 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from requests>=2.19.0->datasets) (1.26.16)
Requirement already satisfied: certifi>=2017.4.17 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from requests>=2.19.0->datasets) (2023.7.22)
Requirement already satisfied: python-dateutil>=2.8.2 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from pandas->datasets) (2.8.2)
Requirement already satisfied: pytz>=2020.1 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from pandas->datasets) (2023.3.post1)
Requirement already satisfied: tzdata>=2022.1 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from pandas->datasets) (2023.3)
Requirement already satisfied: six>=1.5 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)

Output: Defaulting to user installation because normal site-packages is not writeable
Requirement already satisfied: accelerate in /u/bzd2/.local/lib/python3.9/site-packages (0.25.0)
Requirement already satisfied: numpy>=1.17 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from accelerate) (1.24.3)
Requirement already satisfied: packaging>=20.0 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from accelerate) (23.1)
Requirement already satisfied: psutil in /sw/external/python/anaconda3/lib/python3.9/site-packages (from accelerate) (5.9.0)
Requirement already satisfied: pyyaml in /sw/external/python/anaconda3/lib/python3.9/site-packages (from accelerate) (5.4.1)
Requirement already satisfied: torch>=1.10.0 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from accelerate) (2.2.0.dev20231003+cu121)
Requirement already satisfied: huggingface-hub in /u/bzd2/.local/lib/python3.9/site-packages (from accelerate) (0.19.4)
Requirement already satisfied: safetensors>=0.3.1 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from accelerate) (0.3.2)
Requirement already satisfied: filelock in /sw/external/python/anaconda3/lib/python3.9/site-packages (from torch>=1.10.0->accelerate) (3.9.0)
Requirement already satisfied: typing-extensions in /sw/external/python/anaconda3/lib/python3.9/site-packages (from torch>=1.10.0->accelerate) (4.7.1)
Requirement already satisfied: sympy in /sw/external/python/anaconda3/lib/python3.9/site-packages (from torch>=1.10.0->accelerate) (1.11.1)
Requirement already satisfied: networkx in /sw/external/python/anaconda3/lib/python3.9/site-packages (from torch>=1.10.0->accelerate) (3.1)
Requirement already satisfied: jinja2 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from torch>=1.10.0->accelerate) (3.1.2)
Requirement already satisfied: fsspec in /u/bzd2/.local/lib/python3.9/site-packages (from torch>=1.10.0->accelerate) (2023.6.0)
Requirement already satisfied: pytorch-triton==2.1.0+6e4932cda8 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from torch>=1.10.0->accelerate) (2.1.0+6e4932cda8)
Requirement already satisfied: requests in /sw/external/python/anaconda3/lib/python3.9/site-packages (from huggingface-hub->accelerate) (2.31.0)
Requirement already satisfied: tqdm>=4.42.1 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from huggingface-hub->accelerate) (4.65.0)
Requirement already satisfied: MarkupSafe>=2.0 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.1)
Requirement already satisfied: charset-normalizer<4,>=2 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from requests->huggingface-hub->accelerate) (2.0.4)
Requirement already satisfied: idna<4,>=2.5 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from requests->huggingface-hub->accelerate) (3.4)
Requirement already satisfied: urllib3<3,>=1.21.1 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from requests->huggingface-hub->accelerate) (1.26.16)
Requirement already satisfied: certifi>=2017.4.17 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from requests->huggingface-hub->accelerate) (2023.7.22)
Requirement already satisfied: mpmath>=0.19 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)

Output: Defaulting to user installation because normal site-packages is not writeable
Requirement already satisfied: huggingface_hub in /u/bzd2/.local/lib/python3.9/site-packages (0.19.4)
Requirement already satisfied: filelock in /sw/external/python/anaconda3/lib/python3.9/site-packages (from huggingface_hub) (3.9.0)
Requirement already satisfied: fsspec>=2023.5.0 in /u/bzd2/.local/lib/python3.9/site-packages (from huggingface_hub) (2023.6.0)
Requirement already satisfied: requests in /sw/external/python/anaconda3/lib/python3.9/site-packages (from huggingface_hub) (2.31.0)
Requirement already satisfied: tqdm>=4.42.1 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from huggingface_hub) (4.65.0)
Requirement already satisfied: pyyaml>=5.1 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from huggingface_hub) (5.4.1)
Requirement already satisfied: typing-extensions>=3.7.4.3 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from huggingface_hub) (4.7.1)
Requirement already satisfied: packaging>=20.9 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from huggingface_hub) (23.1)
Requirement already satisfied: charset-normalizer<4,>=2 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from requests->huggingface_hub) (2.0.4)
Requirement already satisfied: idna<4,>=2.5 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from requests->huggingface_hub) (3.4)
Requirement already satisfied: urllib3<3,>=1.21.1 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from requests->huggingface_hub) (1.26.16)
Requirement already satisfied: certifi>=2017.4.17 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from requests->huggingface_hub) (2023.7.22)

Output: Defaulting to user installation because normal site-packages is not writeable
Requirement already satisfied: bitsandbytes in /u/bzd2/.local/lib/python3.9/site-packages (0.41.3.post2)

Output: Defaulting to user installation because normal site-packages is not writeable
Requirement already satisfied: wandb in /u/bzd2/.local/lib/python3.9/site-packages (0.16.1)
Requirement already satisfied: Click!=8.0.0,>=7.1 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from wandb) (8.0.4)
Requirement already satisfied: GitPython!=3.1.29,>=1.0.0 in /u/bzd2/.local/lib/python3.9/site-packages (from wandb) (3.1.40)
Requirement already satisfied: requests<3,>=2.0.0 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from wandb) (2.31.0)
Requirement already satisfied: psutil>=5.0.0 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from wandb) (5.9.0)
Requirement already satisfied: sentry-sdk>=1.0.0 in /u/bzd2/.local/lib/python3.9/site-packages (from wandb) (1.38.0)
Requirement already satisfied: docker-pycreds>=0.4.0 in /u/bzd2/.local/lib/python3.9/site-packages (from wandb) (0.4.0)
Requirement already satisfied: PyYAML in /sw/external/python/anaconda3/lib/python3.9/site-packages (from wandb) (5.4.1)
Requirement already satisfied: setproctitle in /sw/external/python/anaconda3/lib/python3.9/site-packages (from wandb) (1.3.2)
Requirement already satisfied: setuptools in /sw/external/python/anaconda3/lib/python3.9/site-packages (from wandb) (68.0.0)
Requirement already satisfied: appdirs>=1.4.3 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from wandb) (1.4.4)
Requirement already satisfied: typing-extensions in /sw/external/python/anaconda3/lib/python3.9/site-packages (from wandb) (4.7.1)
Requirement already satisfied: protobuf!=4.21.0,<5,>=3.15.0 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from wandb) (3.20.3)
Requirement already satisfied: six>=1.4.0 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)
Requirement already satisfied: gitdb<5,>=4.0.1 in /u/bzd2/.local/lib/python3.9/site-packages (from GitPython!=3.1.29,>=1.0.0->wandb) (4.0.11)
Requirement already satisfied: charset-normalizer<4,>=2 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from requests<3,>=2.0.0->wandb) (2.0.4)
Requirement already satisfied: idna<4,>=2.5 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from requests<3,>=2.0.0->wandb) (3.4)
Requirement already satisfied: urllib3<3,>=1.21.1 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from requests<3,>=2.0.0->wandb) (1.26.16)
Requirement already satisfied: certifi>=2017.4.17 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from requests<3,>=2.0.0->wandb) (2023.7.22)
Requirement already satisfied: smmap<6,>=3.0.1 in /u/bzd2/.local/lib/python3.9/site-packages (from gitdb<5,>=4.0.1->GitPython!=3.1.29,>=1.0.0->wandb) (5.0.1)

Output: Defaulting to user installation because normal site-packages is not writeable
Requirement already satisfied: scikit-learn in /sw/external/python/anaconda3/lib/python3.9/site-packages (1.3.0)
Requirement already satisfied: numpy>=1.17.3 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from scikit-learn) (1.24.3)
Requirement already satisfied: scipy>=1.5.0 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from scikit-learn) (1.8.1)
Requirement already satisfied: joblib>=1.1.1 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from scikit-learn) (1.2.0)
Requirement already satisfied: threadpoolctl>=2.0.0 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from scikit-learn) (2.2.0)

Output: Defaulting to user installation because normal site-packages is not writeable
Requirement already satisfied: code_bert_score in /u/bzd2/.local/lib/python3.9/site-packages (0.4.1)
Requirement already satisfied: torch>=1.0.0 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from code_bert_score) (2.2.0.dev20231003+cu121)
Requirement already satisfied: numpy in /sw/external/python/anaconda3/lib/python3.9/site-packages (from code_bert_score) (1.24.3)
Requirement already satisfied: pandas>=1.0.1 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from code_bert_score) (2.0.3)
Requirement already satisfied: requests in /sw/external/python/anaconda3/lib/python3.9/site-packages (from code_bert_score) (2.31.0)
Requirement already satisfied: tqdm>=4.31.1 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from code_bert_score) (4.65.0)
Requirement already satisfied: matplotlib in /sw/external/python/anaconda3/lib/python3.9/site-packages (from code_bert_score) (3.7.2)
Requirement already satisfied: transformers>=3.0.0 in /u/bzd2/.local/lib/python3.9/site-packages (from code_bert_score) (4.37.0.dev0)
Requirement already satisfied: python-dateutil>=2.8.2 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from pandas>=1.0.1->code_bert_score) (2.8.2)
Requirement already satisfied: pytz>=2020.1 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from pandas>=1.0.1->code_bert_score) (2023.3.post1)
Requirement already satisfied: tzdata>=2022.1 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from pandas>=1.0.1->code_bert_score) (2023.3)
Requirement already satisfied: filelock in /sw/external/python/anaconda3/lib/python3.9/site-packages (from torch>=1.0.0->code_bert_score) (3.9.0)
Requirement already satisfied: typing-extensions in /sw/external/python/anaconda3/lib/python3.9/site-packages (from torch>=1.0.0->code_bert_score) (4.7.1)
Requirement already satisfied: sympy in /sw/external/python/anaconda3/lib/python3.9/site-packages (from torch>=1.0.0->code_bert_score) (1.11.1)
Requirement already satisfied: networkx in /sw/external/python/anaconda3/lib/python3.9/site-packages (from torch>=1.0.0->code_bert_score) (3.1)
Requirement already satisfied: jinja2 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from torch>=1.0.0->code_bert_score) (3.1.2)
Requirement already satisfied: fsspec in /u/bzd2/.local/lib/python3.9/site-packages (from torch>=1.0.0->code_bert_score) (2023.6.0)
Requirement already satisfied: pytorch-triton==2.1.0+6e4932cda8 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from torch>=1.0.0->code_bert_score) (2.1.0+6e4932cda8)
Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /u/bzd2/.local/lib/python3.9/site-packages (from transformers>=3.0.0->code_bert_score) (0.19.4)
Requirement already satisfied: packaging>=20.0 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from transformers>=3.0.0->code_bert_score) (23.1)
Requirement already satisfied: pyyaml>=5.1 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from transformers>=3.0.0->code_bert_score) (5.4.1)
Requirement already satisfied: regex!=2019.12.17 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from transformers>=3.0.0->code_bert_score) (2022.7.9)
Requirement already satisfied: tokenizers<0.19,>=0.14 in /u/bzd2/.local/lib/python3.9/site-packages (from transformers>=3.0.0->code_bert_score) (0.15.0)
Requirement already satisfied: safetensors>=0.3.1 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from transformers>=3.0.0->code_bert_score) (0.3.2)
Requirement already satisfied: contourpy>=1.0.1 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from matplotlib->code_bert_score) (1.0.5)
Requirement already satisfied: cycler>=0.10 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from matplotlib->code_bert_score) (0.11.0)
Requirement already satisfied: fonttools>=4.22.0 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from matplotlib->code_bert_score) (4.25.0)
Requirement already satisfied: kiwisolver>=1.0.1 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from matplotlib->code_bert_score) (1.4.4)
Requirement already satisfied: pillow>=6.2.0 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from matplotlib->code_bert_score) (9.4.0)
Requirement already satisfied: pyparsing<3.1,>=2.3.1 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from matplotlib->code_bert_score) (3.0.9)
Requirement already satisfied: importlib-resources>=3.2.0 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from matplotlib->code_bert_score) (5.2.0)
Requirement already satisfied: charset-normalizer<4,>=2 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from requests->code_bert_score) (2.0.4)
Requirement already satisfied: idna<4,>=2.5 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from requests->code_bert_score) (3.4)
Requirement already satisfied: urllib3<3,>=1.21.1 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from requests->code_bert_score) (1.26.16)
Requirement already satisfied: certifi>=2017.4.17 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from requests->code_bert_score) (2023.7.22)
Requirement already satisfied: zipp>=3.1.0 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from importlib-resources>=3.2.0->matplotlib->code_bert_score) (3.11.0)
Requirement already satisfied: six>=1.5 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from python-dateutil>=2.8.2->pandas>=1.0.1->code_bert_score) (1.16.0)
Requirement already satisfied: MarkupSafe>=2.0 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from jinja2->torch>=1.0.0->code_bert_score) (2.1.1)
Requirement already satisfied: mpmath>=0.19 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from sympy->torch>=1.0.0->code_bert_score) (1.3.0)

WARNING:__main__:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
2023-12-13 19:12:26.352521: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
2023-12-13 19:12:26.352528: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
2023-12-13 19:12:26.352534: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
2023-12-13 19:12:26.352532: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
2023-12-13 19:12:26.353456: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2023-12-13 19:12:26.353461: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2023-12-13 19:12:26.353465: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2023-12-13 19:12:26.353469: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2023-12-13 19:12:26.359980: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2023-12-13 19:12:26.359982: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2023-12-13 19:12:26.359984: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2023-12-13 19:12:26.359988: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2023-12-13 19:12:27.246167: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-12-13 19:12:27.246163: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-12-13 19:12:27.246163: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-12-13 19:12:27.246170: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
/u/bzd2/.local/lib/python3.9/site-packages/transformers/models/auto/tokenization_auto.py:691: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/transformers/models/auto/tokenization_auto.py:691: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/transformers/models/auto/tokenization_auto.py:691: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/transformers/models/auto/tokenization_auto.py:691: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.
  warnings.warn(
loading file vocab.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-3b/snapshots/e1c5ef4ebb97afa0db09ec3e520f0487ca350bbe/vocab.json
loading file merges.txt from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-3b/snapshots/e1c5ef4ebb97afa0db09ec3e520f0487ca350bbe/merges.txt
loading file tokenizer.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-3b/snapshots/e1c5ef4ebb97afa0db09ec3e520f0487ca350bbe/tokenizer.json
loading file added_tokens.json from cache at None
loading file special_tokens_map.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-3b/snapshots/e1c5ef4ebb97afa0db09ec3e520f0487ca350bbe/special_tokens_map.json
loading file tokenizer_config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-3b/snapshots/e1c5ef4ebb97afa0db09ec3e520f0487ca350bbe/tokenizer_config.json
loading file vocab.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-3b/snapshots/e1c5ef4ebb97afa0db09ec3e520f0487ca350bbe/vocab.json
loading file merges.txt from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-3b/snapshots/e1c5ef4ebb97afa0db09ec3e520f0487ca350bbe/merges.txt
loading file tokenizer.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-3b/snapshots/e1c5ef4ebb97afa0db09ec3e520f0487ca350bbe/tokenizer.json
loading file added_tokens.json from cache at None
loading file special_tokens_map.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-3b/snapshots/e1c5ef4ebb97afa0db09ec3e520f0487ca350bbe/special_tokens_map.json
loading file tokenizer_config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-3b/snapshots/e1c5ef4ebb97afa0db09ec3e520f0487ca350bbe/tokenizer_config.json
loading file vocab.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-3b/snapshots/e1c5ef4ebb97afa0db09ec3e520f0487ca350bbe/vocab.json
loading file merges.txt from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-3b/snapshots/e1c5ef4ebb97afa0db09ec3e520f0487ca350bbe/merges.txt
loading file tokenizer.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-3b/snapshots/e1c5ef4ebb97afa0db09ec3e520f0487ca350bbe/tokenizer.json
loading file added_tokens.json from cache at None
loading file special_tokens_map.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-3b/snapshots/e1c5ef4ebb97afa0db09ec3e520f0487ca350bbe/special_tokens_map.json
loading file tokenizer_config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-3b/snapshots/e1c5ef4ebb97afa0db09ec3e520f0487ca350bbe/tokenizer_config.json
loading file vocab.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-3b/snapshots/e1c5ef4ebb97afa0db09ec3e520f0487ca350bbe/vocab.json
loading file merges.txt from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-3b/snapshots/e1c5ef4ebb97afa0db09ec3e520f0487ca350bbe/merges.txt
loading file tokenizer.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-3b/snapshots/e1c5ef4ebb97afa0db09ec3e520f0487ca350bbe/tokenizer.json
loading file added_tokens.json from cache at None
loading file special_tokens_map.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-3b/snapshots/e1c5ef4ebb97afa0db09ec3e520f0487ca350bbe/special_tokens_map.json
loading file tokenizer_config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-3b/snapshots/e1c5ef4ebb97afa0db09ec3e520f0487ca350bbe/tokenizer_config.json
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/datasets/load.py:2089: FutureWarning: 'use_auth_token' was deprecated in favor of 'token' in version 2.14.0 and will be removed in 3.0.0.
You can remove this warning by passing 'token=True' instead.
  warnings.warn(
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/datasets/load.py:2089: FutureWarning: 'use_auth_token' was deprecated in favor of 'token' in version 2.14.0 and will be removed in 3.0.0.
You can remove this warning by passing 'token=True' instead.
  warnings.warn(
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/datasets/load.py:2089: FutureWarning: 'use_auth_token' was deprecated in favor of 'token' in version 2.14.0 and will be removed in 3.0.0.
You can remove this warning by passing 'token=True' instead.
  warnings.warn(
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/datasets/load.py:2089: FutureWarning: 'use_auth_token' was deprecated in favor of 'token' in version 2.14.0 and will be removed in 3.0.0.
You can remove this warning by passing 'token=True' instead.
  warnings.warn(
Loading the dataset in streaming mode
Loading the dataset in streaming mode
Loading the dataset in streaming mode
Loading the dataset in streaming mode
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<FOR DEBUGGING PURPOSES ONLY!>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
{'download_count': '81335', 'path': 'data/repos/adriagalin/motd/tasks/main.yml', 'input': 'name: Delete /etc/motd file', 'org_name': 'adriagalin', 'output': 'file:\n  path: /etc/motd\n  state: absent\nwhen: ag_motd_add_footer | bool\ntags:\n- motd\n- common\n', 'license': 'GPL-3.0-only', 'repo_name': 'motd', 'download_link': 'https://old-galaxy.ansible.com/adriagalin/motd'}
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<FOR DEBUGGING PURPOSES ONLY!>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
{'download_count': '16571', 'path': 'data/repos/siamaksade/openshift_gogs/tasks/main.yml', 'input': 'name: check if gogs exists', 'org_name': 'siamaksade', 'output': 'shell: oc get service gogs -n gogs\nregister: install_gogs\nignore_errors: true\nchanged_when: false\n', 'license': '', 'repo_name': 'openshift_gogs', 'download_link': 'https://old-galaxy.ansible.com/siamaksade/openshift_gogs'}
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<FOR DEBUGGING PURPOSES ONLY!>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<FOR DEBUGGING PURPOSES ONLY!>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>{'download_count': '688', 'path': 'data/repos/devopsarr/sonarr/integration/targets/sonarr_release_profile/tasks/main.yml', 'input': 'name: assert release profile idempotency', 'org_name': 'devopsarr', 'output': 'assert:\n  that:\n  - result.changed == false\n  - result.ignored == ["repack", "dvdrip"]\n', 'license': '', 'repo_name': 'sonarr', 'download_link': 'https://old-galaxy.ansible.com/devopsarr/sonarr'}

<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<FOR DEBUGGING PURPOSES ONLY!>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
{'download_count': '9401', 'path': 'data/repos/buluma/anaconda/tasks/main.yml', 'input': 'name: Start and enable anaconda', 'org_name': 'buluma', 'output': 'ansible.builtin.service:\n  name: anaconda\n  state: started\n  enabled: true\n', 'license': 'Apache-2.0-Short,Apache-2.0', 'repo_name': 'anaconda', 'download_link': 'https://old-galaxy.ansible.com/buluma/anaconda'}
{'download_count': '81335', 'path': 'data/repos/adriagalin/motd/tasks/main.yml', 'license': 'GPL-3.0-only', 'repo_name': 'motd', 'org_name': 'adriagalin', 'download_link': 'https://old-galaxy.ansible.com/adriagalin/motd', 'input': 'name: Delete /etc/motd file', 'output': 'file:\n  path: /etc/motd\n  state: absent\nwhen: ag_motd_add_footer | bool\ntags:\n- motd\n- common\n'}
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<FOR DEBUGGING PURPOSES ONLY!>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<FOR DEBUGGING PURPOSES ONLY!>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
{'download_count': '1904', 'path': 'data/repos/chrismeyersfsu/ec2_server/tasks/main.yml', 'input': 'name: Add hosts group temporary inventory group without pem path', 'org_name': 'chrismeyersfsu', 'output': "add_host:\n  name: ''\n  groups: ec2_server_hosts\n  ansible_ssh_user: ''\nwith_items: ec2.instances\nwhen: ec2_server_pem_path == ''\n", 'license': '', 'repo_name': 'ec2_server', 'download_link': 'https://old-galaxy.ansible.com/chrismeyersfsu/ec2_server'}
{'download_count': '16571', 'path': 'data/repos/siamaksade/openshift_gogs/tasks/main.yml', 'license': '', 'repo_name': 'openshift_gogs', 'org_name': 'siamaksade', 'download_link': 'https://old-galaxy.ansible.com/siamaksade/openshift_gogs', 'input': 'name: check if gogs exists', 'output': 'shell: oc get service gogs -n gogs\nregister: install_gogs\nignore_errors: true\nchanged_when: false\n'}
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<FOR DEBUGGING PURPOSES ONLY!>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
{'download_count': '688', 'path': 'data/repos/devopsarr/sonarr/integration/targets/sonarr_release_profile/tasks/main.yml', 'license': '', 'repo_name': 'sonarr', 'org_name': 'devopsarr', 'download_link': 'https://old-galaxy.ansible.com/devopsarr/sonarr', 'input': 'name: assert release profile idempotency', 'output': 'assert:\n  that:\n  - result.changed == false\n  - result.ignored == ["repack", "dvdrip"]\n'}
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<FOR DEBUGGING PURPOSES ONLY!>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
{'download_count': '9401', 'path': 'data/repos/buluma/anaconda/tasks/main.yml', 'license': 'Apache-2.0-Short,Apache-2.0', 'repo_name': 'anaconda', 'org_name': 'buluma', 'download_link': 'https://old-galaxy.ansible.com/buluma/anaconda', 'input': 'name: Start and enable anaconda', 'output': 'ansible.builtin.service:\n  name: anaconda\n  state: started\n  enabled: true\n'}
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<FOR DEBUGGING PURPOSES ONLY!>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
{'download_count': '1904', 'path': 'data/repos/chrismeyersfsu/ec2_server/tasks/main.yml', 'license': '', 'repo_name': 'ec2_server', 'org_name': 'chrismeyersfsu', 'download_link': 'https://old-galaxy.ansible.com/chrismeyersfsu/ec2_server', 'input': 'name: Add hosts group temporary inventory group without pem path', 'output': "add_host:\n  name: ''\n  groups: ec2_server_hosts\n  ansible_ssh_user: ''\nwith_items: ec2.instances\nwhen: ec2_server_pem_path == ''\n"}
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<FOR DEBUGGING PURPOSES ONLY!>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>><<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<FOR DEBUGGING PURPOSES ONLY!>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>

{'download_count': '561', 'path': 'data/repos/lifeofguenter/nginx/tasks/nginx.yml', 'input': 'name: Install', 'org_name': 'lifeofguenter', 'output': 'ansible.builtin.command: make install\nargs:\n  chdir: /tmp/nginx-1.24.0\nchanged_when: true\nbecome: true\nnotify: Restart_nginx\n', 'license': 'MIT', 'repo_name': 'nginx', 'download_link': 'https://old-galaxy.ansible.com/lifeofguenter/nginx'}
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<FOR DEBUGGING PURPOSES ONLY!>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
{'download_count': '81335', 'org_name': 'adriagalin', 'download_link': 'https://old-galaxy.ansible.com/adriagalin/motd', 'license': 'GPL-3.0-only', 'repo_name': 'motd', 'output': 'file:\n  path: /etc/motd\n  state: absent\nwhen: ag_motd_add_footer | bool\ntags:\n- motd\n- common\n', 'path': 'data/repos/adriagalin/motd/tasks/main.yml', 'input': 'name: Delete /etc/motd file'}
{'download_count': '34969', 'path': 'data/repos/robertdebock/locale/tasks/assert.yml', 'input': 'name: assert | Test locale_lc_time', 'org_name': 'robertdebock', 'output': 'ansible.builtin.assert:\n  that:\n  - locale_lc_time is defined\n  - locale_lc_time is string\n  - locale_lc_time is not none\n  quiet: true\n', 'license': 'Apache-2.0-Short,Apache-2.0', 'repo_name': 'locale', 'download_link': 'https://old-galaxy.ansible.com/robertdebock/locale'}
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<FOR DEBUGGING PURPOSES ONLY!>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>><<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<FOR DEBUGGING PURPOSES ONLY!>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>

{'download_count': '513', 'path': 'data/repos/brentwg/visual-studio-code/tasks/Debian.yml', 'input': 'name: Create the APT repository (Debian)', 'org_name': 'brentwg', 'output': 'apt_repository:\n  repo: deb [arch=amd64] https://packages.microsoft.com/repos/vscode stable main\n  filename: vscode\n', 'license': '', 'repo_name': 'visual-studio-code', 'download_link': 'https://old-galaxy.ansible.com/brentwg/visual-studio-code'}
{'download_count': '16571', 'org_name': 'siamaksade', 'download_link': 'https://old-galaxy.ansible.com/siamaksade/openshift_gogs', 'license': '', 'repo_name': 'openshift_gogs', 'output': 'shell: oc get service gogs -n gogs\nregister: install_gogs\nignore_errors: true\nchanged_when: false\n', 'path': 'data/repos/siamaksade/openshift_gogs/tasks/main.yml', 'input': 'name: check if gogs exists'}
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<FOR DEBUGGING PURPOSES ONLY!>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>><<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<FOR DEBUGGING PURPOSES ONLY!>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<FOR DEBUGGING PURPOSES ONLY!>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>

{'download_count': '801', 'path': 'data/repos/mahdi22/mariadb_install/tasks/Debian-install.yml', 'input': 'name: Add MariaDB Repository Key for', 'org_name': 'mahdi22', 'output': 'apt_key:\n  url: https://mariadb.org/mariadb_release_signing_key.asc\n  state: present\nwhen: (use_proxy is not defined) or (not use_proxy)\n', 'license': '', 'repo_name': 'mariadb_install', 'download_link': 'https://old-galaxy.ansible.com/mahdi22/mariadb_install'}
{'download_count': '561', 'path': 'data/repos/lifeofguenter/nginx/tasks/nginx.yml', 'license': 'MIT', 'repo_name': 'nginx', 'org_name': 'lifeofguenter', 'download_link': 'https://old-galaxy.ansible.com/lifeofguenter/nginx', 'input': 'name: Install', 'output': 'ansible.builtin.command: make install\nargs:\n  chdir: /tmp/nginx-1.24.0\nchanged_when: true\nbecome: true\nnotify: Restart_nginx\n'}<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<FOR DEBUGGING PURPOSES ONLY!>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
{'download_count': '688', 'org_name': 'devopsarr', 'download_link': 'https://old-galaxy.ansible.com/devopsarr/sonarr', 'license': '', 'repo_name': 'sonarr', 'output': 'assert:\n  that:\n  - result.changed == false\n  - result.ignored == ["repack", "dvdrip"]\n', 'path': 'data/repos/devopsarr/sonarr/integration/targets/sonarr_release_profile/tasks/main.yml', 'input': 'name: assert release profile idempotency'}

<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<FOR DEBUGGING PURPOSES ONLY!>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<FOR DEBUGGING PURPOSES ONLY!>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>{'download_count': '747', 'path': 'data/repos/buluma/packer_rhel/tasks/main.yml', 'input': 'name: ensure necessary packages are installed.', 'org_name': 'buluma', 'output': 'ansible.builtin.yum:\n  name:\n  - wget\n  - perl\n  - cpp\n  - gcc\n  - make\n  - bzip2\n  - kernel-headers\n  - kernel-devel\n  - libselinux-python\n  - elfutils-libelf-devel\n  - cifs-utils\n  state: present\n', 'license': 'Apache-2.0-Short,Apache-2.0', 'repo_name': 'packer_rhel', 'download_link': 'https://old-galaxy.ansible.com/buluma/packer_rhel'}

{'download_count': '34969', 'path': 'data/repos/robertdebock/locale/tasks/assert.yml', 'license': 'Apache-2.0-Short,Apache-2.0', 'repo_name': 'locale', 'org_name': 'robertdebock', 'download_link': 'https://old-galaxy.ansible.com/robertdebock/locale', 'input': 'name: assert | Test locale_lc_time', 'output': 'ansible.builtin.assert:\n  that:\n  - locale_lc_time is defined\n  - locale_lc_time is string\n  - locale_lc_time is not none\n  quiet: true\n'}
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<FOR DEBUGGING PURPOSES ONLY!>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>{'download_count': '9401', 'org_name': 'buluma', 'download_link': 'https://old-galaxy.ansible.com/buluma/anaconda', 'license': 'Apache-2.0-Short,Apache-2.0', 'repo_name': 'anaconda', 'output': 'ansible.builtin.service:\n  name: anaconda\n  state: started\n  enabled: true\n', 'path': 'data/repos/buluma/anaconda/tasks/main.yml', 'input': 'name: Start and enable anaconda'}
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<FOR DEBUGGING PURPOSES ONLY!>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>

<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<FOR DEBUGGING PURPOSES ONLY!>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>{'download_count': '4445', 'path': 'data/repos/darkwizard242/packer/tasks/install_debian.yml', 'input': 'name: Debian/Ubuntu Family | Install unzip if it is currently not in installed state', 'org_name': 'darkwizard242', 'output': 'ansible.builtin.apt:\n  name: unzip\n  state: present\n  force_apt_get: true\n  update_cache: true\n', 'license': 'MIT', 'repo_name': 'packer', 'download_link': 'https://old-galaxy.ansible.com/darkwizard242/packer'}

{'download_count': '513', 'path': 'data/repos/brentwg/visual-studio-code/tasks/Debian.yml', 'license': '', 'repo_name': 'visual-studio-code', 'org_name': 'brentwg', 'download_link': 'https://old-galaxy.ansible.com/brentwg/visual-studio-code', 'input': 'name: Create the APT repository (Debian)', 'output': 'apt_repository:\n  repo: deb [arch=amd64] https://packages.microsoft.com/repos/vscode stable main\n  filename: vscode\n'}
{'download_count': '1904', 'org_name': 'chrismeyersfsu', 'download_link': 'https://old-galaxy.ansible.com/chrismeyersfsu/ec2_server', 'license': '', 'repo_name': 'ec2_server', 'output': "add_host:\n  name: ''\n  groups: ec2_server_hosts\n  ansible_ssh_user: ''\nwith_items: ec2.instances\nwhen: ec2_server_pem_path == ''\n", 'path': 'data/repos/chrismeyersfsu/ec2_server/tasks/main.yml', 'input': 'name: Add hosts group temporary inventory group without pem path'}
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<FOR DEBUGGING PURPOSES ONLY!>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
{'download_count': '801', 'path': 'data/repos/mahdi22/mariadb_install/tasks/Debian-install.yml', 'license': '', 'repo_name': 'mariadb_install', 'org_name': 'mahdi22', 'download_link': 'https://old-galaxy.ansible.com/mahdi22/mariadb_install', 'input': 'name: Add MariaDB Repository Key for', 'output': 'apt_key:\n  url: https://mariadb.org/mariadb_release_signing_key.asc\n  state: present\nwhen: (use_proxy is not defined) or (not use_proxy)\n'}
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<FOR DEBUGGING PURPOSES ONLY!>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
{'download_count': '747', 'path': 'data/repos/buluma/packer_rhel/tasks/main.yml', 'license': 'Apache-2.0-Short,Apache-2.0', 'repo_name': 'packer_rhel', 'org_name': 'buluma', 'download_link': 'https://old-galaxy.ansible.com/buluma/packer_rhel', 'input': 'name: ensure necessary packages are installed.', 'output': 'ansible.builtin.yum:\n  name:\n  - wget\n  - perl\n  - cpp\n  - gcc\n  - make\n  - bzip2\n  - kernel-headers\n  - kernel-devel\n  - libselinux-python\n  - elfutils-libelf-devel\n  - cifs-utils\n  state: present\n'}
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<FOR DEBUGGING PURPOSES ONLY!>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
{'download_count': '4445', 'path': 'data/repos/darkwizard242/packer/tasks/install_debian.yml', 'license': 'MIT', 'repo_name': 'packer', 'org_name': 'darkwizard242', 'download_link': 'https://old-galaxy.ansible.com/darkwizard242/packer', 'input': 'name: Debian/Ubuntu Family | Install unzip if it is currently not in installed state', 'output': 'ansible.builtin.apt:\n  name: unzip\n  state: present\n  force_apt_get: true\n  update_cache: true\n'}
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<FOR DEBUGGING PURPOSES ONLY!>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
{'org_name': 'adriagalin', 'input': 'name: Delete /etc/motd file', 'download_count': '81335', 'repo_name': 'motd', 'path': 'data/repos/adriagalin/motd/tasks/main.yml', 'license': 'GPL-3.0-only', 'download_link': 'https://old-galaxy.ansible.com/adriagalin/motd', 'output': 'file:\n  path: /etc/motd\n  state: absent\nwhen: ag_motd_add_footer | bool\ntags:\n- motd\n- common\n'}
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<FOR DEBUGGING PURPOSES ONLY!>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>><<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<FOR DEBUGGING PURPOSES ONLY!>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>

{'org_name': 'siamaksade', 'input': 'name: check if gogs exists', 'download_count': '16571', 'repo_name': 'openshift_gogs', 'path': 'data/repos/siamaksade/openshift_gogs/tasks/main.yml', 'license': '', 'download_link': 'https://old-galaxy.ansible.com/siamaksade/openshift_gogs', 'output': 'shell: oc get service gogs -n gogs\nregister: install_gogs\nignore_errors: true\nchanged_when: false\n'}{'download_count': '561', 'org_name': 'lifeofguenter', 'download_link': 'https://old-galaxy.ansible.com/lifeofguenter/nginx', 'license': 'MIT', 'repo_name': 'nginx', 'output': 'ansible.builtin.command: make install\nargs:\n  chdir: /tmp/nginx-1.24.0\nchanged_when: true\nbecome: true\nnotify: Restart_nginx\n', 'path': 'data/repos/lifeofguenter/nginx/tasks/nginx.yml', 'input': 'name: Install'}

<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<FOR DEBUGGING PURPOSES ONLY!>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>><<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<FOR DEBUGGING PURPOSES ONLY!>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>

{'org_name': 'devopsarr', 'input': 'name: assert release profile idempotency', 'download_count': '688', 'repo_name': 'sonarr', 'path': 'data/repos/devopsarr/sonarr/integration/targets/sonarr_release_profile/tasks/main.yml', 'license': '', 'download_link': 'https://old-galaxy.ansible.com/devopsarr/sonarr', 'output': 'assert:\n  that:\n  - result.changed == false\n  - result.ignored == ["repack", "dvdrip"]\n'}{'download_count': '34969', 'org_name': 'robertdebock', 'download_link': 'https://old-galaxy.ansible.com/robertdebock/locale', 'license': 'Apache-2.0-Short,Apache-2.0', 'repo_name': 'locale', 'output': 'ansible.builtin.assert:\n  that:\n  - locale_lc_time is defined\n  - locale_lc_time is string\n  - locale_lc_time is not none\n  quiet: true\n', 'path': 'data/repos/robertdebock/locale/tasks/assert.yml', 'input': 'name: assert | Test locale_lc_time'}

<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<FOR DEBUGGING PURPOSES ONLY!>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>><<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<FOR DEBUGGING PURPOSES ONLY!>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>

{'download_count': '513', 'org_name': 'brentwg', 'download_link': 'https://old-galaxy.ansible.com/brentwg/visual-studio-code', 'license': '', 'repo_name': 'visual-studio-code', 'output': 'apt_repository:\n  repo: deb [arch=amd64] https://packages.microsoft.com/repos/vscode stable main\n  filename: vscode\n', 'path': 'data/repos/brentwg/visual-studio-code/tasks/Debian.yml', 'input': 'name: Create the APT repository (Debian)'}{'org_name': 'buluma', 'input': 'name: Start and enable anaconda', 'download_count': '9401', 'repo_name': 'anaconda', 'path': 'data/repos/buluma/anaconda/tasks/main.yml', 'license': 'Apache-2.0-Short,Apache-2.0', 'download_link': 'https://old-galaxy.ansible.com/buluma/anaconda', 'output': 'ansible.builtin.service:\n  name: anaconda\n  state: started\n  enabled: true\n'}

<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<FOR DEBUGGING PURPOSES ONLY!>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>><<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<FOR DEBUGGING PURPOSES ONLY!>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>

{'download_count': '801', 'org_name': 'mahdi22', 'download_link': 'https://old-galaxy.ansible.com/mahdi22/mariadb_install', 'license': '', 'repo_name': 'mariadb_install', 'output': 'apt_key:\n  url: https://mariadb.org/mariadb_release_signing_key.asc\n  state: present\nwhen: (use_proxy is not defined) or (not use_proxy)\n', 'path': 'data/repos/mahdi22/mariadb_install/tasks/Debian-install.yml', 'input': 'name: Add MariaDB Repository Key for'}{'org_name': 'chrismeyersfsu', 'input': 'name: Add hosts group temporary inventory group without pem path', 'download_count': '1904', 'repo_name': 'ec2_server', 'path': 'data/repos/chrismeyersfsu/ec2_server/tasks/main.yml', 'license': '', 'download_link': 'https://old-galaxy.ansible.com/chrismeyersfsu/ec2_server', 'output': "add_host:\n  name: ''\n  groups: ec2_server_hosts\n  ansible_ssh_user: ''\nwith_items: ec2.instances\nwhen: ec2_server_pem_path == ''\n"}

<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<FOR DEBUGGING PURPOSES ONLY!>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
{'download_count': '747', 'org_name': 'buluma', 'download_link': 'https://old-galaxy.ansible.com/buluma/packer_rhel', 'license': 'Apache-2.0-Short,Apache-2.0', 'repo_name': 'packer_rhel', 'output': 'ansible.builtin.yum:\n  name:\n  - wget\n  - perl\n  - cpp\n  - gcc\n  - make\n  - bzip2\n  - kernel-headers\n  - kernel-devel\n  - libselinux-python\n  - elfutils-libelf-devel\n  - cifs-utils\n  state: present\n', 'path': 'data/repos/buluma/packer_rhel/tasks/main.yml', 'input': 'name: ensure necessary packages are installed.'}
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<FOR DEBUGGING PURPOSES ONLY!>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
{'download_count': '4445', 'org_name': 'darkwizard242', 'download_link': 'https://old-galaxy.ansible.com/darkwizard242/packer', 'license': 'MIT', 'repo_name': 'packer', 'output': 'ansible.builtin.apt:\n  name: unzip\n  state: present\n  force_apt_get: true\n  update_cache: true\n', 'path': 'data/repos/darkwizard242/packer/tasks/install_debian.yml', 'input': 'name: Debian/Ubuntu Family | Install unzip if it is currently not in installed state'}
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<FOR DEBUGGING PURPOSES ONLY!>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
{'org_name': 'lifeofguenter', 'input': 'name: Install', 'download_count': '561', 'repo_name': 'nginx', 'path': 'data/repos/lifeofguenter/nginx/tasks/nginx.yml', 'license': 'MIT', 'download_link': 'https://old-galaxy.ansible.com/lifeofguenter/nginx', 'output': 'ansible.builtin.command: make install\nargs:\n  chdir: /tmp/nginx-1.24.0\nchanged_when: true\nbecome: true\nnotify: Restart_nginx\n'}
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<FOR DEBUGGING PURPOSES ONLY!>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
{'org_name': 'robertdebock', 'input': 'name: assert | Test locale_lc_time', 'download_count': '34969', 'repo_name': 'locale', 'path': 'data/repos/robertdebock/locale/tasks/assert.yml', 'license': 'Apache-2.0-Short,Apache-2.0', 'download_link': 'https://old-galaxy.ansible.com/robertdebock/locale', 'output': 'ansible.builtin.assert:\n  that:\n  - locale_lc_time is defined\n  - locale_lc_time is string\n  - locale_lc_time is not none\n  quiet: true\n'}
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<FOR DEBUGGING PURPOSES ONLY!>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
{'org_name': 'brentwg', 'input': 'name: Create the APT repository (Debian)', 'download_count': '513', 'repo_name': 'visual-studio-code', 'path': 'data/repos/brentwg/visual-studio-code/tasks/Debian.yml', 'license': '', 'download_link': 'https://old-galaxy.ansible.com/brentwg/visual-studio-code', 'output': 'apt_repository:\n  repo: deb [arch=amd64] https://packages.microsoft.com/repos/vscode stable main\n  filename: vscode\n'}
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<FOR DEBUGGING PURPOSES ONLY!>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
{'org_name': 'mahdi22', 'input': 'name: Add MariaDB Repository Key for', 'download_count': '801', 'repo_name': 'mariadb_install', 'path': 'data/repos/mahdi22/mariadb_install/tasks/Debian-install.yml', 'license': '', 'download_link': 'https://old-galaxy.ansible.com/mahdi22/mariadb_install', 'output': 'apt_key:\n  url: https://mariadb.org/mariadb_release_signing_key.asc\n  state: present\nwhen: (use_proxy is not defined) or (not use_proxy)\n'}
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<FOR DEBUGGING PURPOSES ONLY!>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
{'org_name': 'buluma', 'input': 'name: ensure necessary packages are installed.', 'download_count': '747', 'repo_name': 'packer_rhel', 'path': 'data/repos/buluma/packer_rhel/tasks/main.yml', 'license': 'Apache-2.0-Short,Apache-2.0', 'download_link': 'https://old-galaxy.ansible.com/buluma/packer_rhel', 'output': 'ansible.builtin.yum:\n  name:\n  - wget\n  - perl\n  - cpp\n  - gcc\n  - make\n  - bzip2\n  - kernel-headers\n  - kernel-devel\n  - libselinux-python\n  - elfutils-libelf-devel\n  - cifs-utils\n  state: present\n'}
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<FOR DEBUGGING PURPOSES ONLY!>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
{'org_name': 'darkwizard242', 'input': 'name: Debian/Ubuntu Family | Install unzip if it is currently not in installed state', 'download_count': '4445', 'repo_name': 'packer', 'path': 'data/repos/darkwizard242/packer/tasks/install_debian.yml', 'license': 'MIT', 'download_link': 'https://old-galaxy.ansible.com/darkwizard242/packer', 'output': 'ansible.builtin.apt:\n  name: unzip\n  state: present\n  force_apt_get: true\n  update_cache: true\n'}
  0%|          | 0/400 [00:00<?, ?it/s]  0%|          | 0/400 [00:00<?, ?it/s]  0%|          | 0/400 [00:00<?, ?it/s]  0%|          | 0/400 [00:00<?, ?it/s]  0%|          | 1/400 [00:00<02:46,  2.39it/s]  0%|          | 1/400 [00:00<02:46,  2.39it/s]  0%|          | 1/400 [00:00<02:46,  2.39it/s]  0%|          | 1/400 [00:00<02:50,  2.33it/s]100%|| 400/400 [00:00<00:00, 825.85it/s]100%|| 400/400 [00:00<00:00, 825.78it/s]

The character to token ratio of the dataset is: 3.18
The character to token ratio of the dataset is: 3.18
Loading the model
Loading the model
100%|| 400/400 [00:00<00:00, 810.62it/s]
100%|| 400/400 [00:00<00:00, 807.82it/s]
The character to token ratio of the dataset is: 3.18
Loading the model
The character to token ratio of the dataset is: 3.18
Loading the model
Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
/u/bzd2/.local/lib/python3.9/site-packages/transformers/models/auto/auto_factory.py:472: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.
  warnings.warn(
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-3b/snapshots/e1c5ef4ebb97afa0db09ec3e520f0487ca350bbe/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "bigcode/starcoderbase-3b",
  "activation_function": "gelu_pytorch_tanh",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 2816,
  "n_head": 22,
  "n_inner": 11264,
  "n_layer": 36,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.37.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

/u/bzd2/.local/lib/python3.9/site-packages/transformers/models/auto/auto_factory.py:472: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/transformers/models/auto/auto_factory.py:472: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/transformers/models/auto/auto_factory.py:472: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.
  warnings.warn(
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-3b/snapshots/e1c5ef4ebb97afa0db09ec3e520f0487ca350bbe/config.json
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-3b/snapshots/e1c5ef4ebb97afa0db09ec3e520f0487ca350bbe/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "bigcode/starcoderbase-3b",
  "activation_function": "gelu_pytorch_tanh",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 2816,
  "n_head": 22,
  "n_inner": 11264,
  "n_layer": 36,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.37.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

Model config GPTBigCodeConfig {
  "_name_or_path": "bigcode/starcoderbase-3b",
  "activation_function": "gelu_pytorch_tanh",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 2816,
  "n_head": 22,
  "n_inner": 11264,
  "n_layer": 36,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.37.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-3b/snapshots/e1c5ef4ebb97afa0db09ec3e520f0487ca350bbe/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "bigcode/starcoderbase-3b",
  "activation_function": "gelu_pytorch_tanh",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 2816,
  "n_head": 22,
  "n_inner": 11264,
  "n_layer": 36,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.37.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

Overriding torch_dtype=None with `torch_dtype=torch.float16` due to requirements of `bitsandbytes` to enable model loading in 8-bit or 4-bit. Pass your own torch_dtype to specify the dtype of the remaining non-linear layers or pass torch_dtype=torch.float16 to remove this warning.
Overriding torch_dtype=None with `torch_dtype=torch.float16` due to requirements of `bitsandbytes` to enable model loading in 8-bit or 4-bit. Pass your own torch_dtype to specify the dtype of the remaining non-linear layers or pass torch_dtype=torch.float16 to remove this warning.
Overriding torch_dtype=None with `torch_dtype=torch.float16` due to requirements of `bitsandbytes` to enable model loading in 8-bit or 4-bit. Pass your own torch_dtype to specify the dtype of the remaining non-linear layers or pass torch_dtype=torch.float16 to remove this warning.
Overriding torch_dtype=None with `torch_dtype=torch.float16` due to requirements of `bitsandbytes` to enable model loading in 8-bit or 4-bit. Pass your own torch_dtype to specify the dtype of the remaining non-linear layers or pass torch_dtype=torch.float16 to remove this warning.
loading weights file pytorch_model.bin from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-3b/snapshots/e1c5ef4ebb97afa0db09ec3e520f0487ca350bbe/pytorch_model.bin.index.json
loading weights file pytorch_model.bin from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-3b/snapshots/e1c5ef4ebb97afa0db09ec3e520f0487ca350bbe/pytorch_model.bin.index.json
loading weights file pytorch_model.bin from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-3b/snapshots/e1c5ef4ebb97afa0db09ec3e520f0487ca350bbe/pytorch_model.bin.index.json
loading weights file pytorch_model.bin from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-3b/snapshots/e1c5ef4ebb97afa0db09ec3e520f0487ca350bbe/pytorch_model.bin.index.json
Instantiating GPTBigCodeForCausalLM model under default dtype torch.float16.
Instantiating GPTBigCodeForCausalLM model under default dtype torch.float16.
Instantiating GPTBigCodeForCausalLM model under default dtype torch.float16.
Instantiating GPTBigCodeForCausalLM model under default dtype torch.float16.
Generate config GenerationConfig {
  "bos_token_id": 0,
  "eos_token_id": 0
}

Generate config GenerationConfig {
  "bos_token_id": 0,
  "eos_token_id": 0
}

Generate config GenerationConfig {
  "bos_token_id": 0,
  "eos_token_id": 0
}

Generate config GenerationConfig {
  "bos_token_id": 0,
  "eos_token_id": 0
}

huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Detected 8-bit loading: activating 8-bit loading for this model
Detected 8-bit loading: activating 8-bit loading for this model
Detected 8-bit loading: activating 8-bit loading for this model
Detected 8-bit loading: activating 8-bit loading for this model
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|     | 1/2 [00:14<00:14, 14.17s/it]Loading checkpoint shards:  50%|     | 1/2 [00:14<00:14, 14.19s/it]Loading checkpoint shards:  50%|     | 1/2 [00:14<00:14, 14.20s/it]Loading checkpoint shards:  50%|     | 1/2 [00:14<00:14, 14.20s/it]Loading checkpoint shards: 100%|| 2/2 [00:17<00:00,  7.77s/it]Loading checkpoint shards: 100%|| 2/2 [00:17<00:00,  8.73s/it]
All model checkpoint weights were used when initializing GPTBigCodeForCausalLM.

All the weights of GPTBigCodeForCausalLM were initialized from the model checkpoint at bigcode/starcoderbase-3b.
If your task is similar to the task the model of the checkpoint was trained on, you can already use GPTBigCodeForCausalLM for predictions without further training.
Loading checkpoint shards: 100%|| 2/2 [00:17<00:00,  7.77s/it]Loading checkpoint shards: 100%|| 2/2 [00:17<00:00,  8.73s/it]
All model checkpoint weights were used when initializing GPTBigCodeForCausalLM.

Loading checkpoint shards: 100%|| 2/2 [00:17<00:00,  7.77s/it]All the weights of GPTBigCodeForCausalLM were initialized from the model checkpoint at bigcode/starcoderbase-3b.
If your task is similar to the task the model of the checkpoint was trained on, you can already use GPTBigCodeForCausalLM for predictions without further training.
Loading checkpoint shards: 100%|| 2/2 [00:17<00:00,  8.73s/it]
All model checkpoint weights were used when initializing GPTBigCodeForCausalLM.

All the weights of GPTBigCodeForCausalLM were initialized from the model checkpoint at bigcode/starcoderbase-3b.
If your task is similar to the task the model of the checkpoint was trained on, you can already use GPTBigCodeForCausalLM for predictions without further training.
Loading checkpoint shards: 100%|| 2/2 [00:17<00:00,  7.77s/it]Loading checkpoint shards: 100%|| 2/2 [00:17<00:00,  8.73s/it]
All model checkpoint weights were used when initializing GPTBigCodeForCausalLM.

All the weights of GPTBigCodeForCausalLM were initialized from the model checkpoint at bigcode/starcoderbase-3b.
If your task is similar to the task the model of the checkpoint was trained on, you can already use GPTBigCodeForCausalLM for predictions without further training.
loading configuration file generation_config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-3b/snapshots/e1c5ef4ebb97afa0db09ec3e520f0487ca350bbe/generation_config.json
loading configuration file generation_config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-3b/snapshots/e1c5ef4ebb97afa0db09ec3e520f0487ca350bbe/generation_config.json
loading configuration file generation_config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-3b/snapshots/e1c5ef4ebb97afa0db09ec3e520f0487ca350bbe/generation_config.json
loading configuration file generation_config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-3b/snapshots/e1c5ef4ebb97afa0db09ec3e520f0487ca350bbe/generation_config.json
Generate config GenerationConfig {
  "bos_token_id": 0,
  "eos_token_id": 0
}

Generate config GenerationConfig {
  "bos_token_id": 0,
  "eos_token_id": 0
}

Generate config GenerationConfig {
  "bos_token_id": 0,
  "eos_token_id": 0
}

Generate config GenerationConfig {
  "bos_token_id": 0,
  "eos_token_id": 0
}

/u/bzd2/.local/lib/python3.9/site-packages/peft/utils/other.py:141: FutureWarning: prepare_model_for_int8_training is deprecated and will be removed in a future version. Use prepare_model_for_kbit_training instead.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/peft/utils/other.py:141: FutureWarning: prepare_model_for_int8_training is deprecated and will be removed in a future version. Use prepare_model_for_kbit_training instead.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/peft/utils/other.py:141: FutureWarning: prepare_model_for_int8_training is deprecated and will be removed in a future version. Use prepare_model_for_kbit_training instead.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/peft/utils/other.py:141: FutureWarning: prepare_model_for_int8_training is deprecated and will be removed in a future version. Use prepare_model_for_kbit_training instead.
  warnings.warn(
trainable params: 14745600 || all params: 3058056704 || trainable%: 0.482188573570675
Starting main loop
PyTorch: setting up devices
trainable params: 14745600 || all params: 3058056704 || trainable%: 0.482188573570675
Starting main loop
trainable params: 14745600 || all params: 3058056704 || trainable%: 0.482188573570675
Starting main loop
PyTorch: setting up devices
PyTorch: setting up devices
trainable params: 14745600 || all params: 3058056704 || trainable%: 0.482188573570675
Starting main loop
Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
PyTorch: setting up devices
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Training...
Training...Training...

max_steps is given, it will override any value given in num_train_epochs
Using auto half precision backend
Training...
Currently training with a batch size of: 1
***** Running training *****
  Num examples = 208,000
  Num Epochs = 9,223,372,036,854,775,807
  Instantaneous batch size per device = 1
  Total train batch size (w. parallel, distributed & accumulation) = 64
  Gradient Accumulation steps = 16
  Total optimization steps = 3,250
  Number of trainable parameters = 14,745,600
Automatic Weights & Biases logging enabled, to disable set os.environ["WANDB_DISABLED"] = "true"
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
wandb: Currently logged in as: hetarthvader (complex_dnn). Use `wandb login --relogin` to force relogin
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
wandb: Tracking run with wandb version 0.16.1
wandb: Run data is saved locally in /projects/bbvz/bzd2/wandb/run-20231213_191346-nctaka3q
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run FinalRuns-/projects/bbvz/bzd2/checkpoints_experiment_10
wandb:  View project at https://wandb.ai/complex_dnn/huggingface
wandb:  View run at https://wandb.ai/complex_dnn/huggingface/runs/nctaka3q
  0%|          | 0/3250 [00:00<?, ?it/s]/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 2.0072, 'learning_rate': 5e-05, 'epoch': 0.0}
{'loss': 2.0776, 'learning_rate': 0.0001, 'epoch': 0.0}
{'loss': 1.9282, 'learning_rate': 9.999997661121407e-05, 'epoch': 0.0}
{'loss': 1.9373, 'learning_rate': 9.999990644487813e-05, 'epoch': 0.0}
{'loss': 1.8533, 'learning_rate': 9.999978950105786e-05, 'epoch': 0.0}
{'loss': 1.8856, 'learning_rate': 9.999962577986265e-05, 'epoch': 0.0}
  0%|          | 1/3250 [00:15<13:34:10, 15.04s/it]                                                     0%|          | 1/3250 [00:15<13:34:10, 15.04s/it]  0%|          | 2/3250 [00:25<11:08:51, 12.36s/it]                                                     0%|          | 2/3250 [00:25<11:08:51, 12.36s/it]  0%|          | 3/3250 [00:35<10:18:29, 11.43s/it]                                                     0%|          | 3/3250 [00:35<10:18:29, 11.43s/it]  0%|          | 4/3250 [00:46<9:55:04, 11.00s/it]                                                     0%|          | 4/3250 [00:46<9:55:04, 11.00s/it]  0%|          | 5/3250 [00:56<9:42:07, 10.76s/it]                                                    0%|          | 5/3250 [00:56<9:42:07, 10.76s/it]  0%|          | 6/3250 [01:06<9:34:18, 10.62s/it]                                                    0%|          | 6/3250 [01:06<9:34:18, 10.62s/it]  0%|          | 7/3250 [01:17<9:29:01, 10.53s/it]                                      {'loss': 2.103, 'learning_rate': 9.999941528144567e-05, 'epoch': 0.0}
{'loss': 1.7569, 'learning_rate': 9.999915800600383e-05, 'epoch': 0.0}
{'loss': 1.715, 'learning_rate': 9.999885395377788e-05, 'epoch': 0.0}
{'loss': 1.7241, 'learning_rate': 9.999850312505221e-05, 'epoch': 0.0}
              0%|          | 7/3250 [01:17<9:29:01, 10.53s/it]  0%|          | 8/3250 [01:27<9:24:50, 10.45s/it]                                                    0%|          | 8/3250 [01:27<9:24:50, 10.45s/it]  0%|          | 9/3250 [01:37<9:22:08, 10.41s/it]                                                    0%|          | 9/3250 [01:37<9:22:08, 10.41s/it]  0%|          | 10/3250 [01:48<9:20:27, 10.38s/it]                                                     0%|          | 10/3250 [01:48<9:20:27, 10.38s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 1.560722827911377, 'eval_runtime': 2.1758, 'eval_samples_per_second': 5.515, 'eval_steps_per_second': 1.379, 'epoch': 0.0}
                                                     0%|          | 10/3250 [01:50<9:20:27, 10.38s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_10/tmp-checkpoint-10
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-10
 /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-10
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-10
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-10
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-10/pytorch_model.bin
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-10/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-10/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-10/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 1.6507, 'learning_rate': 9.99981055201551e-05, 'epoch': 0.0}
{'loss': 1.5422, 'learning_rate': 9.999766113945847e-05, 'epoch': 0.0}
{'loss': 1.6285, 'learning_rate': 9.999716998337812e-05, 'epoch': 0.0}
{'loss': 1.6031, 'learning_rate': 9.99966320523735e-05, 'epoch': 0.0}
{'loss': 1.5085, 'learning_rate': 9.999604734694792e-05, 'epoch': 0.0}
{'loss': 1.4902, 'learning_rate': 9.999541586764836e-05, 'epoch': 0.0}
  0%|          | 11/3250 [02:01<10:11:32, 11.33s/it]                                                      0%|          | 11/3250 [02:01<10:11:32, 11.33s/it]  0%|          | 12/3250 [02:11<9:54:31, 11.02s/it]                                                      0%|          | 12/3250 [02:11<9:54:31, 11.02s/it]  0%|          | 13/3250 [02:22<9:43:03, 10.81s/it]                                                     0%|          | 13/3250 [02:22<9:43:03, 10.81s/it]  0%|          | 14/3250 [02:32<9:34:57, 10.66s/it]                                                     0%|          | 14/3250 [02:32<9:34:57, 10.66s/it]  0%|          | 15/3250 [02:42<9:29:26, 10.56s/it]                                                     0%|          | 15/3250 [02:42<9:29:26, 10.56s/it]  0%|          | 16/3250 [02:53<9:25:45, 10.50s/it]                                                     0%|          | 16/3250 [02:53<9:25:45, 10.50s/it]  1%|          | 17/3250 [03:03<9:29:10, 10.56s/it]                         {'loss': 1.3895, 'learning_rate': 9.999473761506563e-05, 'epoch': 0.01}
{'loss': 1.3985, 'learning_rate': 9.999401258983425e-05, 'epoch': 0.01}
{'loss': 1.3924, 'learning_rate': 9.999324079263253e-05, 'epoch': 0.01}
{'loss': 1.3765, 'learning_rate': 9.999242222418252e-05, 'epoch': 0.01}
                            1%|          | 17/3250 [03:03<9:29:10, 10.56s/it]  1%|          | 18/3250 [03:14<9:29:33, 10.57s/it]                                                     1%|          | 18/3250 [03:14<9:29:33, 10.57s/it]  1%|          | 19/3250 [03:24<9:25:51, 10.51s/it]                                                     1%|          | 19/3250 [03:24<9:25:51, 10.51s/it]  1%|          | 20/3250 [03:35<9:23:19, 10.46s/it]                                                     1%|          | 20/3250 [03:35<9:23:19, 10.46s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 1.300174355506897, 'eval_runtime': 2.1229, 'eval_samples_per_second': 5.653, 'eval_steps_per_second': 1.413, 'epoch': 0.01}
                                                     1%|          | 20/3250 [03:37<9:23:19, 10.46s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_10/tmp-checkpoint-20
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-20I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-20

I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-20
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-20
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-20/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-20/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-20/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-20/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 1.3593, 'learning_rate': 9.999155688525004e-05, 'epoch': 0.01}
{'loss': 1.377, 'learning_rate': 9.999064477664466e-05, 'epoch': 0.01}
{'loss': 1.3446, 'learning_rate': 9.998968589921969e-05, 'epoch': 0.01}
{'loss': 1.3314, 'learning_rate': 9.998868025387223e-05, 'epoch': 0.01}
{'loss': 1.3173, 'learning_rate': 9.998762784154308e-05, 'epoch': 0.01}
{'loss': 1.2903, 'learning_rate': 9.998652866321687e-05, 'epoch': 0.01}
  1%|          | 21/3250 [03:48<10:04:48, 11.24s/it]                                                      1%|          | 21/3250 [03:48<10:04:48, 11.24s/it]  1%|          | 22/3250 [03:58<9:51:10, 10.99s/it]                                                      1%|          | 22/3250 [03:58<9:51:10, 10.99s/it]  1%|          | 23/3250 [04:09<9:40:57, 10.80s/it]                                                     1%|          | 23/3250 [04:09<9:40:57, 10.80s/it]  1%|          | 24/3250 [04:19<9:33:42, 10.67s/it]                                                     1%|          | 24/3250 [04:19<9:33:42, 10.67s/it]  1%|          | 25/3250 [04:29<9:28:36, 10.58s/it]                                                     1%|          | 25/3250 [04:29<9:28:36, 10.58s/it]  1%|          | 26/3250 [04:40<9:25:12, 10.52s/it]                                                     1%|          | 26/3250 [04:40<9:25:12, 10.52s/it]  1%|          | 27/3250 [04:50<9:22:44, 10.48s/it]                         {'loss': 1.3078, 'learning_rate': 9.99853827199219e-05, 'epoch': 0.01}
{'loss': 1.2938, 'learning_rate': 9.998419001273029e-05, 'epoch': 0.01}
{'loss': 1.3189, 'learning_rate': 9.998295054275786e-05, 'epoch': 0.01}
{'loss': 1.3285, 'learning_rate': 9.99816643111642e-05, 'epoch': 0.01}
                            1%|          | 27/3250 [04:50<9:22:44, 10.48s/it]  1%|          | 28/3250 [05:00<9:21:49, 10.46s/it]                                                     1%|          | 28/3250 [05:00<9:21:49, 10.46s/it]  1%|          | 29/3250 [05:11<9:20:17, 10.44s/it]                                                     1%|          | 29/3250 [05:11<9:20:17, 10.44s/it]  1%|          | 30/3250 [05:21<9:19:13, 10.42s/it]                                                     1%|          | 30/3250 [05:21<9:19:13, 10.42s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 1.2214070558547974, 'eval_runtime': 2.1276, 'eval_samples_per_second': 5.64, 'eval_steps_per_second': 1.41, 'epoch': 0.01}
                                                     1%|          | 30/3250 [05:23<9:19:13, 10.42s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_10/tmp-checkpoint-30
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-30
/projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-30
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-30
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-30
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-30/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-30/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-30/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-30/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 1.2611, 'learning_rate': 9.998033131915266e-05, 'epoch': 0.01}
{'loss': 1.272, 'learning_rate': 9.997895156797028e-05, 'epoch': 0.01}
{'loss': 1.2476, 'learning_rate': 9.997752505890794e-05, 'epoch': 0.01}
{'loss': 1.2341, 'learning_rate': 9.997605179330019e-05, 'epoch': 0.01}
{'loss': 1.2475, 'learning_rate': 9.997453177252534e-05, 'epoch': 0.01}
{'loss': 1.2741, 'learning_rate': 9.997296499800545e-05, 'epoch': 0.01}
  1%|          | 31/3250 [05:34<10:03:25, 11.25s/it]                                                      1%|          | 31/3250 [05:34<10:03:25, 11.25s/it]  1%|          | 32/3250 [05:45<9:49:19, 10.99s/it]                                                      1%|          | 32/3250 [05:45<9:49:19, 10.99s/it]  1%|          | 33/3250 [05:55<9:43:28, 10.88s/it]                                                     1%|          | 33/3250 [05:55<9:43:28, 10.88s/it]  1%|          | 34/3250 [06:06<9:35:25, 10.74s/it]                                                     1%|          | 34/3250 [06:06<9:35:25, 10.74s/it]  1%|          | 35/3250 [06:16<9:29:41, 10.63s/it]                                                     1%|          | 35/3250 [06:16<9:29:41, 10.63s/it]  1%|          | 36/3250 [06:27<9:25:54, 10.56s/it]                                                     1%|          | 36/3250 [06:27<9:25:54, 10.56s/it]  1%|          | 37/3250 [06:37<9:22:54, 10.51s/it]                         {'loss': 1.6839, 'learning_rate': 9.997135147120633e-05, 'epoch': 0.01}
{'loss': 1.1441, 'learning_rate': 9.99696911936375e-05, 'epoch': 0.01}
{'loss': 1.2219, 'learning_rate': 9.996798416685228e-05, 'epoch': 0.01}
{'loss': 1.289, 'learning_rate': 9.99662303924476e-05, 'epoch': 0.01}
                            1%|          | 37/3250 [06:37<9:22:54, 10.51s/it]  1%|          | 38/3250 [06:47<9:20:56, 10.48s/it]                                                     1%|          | 38/3250 [06:47<9:20:56, 10.48s/it]  1%|          | 39/3250 [06:58<9:19:34, 10.46s/it]                                                     1%|          | 39/3250 [06:58<9:19:34, 10.46s/it]  1%|          | 40/3250 [07:08<9:18:39, 10.44s/it]                                                     1%|          | 40/3250 [07:08<9:18:39, 10.44s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 1.1744650602340698, 'eval_runtime': 2.1297, 'eval_samples_per_second': 5.634, 'eval_steps_per_second': 1.409, 'epoch': 0.01}
                                                     1%|          | 40/3250 [07:10<9:18:39, 10.44s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_10/tmp-checkpoint-40
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-40I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-40

I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-40
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-40
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-40/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-40/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-40/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-40/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 1.2277, 'learning_rate': 9.996442987206428e-05, 'epoch': 0.01}
{'loss': 1.2134, 'learning_rate': 9.996258260738676e-05, 'epoch': 0.01}
{'loss': 1.2323, 'learning_rate': 9.996068860014325e-05, 'epoch': 0.01}
{'loss': 1.2637, 'learning_rate': 9.995874785210573e-05, 'epoch': 0.01}
{'loss': 1.2504, 'learning_rate': 9.995676036508982e-05, 'epoch': 0.01}
{'loss': 1.2001, 'learning_rate': 9.995472614095495e-05, 'epoch': 0.01}
  1%|         | 41/3250 [07:21<9:58:27, 11.19s/it]                                                     1%|         | 41/3250 [07:21<9:58:27, 11.19s/it]  1%|         | 42/3250 [07:32<9:45:35, 10.95s/it]                                                     1%|         | 42/3250 [07:32<9:45:35, 10.95s/it]  1%|         | 43/3250 [07:42<9:36:30, 10.79s/it]                                                     1%|         | 43/3250 [07:42<9:36:30, 10.79s/it]  1%|         | 44/3250 [07:52<9:30:05, 10.67s/it]                                                     1%|         | 44/3250 [07:52<9:30:05, 10.67s/it]  1%|         | 45/3250 [08:03<9:25:52, 10.59s/it]                                                     1%|         | 45/3250 [08:03<9:25:52, 10.59s/it]  1%|         | 46/3250 [08:13<9:22:57, 10.54s/it]                                                     1%|         | 46/3250 [08:13<9:22:57, 10.54s/it]  1%|         | 47/3250 [08:24<9:20:36, 10.50s/it]   {'loss': 1.1548, 'learning_rate': 9.995264518160425e-05, 'epoch': 0.01}
{'loss': 1.2101, 'learning_rate': 9.995051748898453e-05, 'epoch': 0.01}
{'loss': 1.1866, 'learning_rate': 9.994834306508638e-05, 'epoch': 0.02}
{'loss': 1.1943, 'learning_rate': 9.994612191194406e-05, 'epoch': 0.02}
                                                  1%|         | 47/3250 [08:24<9:20:36, 10.50s/it]  1%|         | 48/3250 [08:34<9:19:14, 10.48s/it]                                                     1%|         | 48/3250 [08:34<9:19:14, 10.48s/it]  2%|         | 49/3250 [08:44<9:17:52, 10.46s/it]                                                     2%|         | 49/3250 [08:44<9:17:52, 10.46s/it]  2%|         | 50/3250 [08:55<9:26:49, 10.63s/it]                                                     2%|         | 50/3250 [08:55<9:26:49, 10.63s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 1.1414331197738647, 'eval_runtime': 2.1206, 'eval_samples_per_second': 5.659, 'eval_steps_per_second': 1.415, 'epoch': 0.02}
                                                     2%|         | 50/3250 [08:58<9:26:49, 10.63s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_10/tmp-checkpoint-50
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-50I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-50

I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-50
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-50
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-50/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-50/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-50/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-50/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 1.1739, 'learning_rate': 9.99438540316356e-05, 'epoch': 0.02}
{'loss': 1.1823, 'learning_rate': 9.99415394262827e-05, 'epoch': 0.02}
{'loss': 1.185, 'learning_rate': 9.993917809805081e-05, 'epoch': 0.02}
{'loss': 1.1899, 'learning_rate': 9.993677004914907e-05, 'epoch': 0.02}
{'loss': 1.1792, 'learning_rate': 9.993431528183032e-05, 'epoch': 0.02}
{'loss': 1.1579, 'learning_rate': 9.993181379839113e-05, 'epoch': 0.02}
  2%|         | 51/3250 [09:08<10:04:08, 11.33s/it]                                                      2%|         | 51/3250 [09:08<10:04:08, 11.33s/it]  2%|         | 52/3250 [09:19<9:49:04, 11.05s/it]                                                      2%|         | 52/3250 [09:19<9:49:04, 11.05s/it]  2%|         | 53/3250 [09:29<9:38:14, 10.85s/it]                                                     2%|         | 53/3250 [09:29<9:38:14, 10.85s/it]  2%|         | 54/3250 [09:40<9:32:05, 10.74s/it]                                                     2%|         | 54/3250 [09:40<9:32:05, 10.74s/it]  2%|         | 55/3250 [09:50<9:26:14, 10.63s/it]                                                     2%|         | 55/3250 [09:50<9:26:14, 10.63s/it]  2%|         | 56/3250 [10:00<9:21:53, 10.56s/it]                                                     2%|         | 56/3250 [10:00<9:21:53, 10.56s/it]  2%|         | 57/3250 [10:11<9:19:02, 10.51s/it]{'loss': 1.1573, 'learning_rate': 9.992926560117176e-05, 'epoch': 0.02}
{'loss': 1.1989, 'learning_rate': 9.992667069255619e-05, 'epoch': 0.02}
{'loss': 1.1545, 'learning_rate': 9.992402907497209e-05, 'epoch': 0.02}
{'loss': 1.2325, 'learning_rate': 9.992134075089084e-05, 'epoch': 0.02}
                                                     2%|         | 57/3250 [10:11<9:19:02, 10.51s/it]  2%|         | 58/3250 [10:21<9:16:50, 10.47s/it]                                                     2%|         | 58/3250 [10:21<9:16:50, 10.47s/it]  2%|         | 59/3250 [10:32<9:15:31, 10.45s/it]                                                     2%|         | 59/3250 [10:32<9:15:31, 10.45s/it]  2%|         | 60/3250 [10:42<9:14:33, 10.43s/it]                                                     2%|         | 60/3250 [10:42<9:14:33, 10.43s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 1.117668628692627, 'eval_runtime': 2.1266, 'eval_samples_per_second': 5.643, 'eval_steps_per_second': 1.411, 'epoch': 0.02}
                                                     2%|         | 60/3250 [10:44<9:14:33, 10.43s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_10/tmp-checkpoint-60
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-60I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-60

I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-60
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-60
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-60/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-60/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-60/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-60/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 1.1439, 'learning_rate': 9.991860572282747e-05, 'epoch': 0.02}
{'loss': 1.1841, 'learning_rate': 9.991582399334076e-05, 'epoch': 0.02}
{'loss': 1.1565, 'learning_rate': 9.991299556503318e-05, 'epoch': 0.02}
{'loss': 1.142, 'learning_rate': 9.991012044055084e-05, 'epoch': 0.02}
{'loss': 1.1417, 'learning_rate': 9.990719862258358e-05, 'epoch': 0.02}
{'loss': 1.1507, 'learning_rate': 9.990423011386489e-05, 'epoch': 0.02}
  2%|         | 61/3250 [10:55<9:54:36, 11.19s/it]                                                     2%|         | 61/3250 [10:55<9:54:36, 11.19s/it]  2%|         | 62/3250 [11:05<9:41:34, 10.95s/it]                                                     2%|         | 62/3250 [11:05<9:41:34, 10.95s/it]  2%|         | 63/3250 [11:16<9:32:35, 10.78s/it]                                                     2%|         | 63/3250 [11:16<9:32:35, 10.78s/it]  2%|         | 64/3250 [11:26<9:26:21, 10.67s/it]                                                     2%|         | 64/3250 [11:26<9:26:21, 10.67s/it]  2%|         | 65/3250 [11:37<9:21:59, 10.59s/it]                                                     2%|         | 65/3250 [11:37<9:21:59, 10.59s/it]  2%|         | 66/3250 [11:47<9:23:09, 10.61s/it]                                                     2%|         | 66/3250 [11:47<9:23:09, 10.61s/it]  2%|         | 67/3250 [11:58<9:19:23, 10.54s/it]   {'loss': 1.1265, 'learning_rate': 9.990121491717201e-05, 'epoch': 0.02}
{'loss': 1.5332, 'learning_rate': 9.989815303532577e-05, 'epoch': 0.02}
{'loss': 1.1353, 'learning_rate': 9.989504447119073e-05, 'epoch': 0.02}
{'loss': 1.1839, 'learning_rate': 9.989188922767512e-05, 'epoch': 0.02}
                                                  2%|         | 67/3250 [11:58<9:19:23, 10.54s/it]  2%|         | 68/3250 [12:08<9:16:35, 10.50s/it]                                                     2%|         | 68/3250 [12:08<9:16:35, 10.50s/it]  2%|         | 69/3250 [12:18<9:14:39, 10.46s/it]                                                     2%|         | 69/3250 [12:18<9:14:39, 10.46s/it]  2%|         | 70/3250 [12:29<9:12:59, 10.43s/it]                                                     2%|         | 70/3250 [12:29<9:12:59, 10.43s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 1.0973668098449707, 'eval_runtime': 2.1273, 'eval_samples_per_second': 5.641, 'eval_steps_per_second': 1.41, 'epoch': 0.02}
                                                     2%|         | 70/3250 [12:31<9:12:59, 10.43s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_10/tmp-checkpoint-70
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-70
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-70
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-70
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-70
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-70/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-70/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-70/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-70/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 1.1518, 'learning_rate': 9.988868730773082e-05, 'epoch': 0.02}
{'loss': 1.1392, 'learning_rate': 9.98854387143534e-05, 'epoch': 0.02}
{'loss': 1.1153, 'learning_rate': 9.988214345058209e-05, 'epoch': 0.02}
{'loss': 1.181, 'learning_rate': 9.987880151949976e-05, 'epoch': 0.02}
{'loss': 1.162, 'learning_rate': 9.987541292423298e-05, 'epoch': 0.02}
{'loss': 1.1263, 'learning_rate': 9.987197766795193e-05, 'epoch': 0.02}
  2%|         | 71/3250 [12:42<9:55:11, 11.23s/it]                                                     2%|         | 71/3250 [12:42<9:55:11, 11.23s/it]  2%|         | 72/3250 [12:52<9:42:25, 11.00s/it]                                                     2%|         | 72/3250 [12:52<9:42:25, 11.00s/it]  2%|         | 73/3250 [13:03<9:33:11, 10.83s/it]                                                     2%|         | 73/3250 [13:03<9:33:11, 10.83s/it]  2%|         | 74/3250 [13:13<9:26:46, 10.71s/it]                                                     2%|         | 74/3250 [13:13<9:26:46, 10.71s/it]  2%|         | 75/3250 [13:24<9:22:17, 10.63s/it]                                                     2%|         | 75/3250 [13:24<9:22:17, 10.63s/it]  2%|         | 76/3250 [13:34<9:19:15, 10.57s/it]                                                     2%|         | 76/3250 [13:34<9:19:15, 10.57s/it]  2%|         | 77/3250 [13:44<9:16:52, 10.53s/it]   {'loss': 1.1102, 'learning_rate': 9.986849575387049e-05, 'epoch': 0.02}
{'loss': 1.1223, 'learning_rate': 9.986496718524616e-05, 'epoch': 0.02}
{'loss': 1.113, 'learning_rate': 9.986139196538011e-05, 'epoch': 0.02}
{'loss': 1.1246, 'learning_rate': 9.985777009761713e-05, 'epoch': 0.02}
                                                  2%|         | 77/3250 [13:44<9:16:52, 10.53s/it]  2%|         | 78/3250 [13:55<9:15:27, 10.51s/it]                                                     2%|         | 78/3250 [13:55<9:15:27, 10.51s/it]  2%|         | 79/3250 [14:05<9:13:41, 10.48s/it]                                                     2%|         | 79/3250 [14:05<9:13:41, 10.48s/it]  2%|         | 80/3250 [14:16<9:12:08, 10.45s/it]                                                     2%|         | 80/3250 [14:16<9:12:08, 10.45s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 1.08137845993042, 'eval_runtime': 2.1263, 'eval_samples_per_second': 5.644, 'eval_steps_per_second': 1.411, 'epoch': 0.02}
                                                     2%|         | 80/3250 [14:18<9:12:08, 10.45s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_10/tmp-checkpoint-80
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  I AM HERE AND I AM SAVING THE MODEL
/projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-80the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-80

I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-80
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-80
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-80/pytorch_model.bin
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-80/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-80/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-80/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 1.1076, 'learning_rate': 9.985410158534567e-05, 'epoch': 0.02}
{'loss': 1.1076, 'learning_rate': 9.98503864319978e-05, 'epoch': 0.03}
{'loss': 1.1017, 'learning_rate': 9.984662464104926e-05, 'epoch': 0.03}
{'loss': 1.1207, 'learning_rate': 9.984281621601938e-05, 'epoch': 0.03}
{'loss': 1.1187, 'learning_rate': 9.983896116047113e-05, 'epoch': 0.03}
{'loss': 1.09, 'learning_rate': 9.983505947801115e-05, 'epoch': 0.03}
  2%|         | 81/3250 [14:29<9:51:15, 11.19s/it]                                                     2%|         | 81/3250 [14:29<9:51:15, 11.19s/it]  3%|         | 82/3250 [14:40<9:48:07, 11.14s/it]                                                     3%|         | 82/3250 [14:40<9:48:07, 11.14s/it]  3%|         | 83/3250 [14:50<9:36:06, 10.91s/it]                                                     3%|         | 83/3250 [14:50<9:36:06, 10.91s/it]  3%|         | 84/3250 [15:00<9:27:43, 10.76s/it]                                                     3%|         | 84/3250 [15:00<9:27:43, 10.76s/it]  3%|         | 85/3250 [15:11<9:21:24, 10.64s/it]                                                     3%|         | 85/3250 [15:11<9:21:24, 10.64s/it]  3%|         | 86/3250 [15:21<9:17:06, 10.56s/it]                                                     3%|         | 86/3250 [15:21<9:17:06, 10.56s/it]  3%|         | 87/3250 [15:32<9:14:07, 10.51s/it]   {'loss': 1.1191, 'learning_rate': 9.983111117228961e-05, 'epoch': 0.03}
{'loss': 1.1133, 'learning_rate': 9.982711624700041e-05, 'epoch': 0.03}
{'loss': 1.1095, 'learning_rate': 9.982307470588098e-05, 'epoch': 0.03}
{'loss': 1.1633, 'learning_rate': 9.981898655271235e-05, 'epoch': 0.03}
                                                  3%|         | 87/3250 [15:32<9:14:07, 10.51s/it]  3%|         | 88/3250 [15:42<9:11:40, 10.47s/it]                                                     3%|         | 88/3250 [15:42<9:11:40, 10.47s/it]  3%|         | 89/3250 [15:52<9:10:07, 10.44s/it]                                                     3%|         | 89/3250 [15:52<9:10:07, 10.44s/it]  3%|         | 90/3250 [16:03<9:10:08, 10.45s/it]                                                     3%|         | 90/3250 [16:03<9:10:08, 10.45s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 1.0651427507400513, 'eval_runtime': 2.1932, 'eval_samples_per_second': 5.471, 'eval_steps_per_second': 1.368, 'epoch': 0.03}
                                                     3%|         | 90/3250 [16:05<9:10:08, 10.45s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_10/tmp-checkpoint-90
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in I AM HERE AND I AM SAVING THE MODEL 
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-90
/projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-90
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-90
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-90
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-90/pytorch_model.bin
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-90/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-90/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-90/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 1.1211, 'learning_rate': 9.981485179131929e-05, 'epoch': 0.03}
{'loss': 1.1024, 'learning_rate': 9.981067042557e-05, 'epoch': 0.03}
{'loss': 1.1151, 'learning_rate': 9.980644245937639e-05, 'epoch': 0.03}
{'loss': 1.0499, 'learning_rate': 9.980216789669395e-05, 'epoch': 0.03}
{'loss': 1.1149, 'learning_rate': 9.979784674152175e-05, 'epoch': 0.03}
{'loss': 1.0737, 'learning_rate': 9.979347899790246e-05, 'epoch': 0.03}
  3%|         | 91/3250 [16:16<9:51:29, 11.23s/it]                                                     3%|         | 91/3250 [16:16<9:51:29, 11.23s/it]  3%|         | 92/3250 [16:26<9:37:42, 10.98s/it]                                                     3%|         | 92/3250 [16:26<9:37:42, 10.98s/it]  3%|         | 93/3250 [16:37<9:28:37, 10.81s/it]                                                     3%|         | 93/3250 [16:37<9:28:37, 10.81s/it]  3%|         | 94/3250 [16:47<9:22:18, 10.69s/it]                                                     3%|         | 94/3250 [16:47<9:22:18, 10.69s/it]  3%|         | 95/3250 [16:57<9:17:45, 10.61s/it]                                                     3%|         | 95/3250 [16:57<9:17:45, 10.61s/it]  3%|         | 96/3250 [17:08<9:14:28, 10.55s/it]                                                     3%|         | 96/3250 [17:08<9:14:28, 10.55s/it]  3%|         | 97/3250 [17:18<9:12:16, 10.51s/it]   {'loss': 1.1153, 'learning_rate': 9.978906466992229e-05, 'epoch': 0.03}
{'loss': 1.4939, 'learning_rate': 9.978460376171112e-05, 'epoch': 0.03}
{'loss': 1.073, 'learning_rate': 9.978009627744234e-05, 'epoch': 0.03}
{'loss': 1.0983, 'learning_rate': 9.977554222133292e-05, 'epoch': 0.03}
                                                  3%|         | 97/3250 [17:18<9:12:16, 10.51s/it]  3%|         | 98/3250 [17:29<9:10:29, 10.48s/it]                                                     3%|         | 98/3250 [17:29<9:10:29, 10.48s/it]  3%|         | 99/3250 [17:39<9:13:25, 10.54s/it]                                                     3%|         | 99/3250 [17:39<9:13:25, 10.54s/it]  3%|         | 100/3250 [17:50<9:11:06, 10.50s/it]                                                      3%|         | 100/3250 [17:50<9:11:06, 10.50s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 1.0529018640518188, 'eval_runtime': 2.1263, 'eval_samples_per_second': 5.644, 'eval_steps_per_second': 1.411, 'epoch': 0.03}
                                                      3%|         | 100/3250 [17:52<9:11:06, 10.50s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_10/tmp-checkpoint-100
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-100
/projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-100
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-100
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-100
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-100/pytorch_model.binthe pytorch model path is
 /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-100/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-100/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-100/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 1.1182, 'learning_rate': 9.977094159764342e-05, 'epoch': 0.03}
{'loss': 1.0851, 'learning_rate': 9.976629441067797e-05, 'epoch': 0.03}
{'loss': 1.0601, 'learning_rate': 9.976160066478425e-05, 'epoch': 0.03}
{'loss': 1.0887, 'learning_rate': 9.97568603643535e-05, 'epoch': 0.03}
{'loss': 1.1405, 'learning_rate': 9.975207351382051e-05, 'epoch': 0.03}
{'loss': 1.093, 'learning_rate': 9.974724011766363e-05, 'epoch': 0.03}
  3%|         | 101/3250 [18:03<9:49:55, 11.24s/it]                                                      3%|         | 101/3250 [18:03<9:49:55, 11.24s/it]  3%|         | 102/3250 [18:13<9:36:31, 10.99s/it]                                                      3%|         | 102/3250 [18:13<9:36:31, 10.99s/it]  3%|         | 103/3250 [18:24<9:27:16, 10.82s/it]                                                      3%|         | 103/3250 [18:24<9:27:16, 10.82s/it]  3%|         | 104/3250 [18:34<9:20:36, 10.69s/it]                                                      3%|         | 104/3250 [18:34<9:20:36, 10.69s/it]  3%|         | 105/3250 [18:44<9:15:43, 10.60s/it]                                                      3%|         | 105/3250 [18:44<9:15:43, 10.60s/it]  3%|         | 106/3250 [18:55<9:12:25, 10.54s/it]                                                      3%|         | 106/3250 [18:55<9:12:25, 10.54s/it]  3%|         | 107/3250 [19:05<9:10{'loss': 1.0894, 'learning_rate': 9.974236018040474e-05, 'epoch': 0.03}
{'loss': 1.0387, 'learning_rate': 9.973743370660928e-05, 'epoch': 0.03}
{'loss': 1.088, 'learning_rate': 9.973246070088624e-05, 'epoch': 0.03}
{'loss': 1.0799, 'learning_rate': 9.972744116788809e-05, 'epoch': 0.03}
:17, 10.51s/it]                                                      3%|         | 107/3250 [19:05<9:10:17, 10.51s/it]  3%|         | 108/3250 [19:16<9:08:24, 10.47s/it]                                                      3%|         | 108/3250 [19:16<9:08:24, 10.47s/it]  3%|         | 109/3250 [19:26<9:07:20, 10.46s/it]                                                      3%|         | 109/3250 [19:26<9:07:20, 10.46s/it]  3%|         | 110/3250 [19:36<9:06:05, 10.43s/it]                                                      3%|         | 110/3250 [19:36<9:06:05, 10.43s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 1.0434951782226562, 'eval_runtime': 2.1265, 'eval_samples_per_second': 5.643, 'eval_steps_per_second': 1.411, 'epoch': 0.03}
                                                      3%|         | 110/3250 [19:39<9:06:05, 10.43s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_10/tmp-checkpoint-110
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in I AM HERE AND I AM SAVING THE MODEL
 the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-110
/projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-110
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-110
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-110
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-110/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-110/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-110/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-110/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 1.0618, 'learning_rate': 9.972237511231087e-05, 'epoch': 0.03}
{'loss': 1.047, 'learning_rate': 9.971726253889416e-05, 'epoch': 0.03}
{'loss': 1.0657, 'learning_rate': 9.9712103452421e-05, 'epoch': 0.03}
{'loss': 1.0716, 'learning_rate': 9.970689785771798e-05, 'epoch': 0.04}
{'loss': 1.0798, 'learning_rate': 9.970164575965523e-05, 'epoch': 0.04}
{'loss': 1.0551, 'learning_rate': 9.969634716314635e-05, 'epoch': 0.04}
  3%|         | 111/3250 [19:49<9:46:38, 11.21s/it]                                                      3%|         | 111/3250 [19:49<9:46:38, 11.21s/it]  3%|         | 112/3250 [20:00<9:33:30, 10.97s/it]                                                      3%|         | 112/3250 [20:00<9:33:30, 10.97s/it]  3%|         | 113/3250 [20:10<9:24:14, 10.79s/it]                                                      3%|         | 113/3250 [20:10<9:24:14, 10.79s/it]  4%|         | 114/3250 [20:21<9:17:38, 10.67s/it]                                                      4%|         | 114/3250 [20:21<9:17:38, 10.67s/it]  4%|         | 115/3250 [20:32<9:23:13, 10.78s/it]                                                      4%|         | 115/3250 [20:32<9:23:13, 10.78s/it]  4%|         | 116/3250 [20:42<9:17:12, 10.67s/it]                                                      4%|         | 116/3250 [20:42<9:17:12, 10.67s/it]  4%|         | 117/3250 [20:52<9:12{'loss': 1.0474, 'learning_rate': 9.969100207314845e-05, 'epoch': 0.04}
{'loss': 1.0891, 'learning_rate': 9.968561049466214e-05, 'epoch': 0.04}
{'loss': 1.0725, 'learning_rate': 9.968017243273149e-05, 'epoch': 0.04}
{'loss': 1.0941, 'learning_rate': 9.967468789244412e-05, 'epoch': 0.04}
:55, 10.59s/it]                                                      4%|         | 117/3250 [20:52<9:12:55, 10.59s/it]  4%|         | 118/3250 [21:03<9:09:56, 10.54s/it]                                                      4%|         | 118/3250 [21:03<9:09:56, 10.54s/it]  4%|         | 119/3250 [21:13<9:07:30, 10.49s/it]                                                      4%|         | 119/3250 [21:13<9:07:30, 10.49s/it]  4%|         | 120/3250 [21:24<9:05:54, 10.46s/it]                                                      4%|         | 120/3250 [21:24<9:05:54, 10.46s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 1.032513976097107, 'eval_runtime': 2.1178, 'eval_samples_per_second': 5.666, 'eval_steps_per_second': 1.417, 'epoch': 0.04}
                                                      4%|         | 120/3250 [21:26<9:05:54, 10.46s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_10/tmp-checkpoint-120
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-120
/projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-120
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-120
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-120
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-120/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-120/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-120/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-120/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 1.1073, 'learning_rate': 9.966915687893108e-05, 'epoch': 0.04}
{'loss': 1.051, 'learning_rate': 9.966357939736693e-05, 'epoch': 0.04}
{'loss': 1.0455, 'learning_rate': 9.965795545296967e-05, 'epoch': 0.04}
{'loss': 1.0413, 'learning_rate': 9.965228505100084e-05, 'epoch': 0.04}
{'loss': 1.0634, 'learning_rate': 9.964656819676533e-05, 'epoch': 0.04}
{'loss': 1.0505, 'learning_rate': 9.964080489561159e-05, 'epoch': 0.04}
  4%|         | 121/3250 [21:37<9:45:03, 11.22s/it]                                                      4%|         | 121/3250 [21:37<9:45:03, 11.22s/it]  4%|         | 122/3250 [21:47<9:32:03, 10.97s/it]                                                      4%|         | 122/3250 [21:47<9:32:03, 10.97s/it]  4%|         | 123/3250 [21:57<9:22:58, 10.80s/it]                                                      4%|         | 123/3250 [21:57<9:22:58, 10.80s/it]  4%|         | 124/3250 [22:08<9:16:18, 10.68s/it]                                                      4%|         | 124/3250 [22:08<9:16:18, 10.68s/it]  4%|         | 125/3250 [22:18<9:11:43, 10.59s/it]                                                      4%|         | 125/3250 [22:18<9:11:43, 10.59s/it]  4%|         | 126/3250 [22:29<9:08:39, 10.54s/it]                                                      4%|         | 126/3250 [22:29<9:08:39, 10.54s/it]  4%|         | 127/3250 [22:39<9:06{'loss': 1.076, 'learning_rate': 9.963499515293147e-05, 'epoch': 0.04}
{'loss': 1.5071, 'learning_rate': 9.962913897416028e-05, 'epoch': 0.04}
{'loss': 0.9778, 'learning_rate': 9.96232363647768e-05, 'epoch': 0.04}
{'loss': 1.0329, 'learning_rate': 9.961728733030318e-05, 'epoch': 0.04}
:13, 10.49s/it]                                                      4%|         | 127/3250 [22:39<9:06:13, 10.49s/it]  4%|         | 128/3250 [22:49<9:04:09, 10.46s/it]                                                      4%|         | 128/3250 [22:49<9:04:09, 10.46s/it]  4%|         | 129/3250 [23:00<9:03:20, 10.45s/it]                                                      4%|         | 129/3250 [23:00<9:03:20, 10.45s/it]  4%|         | 130/3250 [23:10<9:02:11, 10.43s/it]                                                      4%|         | 130/3250 [23:10<9:02:11, 10.43s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 1.0236294269561768, 'eval_runtime': 2.1214, 'eval_samples_per_second': 5.657, 'eval_steps_per_second': 1.414, 'epoch': 0.04}
                                                      4%|         | 130/3250 [23:12<9:02:11, 10.43s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_10/tmp-checkpoint-130
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-130
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-130
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-130
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-130
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-130/pytorch_model.bin
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-130/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-130/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-130/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 1.0895, 'learning_rate': 9.961129187630509e-05, 'epoch': 0.04}
{'loss': 1.0558, 'learning_rate': 9.960525000839159e-05, 'epoch': 0.04}
{'loss': 1.0337, 'learning_rate': 9.959916173221511e-05, 'epoch': 0.04}
{'loss': 1.0489, 'learning_rate': 9.959302705347158e-05, 'epoch': 0.04}
{'loss': 1.0943, 'learning_rate': 9.958684597790031e-05, 'epoch': 0.04}
{'loss': 1.0777, 'learning_rate': 9.958061851128402e-05, 'epoch': 0.04}
  4%|         | 131/3250 [23:23<9:41:24, 11.18s/it]                                                      4%|         | 131/3250 [23:23<9:41:24, 11.18s/it]  4%|         | 132/3250 [23:34<9:32:30, 11.02s/it]                                                      4%|         | 132/3250 [23:34<9:32:30, 11.02s/it]  4%|         | 133/3250 [23:44<9:22:36, 10.83s/it]                                                      4%|         | 133/3250 [23:44<9:22:36, 10.83s/it]  4%|         | 134/3250 [23:55<9:15:31, 10.70s/it]                                                      4%|         | 134/3250 [23:55<9:15:31, 10.70s/it]  4%|         | 135/3250 [24:05<9:10:26, 10.60s/it]                                                      4%|         | 135/3250 [24:05<9:10:26, 10.60s/it]  4%|         | 136/3250 [24:15<9:07:05, 10.54s/it]                                                      4%|         | 136/3250 [24:15<9:07:05, 10.54s/it]  4%|         | 137/3250 [24:26<9:04{'loss': 1.0424, 'learning_rate': 9.957434465944879e-05, 'epoch': 0.04}
{'loss': 0.9945, 'learning_rate': 9.956802442826416e-05, 'epoch': 0.04}
{'loss': 1.0611, 'learning_rate': 9.956165782364303e-05, 'epoch': 0.04}
{'loss': 1.0228, 'learning_rate': 9.955524485154168e-05, 'epoch': 0.04}
:37, 10.50s/it]                                                      4%|         | 137/3250 [24:26<9:04:37, 10.50s/it]  4%|         | 138/3250 [24:36<9:02:37, 10.46s/it]                                                      4%|         | 138/3250 [24:36<9:02:37, 10.46s/it]  4%|         | 139/3250 [24:46<9:01:31, 10.44s/it]                                                      4%|         | 139/3250 [24:46<9:01:31, 10.44s/it]  4%|         | 140/3250 [24:57<9:00:32, 10.43s/it]                                                      4%|         | 140/3250 [24:57<9:00:32, 10.43s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 1.0148273706436157, 'eval_runtime': 2.1159, 'eval_samples_per_second': 5.671, 'eval_steps_per_second': 1.418, 'epoch': 0.04}
                                                      4%|         | 140/3250 [24:59<9:00:32, 10.43s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_10/tmp-checkpoint-140
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-140I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-140

I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-140
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-140
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-140/pytorch_model.bin
the pytorch model path is the pytorch model path is/projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-140/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-140/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-140/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 1.0391, 'learning_rate': 9.954878551795976e-05, 'epoch': 0.04}
{'loss': 1.0196, 'learning_rate': 9.954227982894034e-05, 'epoch': 0.04}
{'loss': 1.0312, 'learning_rate': 9.953572779056981e-05, 'epoch': 0.04}
{'loss': 1.0449, 'learning_rate': 9.952912940897793e-05, 'epoch': 0.04}
{'loss': 1.0399, 'learning_rate': 9.952248469033785e-05, 'epoch': 0.04}
{'loss': 1.0381, 'learning_rate': 9.951579364086602e-05, 'epoch': 0.04}
  4%|         | 141/3250 [25:10<9:39:49, 11.19s/it]                                                      4%|         | 141/3250 [25:10<9:39:49, 11.19s/it]  4%|         | 142/3250 [25:20<9:27:50, 10.96s/it]                                                      4%|         | 142/3250 [25:20<9:27:50, 10.96s/it]  4%|         | 143/3250 [25:31<9:19:29, 10.80s/it]                                                      4%|         | 143/3250 [25:31<9:19:29, 10.80s/it]  4%|         | 144/3250 [25:41<9:13:21, 10.69s/it]                                                      4%|         | 144/3250 [25:41<9:13:21, 10.69s/it]  4%|         | 145/3250 [25:52<9:09:15, 10.61s/it]                                                      4%|         | 145/3250 [25:52<9:09:15, 10.61s/it]  4%|         | 146/3250 [26:02<9:06:18, 10.56s/it]                                                      4%|         | 146/3250 [26:02<9:06:18, 10.56s/it]  5%|         | 147/3250 [26:12<9:04{'loss': 1.0369, 'learning_rate': 9.950905626682228e-05, 'epoch': 0.05}
{'loss': 1.0242, 'learning_rate': 9.95022725745098e-05, 'epoch': 0.05}
{'loss': 1.0755, 'learning_rate': 9.949544257027502e-05, 'epoch': 0.05}
{'loss': 1.0286, 'learning_rate': 9.948856626050781e-05, 'epoch': 0.05}
:00, 10.52s/it]                                                      5%|         | 147/3250 [26:12<9:04:00, 10.52s/it]  5%|         | 148/3250 [26:23<9:07:44, 10.59s/it]                                                      5%|         | 148/3250 [26:23<9:07:44, 10.59s/it]  5%|         | 149/3250 [26:34<9:05:00, 10.55s/it]                                                      5%|         | 149/3250 [26:34<9:05:00, 10.55s/it]  5%|         | 150/3250 [26:44<9:02:58, 10.51s/it]                                                      5%|         | 150/3250 [26:44<9:02:58, 10.51s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 1.0077091455459595, 'eval_runtime': 2.35, 'eval_samples_per_second': 5.106, 'eval_steps_per_second': 1.277, 'epoch': 0.05}
                                                      5%|         | 150/3250 [26:46<9:02:58, 10.51s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_10/tmp-checkpoint-150
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-150I AM HERE AND I AM SAVING THE MODEL

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-150
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-150
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-150
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-150/pytorch_model.bin 
/projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-150/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-150/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-150/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 1.0967, 'learning_rate': 9.94816436516413e-05, 'epoch': 0.05}
{'loss': 1.0006, 'learning_rate': 9.947467475015196e-05, 'epoch': 0.05}
{'loss': 1.0449, 'learning_rate': 9.94676595625595e-05, 'epoch': 0.05}
{'loss': 1.0253, 'learning_rate': 9.946059809542707e-05, 'epoch': 0.05}
{'loss': 1.012, 'learning_rate': 9.945349035536097e-05, 'epoch': 0.05}
{'loss': 1.0362, 'learning_rate': 9.944633634901088e-05, 'epoch': 0.05}
  5%|         | 151/3250 [26:57<9:47:13, 11.37s/it]                                                      5%|         | 151/3250 [26:57<9:47:13, 11.37s/it]  5%|         | 152/3250 [27:08<9:32:23, 11.09s/it]                                                      5%|         | 152/3250 [27:08<9:32:23, 11.09s/it]  5%|         | 153/3250 [27:18<9:21:59, 10.89s/it]                                                      5%|         | 153/3250 [27:18<9:21:59, 10.89s/it]  5%|         | 154/3250 [27:29<9:14:37, 10.75s/it]                                                      5%|         | 154/3250 [27:29<9:14:37, 10.75s/it]  5%|         | 155/3250 [27:39<9:09:44, 10.66s/it]                                                      5%|         | 155/3250 [27:39<9:09:44, 10.66s/it]  5%|         | 156/3250 [27:50<9:06:02, 10.59s/it]                                                      5%|         | 156/3250 [27:50<9:06:02, 10.59s/it]  5%|         | 157/3250 [28:00<9:03{'loss': 1.0245, 'learning_rate': 9.943913608306975e-05, 'epoch': 0.05}
{'loss': 1.0184, 'learning_rate': 9.94318895642738e-05, 'epoch': 0.05}
{'loss': 1.4269, 'learning_rate': 9.94245967994025e-05, 'epoch': 0.05}
{'loss': 1.0196, 'learning_rate': 9.941725779527861e-05, 'epoch': 0.05}
:38, 10.55s/it]                                                      5%|         | 157/3250 [28:00<9:03:38, 10.55s/it]  5%|         | 158/3250 [28:10<9:01:42, 10.51s/it]                                                      5%|         | 158/3250 [28:10<9:01:42, 10.51s/it]  5%|         | 159/3250 [28:21<9:00:12, 10.49s/it]                                                      5%|         | 159/3250 [28:21<9:00:12, 10.49s/it]  5%|         | 160/3250 [28:31<8:59:15, 10.47s/it]                                                      5%|         | 160/3250 [28:31<8:59:15, 10.47s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 1.0024253129959106, 'eval_runtime': 2.1196, 'eval_samples_per_second': 5.661, 'eval_steps_per_second': 1.415, 'epoch': 0.05}
                                                      5%|         | 160/3250 [28:33<8:59:15, 10.47s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_10/tmp-checkpoint-160
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-160
 /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-160
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-160
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-160
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-160/pytorch_model.bin
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-160/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-160/pytorch_model.binthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-160/pytorch_model.bin

/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 1.0648, 'learning_rate': 9.940987255876817e-05, 'epoch': 0.05}
{'loss': 1.0421, 'learning_rate': 9.940244109678043e-05, 'epoch': 0.05}
{'loss': 1.0265, 'learning_rate': 9.939496341626791e-05, 'epoch': 0.05}
{'loss': 0.9893, 'learning_rate': 9.938743952422636e-05, 'epoch': 0.05}
{'loss': 1.0683, 'learning_rate': 9.937986942769477e-05, 'epoch': 0.05}
{'loss': 1.0581, 'learning_rate': 9.937225313375535e-05, 'epoch': 0.05}
  5%|         | 161/3250 [28:44<9:38:04, 11.23s/it]                                                      5%|         | 161/3250 [28:44<9:38:04, 11.23s/it]  5%|         | 162/3250 [28:55<9:25:31, 10.99s/it]                                                      5%|         | 162/3250 [28:55<9:25:31, 10.99s/it]  5%|         | 163/3250 [29:05<9:16:42, 10.82s/it]                                                      5%|         | 163/3250 [29:05<9:16:42, 10.82s/it]  5%|         | 164/3250 [29:16<9:13:05, 10.75s/it]                                                      5%|         | 164/3250 [29:16<9:13:05, 10.75s/it]  5%|         | 165/3250 [29:26<9:08:10, 10.66s/it]                                                      5%|         | 165/3250 [29:26<9:08:10, 10.66s/it]  5%|         | 166/3250 [29:37<9:04:27, 10.59s/it]                                                      5%|         | 166/3250 [29:37<9:04:27, 10.59s/it]  5%|         | 167/3250 [29:47<9:01{'loss': 1.0093, 'learning_rate': 9.936459064953355e-05, 'epoch': 0.05}
{'loss': 1.0034, 'learning_rate': 9.935688198219801e-05, 'epoch': 0.05}
{'loss': 1.016, 'learning_rate': 9.934912713896057e-05, 'epoch': 0.05}
{'loss': 1.0009, 'learning_rate': 9.934132612707632e-05, 'epoch': 0.05}
:56, 10.55s/it]                                                      5%|         | 167/3250 [29:47<9:01:56, 10.55s/it]  5%|         | 168/3250 [29:58<8:59:58, 10.51s/it]                                                      5%|         | 168/3250 [29:58<8:59:58, 10.51s/it]  5%|         | 169/3250 [30:08<8:58:45, 10.49s/it]                                                      5%|         | 169/3250 [30:08<8:58:45, 10.49s/it]  5%|         | 170/3250 [30:18<8:57:50, 10.48s/it]                                                      5%|         | 170/3250 [30:18<8:57:50, 10.48s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.995637059211731, 'eval_runtime': 2.1223, 'eval_samples_per_second': 5.654, 'eval_steps_per_second': 1.414, 'epoch': 0.05}
                                                      5%|         | 170/3250 [30:21<8:57:50, 10.48s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_10/tmp-checkpoint-170
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-170
/projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-170
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-170
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-170
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-170/pytorch_model.bin
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-170/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-170/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-170/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 1.0098, 'learning_rate': 9.933347895384346e-05, 'epoch': 0.05}
{'loss': 0.9937, 'learning_rate': 9.932558562660347e-05, 'epoch': 0.05}
{'loss': 0.9995, 'learning_rate': 9.931764615274093e-05, 'epoch': 0.05}
{'loss': 1.0024, 'learning_rate': 9.930966053968364e-05, 'epoch': 0.05}
{'loss': 1.0357, 'learning_rate': 9.930162879490257e-05, 'epoch': 0.05}
{'loss': 0.998, 'learning_rate': 9.92935509259118e-05, 'epoch': 0.05}
  5%|         | 171/3250 [30:31<9:35:57, 11.22s/it]                                                      5%|         | 171/3250 [30:31<9:35:57, 11.22s/it]  5%|         | 172/3250 [30:42<9:23:34, 10.99s/it]                                                      5%|         | 172/3250 [30:42<9:23:34, 10.99s/it]  5%|         | 173/3250 [30:52<9:14:55, 10.82s/it]                                                      5%|         | 173/3250 [30:52<9:14:55, 10.82s/it]  5%|         | 174/3250 [31:03<9:09:50, 10.73s/it]                                                      5%|         | 174/3250 [31:03<9:09:50, 10.73s/it]  5%|         | 175/3250 [31:13<9:05:13, 10.64s/it]                                                      5%|         | 175/3250 [31:13<9:05:13, 10.64s/it]  5%|         | 176/3250 [31:24<9:01:48, 10.58s/it]                                                      5%|         | 176/3250 [31:24<9:01:48, 10.58s/it]  5%|         | 177/3250 [31:34<8:59{'loss': 0.986, 'learning_rate': 9.928542694026862e-05, 'epoch': 0.05}
{'loss': 1.0399, 'learning_rate': 9.927725684557338e-05, 'epoch': 0.05}
{'loss': 1.0205, 'learning_rate': 9.926904064946969e-05, 'epoch': 0.06}
{'loss': 1.0024, 'learning_rate': 9.92607783596442e-05, 'epoch': 0.06}
:19, 10.53s/it]                                                      5%|         | 177/3250 [31:34<8:59:19, 10.53s/it]  5%|         | 178/3250 [31:44<8:57:37, 10.50s/it]                                                      5%|         | 178/3250 [31:44<8:57:37, 10.50s/it]  6%|         | 179/3250 [31:55<8:56:22, 10.48s/it]                                                      6%|         | 179/3250 [31:55<8:56:22, 10.48s/it]  6%|         | 180/3250 [32:05<8:55:32, 10.47s/it]                                                      6%|         | 180/3250 [32:05<8:55:32, 10.47s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.9886634349822998, 'eval_runtime': 2.1223, 'eval_samples_per_second': 5.654, 'eval_steps_per_second': 1.414, 'epoch': 0.06}
                                                      6%|         | 180/3250 [32:07<8:55:32, 10.47s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_10/tmp-checkpoint-180
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-180
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-180
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-180
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-180
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-180/pytorch_model.bin
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-180/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-180/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-180/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 1.0687, 'learning_rate': 9.925246998382671e-05, 'epoch': 0.06}
{'loss': 1.0156, 'learning_rate': 9.924411552979012e-05, 'epoch': 0.06}
{'loss': 0.9915, 'learning_rate': 9.923571500535047e-05, 'epoch': 0.06}
{'loss': 1.0154, 'learning_rate': 9.922726841836684e-05, 'epoch': 0.06}
{'loss': 0.9547, 'learning_rate': 9.921877577674152e-05, 'epoch': 0.06}
{'loss': 1.0261, 'learning_rate': 9.921023708841975e-05, 'epoch': 0.06}
  6%|         | 181/3250 [32:19<9:44:29, 11.43s/it]                                                      6%|         | 181/3250 [32:19<9:44:29, 11.43s/it]  6%|         | 182/3250 [32:29<9:28:46, 11.12s/it]                                                      6%|         | 182/3250 [32:29<9:28:46, 11.12s/it]  6%|         | 183/3250 [32:40<9:17:49, 10.91s/it]                                                      6%|         | 183/3250 [32:40<9:17:49, 10.91s/it]  6%|         | 184/3250 [32:50<9:09:57, 10.76s/it]                                                      6%|         | 184/3250 [32:50<9:09:57, 10.76s/it]  6%|         | 185/3250 [33:01<9:04:25, 10.66s/it]                                                      6%|         | 185/3250 [33:01<9:04:25, 10.66s/it]  6%|         | 186/3250 [33:11<9:00:27, 10.58s/it]                                                      6%|         | 186/3250 [33:11<9:00:27, 10.58s/it]  6%|         | 187/3250 [33:21<8:57{'loss': 0.9791, 'learning_rate': 9.920165236138994e-05, 'epoch': 0.06}
{'loss': 1.0157, 'learning_rate': 9.919302160368353e-05, 'epoch': 0.06}
{'loss': 1.4147, 'learning_rate': 9.918434482337506e-05, 'epoch': 0.06}
{'loss': 0.99, 'learning_rate': 9.917562202858208e-05, 'epoch': 0.06}
:52, 10.54s/it]                                                      6%|         | 187/3250 [33:21<8:57:52, 10.54s/it]  6%|         | 188/3250 [33:32<8:56:11, 10.51s/it]                                                      6%|         | 188/3250 [33:32<8:56:11, 10.51s/it]  6%|         | 189/3250 [33:42<8:54:24, 10.48s/it]                                                      6%|         | 189/3250 [33:42<8:54:24, 10.48s/it]  6%|         | 190/3250 [33:53<8:53:21, 10.46s/it]                                                      6%|         | 190/3250 [33:53<8:53:21, 10.46s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.9841294884681702, 'eval_runtime': 2.1263, 'eval_samples_per_second': 5.644, 'eval_steps_per_second': 1.411, 'epoch': 0.06}
                                                      6%|         | 190/3250 [33:55<8:53:21, 10.46s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_10/tmp-checkpoint-190
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  I AM HERE AND I AM SAVING THE MODEL
/projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-190the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-190

I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-190
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-190
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-190/pytorch_model.bin
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-190/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-190/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-190/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 1.0074, 'learning_rate': 9.916685322746524e-05, 'epoch': 0.06}
{'loss': 1.0354, 'learning_rate': 9.915803842822817e-05, 'epoch': 0.06}
{'loss': 0.9997, 'learning_rate': 9.914917763911759e-05, 'epoch': 0.06}
{'loss': 0.9727, 'learning_rate': 9.914027086842322e-05, 'epoch': 0.06}
{'loss': 0.9986, 'learning_rate': 9.913131812447781e-05, 'epoch': 0.06}
{'loss': 1.0502, 'learning_rate': 9.912231941565711e-05, 'epoch': 0.06}
  6%|         | 191/3250 [34:06<9:32:23, 11.23s/it]                                                      6%|         | 191/3250 [34:06<9:32:23, 11.23s/it]  6%|         | 192/3250 [34:16<9:19:39, 10.98s/it]                                                      6%|         | 192/3250 [34:16<9:19:39, 10.98s/it]  6%|         | 193/3250 [34:27<9:10:42, 10.81s/it]                                                      6%|         | 193/3250 [34:27<9:10:42, 10.81s/it]  6%|         | 194/3250 [34:37<9:04:38, 10.69s/it]                                                      6%|         | 194/3250 [34:37<9:04:38, 10.69s/it]  6%|         | 195/3250 [34:47<9:00:17, 10.61s/it]                                                      6%|         | 195/3250 [34:47<9:00:17, 10.61s/it]  6%|         | 196/3250 [34:58<8:57:12, 10.55s/it]                                                      6%|         | 196/3250 [34:58<8:57:12, 10.55s/it]  6%|         | 197/3250 [35:09<9:00{'loss': 1.0114, 'learning_rate': 9.911327475037985e-05, 'epoch': 0.06}
{'loss': 1.0055, 'learning_rate': 9.91041841371078e-05, 'epoch': 0.06}
{'loss': 0.9508, 'learning_rate': 9.909504758434571e-05, 'epoch': 0.06}
{'loss': 0.9957, 'learning_rate': 9.908586510064127e-05, 'epoch': 0.06}
:22, 10.62s/it]                                                      6%|         | 197/3250 [35:09<9:00:22, 10.62s/it]  6%|         | 198/3250 [35:19<8:57:13, 10.56s/it]                                                      6%|         | 198/3250 [35:19<8:57:13, 10.56s/it]  6%|         | 199/3250 [35:29<8:54:54, 10.52s/it]                                                      6%|         | 199/3250 [35:29<8:54:54, 10.52s/it]  6%|         | 200/3250 [35:40<8:53:02, 10.49s/it]                                                      6%|         | 200/3250 [35:40<8:53:02, 10.49s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.9803223013877869, 'eval_runtime': 2.121, 'eval_samples_per_second': 5.658, 'eval_steps_per_second': 1.414, 'epoch': 0.06}
                                                      6%|         | 200/3250 [35:42<8:53:02, 10.49s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_10/tmp-checkpoint-200
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  I AM HERE AND I AM SAVING THE MODEL
/projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-200the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-200

I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-200
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-200
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-200/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-200/pytorch_model.binthe pytorch model path is
 /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-200/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-200/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.992, 'learning_rate': 9.907663669458518e-05, 'epoch': 0.06}
{'loss': 0.992, 'learning_rate': 9.906736237481109e-05, 'epoch': 0.06}
{'loss': 0.9865, 'learning_rate': 9.905804214999558e-05, 'epoch': 0.06}
{'loss': 0.9911, 'learning_rate': 9.904867602885824e-05, 'epoch': 0.06}
{'loss': 0.9979, 'learning_rate': 9.903926402016153e-05, 'epoch': 0.06}
{'loss': 0.9962, 'learning_rate': 9.902980613271087e-05, 'epoch': 0.06}
  6%|         | 201/3250 [35:53<9:30:28, 11.23s/it]                                                      6%|         | 201/3250 [35:53<9:30:28, 11.23s/it]  6%|         | 202/3250 [36:03<9:17:51, 10.98s/it]                                                      6%|         | 202/3250 [36:03<9:17:51, 10.98s/it]  6%|         | 203/3250 [36:14<9:09:00, 10.81s/it]                                                      6%|         | 203/3250 [36:14<9:09:00, 10.81s/it]  6%|         | 204/3250 [36:24<9:03:01, 10.70s/it]                                                      6%|         | 204/3250 [36:24<9:03:01, 10.70s/it]  6%|         | 205/3250 [36:34<8:58:30, 10.61s/it]                                                      6%|         | 205/3250 [36:34<8:58:30, 10.61s/it]  6%|         | 206/3250 [36:45<8:55:27, 10.55s/it]                                                      6%|         | 206/3250 [36:45<8:55:27, 10.55s/it]  6%|         | 207/3250 [36:55<8:53{'loss': 0.9814, 'learning_rate': 9.90203023753546e-05, 'epoch': 0.06}
{'loss': 0.9974, 'learning_rate': 9.9010752756984e-05, 'epoch': 0.06}
{'loss': 1.0148, 'learning_rate': 9.900115728653319e-05, 'epoch': 0.06}
{'loss': 0.9922, 'learning_rate': 9.899151597297923e-05, 'epoch': 0.06}
:02, 10.51s/it]                                                      6%|         | 207/3250 [36:55<8:53:02, 10.51s/it]  6%|         | 208/3250 [37:06<8:51:22, 10.48s/it]                                                      6%|         | 208/3250 [37:06<8:51:22, 10.48s/it]  6%|         | 209/3250 [37:16<8:50:10, 10.46s/it]                                                      6%|         | 209/3250 [37:16<8:50:10, 10.46s/it]  6%|         | 210/3250 [37:27<8:49:16, 10.45s/it]                                                      6%|         | 210/3250 [37:27<8:49:16, 10.45s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.9727394580841064, 'eval_runtime': 2.1151, 'eval_samples_per_second': 5.674, 'eval_steps_per_second': 1.418, 'epoch': 0.06}
                                                      6%|         | 210/3250 [37:29<8:49:16, 10.45s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_10/tmp-checkpoint-210
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-210I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in 
 /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-210
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-210
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-210
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-210/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-210/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-210/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-210/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 1.024, 'learning_rate': 9.89818288253421e-05, 'epoch': 0.06}
{'loss': 1.0138, 'learning_rate': 9.897209585268458e-05, 'epoch': 0.07}
{'loss': 0.9806, 'learning_rate': 9.896231706411242e-05, 'epoch': 0.07}
{'loss': 0.9666, 'learning_rate': 9.895249246877415e-05, 'epoch': 0.07}
{'loss': 0.9721, 'learning_rate': 9.894262207586116e-05, 'epoch': 0.07}
{'loss': 0.9908, 'learning_rate': 9.893270589460775e-05, 'epoch': 0.07}
  6%|         | 211/3250 [37:40<9:28:46, 11.23s/it]                                                      6%|         | 211/3250 [37:40<9:28:46, 11.23s/it]  7%|         | 212/3250 [37:50<9:16:09, 10.98s/it]                                                      7%|         | 212/3250 [37:50<9:16:09, 10.98s/it]  7%|         | 213/3250 [38:00<9:07:23, 10.81s/it]                                                      7%|         | 213/3250 [38:00<9:07:23, 10.81s/it]  7%|         | 214/3250 [38:11<9:04:47, 10.77s/it]                                                      7%|         | 214/3250 [38:11<9:04:47, 10.77s/it]  7%|         | 215/3250 [38:22<8:59:22, 10.66s/it]                                                      7%|         | 215/3250 [38:22<8:59:22, 10.66s/it]  7%|         | 216/3250 [38:32<8:55:23, 10.59s/it]                                                      7%|         | 216/3250 [38:32<8:55:23, 10.59s/it]  7%|         | 217/3250 [38:42<8:52{'loss': 0.9743, 'learning_rate': 9.8922743934291e-05, 'epoch': 0.07}
{'loss': 1.0018, 'learning_rate': 9.891273620423083e-05, 'epoch': 0.07}
{'loss': 1.4401, 'learning_rate': 9.890268271379e-05, 'epoch': 0.07}
{'loss': 0.9036, 'learning_rate': 9.889258347237404e-05, 'epoch': 0.07}
:42, 10.54s/it]                                                      7%|         | 217/3250 [38:42<8:52:42, 10.54s/it]  7%|         | 218/3250 [38:53<8:50:39, 10.50s/it]                                                      7%|         | 218/3250 [38:53<8:50:39, 10.50s/it]  7%|         | 219/3250 [39:03<8:49:01, 10.47s/it]                                                      7%|         | 219/3250 [39:03<8:49:01, 10.47s/it]  7%|         | 220/3250 [39:14<8:48:03, 10.46s/it]                                                      7%|         | 220/3250 [39:14<8:48:03, 10.46s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.9705851674079895, 'eval_runtime': 2.3551, 'eval_samples_per_second': 5.095, 'eval_steps_per_second': 1.274, 'epoch': 0.07}
                                                      7%|         | 220/3250 [39:16<8:48:03, 10.46s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_10/tmp-checkpoint-220
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  I AM HERE AND I AM SAVING THE MODEL
/projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-220the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-220

I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-220
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-220
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-220/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-220/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-220/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-220/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.957, 'learning_rate': 9.888243848943136e-05, 'epoch': 0.07}
{'loss': 1.0103, 'learning_rate': 9.887224777445308e-05, 'epoch': 0.07}
{'loss': 0.9861, 'learning_rate': 9.886201133697314e-05, 'epoch': 0.07}
{'loss': 0.9713, 'learning_rate': 9.885172918656826e-05, 'epoch': 0.07}
{'loss': 0.9706, 'learning_rate': 9.884140133285791e-05, 'epoch': 0.07}
{'loss': 1.0123, 'learning_rate': 9.883102778550434e-05, 'epoch': 0.07}
  7%|         | 221/3250 [39:27<9:30:49, 11.31s/it]                                                      7%|         | 221/3250 [39:27<9:30:49, 11.31s/it]  7%|         | 222/3250 [39:37<9:17:13, 11.04s/it]                                                      7%|         | 222/3250 [39:37<9:17:13, 11.04s/it]  7%|         | 223/3250 [39:48<9:07:32, 10.85s/it]                                                      7%|         | 223/3250 [39:48<9:07:32, 10.85s/it]  7%|         | 224/3250 [39:58<9:00:44, 10.72s/it]                                                      7%|         | 224/3250 [39:58<9:00:44, 10.72s/it]  7%|         | 225/3250 [40:09<8:55:54, 10.63s/it]                                                      7%|         | 225/3250 [40:09<8:55:54, 10.63s/it]  7%|         | 226/3250 [40:19<8:52:39, 10.57s/it]                                                      7%|         | 226/3250 [40:19<8:52:39, 10.57s/it]  7%|         | 227/3250 [40:29<8:50{'loss': 0.9998, 'learning_rate': 9.882060855421253e-05, 'epoch': 0.07}
{'loss': 0.9646, 'learning_rate': 9.881014364873021e-05, 'epoch': 0.07}
{'loss': 0.9218, 'learning_rate': 9.879963307884784e-05, 'epoch': 0.07}
{'loss': 0.9861, 'learning_rate': 9.87890768543986e-05, 'epoch': 0.07}
:05, 10.52s/it]                                                      7%|         | 227/3250 [40:29<8:50:05, 10.52s/it]  7%|         | 228/3250 [40:40<8:48:41, 10.50s/it]                                                      7%|         | 228/3250 [40:40<8:48:41, 10.50s/it]  7%|         | 229/3250 [40:50<8:47:27, 10.48s/it]                                                      7%|         | 229/3250 [40:50<8:47:27, 10.48s/it]  7%|         | 230/3250 [41:01<8:51:29, 10.56s/it]                                                      7%|         | 230/3250 [41:01<8:51:29, 10.56s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.9666674137115479, 'eval_runtime': 2.1257, 'eval_samples_per_second': 5.645, 'eval_steps_per_second': 1.411, 'epoch': 0.07}
                                                      7%|         | 230/3250 [41:03<8:51:29, 10.56s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_10/tmp-checkpoint-230
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-230
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-230
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-230
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-230
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-230/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-230/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-230/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-230/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.9617, 'learning_rate': 9.877847498525837e-05, 'epoch': 0.07}
{'loss': 0.97, 'learning_rate': 9.876782748134576e-05, 'epoch': 0.07}
{'loss': 0.9584, 'learning_rate': 9.875713435262203e-05, 'epoch': 0.07}
{'loss': 0.9643, 'learning_rate': 9.874639560909117e-05, 'epoch': 0.07}
{'loss': 0.9798, 'learning_rate': 9.873561126079985e-05, 'epoch': 0.07}
{'loss': 0.9675, 'learning_rate': 9.872478131783736e-05, 'epoch': 0.07}
  7%|         | 231/3250 [41:14<9:30:12, 11.33s/it]                                                      7%|         | 231/3250 [41:14<9:30:12, 11.33s/it]  7%|         | 232/3250 [41:25<9:16:04, 11.06s/it]                                                      7%|         | 232/3250 [41:25<9:16:04, 11.06s/it]  7%|         | 233/3250 [41:35<9:06:07, 10.86s/it]                                                      7%|         | 233/3250 [41:35<9:06:07, 10.86s/it]  7%|         | 234/3250 [41:45<8:59:31, 10.73s/it]                                                      7%|         | 234/3250 [41:45<8:59:31, 10.73s/it]  7%|         | 235/3250 [41:56<8:54:29, 10.64s/it]                                                      7%|         | 235/3250 [41:56<8:54:29, 10.64s/it]  7%|         | 236/3250 [42:06<8:51:02, 10.57s/it]                                                      7%|         | 236/3250 [42:06<8:51:02, 10.57s/it]  7%|         | 237/3250 [42:17<8:48{'loss': 0.9701, 'learning_rate': 9.871390579033564e-05, 'epoch': 0.07}
{'loss': 0.971, 'learning_rate': 9.870298468846936e-05, 'epoch': 0.07}
{'loss': 0.9561, 'learning_rate': 9.869201802245573e-05, 'epoch': 0.07}
{'loss': 1.006, 'learning_rate': 9.868100580255466e-05, 'epoch': 0.07}
:24, 10.52s/it]                                                      7%|         | 237/3250 [42:17<8:48:24, 10.52s/it]  7%|         | 238/3250 [42:27<8:46:38, 10.49s/it]                                                      7%|         | 238/3250 [42:27<8:46:38, 10.49s/it]  7%|         | 239/3250 [42:37<8:45:18, 10.47s/it]                                                      7%|         | 239/3250 [42:37<8:45:18, 10.47s/it]  7%|         | 240/3250 [42:48<8:43:58, 10.44s/it]                                                      7%|         | 240/3250 [42:48<8:43:58, 10.44s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.9608665704727173, 'eval_runtime': 2.1174, 'eval_samples_per_second': 5.667, 'eval_steps_per_second': 1.417, 'epoch': 0.07}
                                                      7%|         | 240/3250 [42:50<8:43:58, 10.44s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_10/tmp-checkpoint-240
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-240
 /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-240
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-240
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-240
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-240/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-240/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-240/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-240/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.9568, 'learning_rate': 9.866994803906862e-05, 'epoch': 0.07}
{'loss': 1.0345, 'learning_rate': 9.865884474234274e-05, 'epoch': 0.07}
{'loss': 0.9475, 'learning_rate': 9.864769592276472e-05, 'epoch': 0.07}
{'loss': 0.9761, 'learning_rate': 9.863650159076485e-05, 'epoch': 0.08}
{'loss': 0.9494, 'learning_rate': 9.8625261756816e-05, 'epoch': 0.08}
{'loss': 0.9456, 'learning_rate': 9.861397643143362e-05, 'epoch': 0.08}
  7%|         | 241/3250 [43:01<9:21:04, 11.19s/it]                                                      7%|         | 241/3250 [43:01<9:21:04, 11.19s/it]  7%|         | 242/3250 [43:11<9:09:06, 10.95s/it]                                                      7%|         | 242/3250 [43:11<9:09:06, 10.95s/it]  7%|         | 243/3250 [43:22<9:00:39, 10.79s/it]                                                      7%|         | 243/3250 [43:22<9:00:39, 10.79s/it]  8%|         | 244/3250 [43:32<8:54:48, 10.67s/it]                                                      8%|         | 244/3250 [43:32<8:54:48, 10.67s/it]  8%|         | 245/3250 [43:42<8:50:41, 10.60s/it]                                                      8%|         | 245/3250 [43:42<8:50:41, 10.60s/it]  8%|         | 246/3250 [43:53<8:50:27, 10.59s/it]                                                      8%|         | 246/3250 [43:53<8:50:27, 10.59s/it]  8%|         | 247/3250 [44:03<8:47{'loss': 0.9556, 'learning_rate': 9.86026456251757e-05, 'epoch': 0.08}
{'loss': 0.9525, 'learning_rate': 9.859126934864281e-05, 'epoch': 0.08}
{'loss': 0.9573, 'learning_rate': 9.857984761247803e-05, 'epoch': 0.08}
{'loss': 1.3784, 'learning_rate': 9.856838042736699e-05, 'epoch': 0.08}
:46, 10.54s/it]                                                      8%|         | 247/3250 [44:03<8:47:46, 10.54s/it]  8%|         | 248/3250 [44:14<8:45:49, 10.51s/it]                                                      8%|         | 248/3250 [44:14<8:45:49, 10.51s/it]  8%|         | 249/3250 [44:24<8:44:12, 10.48s/it]                                                      8%|         | 249/3250 [44:24<8:44:12, 10.48s/it]  8%|         | 250/3250 [44:35<8:43:01, 10.46s/it]                                                      8%|         | 250/3250 [44:35<8:43:01, 10.46s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.9580448865890503, 'eval_runtime': 2.3552, 'eval_samples_per_second': 5.095, 'eval_steps_per_second': 1.274, 'epoch': 0.08}
                                                      8%|         | 250/3250 [44:37<8:43:01, 10.46s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_10/tmp-checkpoint-250
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-250I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-250

I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-250
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-250
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-250/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-250/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-250/pytorch_model.binthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-250/pytorch_model.bin

/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.9483, 'learning_rate': 9.855686780403783e-05, 'epoch': 0.08}
{'loss': 0.9842, 'learning_rate': 9.854530975326117e-05, 'epoch': 0.08}
{'loss': 0.9763, 'learning_rate': 9.853370628585021e-05, 'epoch': 0.08}
{'loss': 0.9548, 'learning_rate': 9.852205741266058e-05, 'epoch': 0.08}
{'loss': 0.9313, 'learning_rate': 9.851036314459037e-05, 'epoch': 0.08}
{'loss': 1.0112, 'learning_rate': 9.84986234925802e-05, 'epoch': 0.08}
  8%|         | 251/3250 [44:48<9:24:51, 11.30s/it]                                                      8%|         | 251/3250 [44:48<9:24:51, 11.30s/it]  8%|         | 252/3250 [44:58<9:11:09, 11.03s/it]                                                      8%|         | 252/3250 [44:58<9:11:09, 11.03s/it]  8%|         | 253/3250 [45:09<9:01:43, 10.85s/it]                                                      8%|         | 253/3250 [45:09<9:01:43, 10.85s/it]  8%|         | 254/3250 [45:19<8:54:43, 10.71s/it]                                                      8%|         | 254/3250 [45:19<8:54:43, 10.71s/it]  8%|         | 255/3250 [45:30<8:50:00, 10.62s/it]                                                      8%|         | 255/3250 [45:30<8:50:00, 10.62s/it]  8%|         | 256/3250 [45:40<8:46:54, 10.56s/it]                                                      8%|         | 256/3250 [45:40<8:46:54, 10.56s/it]  8%|         | 257/3250 [45:50<8:44{'loss': 0.9918, 'learning_rate': 9.848683846761311e-05, 'epoch': 0.08}
{'loss': 0.9442, 'learning_rate': 9.847500808071457e-05, 'epoch': 0.08}
{'loss': 0.9396, 'learning_rate': 9.846313234295256e-05, 'epoch': 0.08}
{'loss': 0.9552, 'learning_rate': 9.845121126543742e-05, 'epoch': 0.08}
:22, 10.51s/it]                                                      8%|         | 257/3250 [45:50<8:44:22, 10.51s/it]  8%|         | 258/3250 [46:01<8:42:56, 10.49s/it]                                                      8%|         | 258/3250 [46:01<8:42:56, 10.49s/it]  8%|         | 259/3250 [46:11<8:41:29, 10.46s/it]                                                      8%|         | 259/3250 [46:11<8:41:29, 10.46s/it]  8%|         | 260/3250 [46:22<8:40:09, 10.44s/it]                                                      8%|         | 260/3250 [46:22<8:40:09, 10.44s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.9549394845962524, 'eval_runtime': 2.1069, 'eval_samples_per_second': 5.695, 'eval_steps_per_second': 1.424, 'epoch': 0.08}
                                                      8%|         | 260/3250 [46:24<8:40:09, 10.44s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_10/tmp-checkpoint-260
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-260I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-260

I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-260
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-260
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-260/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-260/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-260/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-260/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.9346, 'learning_rate': 9.843924485932194e-05, 'epoch': 0.08}
{'loss': 0.9424, 'learning_rate': 9.842723313580128e-05, 'epoch': 0.08}
{'loss': 0.9406, 'learning_rate': 9.841517610611309e-05, 'epoch': 0.08}
{'loss': 0.938, 'learning_rate': 9.840307378153726e-05, 'epoch': 0.08}
{'loss': 0.9475, 'learning_rate': 9.83909261733962e-05, 'epoch': 0.08}
{'loss': 0.9749, 'learning_rate': 9.837873329305459e-05, 'epoch': 0.08}
  8%|         | 261/3250 [46:35<9:17:25, 11.19s/it]                                                      8%|         | 261/3250 [46:35<9:17:25, 11.19s/it]  8%|         | 262/3250 [46:45<9:05:16, 10.95s/it]                                                      8%|         | 262/3250 [46:45<9:05:16, 10.95s/it]  8%|         | 263/3250 [46:56<9:02:07, 10.89s/it]                                                      8%|         | 263/3250 [46:56<9:02:07, 10.89s/it]  8%|         | 264/3250 [47:06<8:54:36, 10.74s/it]                                                      8%|         | 264/3250 [47:06<8:54:36, 10.74s/it]  8%|         | 265/3250 [47:16<8:49:00, 10.63s/it]                                                      8%|         | 265/3250 [47:16<8:49:00, 10.63s/it]  8%|         | 266/3250 [47:27<8:45:08, 10.56s/it]                                                      8%|         | 266/3250 [47:27<8:45:08, 10.56s/it]  8%|         | 267/3250 [47:37<8:42{'loss': 0.9368, 'learning_rate': 9.836649515191949e-05, 'epoch': 0.08}
{'loss': 0.9306, 'learning_rate': 9.835421176144035e-05, 'epoch': 0.08}
{'loss': 0.9764, 'learning_rate': 9.834188313310886e-05, 'epoch': 0.08}
{'loss': 0.9681, 'learning_rate': 9.832950927845914e-05, 'epoch': 0.08}
:23, 10.51s/it]                                                      8%|         | 267/3250 [47:37<8:42:23, 10.51s/it]  8%|         | 268/3250 [47:48<8:40:24, 10.47s/it]                                                      8%|         | 268/3250 [47:48<8:40:24, 10.47s/it]  8%|         | 269/3250 [47:58<8:38:50, 10.44s/it]                                                      8%|         | 269/3250 [47:58<8:38:50, 10.44s/it]  8%|         | 270/3250 [48:08<8:37:51, 10.43s/it]                                                      8%|         | 270/3250 [48:08<8:37:51, 10.43s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.9513084888458252, 'eval_runtime': 2.1165, 'eval_samples_per_second': 5.67, 'eval_steps_per_second': 1.417, 'epoch': 0.08}
                                                      8%|         | 270/3250 [48:11<8:37:51, 10.43s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_10/tmp-checkpoint-270
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-270I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-270

I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-270
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-270
the pytorch model path isthe pytorch model path isthe pytorch model path is  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-270/pytorch_model.bin/projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-270/pytorch_model.bin

 /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-270/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-270/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.934, 'learning_rate': 9.831709020906754e-05, 'epoch': 0.08}
{'loss': 1.0203, 'learning_rate': 9.830462593655274e-05, 'epoch': 0.08}
{'loss': 0.9531, 'learning_rate': 9.829211647257571e-05, 'epoch': 0.08}
{'loss': 0.9409, 'learning_rate': 9.82795618288397e-05, 'epoch': 0.08}
{'loss': 0.9392, 'learning_rate': 9.826696201709021e-05, 'epoch': 0.08}
{'loss': 0.9099, 'learning_rate': 9.825431704911503e-05, 'epoch': 0.08}
  8%|         | 271/3250 [48:21<9:16:44, 11.21s/it]                                                      8%|         | 271/3250 [48:21<9:16:44, 11.21s/it]  8%|         | 272/3250 [48:32<9:04:49, 10.98s/it]                                                      8%|         | 272/3250 [48:32<9:04:49, 10.98s/it]  8%|         | 273/3250 [48:42<8:56:16, 10.81s/it]                                                      8%|         | 273/3250 [48:42<8:56:16, 10.81s/it]  8%|         | 274/3250 [48:53<8:50:18, 10.69s/it]                                                      8%|         | 274/3250 [48:53<8:50:18, 10.69s/it]  8%|         | 275/3250 [49:03<8:46:22, 10.62s/it]                                                      8%|         | 275/3250 [49:03<8:46:22, 10.62s/it]  8%|         | 276/3250 [49:14<8:43:23, 10.56s/it]                                                      8%|         | 276/3250 [49:14<8:43:23, 10.56s/it]  9%|         | 277/3250 [49:24<8:41{'loss': 0.9715, 'learning_rate': 9.824162693674417e-05, 'epoch': 0.09}
{'loss': 0.9178, 'learning_rate': 9.82288916918499e-05, 'epoch': 0.09}
{'loss': 0.9584, 'learning_rate': 9.821611132634666e-05, 'epoch': 0.09}
{'loss': 1.3694, 'learning_rate': 9.820328585219117e-05, 'epoch': 0.09}
:30, 10.53s/it]                                                      9%|         | 277/3250 [49:24<8:41:30, 10.53s/it]  9%|         | 278/3250 [49:34<8:39:48, 10.49s/it]                                                      9%|         | 278/3250 [49:34<8:39:48, 10.49s/it]  9%|         | 279/3250 [49:45<8:42:31, 10.55s/it]                                                      9%|         | 279/3250 [49:45<8:42:31, 10.55s/it]  9%|         | 280/3250 [49:56<8:40:06, 10.51s/it]                                                      9%|         | 280/3250 [49:56<8:40:06, 10.51s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.9461045861244202, 'eval_runtime': 2.3576, 'eval_samples_per_second': 5.09, 'eval_steps_per_second': 1.272, 'epoch': 0.09}
                                                      9%|         | 280/3250 [49:58<8:40:06, 10.51s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_10/tmp-checkpoint-280
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-280
/projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-280
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-280
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-280
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-280/pytorch_model.binthe pytorch model path is
 /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-280/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-280/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-280/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.9195, 'learning_rate': 9.819041528138231e-05, 'epoch': 0.09}
{'loss': 0.9473, 'learning_rate': 9.817749962596115e-05, 'epoch': 0.09}
{'loss': 0.9851, 'learning_rate': 9.816453889801098e-05, 'epoch': 0.09}
{'loss': 0.9442, 'learning_rate': 9.815153310965718e-05, 'epoch': 0.09}
{'loss': 0.9076, 'learning_rate': 9.81384822730674e-05, 'epoch': 0.09}
{'loss': 0.943, 'learning_rate': 9.812538640045132e-05, 'epoch': 0.09}
  9%|         | 281/3250 [50:09<9:19:45, 11.31s/it]                                                      9%|         | 281/3250 [50:09<9:19:45, 11.31s/it]  9%|         | 282/3250 [50:19<9:06:18, 11.04s/it]                                                      9%|         | 282/3250 [50:19<9:06:18, 11.04s/it]  9%|         | 283/3250 [50:30<8:56:56, 10.86s/it]                                                      9%|         | 283/3250 [50:30<8:56:56, 10.86s/it]  9%|         | 284/3250 [50:40<8:50:06, 10.72s/it]                                                      9%|         | 284/3250 [50:40<8:50:06, 10.72s/it]  9%|         | 285/3250 [50:50<8:45:17, 10.63s/it]                                                      9%|         | 285/3250 [50:50<8:45:17, 10.63s/it]  9%|         | 286/3250 [51:01<8:42:26, 10.58s/it]                                                      9%|         | 286/3250 [51:01<8:42:26, 10.58s/it]  9%|         | 287/3250 [51:11<8:39{'loss': 0.9964, 'learning_rate': 9.811224550406082e-05, 'epoch': 0.09}
{'loss': 0.9527, 'learning_rate': 9.809905959618985e-05, 'epoch': 0.09}
{'loss': 0.9424, 'learning_rate': 9.808582868917458e-05, 'epoch': 0.09}
{'loss': 0.8969, 'learning_rate': 9.807255279539313e-05, 'epoch': 0.09}
:45, 10.53s/it]                                                      9%|         | 287/3250 [51:11<8:39:45, 10.53s/it]  9%|         | 288/3250 [51:22<8:37:42, 10.49s/it]                                                      9%|         | 288/3250 [51:22<8:37:42, 10.49s/it]  9%|         | 289/3250 [51:32<8:36:33, 10.47s/it]                                                      9%|         | 289/3250 [51:32<8:36:33, 10.47s/it]  9%|         | 290/3250 [51:42<8:35:33, 10.45s/it]                                                      9%|         | 290/3250 [51:42<8:35:33, 10.45s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.9438743591308594, 'eval_runtime': 2.1005, 'eval_samples_per_second': 5.713, 'eval_steps_per_second': 1.428, 'epoch': 0.09}
                                                      9%|         | 290/3250 [51:45<8:35:33, 10.45s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_10/tmp-checkpoint-290
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  I AM HERE AND I AM SAVING THE MODEL
/projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-290the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-290

I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-290
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-290
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-290/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-290/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-290/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-290/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.9456, 'learning_rate': 9.805923192726581e-05, 'epoch': 0.09}
{'loss': 0.9428, 'learning_rate': 9.804586609725499e-05, 'epoch': 0.09}
{'loss': 0.9374, 'learning_rate': 9.803245531786506e-05, 'epoch': 0.09}
{'loss': 0.9163, 'learning_rate': 9.801899960164253e-05, 'epoch': 0.09}
{'loss': 0.9408, 'learning_rate': 9.800549896117589e-05, 'epoch': 0.09}
{'loss': 0.9425, 'learning_rate': 9.79919534090957e-05, 'epoch': 0.09}
  9%|         | 291/3250 [51:55<9:12:19, 11.20s/it]                                                      9%|         | 291/3250 [51:55<9:12:19, 11.20s/it]  9%|         | 292/3250 [52:06<9:00:27, 10.96s/it]                                                      9%|         | 292/3250 [52:06<9:00:27, 10.96s/it]  9%|         | 293/3250 [52:16<8:51:57, 10.79s/it]                                                      9%|         | 293/3250 [52:16<8:51:57, 10.79s/it]  9%|         | 294/3250 [52:27<8:46:04, 10.68s/it]                                                      9%|         | 294/3250 [52:27<8:46:04, 10.68s/it]  9%|         | 295/3250 [52:37<8:41:51, 10.60s/it]                                                      9%|         | 295/3250 [52:37<8:41:51, 10.60s/it]  9%|         | 296/3250 [52:48<8:44:12, 10.65s/it]                                                      9%|         | 296/3250 [52:48<8:44:12, 10.65s/it]  9%|         | 297/3250 [52:58<8:40{'loss': 0.9507, 'learning_rate': 9.79783629580745e-05, 'epoch': 0.09}
{'loss': 0.9341, 'learning_rate': 9.796472762082687e-05, 'epoch': 0.09}
{'loss': 0.918, 'learning_rate': 9.795104741010938e-05, 'epoch': 0.09}
{'loss': 0.9724, 'learning_rate': 9.793732233872056e-05, 'epoch': 0.09}
:35, 10.58s/it]                                                      9%|         | 297/3250 [52:58<8:40:35, 10.58s/it]  9%|         | 298/3250 [53:09<8:37:58, 10.53s/it]                                                      9%|         | 298/3250 [53:09<8:37:58, 10.53s/it]  9%|         | 299/3250 [53:19<8:36:13, 10.50s/it]                                                      9%|         | 299/3250 [53:19<8:36:13, 10.50s/it]  9%|         | 300/3250 [53:29<8:34:55, 10.47s/it]                                                      9%|         | 300/3250 [53:29<8:34:55, 10.47s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.9389969110488892, 'eval_runtime': 2.1104, 'eval_samples_per_second': 5.686, 'eval_steps_per_second': 1.422, 'epoch': 0.09}
                                                      9%|         | 300/3250 [53:32<8:34:55, 10.47s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_10/tmp-checkpoint-300
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-300I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-300

I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-300
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-300
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-300/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-300/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-300/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-300/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.9355, 'learning_rate': 9.792355241950088e-05, 'epoch': 0.09}
{'loss': 0.9561, 'learning_rate': 9.790973766533288e-05, 'epoch': 0.09}
{'loss': 0.9634, 'learning_rate': 9.789587808914093e-05, 'epoch': 0.09}
{'loss': 0.9227, 'learning_rate': 9.78819737038914e-05, 'epoch': 0.09}
{'loss': 0.9071, 'learning_rate': 9.786802452259251e-05, 'epoch': 0.09}
{'loss': 0.9287, 'learning_rate': 9.785403055829449e-05, 'epoch': 0.09}
  9%|         | 301/3250 [53:42<9:10:44, 11.21s/it]                                                      9%|         | 301/3250 [53:42<9:10:44, 11.21s/it]  9%|         | 302/3250 [53:53<8:59:05, 10.97s/it]                                                      9%|         | 302/3250 [53:53<8:59:05, 10.97s/it]  9%|         | 303/3250 [54:03<8:50:34, 10.80s/it]                                                      9%|         | 303/3250 [54:03<8:50:34, 10.80s/it]  9%|         | 304/3250 [54:14<8:44:39, 10.69s/it]                                                      9%|         | 304/3250 [54:14<8:44:39, 10.69s/it]  9%|         | 305/3250 [54:24<8:40:20, 10.60s/it]                                                      9%|         | 305/3250 [54:24<8:40:20, 10.60s/it]  9%|         | 306/3250 [54:34<8:37:37, 10.55s/it]                                                      9%|         | 306/3250 [54:34<8:37:37, 10.55s/it]  9%|         | 307/3250 [54:45<8:35{'loss': 0.9324, 'learning_rate': 9.783999182408941e-05, 'epoch': 0.09}
{'loss': 0.9271, 'learning_rate': 9.78259083331112e-05, 'epoch': 0.09}
{'loss': 0.967, 'learning_rate': 9.781178009853568e-05, 'epoch': 0.1}
{'loss': 1.404, 'learning_rate': 9.779760713358059e-05, 'epoch': 0.1}
:24, 10.51s/it]                                                      9%|         | 307/3250 [54:45<8:35:24, 10.51s/it]  9%|         | 308/3250 [54:55<8:34:03, 10.48s/it]                                                      9%|         | 308/3250 [54:55<8:34:03, 10.48s/it] 10%|         | 309/3250 [55:06<8:32:59, 10.47s/it]                                                     10%|         | 309/3250 [55:06<8:32:59, 10.47s/it] 10%|         | 310/3250 [55:16<8:31:55, 10.45s/it]                                                     10%|         | 310/3250 [55:16<8:31:55, 10.45s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.9372246861457825, 'eval_runtime': 2.1082, 'eval_samples_per_second': 5.692, 'eval_steps_per_second': 1.423, 'epoch': 0.1}
                                                     10%|         | 310/3250 [55:18<8:31:55, 10.45s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_10/tmp-checkpoint-310
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in I AM HERE AND I AM SAVING THE MODEL
 the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-310
/projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-310
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-310
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-310
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-310/pytorch_model.bin
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-310/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-310/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-310/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.8571, 'learning_rate': 9.778338945150542e-05, 'epoch': 0.1}
{'loss': 0.909, 'learning_rate': 9.776912706561156e-05, 'epoch': 0.1}
{'loss': 0.9584, 'learning_rate': 9.775481998924222e-05, 'epoch': 0.1}
{'loss': 0.9381, 'learning_rate': 9.77404682357824e-05, 'epoch': 0.1}
{'loss': 0.9172, 'learning_rate': 9.77260718186589e-05, 'epoch': 0.1}
{'loss': 0.9276, 'learning_rate': 9.771163075134029e-05, 'epoch': 0.1}
 10%|         | 311/3250 [55:29<9:08:51, 11.20s/it]                                                     10%|         | 311/3250 [55:29<9:08:51, 11.20s/it] 10%|         | 312/3250 [55:40<9:04:30, 11.12s/it]                                                     10%|         | 312/3250 [55:40<9:04:30, 11.12s/it] 10%|         | 313/3250 [55:50<8:53:56, 10.91s/it]                                                     10%|         | 313/3250 [55:50<8:53:56, 10.91s/it] 10%|         | 314/3250 [56:01<8:46:23, 10.76s/it]                                                     10%|         | 314/3250 [56:01<8:46:23, 10.76s/it] 10%|         | 315/3250 [56:11<8:41:08, 10.65s/it]                                                     10%|         | 315/3250 [56:11<8:41:08, 10.65s/it] 10%|         | 316/3250 [56:22<8:37:28, 10.58s/it]                                                     10%|         | 316/3250 [56:22<8:37:28, 10.58s/it] 10%|         | 317/3250 [56:32<8:34{'loss': 0.9658, 'learning_rate': 9.769714504733694e-05, 'epoch': 0.1}
{'loss': 0.9571, 'learning_rate': 9.768261472020099e-05, 'epoch': 0.1}
{'loss': 0.9, 'learning_rate': 9.76680397835263e-05, 'epoch': 0.1}
{'loss': 0.858, 'learning_rate': 9.765342025094847e-05, 'epoch': 0.1}
:55, 10.53s/it]                                                     10%|         | 317/3250 [56:32<8:34:55, 10.53s/it] 10%|         | 318/3250 [56:42<8:32:56, 10.50s/it]                                                     10%|         | 318/3250 [56:42<8:32:56, 10.50s/it] 10%|         | 319/3250 [56:53<8:31:27, 10.47s/it]                                                     10%|         | 319/3250 [56:53<8:31:27, 10.47s/it] 10%|         | 320/3250 [57:03<8:30:15, 10.45s/it]                                                     10%|         | 320/3250 [57:03<8:30:15, 10.45s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.9353635311126709, 'eval_runtime': 2.1119, 'eval_samples_per_second': 5.682, 'eval_steps_per_second': 1.421, 'epoch': 0.1}
                                                     10%|         | 320/3250 [57:05<8:30:15, 10.45s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_10/tmp-checkpoint-320
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-320
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-320
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-320
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-320
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-320/pytorch_model.binthe pytorch model path is
 /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-320/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-320/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-320/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.9457, 'learning_rate': 9.763875613614482e-05, 'epoch': 0.1}
{'loss': 0.9133, 'learning_rate': 9.762404745283439e-05, 'epoch': 0.1}
{'loss': 0.9208, 'learning_rate': 9.760929421477791e-05, 'epoch': 0.1}
{'loss': 0.912, 'learning_rate': 9.759449643577778e-05, 'epoch': 0.1}
{'loss': 0.9119, 'learning_rate': 9.757965412967811e-05, 'epoch': 0.1}
{'loss': 0.9232, 'learning_rate': 9.756476731036461e-05, 'epoch': 0.1}
 10%|         | 321/3250 [57:16<9:07:35, 11.22s/it]                                                     10%|         | 321/3250 [57:16<9:07:35, 11.22s/it] 10%|         | 322/3250 [57:27<8:55:24, 10.97s/it]                                                     10%|         | 322/3250 [57:27<8:55:24, 10.97s/it] 10%|         | 323/3250 [57:37<8:46:47, 10.80s/it]                                                     10%|         | 323/3250 [57:37<8:46:47, 10.80s/it] 10%|         | 324/3250 [57:48<8:40:52, 10.68s/it]                                                     10%|         | 324/3250 [57:48<8:40:52, 10.68s/it] 10%|         | 325/3250 [57:58<8:36:36, 10.60s/it]                                                     10%|         | 325/3250 [57:58<8:36:36, 10.60s/it] 10%|         | 326/3250 [58:08<8:33:35, 10.54s/it]                                                     10%|         | 326/3250 [58:08<8:33:35, 10.54s/it] 10%|         | 327/3250 [58:19<8:31{'loss': 0.9252, 'learning_rate': 9.75498359917647e-05, 'epoch': 0.1}
{'loss': 0.9241, 'learning_rate': 9.753486018784736e-05, 'epoch': 0.1}
{'loss': 0.924, 'learning_rate': 9.751983991262326e-05, 'epoch': 0.1}
{'loss': 0.9147, 'learning_rate': 9.750477518014461e-05, 'epoch': 0.1}
:30, 10.50s/it]                                                     10%|         | 327/3250 [58:19<8:31:30, 10.50s/it] 10%|         | 328/3250 [58:29<8:35:00, 10.58s/it]                                                     10%|         | 328/3250 [58:29<8:35:00, 10.58s/it] 10%|         | 329/3250 [58:40<8:32:26, 10.53s/it]                                                     10%|         | 329/3250 [58:40<8:32:26, 10.53s/it] 10%|         | 330/3250 [58:50<8:30:45, 10.50s/it]                                                     10%|         | 330/3250 [58:50<8:30:45, 10.50s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.9310925006866455, 'eval_runtime': 2.1177, 'eval_samples_per_second': 5.666, 'eval_steps_per_second': 1.417, 'epoch': 0.1}
                                                     10%|         | 330/3250 [58:52<8:30:45, 10.50s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_10/tmp-checkpoint-330
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-330I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-330

I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-330
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-330
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-330/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-330/pytorch_model.binthe pytorch model path is
 /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-330/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-330/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.949, 'learning_rate': 9.748966600450525e-05, 'epoch': 0.1}
{'loss': 0.9078, 'learning_rate': 9.74745123998406e-05, 'epoch': 0.1}
{'loss': 0.9799, 'learning_rate': 9.745931438032763e-05, 'epoch': 0.1}
{'loss': 0.8841, 'learning_rate': 9.744407196018488e-05, 'epoch': 0.1}
{'loss': 0.9146, 'learning_rate': 9.74287851536724e-05, 'epoch': 0.1}
{'loss': 0.9158, 'learning_rate': 9.74134539750918e-05, 'epoch': 0.1}
 10%|         | 331/3250 [59:03<9:08:04, 11.27s/it]                                                     10%|         | 331/3250 [59:03<9:08:04, 11.27s/it] 10%|         | 332/3250 [59:14<8:55:36, 11.01s/it]                                                     10%|         | 332/3250 [59:14<8:55:36, 11.01s/it] 10%|         | 333/3250 [59:24<8:46:36, 10.83s/it]                                                     10%|         | 333/3250 [59:24<8:46:36, 10.83s/it] 10%|         | 334/3250 [59:35<8:40:30, 10.71s/it]                                                     10%|         | 334/3250 [59:35<8:40:30, 10.71s/it] 10%|         | 335/3250 [59:45<8:36:03, 10.62s/it]                                                     10%|         | 335/3250 [59:45<8:36:03, 10.62s/it] 10%|         | 336/3250 [59:55<8:33:06, 10.57s/it]                                                     10%|         | 336/3250 [59:55<8:33:06, 10.57s/it] 10%|         | 337/3250 [1:00:06<8:{'loss': 0.9128, 'learning_rate': 9.739807843878617e-05, 'epoch': 0.1}
{'loss': 0.9103, 'learning_rate': 9.738265855914013e-05, 'epoch': 0.1}
{'loss': 0.9234, 'learning_rate': 9.736719435057976e-05, 'epoch': 0.1}
{'loss': 0.9096, 'learning_rate': 9.735168582757264e-05, 'epoch': 0.1}
31:22, 10.53s/it]                                                       10%|         | 337/3250 [1:00:06<8:31:22, 10.53s/it] 10%|         | 338/3250 [1:00:16<8:29:57, 10.51s/it]                                                       10%|         | 338/3250 [1:00:16<8:29:57, 10.51s/it] 10%|         | 339/3250 [1:00:27<8:28:34, 10.48s/it]                                                       10%|         | 339/3250 [1:00:27<8:28:34, 10.48s/it] 10%|         | 340/3250 [1:00:37<8:27:25, 10.46s/it]                                                       10%|         | 340/3250 [1:00:37<8:27:25, 10.46s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.9268626570701599, 'eval_runtime': 2.1208, 'eval_samples_per_second': 5.658, 'eval_steps_per_second': 1.415, 'epoch': 0.1}
                                                       10%|         | 340/3250 [1:00:39<8:27:25, 10.46s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_10/tmp-checkpoint-340
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-340
/projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-340
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-340
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-340
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-340/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-340/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-340/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-340/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 1.3379, 'learning_rate': 9.733613300462776e-05, 'epoch': 0.1}
{'loss': 0.9063, 'learning_rate': 9.732053589629561e-05, 'epoch': 0.11}
{'loss': 0.9453, 'learning_rate': 9.730489451716809e-05, 'epoch': 0.11}
{'loss': 0.919, 'learning_rate': 9.728920888187849e-05, 'epoch': 0.11}
{'loss': 0.9157, 'learning_rate': 9.727347900510155e-05, 'epoch': 0.11}
{'loss': 0.8902, 'learning_rate': 9.725770490155338e-05, 'epoch': 0.11}
 10%|         | 341/3250 [1:00:50<9:03:58, 11.22s/it]                                                       10%|         | 341/3250 [1:00:50<9:03:58, 11.22s/it] 11%|         | 342/3250 [1:01:01<8:52:48, 10.99s/it]                                                       11%|         | 342/3250 [1:01:01<8:52:48, 10.99s/it] 11%|         | 343/3250 [1:01:11<8:44:14, 10.82s/it]                                                       11%|         | 343/3250 [1:01:11<8:44:14, 10.82s/it] 11%|         | 344/3250 [1:01:22<8:38:17, 10.70s/it]                                                       11%|         | 344/3250 [1:01:22<8:38:17, 10.70s/it] 11%|         | 345/3250 [1:01:32<8:41:15, 10.77s/it]                                                       11%|         | 345/3250 [1:01:32<8:41:15, 10.77s/it] 11%|         | 346/3250 [1:01:43<8:35:38, 10.65s/it]                                                       11%|         | 346/3250 [1:01:43<8:35:38, 10.65s/it] 11{'loss': 0.9692, 'learning_rate': 9.724188658599146e-05, 'epoch': 0.11}
{'loss': 0.929, 'learning_rate': 9.722602407321463e-05, 'epoch': 0.11}
{'loss': 0.8996, 'learning_rate': 9.721011737806309e-05, 'epoch': 0.11}
{'loss': 0.8964, 'learning_rate': 9.719416651541839e-05, 'epoch': 0.11}
%|         | 347/3250 [1:01:53<8:32:04, 10.58s/it]                                                       11%|         | 347/3250 [1:01:53<8:32:04, 10.58s/it] 11%|         | 348/3250 [1:02:04<8:29:32, 10.54s/it]                                                       11%|         | 348/3250 [1:02:04<8:29:32, 10.54s/it] 11%|         | 349/3250 [1:02:14<8:27:34, 10.50s/it]                                                       11%|         | 349/3250 [1:02:14<8:27:34, 10.50s/it] 11%|         | 350/3250 [1:02:24<8:26:10, 10.47s/it]                                                       11%|         | 350/3250 [1:02:24<8:26:10, 10.47s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.9245646595954895, 'eval_runtime': 2.1162, 'eval_samples_per_second': 5.671, 'eval_steps_per_second': 1.418, 'epoch': 0.11}
                                                       11%|         | 350/3250 [1:02:27<8:26:10, 10.47s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_10/tmp-checkpoint-350
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-350I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in 
 /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-350
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-350
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-350
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-350/pytorch_model.binthe pytorch model path is
 /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-350/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-350/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-350/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.9056, 'learning_rate': 9.717817150020336e-05, 'epoch': 0.11}
{'loss': 0.8895, 'learning_rate': 9.716213234738215e-05, 'epoch': 0.11}
{'loss': 0.9048, 'learning_rate': 9.714604907196025e-05, 'epoch': 0.11}
{'loss': 0.9042, 'learning_rate': 9.712992168898436e-05, 'epoch': 0.11}
{'loss': 0.9008, 'learning_rate': 9.711375021354248e-05, 'epoch': 0.11}
{'loss': 0.8985, 'learning_rate': 9.709753466076387e-05, 'epoch': 0.11}
 11%|         | 351/3250 [1:02:37<9:01:44, 11.21s/it]                                                       11%|         | 351/3250 [1:02:37<9:01:44, 11.21s/it] 11%|         | 352/3250 [1:02:48<8:49:54, 10.97s/it]                                                       11%|         | 352/3250 [1:02:48<8:49:54, 10.97s/it] 11%|         | 353/3250 [1:02:58<8:41:34, 10.80s/it]                                                       11%|         | 353/3250 [1:02:58<8:41:34, 10.80s/it] 11%|         | 354/3250 [1:03:09<8:35:29, 10.68s/it]                                                       11%|         | 354/3250 [1:03:09<8:35:29, 10.68s/it] 11%|         | 355/3250 [1:03:19<8:31:29, 10.60s/it]                                                       11%|         | 355/3250 [1:03:19<8:31:29, 10.60s/it] 11%|         | 356/3250 [1:03:29<8:28:47, 10.55s/it]                                                       11%|         | 356/3250 [1:03:29<8:28:47, 10.55s/it] 11{'loss': 0.9347, 'learning_rate': 9.708127504581902e-05, 'epoch': 0.11}
{'loss': 0.8728, 'learning_rate': 9.706497138391961e-05, 'epoch': 0.11}
{'loss': 0.8969, 'learning_rate': 9.704862369031857e-05, 'epoch': 0.11}
{'loss': 0.9311, 'learning_rate': 9.703223198031002e-05, 'epoch': 0.11}
%|         | 357/3250 [1:03:40<8:26:40, 10.51s/it]                                                       11%|         | 357/3250 [1:03:40<8:26:40, 10.51s/it] 11%|         | 358/3250 [1:03:50<8:24:55, 10.48s/it]                                                       11%|         | 358/3250 [1:03:50<8:24:55, 10.48s/it] 11%|         | 359/3250 [1:04:01<8:23:48, 10.46s/it]                                                       11%|         | 359/3250 [1:04:01<8:23:48, 10.46s/it] 11%|         | 360/3250 [1:04:11<8:22:49, 10.44s/it]                                                       11%|         | 360/3250 [1:04:11<8:22:49, 10.44s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.920805037021637, 'eval_runtime': 2.1513, 'eval_samples_per_second': 5.578, 'eval_steps_per_second': 1.394, 'epoch': 0.11}
                                                       11%|         | 360/3250 [1:04:13<8:22:49, 10.44s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_10/tmp-checkpoint-360
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-360I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-360

I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-360
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-360
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-360/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-360/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-360/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-360/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.9143, 'learning_rate': 9.701579626922922e-05, 'epoch': 0.11}
{'loss': 0.886, 'learning_rate': 9.699931657245263e-05, 'epoch': 0.11}
{'loss': 0.9683, 'learning_rate': 9.698279290539788e-05, 'epoch': 0.11}
{'loss': 0.9069, 'learning_rate': 9.696622528352368e-05, 'epoch': 0.11}
{'loss': 0.8927, 'learning_rate': 9.694961372232991e-05, 'epoch': 0.11}
{'loss': 0.8992, 'learning_rate': 9.693295823735753e-05, 'epoch': 0.11}
 11%|         | 361/3250 [1:04:25<9:05:30, 11.33s/it]                                                       11%|         | 361/3250 [1:04:25<9:05:30, 11.33s/it] 11%|         | 362/3250 [1:04:35<8:51:43, 11.05s/it]                                                       11%|         | 362/3250 [1:04:35<8:51:43, 11.05s/it] 11%|         | 363/3250 [1:04:45<8:42:49, 10.87s/it]                                                       11%|         | 363/3250 [1:04:45<8:42:49, 10.87s/it] 11%|         | 364/3250 [1:04:56<8:36:39, 10.74s/it]                                                       11%|         | 364/3250 [1:04:56<8:36:39, 10.74s/it] 11%|         | 365/3250 [1:05:07<8:37:45, 10.77s/it]                                                       11%|         | 365/3250 [1:05:07<8:37:45, 10.77s/it] 11%|        | 366/3250 [1:05:17<8:31:33, 10.64s/it]                                                       11%|        | 366/3250 [1:05:17<8:31:33, 10.64s/it]{'loss': 0.8673, 'learning_rate': 9.69162588441886e-05, 'epoch': 0.11}
{'loss': 0.9244, 'learning_rate': 9.689951555844628e-05, 'epoch': 0.11}
{'loss': 0.8805, 'learning_rate': 9.688272839579477e-05, 'epoch': 0.11}
{'loss': 0.919, 'learning_rate': 9.686589737193929e-05, 'epoch': 0.11}
 11%|        | 367/3250 [1:05:27<8:27:26, 10.56s/it]                                                       11%|        | 367/3250 [1:05:27<8:27:26, 10.56s/it] 11%|        | 368/3250 [1:05:38<8:24:38, 10.51s/it]                                                       11%|        | 368/3250 [1:05:38<8:24:38, 10.51s/it] 11%|        | 369/3250 [1:05:48<8:22:37, 10.47s/it]                                                       11%|        | 369/3250 [1:05:48<8:22:37, 10.47s/it] 11%|        | 370/3250 [1:05:58<8:21:10, 10.44s/it]                                                       11%|        | 370/3250 [1:05:58<8:21:10, 10.44s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.9186421632766724, 'eval_runtime': 2.3555, 'eval_samples_per_second': 5.095, 'eval_steps_per_second': 1.274, 'epoch': 0.11}
                                                       11%|        | 370/3250 [1:06:01<8:21:10, 10.44s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_10/tmp-checkpoint-370
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-370
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-370
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-370
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-370
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-370/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-370/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-370/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-370/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 1.3338, 'learning_rate': 9.684902250262618e-05, 'epoch': 0.11}
{'loss': 0.8801, 'learning_rate': 9.683210380364272e-05, 'epoch': 0.11}
{'loss': 0.9026, 'learning_rate': 9.681514129081724e-05, 'epoch': 0.11}
{'loss': 0.9416, 'learning_rate': 9.6798134980019e-05, 'epoch': 0.12}
{'loss': 0.9038, 'learning_rate': 9.678108488715833e-05, 'epoch': 0.12}
{'loss': 0.8585, 'learning_rate': 9.676399102818646e-05, 'epoch': 0.12}
 11%|        | 371/3250 [1:06:12<9:04:55, 11.36s/it]                                                       11%|        | 371/3250 [1:06:12<9:04:55, 11.36s/it] 11%|        | 372/3250 [1:06:22<8:50:50, 11.07s/it]                                                       11%|        | 372/3250 [1:06:22<8:50:50, 11.07s/it] 11%|        | 373/3250 [1:06:33<8:40:49, 10.86s/it]                                                       11%|        | 373/3250 [1:06:33<8:40:49, 10.86s/it] 12%|        | 374/3250 [1:06:43<8:33:39, 10.72s/it]                                                       12%|        | 374/3250 [1:06:43<8:33:39, 10.72s/it] 12%|        | 375/3250 [1:06:54<8:40:50, 10.87s/it]                                                       12%|        | 375/3250 [1:06:54<8:40:50, 10.87s/it] 12%|        | 376/3250 [1:07:05<8:34:09, 10.73s/it]                                                       12%|        | 376/3250 [1:07:05{'loss': 0.8925, 'learning_rate': 9.674685341909552e-05, 'epoch': 0.12}
{'loss': 0.9581, 'learning_rate': 9.67296720759187e-05, 'epoch': 0.12}
{'loss': 0.8993, 'learning_rate': 9.671244701472999e-05, 'epoch': 0.12}
{'loss': 0.8949, 'learning_rate': 9.669517825164434e-05, 'epoch': 0.12}
<8:34:09, 10.73s/it] 12%|        | 377/3250 [1:07:15<8:28:35, 10.62s/it]                                                       12%|        | 377/3250 [1:07:15<8:28:35, 10.62s/it] 12%|        | 378/3250 [1:07:26<8:28:39, 10.63s/it]                                                       12%|        | 378/3250 [1:07:26<8:28:39, 10.63s/it] 12%|        | 379/3250 [1:07:36<8:24:42, 10.55s/it]                                                       12%|        | 379/3250 [1:07:36<8:24:42, 10.55s/it] 12%|        | 380/3250 [1:07:47<8:21:58, 10.49s/it]                                                       12%|        | 380/3250 [1:07:47<8:21:58, 10.49s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.9174726605415344, 'eval_runtime': 3.0774, 'eval_samples_per_second': 3.899, 'eval_steps_per_second': 0.975, 'epoch': 0.12}
                                                       12%|        | 380/3250 [1:07:50<8:21:58, 10.49s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_10/tmp-checkpoint-380
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in I AM HERE AND I AM SAVING THE MODEL 
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-380
/projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-380
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-380
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-380
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-380/pytorch_model.bin
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-380/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-380/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-380/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.8502, 'learning_rate': 9.667786580281755e-05, 'epoch': 0.12}
{'loss': 0.8902, 'learning_rate': 9.66605096844463e-05, 'epoch': 0.12}
{'loss': 0.897, 'learning_rate': 9.664310991276815e-05, 'epoch': 0.12}
{'loss': 0.8949, 'learning_rate': 9.662566650406146e-05, 'epoch': 0.12}
{'loss': 0.8592, 'learning_rate': 9.660817947464547e-05, 'epoch': 0.12}
{'loss': 0.8987, 'learning_rate': 9.659064884088016e-05, 'epoch': 0.12}
 12%|        | 381/3250 [1:08:00<9:11:15, 11.53s/it]                                                       12%|        | 381/3250 [1:08:00<9:11:15, 11.53s/it] 12%|        | 382/3250 [1:08:11<8:54:23, 11.18s/it]                                                       12%|        | 382/3250 [1:08:11<8:54:23, 11.18s/it] 12%|        | 383/3250 [1:08:21<8:42:19, 10.93s/it]                                                       12%|        | 383/3250 [1:08:21<8:42:19, 10.93s/it] 12%|        | 384/3250 [1:08:32<8:34:16, 10.77s/it]                                                       12%|        | 384/3250 [1:08:32<8:34:16, 10.77s/it] 12%|        | 385/3250 [1:08:42<8:28:15, 10.64s/it]                                                       12%|        | 385/3250 [1:08:42<8:28:15, 10.64s/it] 12%|        | 386/3250 [1:08:52<8:24:13, 10.56s/it]                                                       12%|        | 386/3250 [1:08:52{'loss': 0.8977, 'learning_rate': 9.657307461916635e-05, 'epoch': 0.12}
{'loss': 0.9021, 'learning_rate': 9.655545682594566e-05, 'epoch': 0.12}
{'loss': 0.8935, 'learning_rate': 9.65377954777004e-05, 'epoch': 0.12}
{'loss': 0.8824, 'learning_rate': 9.652009059095369e-05, 'epoch': 0.12}
<8:24:13, 10.56s/it] 12%|        | 387/3250 [1:09:03<8:21:16, 10.51s/it]                                                       12%|        | 387/3250 [1:09:03<8:21:16, 10.51s/it] 12%|        | 388/3250 [1:09:13<8:18:52, 10.46s/it]                                                       12%|        | 388/3250 [1:09:13<8:18:52, 10.46s/it] 12%|        | 389/3250 [1:09:23<8:17:11, 10.43s/it]                                                       12%|        | 389/3250 [1:09:23<8:17:11, 10.43s/it] 12%|        | 390/3250 [1:09:34<8:16:19, 10.41s/it]                                                       12%|        | 390/3250 [1:09:34<8:16:19, 10.41s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.9133641123771667, 'eval_runtime': 2.0963, 'eval_samples_per_second': 5.724, 'eval_steps_per_second': 1.431, 'epoch': 0.12}
                                                       12%|        | 390/3250 [1:09:36<8:16:19, 10.41s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_10/tmp-checkpoint-390
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  I AM HERE AND I AM SAVING THE MODEL/projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-390
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-390

I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-390
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-390
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-390/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-390/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-390/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-390/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.9258, 'learning_rate': 9.650234218226934e-05, 'epoch': 0.12}
{'loss': 0.8861, 'learning_rate': 9.648455026825194e-05, 'epoch': 0.12}
{'loss': 0.912, 'learning_rate': 9.64667148655467e-05, 'epoch': 0.12}
{'loss': 0.9217, 'learning_rate': 9.644883599083958e-05, 'epoch': 0.12}
{'loss': 0.8804, 'learning_rate': 9.643091366085717e-05, 'epoch': 0.12}
{'loss': 0.8625, 'learning_rate': 9.641294789236676e-05, 'epoch': 0.12}
 12%|        | 391/3250 [1:09:47<8:53:17, 11.19s/it]                                                       12%|        | 391/3250 [1:09:47<8:53:17, 11.19s/it] 12%|        | 392/3250 [1:09:57<8:41:21, 10.95s/it]                                                       12%|        | 392/3250 [1:09:57<8:41:21, 10.95s/it] 12%|        | 393/3250 [1:10:07<8:33:08, 10.78s/it]                                                       12%|        | 393/3250 [1:10:07<8:33:08, 10.78s/it] 12%|        | 394/3250 [1:10:18<8:34:15, 10.80s/it]                                                       12%|        | 394/3250 [1:10:18<8:34:15, 10.80s/it] 12%|        | 395/3250 [1:10:29<8:27:47, 10.67s/it]                                                       12%|        | 395/3250 [1:10:29<8:27:47, 10.67s/it] 12%|        | 396/3250 [1:10:39<8:23:08, 10.58s/it]                                                       12%|        | 396/3250 [1:10:39{'loss': 0.8769, 'learning_rate': 9.639493870217622e-05, 'epoch': 0.12}
{'loss': 0.8955, 'learning_rate': 9.637688610713409e-05, 'epoch': 0.12}
{'loss': 0.8753, 'learning_rate': 9.635879012412951e-05, 'epoch': 0.12}
{'loss': 0.911, 'learning_rate': 9.634065077009218e-05, 'epoch': 0.12}
<8:23:08, 10.58s/it] 12%|        | 397/3250 [1:10:49<8:20:01, 10.52s/it]                                                       12%|        | 397/3250 [1:10:49<8:20:01, 10.52s/it] 12%|        | 398/3250 [1:11:00<8:17:46, 10.47s/it]                                                       12%|        | 398/3250 [1:11:00<8:17:46, 10.47s/it] 12%|        | 399/3250 [1:11:10<8:16:17, 10.44s/it]                                                       12%|        | 399/3250 [1:11:10<8:16:17, 10.44s/it] 12%|        | 400/3250 [1:11:21<8:15:13, 10.43s/it]                                                       12%|        | 400/3250 [1:11:21<8:15:13, 10.43s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.9102553725242615, 'eval_runtime': 2.1076, 'eval_samples_per_second': 5.694, 'eval_steps_per_second': 1.423, 'epoch': 0.12}
                                                       12%|        | 400/3250 [1:11:23<8:15:13, 10.43s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_10/tmp-checkpoint-400
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-400
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-400
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-400
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-400
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-400/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-400/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-400/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-400/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 1.3613, 'learning_rate': 9.632246806199241e-05, 'epoch': 0.12}
{'loss': 0.824, 'learning_rate': 9.630424201684105e-05, 'epoch': 0.12}
{'loss': 0.8585, 'learning_rate': 9.628597265168953e-05, 'epoch': 0.12}
{'loss': 0.9128, 'learning_rate': 9.626765998362974e-05, 'epoch': 0.12}
{'loss': 0.8955, 'learning_rate': 9.624930402979416e-05, 'epoch': 0.12}
{'loss': 0.8655, 'learning_rate': 9.62309048073557e-05, 'epoch': 0.12}
 12%|        | 401/3250 [1:11:34<8:50:58, 11.18s/it]                                                       12%|        | 401/3250 [1:11:34<8:50:58, 11.18s/it] 12%|        | 402/3250 [1:11:44<8:39:18, 10.94s/it]                                                       12%|        | 402/3250 [1:11:44<8:39:18, 10.94s/it] 12%|        | 403/3250 [1:11:54<8:31:07, 10.77s/it]                                                       12%|        | 403/3250 [1:11:54<8:31:07, 10.77s/it] 12%|        | 404/3250 [1:12:05<8:25:06, 10.65s/it]                                                       12%|        | 404/3250 [1:12:05<8:25:06, 10.65s/it] 12%|        | 405/3250 [1:12:15<8:21:01, 10.57s/it]                                                       12%|        | 405/3250 [1:12:15<8:21:01, 10.57s/it] 12%|        | 406/3250 [1:12:25<8:18:04, 10.51s/it]                                                       12%|        | 406/3250 [1:12:25{'loss': 0.8671, 'learning_rate': 9.62124623335278e-05, 'epoch': 0.13}
{'loss': 0.9308, 'learning_rate': 9.619397662556435e-05, 'epoch': 0.13}
{'loss': 0.9024, 'learning_rate': 9.617544770075965e-05, 'epoch': 0.13}
{'loss': 0.8694, 'learning_rate': 9.615687557644848e-05, 'epoch': 0.13}
<8:18:04, 10.51s/it] 13%|        | 407/3250 [1:12:36<8:15:54, 10.47s/it]                                                       13%|        | 407/3250 [1:12:36<8:15:54, 10.47s/it] 13%|        | 408/3250 [1:12:46<8:14:05, 10.43s/it]                                                       13%|        | 408/3250 [1:12:46<8:14:05, 10.43s/it] 13%|        | 409/3250 [1:12:56<8:13:04, 10.41s/it]                                                       13%|        | 409/3250 [1:12:56<8:13:04, 10.41s/it] 13%|        | 410/3250 [1:13:07<8:17:18, 10.51s/it]                                                       13%|        | 410/3250 [1:13:07<8:17:18, 10.51s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.9092775583267212, 'eval_runtime': 2.3327, 'eval_samples_per_second': 5.144, 'eval_steps_per_second': 1.286, 'epoch': 0.13}
                                                       13%|        | 410/3250 [1:13:10<8:17:18, 10.51s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_10/tmp-checkpoint-410
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-410
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-410
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-410
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-410
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-410/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-410/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-410/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-410/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.8209, 'learning_rate': 9.613826027000601e-05, 'epoch': 0.13}
{'loss': 0.9034, 'learning_rate': 9.611960179884783e-05, 'epoch': 0.13}
{'loss': 0.8783, 'learning_rate': 9.61009001804299e-05, 'epoch': 0.13}
{'loss': 0.8756, 'learning_rate': 9.608215543224853e-05, 'epoch': 0.13}
{'loss': 0.8737, 'learning_rate': 9.60633675718404e-05, 'epoch': 0.13}
{'loss': 0.8798, 'learning_rate': 9.604453661678253e-05, 'epoch': 0.13}
 13%|        | 411/3250 [1:13:20<8:54:26, 11.30s/it]                                                       13%|        | 411/3250 [1:13:20<8:54:26, 11.30s/it] 13%|        | 412/3250 [1:13:31<8:41:19, 11.02s/it]                                                       13%|        | 412/3250 [1:13:31<8:41:19, 11.02s/it] 13%|        | 413/3250 [1:13:41<8:31:56, 10.83s/it]                                                       13%|        | 413/3250 [1:13:41<8:31:56, 10.83s/it] 13%|        | 414/3250 [1:13:51<8:25:14, 10.69s/it]                                                       13%|        | 414/3250 [1:13:51<8:25:14, 10.69s/it] 13%|        | 415/3250 [1:14:02<8:20:45, 10.60s/it]                                                       13%|        | 415/3250 [1:14:02<8:20:45, 10.60s/it] 13%|        | 416/3250 [1:14:12<8:17:26, 10.53s/it]                                                       13%|        | 416/3250 [1:14:12{'loss': 0.8727, 'learning_rate': 9.602566258469225e-05, 'epoch': 0.13}
{'loss': 0.882, 'learning_rate': 9.600674549322717e-05, 'epoch': 0.13}
{'loss': 0.8804, 'learning_rate': 9.598778536008522e-05, 'epoch': 0.13}
{'loss': 0.8894, 'learning_rate': 9.596878220300454e-05, 'epoch': 0.13}
<8:17:26, 10.53s/it] 13%|        | 417/3250 [1:14:23<8:14:42, 10.48s/it]                                                       13%|        | 417/3250 [1:14:23<8:14:42, 10.48s/it] 13%|        | 418/3250 [1:14:33<8:12:53, 10.44s/it]                                                       13%|        | 418/3250 [1:14:33<8:12:53, 10.44s/it] 13%|        | 419/3250 [1:14:43<8:11:31, 10.42s/it]                                                       13%|        | 419/3250 [1:14:43<8:11:31, 10.42s/it] 13%|        | 420/3250 [1:14:54<8:10:32, 10.40s/it]                                                       13%|        | 420/3250 [1:14:54<8:10:32, 10.40s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.9051452279090881, 'eval_runtime': 2.1165, 'eval_samples_per_second': 5.67, 'eval_steps_per_second': 1.417, 'epoch': 0.13}
                                                       13%|        | 420/3250 [1:14:56<8:10:32, 10.40s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_10/tmp-checkpoint-420
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in I AM HERE AND I AM SAVING THE MODEL 
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-420
/projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-420
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-420
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-420
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-420/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-420/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-420/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-420/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.8715, 'learning_rate': 9.594973603976363e-05, 'epoch': 0.13}
{'loss': 0.9132, 'learning_rate': 9.59306468881811e-05, 'epoch': 0.13}
{'loss': 0.8554, 'learning_rate': 9.591151476611584e-05, 'epoch': 0.13}
{'loss': 0.9374, 'learning_rate': 9.589233969146695e-05, 'epoch': 0.13}
{'loss': 0.8482, 'learning_rate': 9.58731216821737e-05, 'epoch': 0.13}
{'loss': 0.8648, 'learning_rate': 9.585386075621554e-05, 'epoch': 0.13}
 13%|        | 421/3250 [1:15:07<8:46:53, 11.17s/it]                                                       13%|        | 421/3250 [1:15:07<8:46:53, 11.17s/it] 13%|        | 422/3250 [1:15:17<8:35:11, 10.93s/it]                                                       13%|        | 422/3250 [1:15:17<8:35:11, 10.93s/it] 13%|        | 423/3250 [1:15:27<8:27:09, 10.76s/it]                                                       13%|        | 423/3250 [1:15:27<8:27:09, 10.76s/it] 13%|        | 424/3250 [1:15:38<8:21:21, 10.64s/it]                                                       13%|        | 424/3250 [1:15:38<8:21:21, 10.64s/it] 13%|        | 425/3250 [1:15:48<8:17:09, 10.56s/it]                                                       13%|        | 425/3250 [1:15:48<8:17:09, 10.56s/it] 13%|        | 426/3250 [1:15:58<8:14:23, 10.50s/it]                                                       13%|        | 426/3250 [1:15:58{'loss': 0.87, 'learning_rate': 9.583455693161201e-05, 'epoch': 0.13}
{'loss': 0.8718, 'learning_rate': 9.581521022642286e-05, 'epoch': 0.13}
{'loss': 0.8746, 'learning_rate': 9.579582065874793e-05, 'epoch': 0.13}
{'loss': 0.8834, 'learning_rate': 9.577638824672715e-05, 'epoch': 0.13}
<8:14:23, 10.50s/it] 13%|        | 427/3250 [1:16:09<8:15:54, 10.54s/it]                                                       13%|        | 427/3250 [1:16:09<8:15:54, 10.54s/it] 13%|        | 428/3250 [1:16:19<8:13:37, 10.50s/it]                                                       13%|        | 428/3250 [1:16:19<8:13:37, 10.50s/it] 13%|        | 429/3250 [1:16:30<8:11:52, 10.46s/it]                                                       13%|        | 429/3250 [1:16:30<8:11:52, 10.46s/it] 13%|        | 430/3250 [1:16:40<8:10:08, 10.43s/it]                                                       13%|        | 430/3250 [1:16:40<8:10:08, 10.43s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.9020075798034668, 'eval_runtime': 2.103, 'eval_samples_per_second': 5.706, 'eval_steps_per_second': 1.427, 'epoch': 0.13}
                                                       13%|        | 430/3250 [1:16:42<8:10:08, 10.43s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_10/tmp-checkpoint-430
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-430
 /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-430
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-430
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-430
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-430/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-430/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-430/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-430/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.8711, 'learning_rate': 9.575691300854055e-05, 'epoch': 0.13}
{'loss': 1.3017, 'learning_rate': 9.57373949624082e-05, 'epoch': 0.13}
{'loss': 0.8585, 'learning_rate': 9.571783412659027e-05, 'epoch': 0.13}
{'loss': 0.896, 'learning_rate': 9.56982305193869e-05, 'epoch': 0.13}
{'loss': 0.8729, 'learning_rate': 9.567858415913826e-05, 'epoch': 0.13}
{'loss': 0.8681, 'learning_rate': 9.565889506422456e-05, 'epoch': 0.13}
 13%|        | 431/3250 [1:16:53<8:44:45, 11.17s/it]                                                       13%|        | 431/3250 [1:16:53<8:44:45, 11.17s/it] 13%|        | 432/3250 [1:17:03<8:33:12, 10.93s/it]                                                       13%|        | 432/3250 [1:17:03<8:33:12, 10.93s/it] 13%|        | 433/3250 [1:17:14<8:25:06, 10.76s/it]                                                       13%|        | 433/3250 [1:17:14<8:25:06, 10.76s/it] 13%|        | 434/3250 [1:17:24<8:19:25, 10.64s/it]                                                       13%|        | 434/3250 [1:17:24<8:19:25, 10.64s/it] 13%|        | 435/3250 [1:17:35<8:15:14, 10.56s/it]                                                       13%|        | 435/3250 [1:17:35<8:15:14, 10.56s/it] 13%|        | 436/3250 [1:17:45<8:12:16, 10.50s/it]                                                       13%|        | 436/3250 [1:17:45{'loss': 0.8418, 'learning_rate': 9.563916325306594e-05, 'epoch': 0.13}
{'loss': 0.9168, 'learning_rate': 9.561938874412255e-05, 'epoch': 0.13}
{'loss': 0.8955, 'learning_rate': 9.559957155589444e-05, 'epoch': 0.14}
{'loss': 0.8671, 'learning_rate': 9.557971170692161e-05, 'epoch': 0.14}
<8:12:16, 10.50s/it] 13%|        | 437/3250 [1:17:55<8:09:53, 10.45s/it]                                                       13%|        | 437/3250 [1:17:55<8:09:53, 10.45s/it] 13%|        | 438/3250 [1:18:06<8:08:23, 10.42s/it]                                                       13%|        | 438/3250 [1:18:06<8:08:23, 10.42s/it] 14%|        | 439/3250 [1:18:16<8:07:11, 10.40s/it]                                                       14%|        | 439/3250 [1:18:16<8:07:11, 10.40s/it] 14%|        | 440/3250 [1:18:26<8:06:24, 10.39s/it]                                                       14%|        | 440/3250 [1:18:26<8:06:24, 10.39s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.9016030430793762, 'eval_runtime': 2.1017, 'eval_samples_per_second': 5.71, 'eval_steps_per_second': 1.427, 'epoch': 0.14}
                                                       14%|        | 440/3250 [1:18:28<8:06:24, 10.39s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_10/tmp-checkpoint-440
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-440 /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-440

I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-440
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-440
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-440/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-440/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-440/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-440/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.8512, 'learning_rate': 9.555980921578398e-05, 'epoch': 0.14}
{'loss': 0.8677, 'learning_rate': 9.553986410110134e-05, 'epoch': 0.14}
{'loss': 0.8404, 'learning_rate': 9.551987638153339e-05, 'epoch': 0.14}
{'loss': 0.8666, 'learning_rate': 9.549984607577964e-05, 'epoch': 0.14}
{'loss': 0.8571, 'learning_rate': 9.54797732025795e-05, 'epoch': 0.14}
{'loss': 0.8573, 'learning_rate': 9.545965778071218e-05, 'epoch': 0.14}
 14%|        | 441/3250 [1:18:39<8:41:39, 11.14s/it]                                                       14%|        | 441/3250 [1:18:39<8:41:39, 11.14s/it] 14%|        | 442/3250 [1:18:50<8:30:30, 10.91s/it]                                                       14%|        | 442/3250 [1:18:50<8:30:30, 10.91s/it] 14%|        | 443/3250 [1:19:01<8:31:03, 10.92s/it]                                                       14%|        | 443/3250 [1:19:01<8:31:03, 10.92s/it] 14%|        | 444/3250 [1:19:11<8:22:52, 10.75s/it]                                                       14%|        | 444/3250 [1:19:11<8:22:52, 10.75s/it] 14%|        | 445/3250 [1:19:21<8:17:04, 10.63s/it]                                                       14%|        | 445/3250 [1:19:21<8:17:04, 10.63s/it] 14%|        | 446/3250 [1:19:32<8:12:46, 10.54s/it]                                                       14%|        | 446/3250 [1:19:32{'loss': 0.8645, 'learning_rate': 9.543949982899667e-05, 'epoch': 0.14}
{'loss': 0.8822, 'learning_rate': 9.541929936629175e-05, 'epoch': 0.14}
{'loss': 0.8516, 'learning_rate': 9.539905641149605e-05, 'epoch': 0.14}
{'loss': 0.855, 'learning_rate': 9.537877098354786e-05, 'epoch': 0.14}
<8:12:46, 10.54s/it] 14%|        | 447/3250 [1:19:42<8:09:54, 10.49s/it]                                                       14%|        | 447/3250 [1:19:42<8:09:54, 10.49s/it] 14%|        | 448/3250 [1:19:52<8:07:53, 10.45s/it]                                                       14%|        | 448/3250 [1:19:52<8:07:53, 10.45s/it] 14%|        | 449/3250 [1:20:03<8:06:28, 10.42s/it]                                                       14%|        | 449/3250 [1:20:03<8:06:28, 10.42s/it] 14%|        | 450/3250 [1:20:13<8:05:39, 10.41s/it]                                                       14%|        | 450/3250 [1:20:13<8:05:39, 10.41s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8966046571731567, 'eval_runtime': 2.1103, 'eval_samples_per_second': 5.686, 'eval_steps_per_second': 1.422, 'epoch': 0.14}
                                                       14%|        | 450/3250 [1:20:15<8:05:39, 10.41s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_10/tmp-checkpoint-450
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-450
/projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-450
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-450
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-450
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-450/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-450/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-450/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-450/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.8851, 'learning_rate': 9.535844310142524e-05, 'epoch': 0.14}
{'loss': 0.8758, 'learning_rate': 9.533807278414597e-05, 'epoch': 0.14}
{'loss': 0.8408, 'learning_rate': 9.531766005076755e-05, 'epoch': 0.14}
{'loss': 0.9319, 'learning_rate': 9.529720492038712e-05, 'epoch': 0.14}
{'loss': 0.8619, 'learning_rate': 9.527670741214152e-05, 'epoch': 0.14}
{'loss': 0.8472, 'learning_rate': 9.525616754520721e-05, 'epoch': 0.14}
 14%|        | 451/3250 [1:20:26<8:40:17, 11.15s/it]                                                       14%|        | 451/3250 [1:20:26<8:40:17, 11.15s/it] 14%|        | 452/3250 [1:20:36<8:29:07, 10.92s/it]                                                       14%|        | 452/3250 [1:20:36<8:29:07, 10.92s/it] 14%|        | 453/3250 [1:20:47<8:21:05, 10.75s/it]                                                       14%|        | 453/3250 [1:20:47<8:21:05, 10.75s/it] 14%|        | 454/3250 [1:20:57<8:15:30, 10.63s/it]                                                       14%|        | 454/3250 [1:20:57<8:15:30, 10.63s/it] 14%|        | 455/3250 [1:21:07<8:11:33, 10.55s/it]                                                       14%|        | 455/3250 [1:21:07<8:11:33, 10.55s/it] 14%|        | 456/3250 [1:21:18<8:08:35, 10.49s/it]                                                       14%|        | 456/3250 [1:21:18{'loss': 0.8424, 'learning_rate': 9.52355853388003e-05, 'epoch': 0.14}
{'loss': 0.8343, 'learning_rate': 9.521496081217651e-05, 'epoch': 0.14}
{'loss': 0.887, 'learning_rate': 9.519429398463114e-05, 'epoch': 0.14}
{'loss': 0.8429, 'learning_rate': 9.517358487549906e-05, 'epoch': 0.14}
<8:08:35, 10.49s/it] 14%|        | 457/3250 [1:21:28<8:06:36, 10.45s/it]                                                       14%|        | 457/3250 [1:21:28<8:06:36, 10.45s/it] 14%|        | 458/3250 [1:21:38<8:04:56, 10.42s/it]                                                       14%|        | 458/3250 [1:21:38<8:04:56, 10.42s/it] 14%|        | 459/3250 [1:21:49<8:07:23, 10.48s/it]                                                       14%|        | 459/3250 [1:21:49<8:07:23, 10.48s/it] 14%|        | 460/3250 [1:21:59<8:05:32, 10.44s/it]                                                       14%|        | 460/3250 [1:21:59<8:05:32, 10.44s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8939602375030518, 'eval_runtime': 2.1199, 'eval_samples_per_second': 5.661, 'eval_steps_per_second': 1.415, 'epoch': 0.14}
                                                       14%|        | 460/3250 [1:22:02<8:05:32, 10.44s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_10/tmp-checkpoint-460
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-460
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-460
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-460
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-460
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-460/pytorch_model.bin
the pytorch model path isthe pytorch model path is  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-460/pytorch_model.bin/projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-460/pytorch_model.bin

the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-460/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.8816, 'learning_rate': 9.51528335041547e-05, 'epoch': 0.14}
{'loss': 1.3074, 'learning_rate': 9.513203989001207e-05, 'epoch': 0.14}
{'loss': 0.832, 'learning_rate': 9.511120405252464e-05, 'epoch': 0.14}
{'loss': 0.8675, 'learning_rate': 9.509032601118541e-05, 'epoch': 0.14}
{'loss': 0.896, 'learning_rate': 9.506940578552688e-05, 'epoch': 0.14}
{'loss': 0.8658, 'learning_rate': 9.504844339512095e-05, 'epoch': 0.14}
 14%|        | 461/3250 [1:22:13<8:42:08, 11.23s/it]                                                       14%|        | 461/3250 [1:22:13<8:42:08, 11.23s/it] 14%|        | 462/3250 [1:22:23<8:29:16, 10.96s/it]                                                       14%|        | 462/3250 [1:22:23<8:29:16, 10.96s/it] 14%|        | 463/3250 [1:22:33<8:20:16, 10.77s/it]                                                       14%|        | 463/3250 [1:22:33<8:20:16, 10.77s/it] 14%|        | 464/3250 [1:22:43<8:13:46, 10.63s/it]                                                       14%|        | 464/3250 [1:22:43<8:13:46, 10.63s/it] 14%|        | 465/3250 [1:22:54<8:09:30, 10.55s/it]                                                       14%|        | 465/3250 [1:22:54<8:09:30, 10.55s/it] 14%|        | 466/3250 [1:23:04<8:06:40, 10.49s/it]                                                       14%|        | 466/3250 [1:23:04{'loss': 0.8295, 'learning_rate': 9.502743885957907e-05, 'epoch': 0.14}
{'loss': 0.8448, 'learning_rate': 9.500639219855206e-05, 'epoch': 0.14}
{'loss': 0.9208, 'learning_rate': 9.49853034317301e-05, 'epoch': 0.14}
{'loss': 0.8554, 'learning_rate': 9.496417257884285e-05, 'epoch': 0.14}
<8:06:40, 10.49s/it] 14%|        | 467/3250 [1:23:14<8:04:13, 10.44s/it]                                                       14%|        | 467/3250 [1:23:14<8:04:13, 10.44s/it] 14%|        | 468/3250 [1:23:25<8:02:34, 10.41s/it]                                                       14%|        | 468/3250 [1:23:25<8:02:34, 10.41s/it] 14%|        | 469/3250 [1:23:35<8:01:35, 10.39s/it]                                                       14%|        | 469/3250 [1:23:35<8:01:35, 10.39s/it] 14%|        | 470/3250 [1:23:46<8:00:45, 10.38s/it]                                                       14%|        | 470/3250 [1:23:46<8:00:45, 10.38s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8934237957000732, 'eval_runtime': 2.1141, 'eval_samples_per_second': 5.676, 'eval_steps_per_second': 1.419, 'epoch': 0.14}
                                                       14%|        | 470/3250 [1:23:48<8:00:45, 10.38s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_10/tmp-checkpoint-470
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-470
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-470
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-470
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-470
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-470/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-470/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-470/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-470/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.8552, 'learning_rate': 9.494299965965933e-05, 'epoch': 0.14}
{'loss': 0.8029, 'learning_rate': 9.492178469398787e-05, 'epoch': 0.15}
{'loss': 0.8493, 'learning_rate': 9.490052770167617e-05, 'epoch': 0.15}
{'loss': 0.8627, 'learning_rate': 9.487922870261122e-05, 'epoch': 0.15}
{'loss': 0.8551, 'learning_rate': 9.485788771671935e-05, 'epoch': 0.15}
{'loss': 0.8333, 'learning_rate': 9.483650476396615e-05, 'epoch': 0.15}
 14%|        | 471/3250 [1:23:59<8:37:05, 11.16s/it]                                                       14%|        | 471/3250 [1:23:59<8:37:05, 11.16s/it] 15%|        | 472/3250 [1:24:09<8:25:35, 10.92s/it]                                                       15%|        | 472/3250 [1:24:09<8:25:35, 10.92s/it] 15%|        | 473/3250 [1:24:19<8:17:24, 10.75s/it]                                                       15%|        | 473/3250 [1:24:19<8:17:24, 10.75s/it] 15%|        | 474/3250 [1:24:30<8:11:24, 10.62s/it]                                                       15%|        | 474/3250 [1:24:30<8:11:24, 10.62s/it] 15%|        | 475/3250 [1:24:40<8:07:18, 10.54s/it]                                                       15%|        | 475/3250 [1:24:40<8:07:18, 10.54s/it] 15%|        | 476/3250 [1:24:51<8:09:30, 10.59s/it]                                                       15%|        | 476/3250 [1:24:51{'loss': 0.8517, 'learning_rate': 9.481507986435647e-05, 'epoch': 0.15}
{'loss': 0.8575, 'learning_rate': 9.47936130379344e-05, 'epoch': 0.15}
{'loss': 0.8654, 'learning_rate': 9.477210430478327e-05, 'epoch': 0.15}
{'loss': 0.8598, 'learning_rate': 9.475055368502559e-05, 'epoch': 0.15}
<8:09:30, 10.59s/it] 15%|        | 477/3250 [1:25:01<8:06:00, 10.52s/it]                                                       15%|        | 477/3250 [1:25:01<8:06:00, 10.52s/it] 15%|        | 478/3250 [1:25:11<8:03:16, 10.46s/it]                                                       15%|        | 478/3250 [1:25:11<8:03:16, 10.46s/it] 15%|        | 479/3250 [1:25:22<8:01:32, 10.43s/it]                                                       15%|        | 479/3250 [1:25:22<8:01:32, 10.43s/it] 15%|        | 480/3250 [1:25:32<8:00:12, 10.40s/it]                                                       15%|        | 480/3250 [1:25:32<8:00:12, 10.40s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8884730339050293, 'eval_runtime': 2.3301, 'eval_samples_per_second': 5.15, 'eval_steps_per_second': 1.287, 'epoch': 0.15}
                                                       15%|        | 480/3250 [1:25:34<8:00:12, 10.40s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_10/tmp-checkpoint-480
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-480/projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-480

I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-480
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-480
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-480/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-480/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-480/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-480/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.8403, 'learning_rate': 9.472896119882308e-05, 'epoch': 0.15}
{'loss': 0.8848, 'learning_rate': 9.470732686637664e-05, 'epoch': 0.15}
{'loss': 0.8511, 'learning_rate': 9.468565070792628e-05, 'epoch': 0.15}
{'loss': 0.8655, 'learning_rate': 9.466393274375116e-05, 'epoch': 0.15}
{'loss': 0.8705, 'learning_rate': 9.464217299416956e-05, 'epoch': 0.15}
{'loss': 0.8328, 'learning_rate': 9.462037147953886e-05, 'epoch': 0.15}
 15%|        | 481/3250 [1:25:45<8:38:02, 11.23s/it]                                                       15%|        | 481/3250 [1:25:45<8:38:02, 11.23s/it] 15%|        | 482/3250 [1:25:55<8:25:55, 10.97s/it]                                                       15%|        | 482/3250 [1:25:55<8:25:55, 10.97s/it] 15%|        | 483/3250 [1:26:06<8:17:06, 10.78s/it]                                                       15%|        | 483/3250 [1:26:06<8:17:06, 10.78s/it] 15%|        | 484/3250 [1:26:16<8:11:00, 10.65s/it]                                                       15%|        | 484/3250 [1:26:16<8:11:00, 10.65s/it] 15%|        | 485/3250 [1:26:26<8:06:26, 10.56s/it]                                                       15%|        | 485/3250 [1:26:26<8:06:26, 10.56s/it] 15%|        | 486/3250 [1:26:37<8:03:33, 10.50s/it]                                                       15%|        | 486/3250 [1:26:37{'loss': 0.8262, 'learning_rate': 9.459852822025546e-05, 'epoch': 0.15}
{'loss': 0.8286, 'learning_rate': 9.457664323675489e-05, 'epoch': 0.15}
{'loss': 0.8708, 'learning_rate': 9.455471654951165e-05, 'epoch': 0.15}
{'loss': 0.8309, 'learning_rate': 9.453274817903931e-05, 'epoch': 0.15}
<8:03:33, 10.50s/it] 15%|        | 487/3250 [1:26:47<8:01:23, 10.45s/it]                                                       15%|        | 487/3250 [1:26:47<8:01:23, 10.45s/it] 15%|        | 488/3250 [1:26:58<8:00:10, 10.43s/it]                                                       15%|        | 488/3250 [1:26:58<8:00:10, 10.43s/it] 15%|        | 489/3250 [1:27:08<7:59:04, 10.41s/it]                                                       15%|        | 489/3250 [1:27:08<7:59:04, 10.41s/it] 15%|        | 490/3250 [1:27:18<7:58:09, 10.39s/it]                                                       15%|        | 490/3250 [1:27:18<7:58:09, 10.39s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8864140510559082, 'eval_runtime': 2.1109, 'eval_samples_per_second': 5.685, 'eval_steps_per_second': 1.421, 'epoch': 0.15}
                                                       15%|        | 490/3250 [1:27:20<7:58:09, 10.39s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_10/tmp-checkpoint-490
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-490I AM HERE AND I AM SAVING THE MODEL

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-490
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-490
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-490
the pytorch model path isthe pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-490/pytorch_model.bin 
/projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-490/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-490/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-490/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.8729, 'learning_rate': 9.45107381458904e-05, 'epoch': 0.15}
{'loss': 1.3268, 'learning_rate': 9.448868647065642e-05, 'epoch': 0.15}
{'loss': 0.7743, 'learning_rate': 9.446659317396787e-05, 'epoch': 0.15}
{'loss': 0.835, 'learning_rate': 9.444445827649415e-05, 'epoch': 0.15}
{'loss': 0.8728, 'learning_rate': 9.442228179894362e-05, 'epoch': 0.15}
{'loss': 0.8635, 'learning_rate': 9.440006376206349e-05, 'epoch': 0.15}
 15%|        | 491/3250 [1:27:31<8:34:04, 11.18s/it]                                                       15%|        | 491/3250 [1:27:31<8:34:04, 11.18s/it] 15%|        | 492/3250 [1:27:42<8:25:49, 11.00s/it]                                                       15%|        | 492/3250 [1:27:42<8:25:49, 11.00s/it] 15%|        | 493/3250 [1:27:52<8:16:49, 10.81s/it]                                                       15%|        | 493/3250 [1:27:52<8:16:49, 10.81s/it] 15%|        | 494/3250 [1:28:03<8:10:12, 10.67s/it]                                                       15%|        | 494/3250 [1:28:03<8:10:12, 10.67s/it] 15%|        | 495/3250 [1:28:13<8:05:25, 10.57s/it]                                                       15%|        | 495/3250 [1:28:13<8:05:25, 10.57s/it] 15%|        | 496/3250 [1:28:23<8:02:23, 10.51s/it]                                                       15%|        | 496/3250 [1:28:23{'loss': 0.8471, 'learning_rate': 9.437780418663988e-05, 'epoch': 0.15}
{'loss': 0.8293, 'learning_rate': 9.435550309349777e-05, 'epoch': 0.15}
{'loss': 0.8929, 'learning_rate': 9.433316050350099e-05, 'epoch': 0.15}
{'loss': 0.8702, 'learning_rate': 9.431077643755217e-05, 'epoch': 0.15}
<8:02:23, 10.51s/it] 15%|        | 497/3250 [1:28:34<8:00:00, 10.46s/it]                                                       15%|        | 497/3250 [1:28:34<8:00:00, 10.46s/it] 15%|        | 498/3250 [1:28:44<7:58:28, 10.43s/it]                                                       15%|        | 498/3250 [1:28:44<7:58:28, 10.43s/it] 15%|        | 499/3250 [1:28:54<7:57:11, 10.41s/it]                                                       15%|        | 499/3250 [1:28:54<7:57:11, 10.41s/it] 15%|        | 500/3250 [1:29:05<7:56:13, 10.39s/it]                                                       15%|        | 500/3250 [1:29:05<7:56:13, 10.39s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.888084352016449, 'eval_runtime': 2.1025, 'eval_samples_per_second': 5.707, 'eval_steps_per_second': 1.427, 'epoch': 0.15}
                                                       15%|        | 500/3250 [1:29:07<7:56:13, 10.39s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_10/tmp-checkpoint-500
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-500
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-500
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-500
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-500
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-500/pytorch_model.bin
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-500/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-500/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-500/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.831, 'learning_rate': 9.428835091659276e-05, 'epoch': 0.15}
{'loss': 0.7862, 'learning_rate': 9.426588396160299e-05, 'epoch': 0.15}
{'loss': 0.856, 'learning_rate': 9.424337559360183e-05, 'epoch': 0.15}
{'loss': 0.8316, 'learning_rate': 9.422082583364706e-05, 'epoch': 0.16}
{'loss': 0.8442, 'learning_rate': 9.419823470283511e-05, 'epoch': 0.16}
{'loss': 0.8285, 'learning_rate': 9.417560222230115e-05, 'epoch': 0.16}
 15%|        | 501/3250 [1:29:18<8:31:53, 11.17s/it]                                                       15%|        | 501/3250 [1:29:18<8:31:53, 11.17s/it] 15%|        | 502/3250 [1:29:28<8:20:20, 10.92s/it]                                                       15%|        | 502/3250 [1:29:28<8:20:20, 10.92s/it] 15%|        | 503/3250 [1:29:38<8:12:32, 10.76s/it]                                                       15%|        | 503/3250 [1:29:38<8:12:32, 10.76s/it] 16%|        | 504/3250 [1:29:49<8:06:41, 10.63s/it]                                                       16%|        | 504/3250 [1:29:49<8:06:41, 10.63s/it] 16%|        | 505/3250 [1:29:59<8:02:25, 10.54s/it]                                                       16%|        | 505/3250 [1:29:59<8:02:25, 10.54s/it] 16%|        | 506/3250 [1:30:09<7:59:37, 10.49s/it]                                                       16%|        | 506/3250 [1:30:09{'loss': 0.8345, 'learning_rate': 9.415292841321903e-05, 'epoch': 0.16}
{'loss': 0.835, 'learning_rate': 9.413021329680128e-05, 'epoch': 0.16}
{'loss': 0.8393, 'learning_rate': 9.4107456894299e-05, 'epoch': 0.16}
{'loss': 0.8478, 'learning_rate': 9.408465922700206e-05, 'epoch': 0.16}
<7:59:37, 10.49s/it] 16%|        | 507/3250 [1:30:20<7:57:34, 10.45s/it]                                                       16%|        | 507/3250 [1:30:20<7:57:34, 10.45s/it] 16%|        | 508/3250 [1:30:30<7:56:02, 10.42s/it]                                                       16%|        | 508/3250 [1:30:30<7:56:02, 10.42s/it] 16%|        | 509/3250 [1:30:41<8:00:21, 10.51s/it]                                                       16%|        | 509/3250 [1:30:41<8:00:21, 10.51s/it] 16%|        | 510/3250 [1:30:51<7:58:07, 10.47s/it]                                                       16%|        | 510/3250 [1:30:51<7:58:07, 10.47s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8829329609870911, 'eval_runtime': 2.3389, 'eval_samples_per_second': 5.131, 'eval_steps_per_second': 1.283, 'epoch': 0.16}
                                                       16%|        | 510/3250 [1:30:54<7:58:07, 10.47s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_10/tmp-checkpoint-510
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  I AM HERE AND I AM SAVING THE MODEL
/projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-510the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-510

I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-510
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-510
the pytorch model path isthe pytorch model path isthe pytorch model path is  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-510/pytorch_model.bin/projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-510/pytorch_model.bin

 /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-510/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-510/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.8385, 'learning_rate': 9.40618203162388e-05, 'epoch': 0.16}
{'loss': 0.8303, 'learning_rate': 9.403894018337622e-05, 'epoch': 0.16}
{'loss': 0.8742, 'learning_rate': 9.401601884981983e-05, 'epoch': 0.16}
{'loss': 0.8187, 'learning_rate': 9.399305633701373e-05, 'epoch': 0.16}
{'loss': 0.8989, 'learning_rate': 9.397005266644054e-05, 'epoch': 0.16}
{'loss': 0.8193, 'learning_rate': 9.394700785962139e-05, 'epoch': 0.16}
 16%|        | 511/3250 [1:31:04<8:34:22, 11.27s/it]                                                       16%|        | 511/3250 [1:31:04<8:34:22, 11.27s/it] 16%|        | 512/3250 [1:31:15<8:21:40, 10.99s/it]                                                       16%|        | 512/3250 [1:31:15<8:21:40, 10.99s/it] 16%|        | 513/3250 [1:31:25<8:12:51, 10.80s/it]                                                       16%|        | 513/3250 [1:31:25<8:12:51, 10.80s/it] 16%|        | 514/3250 [1:31:36<8:06:55, 10.68s/it]                                                       16%|        | 514/3250 [1:31:36<8:06:55, 10.68s/it] 16%|        | 515/3250 [1:31:46<8:02:26, 10.58s/it]                                                       16%|        | 515/3250 [1:31:46<8:02:26, 10.58s/it] 16%|        | 516/3250 [1:31:56<7:59:46, 10.53s/it]                                                       16%|        | 516/3250 [1:31:56{'loss': 0.8238, 'learning_rate': 9.392392193811584e-05, 'epoch': 0.16}
{'loss': 0.8271, 'learning_rate': 9.390079492352199e-05, 'epoch': 0.16}
{'loss': 0.8303, 'learning_rate': 9.387762683747636e-05, 'epoch': 0.16}
{'loss': 0.8238, 'learning_rate': 9.385441770165385e-05, 'epoch': 0.16}
<7:59:46, 10.53s/it] 16%|        | 517/3250 [1:32:07<7:57:24, 10.48s/it]                                                       16%|        | 517/3250 [1:32:07<7:57:24, 10.48s/it] 16%|        | 518/3250 [1:32:17<7:55:30, 10.44s/it]                                                       16%|        | 518/3250 [1:32:17<7:55:30, 10.44s/it] 16%|        | 519/3250 [1:32:27<7:54:20, 10.42s/it]                                                       16%|        | 519/3250 [1:32:27<7:54:20, 10.42s/it] 16%|        | 520/3250 [1:32:38<7:53:42, 10.41s/it]                                                       16%|        | 520/3250 [1:32:38<7:53:42, 10.41s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8800114393234253, 'eval_runtime': 2.1088, 'eval_samples_per_second': 5.69, 'eval_steps_per_second': 1.423, 'epoch': 0.16}
                                                       16%|        | 520/3250 [1:32:40<7:53:42, 10.41s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_10/tmp-checkpoint-520
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-520
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-520
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-520
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-520
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-520/pytorch_model.bin
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-520/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-520/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-520/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.8457, 'learning_rate': 9.383116753776784e-05, 'epoch': 0.16}
{'loss': 0.8335, 'learning_rate': 9.380787636757002e-05, 'epoch': 0.16}
{'loss': 1.2717, 'learning_rate': 9.378454421285049e-05, 'epoch': 0.16}
{'loss': 0.8139, 'learning_rate': 9.376117109543769e-05, 'epoch': 0.16}
{'loss': 0.8604, 'learning_rate': 9.373775703719836e-05, 'epoch': 0.16}
{'loss': 0.8409, 'learning_rate': 9.371430206003758e-05, 'epoch': 0.16}
 16%|        | 521/3250 [1:32:51<8:28:41, 11.18s/it]                                                       16%|        | 521/3250 [1:32:51<8:28:41, 11.18s/it] 16%|        | 522/3250 [1:33:01<8:17:33, 10.94s/it]                                                       16%|        | 522/3250 [1:33:01<8:17:33, 10.94s/it] 16%|        | 523/3250 [1:33:11<8:09:24, 10.77s/it]                                                       16%|        | 523/3250 [1:33:11<8:09:24, 10.77s/it] 16%|        | 524/3250 [1:33:22<8:03:45, 10.65s/it]                                                       16%|        | 524/3250 [1:33:22<8:03:45, 10.65s/it] 16%|        | 525/3250 [1:33:32<8:02:48, 10.63s/it]                                                       16%|        | 525/3250 [1:33:32<8:02:48, 10.63s/it] 16%|        | 526/3250 [1:33:43<7:58:56, 10.55s/it]                                                       16%|        | 526/3250 [1:33:43{'loss': 0.8376, 'learning_rate': 9.369080618589864e-05, 'epoch': 0.16}
{'loss': 0.8117, 'learning_rate': 9.366726943676321e-05, 'epoch': 0.16}
{'loss': 0.8788, 'learning_rate': 9.364369183465106e-05, 'epoch': 0.16}
{'loss': 0.8521, 'learning_rate': 9.362007340162029e-05, 'epoch': 0.16}
<7:58:56, 10.55s/it] 16%|        | 527/3250 [1:33:53<7:56:05, 10.49s/it]                                                       16%|        | 527/3250 [1:33:53<7:56:05, 10.49s/it] 16%|        | 528/3250 [1:34:04<7:54:02, 10.45s/it]                                                       16%|        | 528/3250 [1:34:04<7:54:02, 10.45s/it] 16%|        | 529/3250 [1:34:14<7:52:40, 10.42s/it]                                                       16%|        | 529/3250 [1:34:14<7:52:40, 10.42s/it] 16%|        | 530/3250 [1:34:24<7:52:03, 10.41s/it]                                                       16%|        | 530/3250 [1:34:24<7:52:03, 10.41s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.882091224193573, 'eval_runtime': 2.1063, 'eval_samples_per_second': 5.697, 'eval_steps_per_second': 1.424, 'epoch': 0.16}
                                                       16%|        | 530/3250 [1:34:26<7:52:03, 10.41s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_10/tmp-checkpoint-530
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-530I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-530

I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-530
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-530
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-530/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-530/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-530/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-530/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.8246, 'learning_rate': 9.359641415976714e-05, 'epoch': 0.16}
{'loss': 0.8105, 'learning_rate': 9.357271413122606e-05, 'epoch': 0.16}
{'loss': 0.8303, 'learning_rate': 9.354897333816963e-05, 'epoch': 0.16}
{'loss': 0.8137, 'learning_rate': 9.35251918028086e-05, 'epoch': 0.16}
{'loss': 0.8227, 'learning_rate': 9.350136954739183e-05, 'epoch': 0.16}
{'loss': 0.836, 'learning_rate': 9.347750659420623e-05, 'epoch': 0.16}
 16%|        | 531/3250 [1:34:37<8:27:07, 11.19s/it]                                                       16%|        | 531/3250 [1:34:37<8:27:07, 11.19s/it] 16%|        | 532/3250 [1:34:48<8:16:26, 10.96s/it]                                                       16%|        | 532/3250 [1:34:48<8:16:26, 10.96s/it] 16%|        | 533/3250 [1:34:58<8:08:34, 10.79s/it]                                                       16%|        | 533/3250 [1:34:58<8:08:34, 10.79s/it] 16%|        | 534/3250 [1:35:08<8:03:10, 10.67s/it]                                                       16%|        | 534/3250 [1:35:08<8:03:10, 10.67s/it] 16%|        | 535/3250 [1:35:19<7:59:11, 10.59s/it]                                                       16%|        | 535/3250 [1:35:19<7:59:11, 10.59s/it] 16%|        | 536/3250 [1:35:29<7:56:32, 10.54s/it]                                                       16%|        | 536/3250 [1:35:29{'loss': 0.8154, 'learning_rate': 9.345360296557684e-05, 'epoch': 0.17}
{'loss': 0.8202, 'learning_rate': 9.342965868386674e-05, 'epoch': 0.17}
{'loss': 0.8492, 'learning_rate': 9.340567377147702e-05, 'epoch': 0.17}
{'loss': 0.8073, 'learning_rate': 9.338164825084682e-05, 'epoch': 0.17}
<7:56:32, 10.54s/it] 17%|        | 537/3250 [1:35:40<7:54:39, 10.50s/it]                                                       17%|        | 537/3250 [1:35:40<7:54:39, 10.50s/it] 17%|        | 538/3250 [1:35:50<7:53:15, 10.47s/it]                                                       17%|        | 538/3250 [1:35:50<7:53:15, 10.47s/it] 17%|        | 539/3250 [1:36:01<7:52:02, 10.45s/it]                                                       17%|        | 539/3250 [1:36:01<7:52:02, 10.45s/it] 17%|        | 540/3250 [1:36:11<7:51:08, 10.43s/it]                                                       17%|        | 540/3250 [1:36:11<7:51:08, 10.43s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8784658312797546, 'eval_runtime': 2.0991, 'eval_samples_per_second': 5.717, 'eval_steps_per_second': 1.429, 'epoch': 0.17}
                                                       17%|        | 540/3250 [1:36:13<7:51:08, 10.43s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_10/tmp-checkpoint-540
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  I AM HERE AND I AM SAVING THE MODEL
/projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-540the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-540

I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-540
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-540
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-540/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-540/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-540/pytorch_model.binthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-540/pytorch_model.bin

/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.828, 'learning_rate': 9.335758214445324e-05, 'epoch': 0.17}
{'loss': 0.841, 'learning_rate': 9.333347547481135e-05, 'epoch': 0.17}
{'loss': 0.8535, 'learning_rate': 9.330932826447421e-05, 'epoch': 0.17}
{'loss': 0.7895, 'learning_rate': 9.328514053603272e-05, 'epoch': 0.17}
{'loss': 0.8825, 'learning_rate': 9.326091231211582e-05, 'epoch': 0.17}
{'loss': 0.8246, 'learning_rate': 9.323664361539019e-05, 'epoch': 0.17}
 17%|        | 541/3250 [1:36:24<8:33:56, 11.38s/it]                                                       17%|        | 541/3250 [1:36:25<8:33:56, 11.38s/it] 17%|        | 542/3250 [1:36:35<8:20:52, 11.10s/it]                                                       17%|        | 542/3250 [1:36:35<8:20:52, 11.10s/it] 17%|        | 543/3250 [1:36:45<8:11:19, 10.89s/it]                                                       17%|        | 543/3250 [1:36:45<8:11:19, 10.89s/it] 17%|        | 544/3250 [1:36:56<8:04:38, 10.75s/it]                                                       17%|        | 544/3250 [1:36:56<8:04:38, 10.75s/it] 17%|        | 545/3250 [1:37:06<7:59:52, 10.64s/it]                                                       17%|        | 545/3250 [1:37:06<7:59:52, 10.64s/it] 17%|        | 546/3250 [1:37:17<7:56:46, 10.58s/it]                                                       17%|        | 546/3250 [1:37:17{'loss': 0.8041, 'learning_rate': 9.32123344685605e-05, 'epoch': 0.17}
{'loss': 0.8082, 'learning_rate': 9.318798489436917e-05, 'epoch': 0.17}
{'loss': 0.7851, 'learning_rate': 9.31635949155965e-05, 'epoch': 0.17}
{'loss': 0.8449, 'learning_rate': 9.313916455506055e-05, 'epoch': 0.17}
<7:56:46, 10.58s/it] 17%|        | 547/3250 [1:37:27<7:54:26, 10.53s/it]                                                       17%|        | 547/3250 [1:37:27<7:54:26, 10.53s/it] 17%|        | 548/3250 [1:37:37<7:52:31, 10.49s/it]                                                       17%|        | 548/3250 [1:37:37<7:52:31, 10.49s/it] 17%|        | 549/3250 [1:37:48<7:51:15, 10.47s/it]                                                       17%|        | 549/3250 [1:37:48<7:51:15, 10.47s/it] 17%|        | 550/3250 [1:37:58<7:50:24, 10.45s/it]                                                       17%|        | 550/3250 [1:37:58<7:50:24, 10.45s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8729734420776367, 'eval_runtime': 2.1219, 'eval_samples_per_second': 5.655, 'eval_steps_per_second': 1.414, 'epoch': 0.17}
                                                       17%|        | 550/3250 [1:38:00<7:50:24, 10.45s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_10/tmp-checkpoint-550
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-550
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-550
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-550
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-550
the pytorch model path isthe pytorch model path is  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-550/pytorch_model.bin/projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-550/pytorch_model.bin

the pytorch model path is the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-550/pytorch_model.bin
/projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-550/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.808, 'learning_rate': 9.311469383561719e-05, 'epoch': 0.17}
{'loss': 0.8404, 'learning_rate': 9.309018278016004e-05, 'epoch': 0.17}
{'loss': 1.2714, 'learning_rate': 9.306563141162046e-05, 'epoch': 0.17}
{'loss': 0.7933, 'learning_rate': 9.30410397529675e-05, 'epoch': 0.17}
{'loss': 0.8279, 'learning_rate': 9.301640782720792e-05, 'epoch': 0.17}
{'loss': 0.8517, 'learning_rate': 9.299173565738617e-05, 'epoch': 0.17}
 17%|        | 551/3250 [1:38:11<8:26:01, 11.25s/it]                                                       17%|        | 551/3250 [1:38:11<8:26:01, 11.25s/it] 17%|        | 552/3250 [1:38:22<8:14:34, 11.00s/it]                                                       17%|        | 552/3250 [1:38:22<8:14:34, 11.00s/it] 17%|        | 553/3250 [1:38:32<8:06:20, 10.82s/it]                                                       17%|        | 553/3250 [1:38:32<8:06:20, 10.82s/it] 17%|        | 554/3250 [1:38:43<8:00:33, 10.69s/it]                                                       17%|        | 554/3250 [1:38:43<8:00:33, 10.69s/it] 17%|        | 555/3250 [1:38:53<7:56:31, 10.61s/it]                                                       17%|        | 555/3250 [1:38:53<7:56:31, 10.61s/it] 17%|        | 556/3250 [1:39:03<7:53:47, 10.55s/it]                                                       17%|        | 556/3250 [1:39:03{'loss': 0.8356, 'learning_rate': 9.296702326658433e-05, 'epoch': 0.17}
{'loss': 0.7798, 'learning_rate': 9.294227067792211e-05, 'epoch': 0.17}
{'loss': 0.8021, 'learning_rate': 9.291747791455682e-05, 'epoch': 0.17}
{'loss': 0.8916, 'learning_rate': 9.289264499968339e-05, 'epoch': 0.17}
<7:53:47, 10.55s/it] 17%|        | 557/3250 [1:39:14<7:51:59, 10.52s/it]                                                       17%|        | 557/3250 [1:39:14<7:51:59, 10.52s/it] 17%|        | 558/3250 [1:39:24<7:53:42, 10.56s/it]                                                       17%|        | 558/3250 [1:39:24<7:53:42, 10.56s/it] 17%|        | 559/3250 [1:39:35<7:51:40, 10.52s/it]                                                       17%|        | 559/3250 [1:39:35<7:51:40, 10.52s/it] 17%|        | 560/3250 [1:39:45<7:49:57, 10.48s/it]                                                       17%|        | 560/3250 [1:39:45<7:49:57, 10.48s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.873011589050293, 'eval_runtime': 2.1141, 'eval_samples_per_second': 5.676, 'eval_steps_per_second': 1.419, 'epoch': 0.17}
                                                       17%|        | 560/3250 [1:39:47<7:49:57, 10.48s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_10/tmp-checkpoint-560
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-560I AM HERE AND I AM SAVING THE MODEL

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-560
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-560
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-560
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-560/pytorch_model.bin
the pytorch model path is the pytorch model path is/projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-560/pytorch_model.bin 
/projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-560/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-560/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.817, 'learning_rate': 9.286777195653426e-05, 'epoch': 0.17}
{'loss': 0.8262, 'learning_rate': 9.284285880837946e-05, 'epoch': 0.17}
{'loss': 0.7787, 'learning_rate': 9.281790557852652e-05, 'epoch': 0.17}
{'loss': 0.8058, 'learning_rate': 9.279291229032048e-05, 'epoch': 0.17}
{'loss': 0.821, 'learning_rate': 9.276787896714382e-05, 'epoch': 0.17}
{'loss': 0.8141, 'learning_rate': 9.27428056324165e-05, 'epoch': 0.17}
 17%|        | 561/3250 [1:39:58<8:23:30, 11.23s/it]                                                       17%|        | 561/3250 [1:39:58<8:23:30, 11.23s/it] 17%|        | 562/3250 [1:40:09<8:12:12, 10.99s/it]                                                       17%|        | 562/3250 [1:40:09<8:12:12, 10.99s/it] 17%|        | 563/3250 [1:40:19<8:04:08, 10.81s/it]                                                       17%|        | 563/3250 [1:40:19<8:04:08, 10.81s/it] 17%|        | 564/3250 [1:40:29<7:58:27, 10.69s/it]                                                       17%|        | 564/3250 [1:40:29<7:58:27, 10.69s/it] 17%|        | 565/3250 [1:40:40<7:54:13, 10.60s/it]                                                       17%|        | 565/3250 [1:40:40<7:54:13, 10.60s/it] 17%|        | 566/3250 [1:40:50<7:51:41, 10.54s/it]                                                       17%|        | 566/3250 [1:40:50{'loss': 0.7922, 'learning_rate': 9.271769230959596e-05, 'epoch': 0.17}
{'loss': 0.8126, 'learning_rate': 9.269253902217696e-05, 'epoch': 0.17}
{'loss': 0.8284, 'learning_rate': 9.266734579369172e-05, 'epoch': 0.18}
{'loss': 0.827, 'learning_rate': 9.264211264770976e-05, 'epoch': 0.18}
<7:51:41, 10.54s/it] 17%|        | 567/3250 [1:41:01<7:50:52, 10.53s/it]                                                       17%|        | 567/3250 [1:41:01<7:50:52, 10.53s/it] 17%|        | 568/3250 [1:41:11<7:48:59, 10.49s/it]                                                       17%|        | 568/3250 [1:41:11<7:48:59, 10.49s/it] 18%|        | 569/3250 [1:41:22<7:47:41, 10.47s/it]                                                       18%|        | 569/3250 [1:41:22<7:47:41, 10.47s/it] 18%|        | 570/3250 [1:41:32<7:46:27, 10.44s/it]                                                       18%|        | 570/3250 [1:41:32<7:46:27, 10.44s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8697276711463928, 'eval_runtime': 2.1104, 'eval_samples_per_second': 5.686, 'eval_steps_per_second': 1.422, 'epoch': 0.18}
                                                       18%|        | 570/3250 [1:41:34<7:46:27, 10.44s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_10/tmp-checkpoint-570
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-570I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-570

I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-570
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-570
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-570/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-570/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-570/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-570/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.829, 'learning_rate': 9.261683960783804e-05, 'epoch': 0.18}
{'loss': 0.8055, 'learning_rate': 9.259152669772078e-05, 'epoch': 0.18}
{'loss': 0.8535, 'learning_rate': 9.256617394103946e-05, 'epoch': 0.18}
{'loss': 0.8143, 'learning_rate': 9.254078136151295e-05, 'epoch': 0.18}
{'loss': 0.8279, 'learning_rate': 9.251534898289726e-05, 'epoch': 0.18}
{'loss': 0.8286, 'learning_rate': 9.248987682898575e-05, 'epoch': 0.18}
 18%|        | 571/3250 [1:41:45<8:20:00, 11.20s/it]                                                       18%|        | 571/3250 [1:41:45<8:20:00, 11.20s/it] 18%|        | 572/3250 [1:41:55<8:09:10, 10.96s/it]                                                       18%|        | 572/3250 [1:41:55<8:09:10, 10.96s/it] 18%|        | 573/3250 [1:42:06<8:01:29, 10.79s/it]                                                       18%|        | 573/3250 [1:42:06<8:01:29, 10.79s/it] 18%|        | 574/3250 [1:42:17<8:04:30, 10.86s/it]                                                       18%|        | 574/3250 [1:42:17<8:04:30, 10.86s/it] 18%|        | 575/3250 [1:42:27<7:58:09, 10.73s/it]                                                       18%|        | 575/3250 [1:42:27<7:58:09, 10.73s/it] 18%|        | 576/3250 [1:42:38<7:53:26, 10.62s/it]                                                       18%|        | 576/3250 [1:42:38{'loss': 0.7944, 'learning_rate': 9.246436492360888e-05, 'epoch': 0.18}
{'loss': 0.7888, 'learning_rate': 9.243881329063435e-05, 'epoch': 0.18}
{'loss': 0.7922, 'learning_rate': 9.241322195396707e-05, 'epoch': 0.18}
{'loss': 0.8446, 'learning_rate': 9.2387590937549e-05, 'epoch': 0.18}
<7:53:26, 10.62s/it] 18%|        | 577/3250 [1:42:48<7:50:27, 10.56s/it]                                                       18%|        | 577/3250 [1:42:48<7:50:27, 10.56s/it] 18%|        | 578/3250 [1:42:58<7:48:12, 10.51s/it]                                                       18%|        | 578/3250 [1:42:58<7:48:12, 10.51s/it] 18%|        | 579/3250 [1:43:09<7:46:34, 10.48s/it]                                                       18%|        | 579/3250 [1:43:09<7:46:34, 10.48s/it] 18%|        | 580/3250 [1:43:19<7:45:17, 10.46s/it]                                                       18%|        | 580/3250 [1:43:19<7:45:17, 10.46s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8668816089630127, 'eval_runtime': 2.1113, 'eval_samples_per_second': 5.684, 'eval_steps_per_second': 1.421, 'epoch': 0.18}
                                                       18%|        | 580/3250 [1:43:21<7:45:17, 10.46s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_10/tmp-checkpoint-580
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-580
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-580
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-580
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-580
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-580/pytorch_model.bin
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-580/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-580/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-580/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.7889, 'learning_rate': 9.23619202653593e-05, 'epoch': 0.18}
{'loss': 0.8488, 'learning_rate': 9.233620996141421e-05, 'epoch': 0.18}
{'loss': 1.2977, 'learning_rate': 9.231046004976704e-05, 'epoch': 0.18}
{'loss': 0.7413, 'learning_rate': 9.228467055450813e-05, 'epoch': 0.18}
{'loss': 0.7876, 'learning_rate': 9.225884149976493e-05, 'epoch': 0.18}
{'loss': 0.8302, 'learning_rate': 9.22329729097018e-05, 'epoch': 0.18}
 18%|        | 581/3250 [1:43:32<8:20:36, 11.25s/it]                                                       18%|        | 581/3250 [1:43:32<8:20:36, 11.25s/it] 18%|        | 582/3250 [1:43:43<8:09:18, 11.00s/it]                                                       18%|        | 582/3250 [1:43:43<8:09:18, 11.00s/it] 18%|        | 583/3250 [1:43:53<8:01:22, 10.83s/it]                                                       18%|        | 583/3250 [1:43:53<8:01:22, 10.83s/it] 18%|        | 584/3250 [1:44:04<7:55:37, 10.70s/it]                                                       18%|        | 584/3250 [1:44:04<7:55:37, 10.70s/it] 18%|        | 585/3250 [1:44:14<7:51:26, 10.61s/it]                                                       18%|        | 585/3250 [1:44:14<7:51:26, 10.61s/it] 18%|        | 586/3250 [1:44:24<7:48:36, 10.55s/it]                                                       18%|        | 586/3250 [1:44:24{'loss': 0.8277, 'learning_rate': 9.220706480852016e-05, 'epoch': 0.18}
{'loss': 0.7913, 'learning_rate': 9.218111722045837e-05, 'epoch': 0.18}
{'loss': 0.8083, 'learning_rate': 9.215513016979172e-05, 'epoch': 0.18}
{'loss': 0.8466, 'learning_rate': 9.212910368083245e-05, 'epoch': 0.18}
<7:48:36, 10.55s/it] 18%|        | 587/3250 [1:44:35<7:46:25, 10.51s/it]                                                       18%|        | 587/3250 [1:44:35<7:46:25, 10.51s/it] 18%|        | 588/3250 [1:44:45<7:44:58, 10.48s/it]                                                       18%|        | 588/3250 [1:44:45<7:44:58, 10.48s/it] 18%|        | 589/3250 [1:44:56<7:43:43, 10.46s/it]                                                       18%|        | 589/3250 [1:44:56<7:43:43, 10.46s/it] 18%|        | 590/3250 [1:45:06<7:42:59, 10.44s/it]                                                       18%|        | 590/3250 [1:45:06<7:42:59, 10.44s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8663354516029358, 'eval_runtime': 2.1252, 'eval_samples_per_second': 5.646, 'eval_steps_per_second': 1.412, 'epoch': 0.18}
                                                       18%|        | 590/3250 [1:45:08<7:42:59, 10.44s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_10/tmp-checkpoint-590
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-590
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-590
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-590
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-590
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-590/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-590/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-590/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-590/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.8254, 'learning_rate': 9.210303777792968e-05, 'epoch': 0.18}
{'loss': 0.808, 'learning_rate': 9.20769324854694e-05, 'epoch': 0.18}
{'loss': 0.7519, 'learning_rate': 9.205078782787445e-05, 'epoch': 0.18}
{'loss': 0.8193, 'learning_rate': 9.202460382960448e-05, 'epoch': 0.18}
{'loss': 0.8065, 'learning_rate': 9.1998380515156e-05, 'epoch': 0.18}
{'loss': 0.8058, 'learning_rate': 9.197211790906227e-05, 'epoch': 0.18}
 18%|        | 591/3250 [1:45:19<8:18:54, 11.26s/it]                                                       18%|        | 591/3250 [1:45:19<8:18:54, 11.26s/it] 18%|        | 592/3250 [1:45:30<8:07:30, 11.00s/it]                                                       18%|        | 592/3250 [1:45:30<8:07:30, 11.00s/it] 18%|        | 593/3250 [1:45:40<7:59:24, 10.83s/it]                                                       18%|        | 593/3250 [1:45:40<7:59:24, 10.83s/it] 18%|        | 594/3250 [1:45:50<7:53:52, 10.71s/it]                                                       18%|        | 594/3250 [1:45:50<7:53:52, 10.71s/it] 18%|        | 595/3250 [1:46:01<7:50:03, 10.62s/it]                                                       18%|        | 595/3250 [1:46:01<7:50:03, 10.62s/it] 18%|        | 596/3250 [1:46:11<7:47:12, 10.56s/it]                                                       18%|        | 596/3250 [1:46:11{'loss': 0.8096, 'learning_rate': 9.194581603589328e-05, 'epoch': 0.18}
{'loss': 0.8035, 'learning_rate': 9.191947492025582e-05, 'epoch': 0.18}
{'loss': 0.7987, 'learning_rate': 9.189309458679331e-05, 'epoch': 0.18}
{'loss': 0.8151, 'learning_rate': 9.186667506018596e-05, 'epoch': 0.18}
<7:47:12, 10.56s/it] 18%|        | 597/3250 [1:46:22<7:45:15, 10.52s/it]                                                       18%|        | 597/3250 [1:46:22<7:45:15, 10.52s/it] 18%|        | 598/3250 [1:46:32<7:43:53, 10.50s/it]                                                       18%|        | 598/3250 [1:46:32<7:43:53, 10.50s/it] 18%|        | 599/3250 [1:46:43<7:42:58, 10.48s/it]                                                       18%|        | 599/3250 [1:46:43<7:42:58, 10.48s/it] 18%|        | 600/3250 [1:46:53<7:42:06, 10.46s/it]                                                       18%|        | 600/3250 [1:46:53<7:42:06, 10.46s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8675668835639954, 'eval_runtime': 2.1121, 'eval_samples_per_second': 5.681, 'eval_steps_per_second': 1.42, 'epoch': 0.18}
                                                       18%|        | 600/3250 [1:46:55<7:42:06, 10.46s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_10/tmp-checkpoint-600
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-600
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-600
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-600
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-600
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-600/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-600/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-600/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-600/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.8191, 'learning_rate': 9.184021636515058e-05, 'epoch': 0.18}
{'loss': 0.8136, 'learning_rate': 9.181371852644063e-05, 'epoch': 0.19}
{'loss': 0.8011, 'learning_rate': 9.178718156884621e-05, 'epoch': 0.19}
{'loss': 0.8375, 'learning_rate': 9.1760605517194e-05, 'epoch': 0.19}
{'loss': 0.7816, 'learning_rate': 9.173399039634729e-05, 'epoch': 0.19}
{'loss': 0.8669, 'learning_rate': 9.170733623120585e-05, 'epoch': 0.19}
 18%|        | 601/3250 [1:47:06<8:19:24, 11.31s/it]                                                       18%|        | 601/3250 [1:47:06<8:19:24, 11.31s/it] 19%|        | 602/3250 [1:47:17<8:07:29, 11.05s/it]                                                       19%|        | 602/3250 [1:47:17<8:07:29, 11.05s/it] 19%|        | 603/3250 [1:47:27<7:59:09, 10.86s/it]                                                       19%|        | 603/3250 [1:47:27<7:59:09, 10.86s/it] 19%|        | 604/3250 [1:47:38<7:53:14, 10.73s/it]                                                       19%|        | 604/3250 [1:47:38<7:53:14, 10.73s/it] 19%|        | 605/3250 [1:47:48<7:49:06, 10.64s/it]                                                       19%|        | 605/3250 [1:47:48<7:49:06, 10.64s/it] 19%|        | 606/3250 [1:47:58<7:45:53, 10.57s/it]                                                       19%|        | 606/3250 [1:47:58{'loss': 0.7693, 'learning_rate': 9.168064304670606e-05, 'epoch': 0.19}
{'loss': 0.7914, 'learning_rate': 9.165391086782074e-05, 'epoch': 0.19}
{'loss': 0.7948, 'learning_rate': 9.162713971955925e-05, 'epoch': 0.19}
{'loss': 0.7762, 'learning_rate': 9.160032962696734e-05, 'epoch': 0.19}
<7:45:53, 10.57s/it] 19%|        | 607/3250 [1:48:09<7:49:02, 10.65s/it]                                                       19%|        | 607/3250 [1:48:09<7:49:02, 10.65s/it] 19%|        | 608/3250 [1:48:20<7:46:04, 10.58s/it]                                                       19%|        | 608/3250 [1:48:20<7:46:04, 10.58s/it] 19%|        | 609/3250 [1:48:30<7:44:04, 10.54s/it]                                                       19%|        | 609/3250 [1:48:30<7:44:04, 10.54s/it] 19%|        | 610/3250 [1:48:41<7:42:16, 10.51s/it]                                                       19%|        | 610/3250 [1:48:41<7:42:16, 10.51s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8612338304519653, 'eval_runtime': 2.3522, 'eval_samples_per_second': 5.102, 'eval_steps_per_second': 1.275, 'epoch': 0.19}
                                                       19%|        | 610/3250 [1:48:43<7:42:16, 10.51s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_10/tmp-checkpoint-610
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-610
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-610
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-610
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-610
the pytorch model path isthe pytorch model path is  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-610/pytorch_model.bin/projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-610/pytorch_model.bin

the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-610/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-610/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.8051, 'learning_rate': 9.157348061512727e-05, 'epoch': 0.19}
{'loss': 0.8235, 'learning_rate': 9.154659270915764e-05, 'epoch': 0.19}
{'loss': 0.7976, 'learning_rate': 9.151966593421347e-05, 'epoch': 0.19}
{'loss': 1.2341, 'learning_rate': 9.149270031548617e-05, 'epoch': 0.19}
{'loss': 0.7779, 'learning_rate': 9.146569587820344e-05, 'epoch': 0.19}
{'loss': 0.8236, 'learning_rate': 9.143865264762931e-05, 'epoch': 0.19}
 19%|        | 611/3250 [1:48:54<8:19:06, 11.35s/it]                                                       19%|        | 611/3250 [1:48:54<8:19:06, 11.35s/it] 19%|        | 612/3250 [1:49:04<8:06:51, 11.07s/it]                                                       19%|        | 612/3250 [1:49:04<8:06:51, 11.07s/it] 19%|        | 613/3250 [1:49:15<7:57:57, 10.87s/it]                                                       19%|        | 613/3250 [1:49:15<7:57:57, 10.87s/it] 19%|        | 614/3250 [1:49:25<7:51:32, 10.73s/it]                                                       19%|        | 614/3250 [1:49:25<7:51:32, 10.73s/it] 19%|        | 615/3250 [1:49:36<7:47:04, 10.64s/it]                                                       19%|        | 615/3250 [1:49:36<7:47:04, 10.64s/it] 19%|        | 616/3250 [1:49:46<7:43:52, 10.57s/it]                                                       19%|        | 616/3250 [1:49:46{'loss': 0.7984, 'learning_rate': 9.141157064906414e-05, 'epoch': 0.19}
{'loss': 0.8133, 'learning_rate': 9.138444990784453e-05, 'epoch': 0.19}
{'loss': 0.7688, 'learning_rate': 9.135729044934331e-05, 'epoch': 0.19}
{'loss': 0.8463, 'learning_rate': 9.133009229896957e-05, 'epoch': 0.19}
<7:43:52, 10.57s/it] 19%|        | 617/3250 [1:49:56<7:41:30, 10.52s/it]                                                       19%|        | 617/3250 [1:49:56<7:41:30, 10.52s/it] 19%|        | 618/3250 [1:50:07<7:39:59, 10.49s/it]                                                       19%|        | 618/3250 [1:50:07<7:39:59, 10.49s/it] 19%|        | 619/3250 [1:50:17<7:38:46, 10.46s/it]                                                       19%|        | 619/3250 [1:50:17<7:38:46, 10.46s/it] 19%|        | 620/3250 [1:50:28<7:37:56, 10.45s/it]                                                       19%|        | 620/3250 [1:50:28<7:37:56, 10.45s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.861973226070404, 'eval_runtime': 2.1116, 'eval_samples_per_second': 5.683, 'eval_steps_per_second': 1.421, 'epoch': 0.19}
                                                       19%|        | 620/3250 [1:50:30<7:37:56, 10.45s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_10/tmp-checkpoint-620
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-620I AM HERE AND I AM SAVING THE MODEL

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-620
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-620
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-620
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-620/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-620/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-620/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-620/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.8197, 'learning_rate': 9.130285548216857e-05, 'epoch': 0.19}
{'loss': 0.7878, 'learning_rate': 9.127558002442174e-05, 'epoch': 0.19}
{'loss': 0.7681, 'learning_rate': 9.124826595124671e-05, 'epoch': 0.19}
{'loss': 0.793, 'learning_rate': 9.122091328819715e-05, 'epoch': 0.19}
{'loss': 0.7823, 'learning_rate': 9.119352206086293e-05, 'epoch': 0.19}
{'loss': 0.7954, 'learning_rate': 9.116609229486992e-05, 'epoch': 0.19}
 19%|        | 621/3250 [1:50:41<8:10:55, 11.20s/it]                                                       19%|        | 621/3250 [1:50:41<8:10:55, 11.20s/it] 19%|        | 622/3250 [1:50:51<8:00:15, 10.96s/it]                                                       19%|        | 622/3250 [1:50:51<8:00:15, 10.96s/it] 19%|        | 623/3250 [1:51:02<7:57:26, 10.90s/it]                                                       19%|        | 623/3250 [1:51:02<7:57:26, 10.90s/it] 19%|        | 624/3250 [1:51:12<7:50:44, 10.76s/it]                                                       19%|        | 624/3250 [1:51:12<7:50:44, 10.76s/it] 19%|        | 625/3250 [1:51:23<7:45:57, 10.65s/it]                                                       19%|        | 625/3250 [1:51:23<7:45:57, 10.65s/it] 19%|        | 626/3250 [1:51:33<7:42:39, 10.58s/it]                                                       19%|        | 626/3250 [1:51:33{'loss': 0.7937, 'learning_rate': 9.113862401588009e-05, 'epoch': 0.19}
{'loss': 0.7798, 'learning_rate': 9.111111724959143e-05, 'epoch': 0.19}
{'loss': 0.795, 'learning_rate': 9.108357202173794e-05, 'epoch': 0.19}
{'loss': 0.8091, 'learning_rate': 9.105598835808957e-05, 'epoch': 0.19}
<7:42:39, 10.58s/it] 19%|        | 627/3250 [1:51:43<7:40:18, 10.53s/it]                                                       19%|        | 627/3250 [1:51:43<7:40:18, 10.53s/it] 19%|        | 628/3250 [1:51:54<7:38:42, 10.50s/it]                                                       19%|        | 628/3250 [1:51:54<7:38:42, 10.50s/it] 19%|        | 629/3250 [1:52:04<7:37:36, 10.48s/it]                                                       19%|        | 629/3250 [1:52:04<7:37:36, 10.48s/it] 19%|        | 630/3250 [1:52:15<7:36:37, 10.46s/it]                                                       19%|        | 630/3250 [1:52:15<7:36:37, 10.46s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8598239421844482, 'eval_runtime': 2.1129, 'eval_samples_per_second': 5.679, 'eval_steps_per_second': 1.42, 'epoch': 0.19}
                                                       19%|        | 630/3250 [1:52:17<7:36:37, 10.46s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_10/tmp-checkpoint-630
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-630
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-630
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-630
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-630
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-630/pytorch_model.binthe pytorch model path is
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-630/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-630/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-630/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.7833, 'learning_rate': 9.10283662844523e-05, 'epoch': 0.19}
{'loss': 0.7993, 'learning_rate': 9.100070582666795e-05, 'epoch': 0.19}
{'loss': 0.7985, 'learning_rate': 9.097300701061434e-05, 'epoch': 0.19}
{'loss': 0.8193, 'learning_rate': 9.094526986220512e-05, 'epoch': 0.2}
{'loss': 0.7556, 'learning_rate': 9.091749440738984e-05, 'epoch': 0.2}
{'loss': 0.8563, 'learning_rate': 9.088968067215383e-05, 'epoch': 0.2}
 19%|        | 631/3250 [1:52:28<8:09:46, 11.22s/it]                                                       19%|        | 631/3250 [1:52:28<8:09:46, 11.22s/it] 19%|        | 632/3250 [1:52:38<7:59:03, 10.98s/it]                                                       19%|        | 632/3250 [1:52:38<7:59:03, 10.98s/it] 19%|        | 633/3250 [1:52:48<7:51:31, 10.81s/it]                                                       19%|        | 633/3250 [1:52:48<7:51:31, 10.81s/it] 20%|        | 634/3250 [1:52:59<7:46:08, 10.69s/it]                                                       20%|        | 634/3250 [1:52:59<7:46:08, 10.69s/it] 20%|        | 635/3250 [1:53:09<7:42:12, 10.61s/it]                                                       20%|        | 635/3250 [1:53:09<7:42:12, 10.61s/it] 20%|        | 636/3250 [1:53:20<7:39:11, 10.54s/it]                                                       20%|        | 636/3250 [1:53:20{'loss': 0.792, 'learning_rate': 9.08618286825183e-05, 'epoch': 0.2}
{'loss': 0.7737, 'learning_rate': 9.08339384645402e-05, 'epoch': 0.2}
{'loss': 0.7783, 'learning_rate': 9.080601004431229e-05, 'epoch': 0.2}
{'loss': 0.7649, 'learning_rate': 9.077804344796302e-05, 'epoch': 0.2}
<7:39:11, 10.54s/it] 20%|        | 637/3250 [1:53:30<7:37:15, 10.50s/it]                                                       20%|        | 637/3250 [1:53:30<7:37:15, 10.50s/it] 20%|        | 638/3250 [1:53:40<7:35:42, 10.47s/it]                                                       20%|        | 638/3250 [1:53:40<7:35:42, 10.47s/it] 20%|        | 639/3250 [1:53:51<7:34:29, 10.44s/it]                                                       20%|        | 639/3250 [1:53:51<7:34:29, 10.44s/it] 20%|        | 640/3250 [1:54:02<7:40:27, 10.59s/it]                                                       20%|        | 640/3250 [1:54:02<7:40:27, 10.59s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8554418683052063, 'eval_runtime': 2.1059, 'eval_samples_per_second': 5.698, 'eval_steps_per_second': 1.425, 'epoch': 0.2}
                                                       20%|        | 640/3250 [1:54:04<7:40:27, 10.59s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_10/tmp-checkpoint-640
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-640I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-640

I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-640
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-640
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-640/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-640/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-640/pytorch_model.binthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-640/pytorch_model.bin

/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.8088, 'learning_rate': 9.075003870165657e-05, 'epoch': 0.2}
{'loss': 0.7709, 'learning_rate': 9.072199583159286e-05, 'epoch': 0.2}
{'loss': 0.8098, 'learning_rate': 9.069391486400741e-05, 'epoch': 0.2}
{'loss': 1.2441, 'learning_rate': 9.066579582517139e-05, 'epoch': 0.2}
{'loss': 0.7754, 'learning_rate': 9.063763874139164e-05, 'epoch': 0.2}
{'loss': 0.785, 'learning_rate': 9.060944363901056e-05, 'epoch': 0.2}
 20%|        | 641/3250 [1:54:15<8:12:19, 11.32s/it]                                                       20%|        | 641/3250 [1:54:15<8:12:19, 11.32s/it] 20%|        | 642/3250 [1:54:25<8:00:01, 11.04s/it]                                                       20%|        | 642/3250 [1:54:25<8:00:01, 11.04s/it] 20%|        | 643/3250 [1:54:36<7:51:33, 10.85s/it]                                                       20%|        | 643/3250 [1:54:36<7:51:33, 10.85s/it] 20%|        | 644/3250 [1:54:46<7:45:18, 10.71s/it]                                                       20%|        | 644/3250 [1:54:46<7:45:18, 10.71s/it] 20%|        | 645/3250 [1:54:56<7:40:49, 10.61s/it]                                                       20%|        | 645/3250 [1:54:56<7:40:49, 10.61s/it] 20%|        | 646/3250 [1:55:07<7:38:07, 10.56s/it]                                                       20%|        | 646/3250 [1:55:07{'loss': 0.8212, 'learning_rate': 9.058121054440612e-05, 'epoch': 0.2}
{'loss': 0.8036, 'learning_rate': 9.055293948399179e-05, 'epoch': 0.2}
{'loss': 0.7461, 'learning_rate': 9.052463048421665e-05, 'epoch': 0.2}
{'loss': 0.7701, 'learning_rate': 9.04962835715652e-05, 'epoch': 0.2}
<7:38:07, 10.56s/it] 20%|        | 647/3250 [1:55:17<7:36:08, 10.51s/it]                                                       20%|        | 647/3250 [1:55:17<7:36:08, 10.51s/it] 20%|        | 648/3250 [1:55:28<7:34:37, 10.48s/it]                                                       20%|        | 648/3250 [1:55:28<7:34:37, 10.48s/it] 20%|        | 649/3250 [1:55:38<7:33:42, 10.47s/it]                                                       20%|        | 649/3250 [1:55:38<7:33:42, 10.47s/it] 20%|        | 650/3250 [1:55:48<7:32:34, 10.44s/it]                                                       20%|        | 650/3250 [1:55:48<7:32:34, 10.44s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8549540042877197, 'eval_runtime': 2.0966, 'eval_samples_per_second': 5.724, 'eval_steps_per_second': 1.431, 'epoch': 0.2}
                                                       20%|        | 650/3250 [1:55:51<7:32:34, 10.44s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_10/tmp-checkpoint-650
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-650
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-650
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-650
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-650
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-650/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-650/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-650/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-650/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.85, 'learning_rate': 9.046789877255746e-05, 'epoch': 0.2}
{'loss': 0.7914, 'learning_rate': 9.043947611374886e-05, 'epoch': 0.2}
{'loss': 0.7914, 'learning_rate': 9.041101562173023e-05, 'epoch': 0.2}
{'loss': 0.7309, 'learning_rate': 9.038251732312783e-05, 'epoch': 0.2}
{'loss': 0.7842, 'learning_rate': 9.035398124460333e-05, 'epoch': 0.2}
{'loss': 0.7953, 'learning_rate': 9.032540741285367e-05, 'epoch': 0.2}
 20%|        | 651/3250 [1:56:01<8:04:30, 11.19s/it]                                                       20%|        | 651/3250 [1:56:01<8:04:30, 11.19s/it] 20%|        | 652/3250 [1:56:12<7:54:09, 10.95s/it]                                                       20%|        | 652/3250 [1:56:12<7:54:09, 10.95s/it] 20%|        | 653/3250 [1:56:22<7:47:09, 10.79s/it]                                                       20%|        | 653/3250 [1:56:22<7:47:09, 10.79s/it] 20%|        | 654/3250 [1:56:33<7:42:06, 10.68s/it]                                                       20%|        | 654/3250 [1:56:33<7:42:06, 10.68s/it] 20%|        | 655/3250 [1:56:43<7:38:29, 10.60s/it]                                                       20%|        | 655/3250 [1:56:43<7:38:29, 10.60s/it] 20%|        | 656/3250 [1:56:54<7:40:42, 10.66s/it]                                                       20%|        | 656/3250 [1:56:54{'loss': 0.7659, 'learning_rate': 9.029679585461113e-05, 'epoch': 0.2}
{'loss': 0.7728, 'learning_rate': 9.026814659664331e-05, 'epoch': 0.2}
{'loss': 0.7815, 'learning_rate': 9.023945966575304e-05, 'epoch': 0.2}
{'loss': 0.7937, 'learning_rate': 9.021073508877845e-05, 'epoch': 0.2}
<7:40:42, 10.66s/it] 20%|        | 657/3250 [1:57:04<7:37:37, 10.59s/it]                                                       20%|        | 657/3250 [1:57:04<7:37:37, 10.59s/it] 20%|        | 658/3250 [1:57:15<7:35:27, 10.54s/it]                                                       20%|        | 658/3250 [1:57:15<7:35:27, 10.54s/it] 20%|        | 659/3250 [1:57:25<7:34:03, 10.51s/it]                                                       20%|        | 659/3250 [1:57:25<7:34:03, 10.51s/it] 20%|        | 660/3250 [1:57:36<7:32:37, 10.49s/it]                                                       20%|        | 660/3250 [1:57:36<7:32:37, 10.49s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8573297262191772, 'eval_runtime': 2.1073, 'eval_samples_per_second': 5.694, 'eval_steps_per_second': 1.424, 'epoch': 0.2}
                                                       20%|        | 660/3250 [1:57:38<7:32:37, 10.49s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_10/tmp-checkpoint-660
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-660 /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-660

I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-660
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-660
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-660/pytorch_model.bin
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-660/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-660/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-660/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.7985, 'learning_rate': 9.018197289259285e-05, 'epoch': 0.2}
{'loss': 0.789, 'learning_rate': 9.015317310410474e-05, 'epoch': 0.2}
{'loss': 0.7726, 'learning_rate': 9.012433575025783e-05, 'epoch': 0.2}
{'loss': 0.812, 'learning_rate': 9.009546085803092e-05, 'epoch': 0.2}
{'loss': 0.7884, 'learning_rate': 9.006654845443797e-05, 'epoch': 0.2}
{'loss': 0.799, 'learning_rate': 9.003759856652803e-05, 'epoch': 0.2}
 20%|        | 661/3250 [1:57:48<8:04:36, 11.23s/it]                                                       20%|        | 661/3250 [1:57:48<8:04:36, 11.23s/it] 20%|        | 662/3250 [1:57:59<7:53:56, 10.99s/it]                                                       20%|        | 662/3250 [1:57:59<7:53:56, 10.99s/it] 20%|        | 663/3250 [1:58:09<7:46:30, 10.82s/it]                                                       20%|        | 663/3250 [1:58:09<7:46:30, 10.82s/it] 20%|        | 664/3250 [1:58:20<7:41:07, 10.70s/it]                                                       20%|        | 664/3250 [1:58:20<7:41:07, 10.70s/it] 20%|        | 665/3250 [1:58:30<7:37:13, 10.61s/it]                                                       20%|        | 665/3250 [1:58:30<7:37:13, 10.61s/it] 20%|        | 666/3250 [1:58:41<7:34:24, 10.55s/it]                                                       20%|        | 666/3250 [1:58:41{'loss': 0.7977, 'learning_rate': 9.000861122138517e-05, 'epoch': 0.21}
{'loss': 0.7683, 'learning_rate': 8.997958644612861e-05, 'epoch': 0.21}
{'loss': 0.759, 'learning_rate': 8.995052426791247e-05, 'epoch': 0.21}
{'loss': 0.7573, 'learning_rate': 8.99214247139259e-05, 'epoch': 0.21}
<7:34:24, 10.55s/it] 21%|        | 667/3250 [1:58:51<7:32:20, 10.51s/it]                                                       21%|        | 667/3250 [1:58:51<7:32:20, 10.51s/it] 21%|        | 668/3250 [1:59:01<7:31:00, 10.48s/it]                                                       21%|        | 668/3250 [1:59:01<7:31:00, 10.48s/it] 21%|        | 669/3250 [1:59:12<7:29:27, 10.45s/it]                                                       21%|        | 669/3250 [1:59:12<7:29:27, 10.45s/it] 21%|        | 670/3250 [1:59:22<7:28:38, 10.43s/it]                                                       21%|        | 670/3250 [1:59:22<7:28:38, 10.43s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8485198616981506, 'eval_runtime': 2.1103, 'eval_samples_per_second': 5.686, 'eval_steps_per_second': 1.422, 'epoch': 0.21}
                                                       21%|        | 670/3250 [1:59:24<7:28:38, 10.43s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_10/tmp-checkpoint-670
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-670I AM HERE AND I AM SAVING THE MODEL

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-670
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-670
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-670
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-670/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-670/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-670/pytorch_model.binthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-670/pytorch_model.bin

/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.8071, 'learning_rate': 8.989228781139307e-05, 'epoch': 0.21}
{'loss': 0.7559, 'learning_rate': 8.986311358757304e-05, 'epoch': 0.21}
{'loss': 0.8053, 'learning_rate': 8.98339020697598e-05, 'epoch': 0.21}
{'loss': 1.2664, 'learning_rate': 8.980465328528219e-05, 'epoch': 0.21}
{'loss': 0.7251, 'learning_rate': 8.977536726150399e-05, 'epoch': 0.21}
{'loss': 0.7567, 'learning_rate': 8.974604402582379e-05, 'epoch': 0.21}
 21%|        | 671/3250 [1:59:35<8:01:30, 11.20s/it]                                                       21%|        | 671/3250 [1:59:35<8:01:30, 11.20s/it] 21%|        | 672/3250 [1:59:46<7:51:02, 10.96s/it]                                                       21%|        | 672/3250 [1:59:46<7:51:02, 10.96s/it] 21%|        | 673/3250 [1:59:56<7:46:39, 10.87s/it]                                                       21%|        | 673/3250 [1:59:56<7:46:39, 10.87s/it] 21%|        | 674/3250 [2:00:07<7:40:12, 10.72s/it]                                                       21%|        | 674/3250 [2:00:07<7:40:12, 10.72s/it] 21%|        | 675/3250 [2:00:17<7:35:35, 10.62s/it]                                                       21%|        | 675/3250 [2:00:17<7:35:35, 10.62s/it] 21%|        | 676/3250 [2:00:27<7:32:14, 10.54s/it]                                                       21%|        | 676/3250 [2:00:27{'loss': 0.796, 'learning_rate': 8.971668360567496e-05, 'epoch': 0.21}
{'loss': 0.7973, 'learning_rate': 8.968728602852569e-05, 'epoch': 0.21}
{'loss': 0.7526, 'learning_rate': 8.965785132187894e-05, 'epoch': 0.21}
{'loss': 0.7692, 'learning_rate': 8.962837951327236e-05, 'epoch': 0.21}
<7:32:14, 10.54s/it] 21%|        | 677/3250 [2:00:38<7:29:47, 10.49s/it]                                                       21%|        | 677/3250 [2:00:38<7:29:47, 10.49s/it] 21%|        | 678/3250 [2:00:48<7:27:59, 10.45s/it]                                                       21%|        | 678/3250 [2:00:48<7:27:59, 10.45s/it] 21%|        | 679/3250 [2:00:58<7:26:48, 10.43s/it]                                                       21%|        | 679/3250 [2:00:58<7:26:48, 10.43s/it] 21%|        | 680/3250 [2:01:09<7:25:59, 10.41s/it]                                                       21%|        | 680/3250 [2:01:09<7:25:59, 10.41s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8511775135993958, 'eval_runtime': 2.3368, 'eval_samples_per_second': 5.135, 'eval_steps_per_second': 1.284, 'epoch': 0.21}
                                                       21%|        | 680/3250 [2:01:11<7:25:59, 10.41s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_10/tmp-checkpoint-680
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-680
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-680
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-680
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-680
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-680/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-680/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-680/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-680/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.8218, 'learning_rate': 8.959887063027837e-05, 'epoch': 0.21}
{'loss': 0.8006, 'learning_rate': 8.956932470050404e-05, 'epoch': 0.21}
{'loss': 0.7616, 'learning_rate': 8.953974175159111e-05, 'epoch': 0.21}
{'loss': 0.7237, 'learning_rate': 8.951012181121594e-05, 'epoch': 0.21}
{'loss': 0.7818, 'learning_rate': 8.948046490708953e-05, 'epoch': 0.21}
{'loss': 0.7657, 'learning_rate': 8.94507710669574e-05, 'epoch': 0.21}
 21%|        | 681/3250 [2:01:22<8:01:12, 11.24s/it]                                                       21%|        | 681/3250 [2:01:22<8:01:12, 11.24s/it] 21%|        | 682/3250 [2:01:32<7:49:44, 10.98s/it]                                                       21%|        | 682/3250 [2:01:32<7:49:44, 10.98s/it] 21%|        | 683/3250 [2:01:43<7:41:51, 10.80s/it]                                                       21%|        | 683/3250 [2:01:43<7:41:51, 10.80s/it] 21%|        | 684/3250 [2:01:53<7:36:15, 10.67s/it]                                                       21%|        | 684/3250 [2:01:53<7:36:15, 10.67s/it] 21%|        | 685/3250 [2:02:03<7:32:36, 10.59s/it]                                                       21%|        | 685/3250 [2:02:03<7:32:36, 10.59s/it] 21%|        | 686/3250 [2:02:14<7:29:37, 10.52s/it]                                                       21%|        | 686/3250 [2:02:14{'loss': 0.776, 'learning_rate': 8.942104031859972e-05, 'epoch': 0.21}
{'loss': 0.7503, 'learning_rate': 8.939127268983108e-05, 'epoch': 0.21}
{'loss': 0.7657, 'learning_rate': 8.936146820850067e-05, 'epoch': 0.21}
{'loss': 0.753, 'learning_rate': 8.933162690249208e-05, 'epoch': 0.21}
<7:29:37, 10.52s/it] 21%|        | 687/3250 [2:02:24<7:27:41, 10.48s/it]                                                       21%|        | 687/3250 [2:02:24<7:27:41, 10.48s/it] 21%|        | 688/3250 [2:02:35<7:26:18, 10.45s/it]                                                       21%|        | 688/3250 [2:02:35<7:26:18, 10.45s/it] 21%|        | 689/3250 [2:02:45<7:30:09, 10.55s/it]                                                       21%|        | 689/3250 [2:02:45<7:30:09, 10.55s/it] 21%|        | 690/3250 [2:02:56<7:27:56, 10.50s/it]                                                       21%|        | 690/3250 [2:02:56<7:27:56, 10.50s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8500939011573792, 'eval_runtime': 2.1096, 'eval_samples_per_second': 5.688, 'eval_steps_per_second': 1.422, 'epoch': 0.21}
                                                       21%|        | 690/3250 [2:02:58<7:27:56, 10.50s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_10/tmp-checkpoint-690
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  I AM HERE AND I AM SAVING THE MODEL/projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-690
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-690

I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-690
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-690
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-690/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-690/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-690/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-690/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.7796, 'learning_rate': 8.930174879972342e-05, 'epoch': 0.21}
{'loss': 0.7851, 'learning_rate': 8.927183392814718e-05, 'epoch': 0.21}
{'loss': 0.7746, 'learning_rate': 8.924188231575024e-05, 'epoch': 0.21}
{'loss': 0.7778, 'learning_rate': 8.921189399055389e-05, 'epoch': 0.21}
{'loss': 0.7961, 'learning_rate': 8.918186898061376e-05, 'epoch': 0.21}
{'loss': 0.7439, 'learning_rate': 8.915180731401978e-05, 'epoch': 0.21}
 21%|       | 691/3250 [2:03:09<7:59:17, 11.24s/it]                                                       21%|       | 691/3250 [2:03:09<7:59:17, 11.24s/it] 21%|       | 692/3250 [2:03:19<7:48:01, 10.98s/it]                                                       21%|       | 692/3250 [2:03:19<7:48:01, 10.98s/it] 21%|       | 693/3250 [2:03:29<7:40:06, 10.80s/it]                                                       21%|       | 693/3250 [2:03:29<7:40:06, 10.80s/it] 21%|       | 694/3250 [2:03:40<7:34:46, 10.68s/it]                                                       21%|       | 694/3250 [2:03:40<7:34:46, 10.68s/it] 21%|       | 695/3250 [2:03:50<7:30:44, 10.58s/it]                                                       21%|       | 695/3250 [2:03:50<7:30:44, 10.58s/it] 21%|       | 696/3250 [2:04:01<7:27:57, 10.52s/it]                                                       21%|  {'loss': 0.8218, 'learning_rate': 8.91217090188962e-05, 'epoch': 0.21}
{'loss': 0.7417, 'learning_rate': 8.90915741234015e-05, 'epoch': 0.21}
{'loss': 0.7674, 'learning_rate': 8.906140265572843e-05, 'epoch': 0.22}
{'loss': 0.7602, 'learning_rate': 8.903119464410397e-05, 'epoch': 0.22}
     | 696/3250 [2:04:01<7:27:57, 10.52s/it] 21%|       | 697/3250 [2:04:11<7:25:57, 10.48s/it]                                                       21%|       | 697/3250 [2:04:11<7:25:57, 10.48s/it] 21%|       | 698/3250 [2:04:21<7:24:31, 10.45s/it]                                                       21%|       | 698/3250 [2:04:21<7:24:31, 10.45s/it] 22%|       | 699/3250 [2:04:32<7:23:22, 10.43s/it]                                                       22%|       | 699/3250 [2:04:32<7:23:22, 10.43s/it] 22%|       | 700/3250 [2:04:42<7:22:47, 10.42s/it]                                                       22%|       | 700/3250 [2:04:42<7:22:47, 10.42s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8444461226463318, 'eval_runtime': 2.1087, 'eval_samples_per_second': 5.691, 'eval_steps_per_second': 1.423, 'epoch': 0.22}
                                                       22%|       | 700/3250 [2:04:44<7:22:47, 10.42s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_10/tmp-checkpoint-700
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-700I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-700

I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-700
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-700
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-700/pytorch_model.bin
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-700/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-700/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-700/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.7555, 'learning_rate': 8.900095011678924e-05, 'epoch': 0.22}
{'loss': 0.7687, 'learning_rate': 8.897066910207958e-05, 'epoch': 0.22}
{'loss': 0.7887, 'learning_rate': 8.894035162830443e-05, 'epoch': 0.22}
{'loss': 0.7712, 'learning_rate': 8.890999772382732e-05, 'epoch': 0.22}
{'loss': 1.2217, 'learning_rate': 8.887960741704592e-05, 'epoch': 0.22}
{'loss': 0.741, 'learning_rate': 8.88491807363919e-05, 'epoch': 0.22}
 22%|       | 701/3250 [2:04:55<7:55:28, 11.19s/it]                                                       22%|       | 701/3250 [2:04:55<7:55:28, 11.19s/it] 22%|       | 702/3250 [2:05:06<7:54:52, 11.18s/it]                                                       22%|       | 702/3250 [2:05:06<7:54:52, 11.18s/it] 22%|       | 703/3250 [2:05:17<7:45:03, 10.96s/it]                                                       22%|       | 703/3250 [2:05:17<7:45:03, 10.96s/it] 22%|       | 704/3250 [2:05:27<7:37:46, 10.79s/it]                                                       22%|       | 704/3250 [2:05:27<7:37:46, 10.79s/it] 22%|       | 705/3250 [2:05:38<7:35:35, 10.74s/it]                                                       22%|       | 705/3250 [2:05:38<7:35:35, 10.74s/it] 22%|       | 706/3250 [2:05:48<7:30:31, 10.63s/it]                                                       22%|  {'loss': 0.7869, 'learning_rate': 8.881871771033102e-05, 'epoch': 0.22}
{'loss': 0.778, 'learning_rate': 8.878821836736297e-05, 'epoch': 0.22}
{'loss': 0.7785, 'learning_rate': 8.875768273602148e-05, 'epoch': 0.22}
{'loss': 0.7415, 'learning_rate': 8.872711084487418e-05, 'epoch': 0.22}
     | 706/3250 [2:05:48<7:30:31, 10.63s/it] 22%|       | 707/3250 [2:05:59<7:27:27, 10.56s/it]                                                       22%|       | 707/3250 [2:05:59<7:27:27, 10.56s/it] 22%|       | 708/3250 [2:06:09<7:24:46, 10.50s/it]                                                       22%|       | 708/3250 [2:06:09<7:24:46, 10.50s/it] 22%|       | 709/3250 [2:06:19<7:23:11, 10.47s/it]                                                       22%|       | 709/3250 [2:06:19<7:23:11, 10.47s/it] 22%|       | 710/3250 [2:06:30<7:21:54, 10.44s/it]                                                       22%|       | 710/3250 [2:06:30<7:21:54, 10.44s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.846416175365448, 'eval_runtime': 2.3842, 'eval_samples_per_second': 5.033, 'eval_steps_per_second': 1.258, 'epoch': 0.22}
                                                       22%|       | 710/3250 [2:06:32<7:21:54, 10.44s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_10/tmp-checkpoint-710
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-710I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-710

I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-710
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-710
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-710/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-710/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-710/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-710/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.8168, 'learning_rate': 8.869650272252267e-05, 'epoch': 0.22}
{'loss': 0.7939, 'learning_rate': 8.866585839760242e-05, 'epoch': 0.22}
{'loss': 0.7534, 'learning_rate': 8.863517789878275e-05, 'epoch': 0.22}
{'loss': 0.7463, 'learning_rate': 8.860446125476687e-05, 'epoch': 0.22}
{'loss': 0.7688, 'learning_rate': 8.857370849429178e-05, 'epoch': 0.22}
{'loss': 0.7555, 'learning_rate': 8.854291964612825e-05, 'epoch': 0.22}
 22%|       | 711/3250 [2:06:43<7:56:20, 11.26s/it]                                                       22%|       | 711/3250 [2:06:43<7:56:20, 11.26s/it] 22%|       | 712/3250 [2:06:53<7:46:19, 11.02s/it]                                                       22%|       | 712/3250 [2:06:53<7:46:19, 11.02s/it] 22%|       | 713/3250 [2:07:04<7:38:02, 10.83s/it]                                                       22%|       | 713/3250 [2:07:04<7:38:02, 10.83s/it] 22%|       | 714/3250 [2:07:14<7:32:10, 10.70s/it]                                                       22%|       | 714/3250 [2:07:14<7:32:10, 10.70s/it] 22%|       | 715/3250 [2:07:25<7:28:58, 10.63s/it]                                                       22%|       | 715/3250 [2:07:25<7:28:58, 10.63s/it] 22%|       | 716/3250 [2:07:35<7:26:04, 10.56s/it]                                                       22%|  {'loss': 0.7627, 'learning_rate': 8.851209473908083e-05, 'epoch': 0.22}
{'loss': 0.7581, 'learning_rate': 8.848123380198783e-05, 'epoch': 0.22}
{'loss': 0.7479, 'learning_rate': 8.845033686372123e-05, 'epoch': 0.22}
{'loss': 0.7679, 'learning_rate': 8.84194039531867e-05, 'epoch': 0.22}
     | 716/3250 [2:07:35<7:26:04, 10.56s/it] 22%|       | 717/3250 [2:07:45<7:23:43, 10.51s/it]                                                       22%|       | 717/3250 [2:07:45<7:23:43, 10.51s/it] 22%|       | 718/3250 [2:07:56<7:22:00, 10.47s/it]                                                       22%|       | 718/3250 [2:07:56<7:22:00, 10.47s/it] 22%|       | 719/3250 [2:08:06<7:20:47, 10.45s/it]                                                       22%|       | 719/3250 [2:08:06<7:20:47, 10.45s/it] 22%|       | 720/3250 [2:08:17<7:21:21, 10.47s/it]                                                       22%|       | 720/3250 [2:08:17<7:21:21, 10.47s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8467698693275452, 'eval_runtime': 2.7843, 'eval_samples_per_second': 4.31, 'eval_steps_per_second': 1.077, 'epoch': 0.22}
                                                       22%|       | 720/3250 [2:08:19<7:21:21, 10.47s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_10/tmp-checkpoint-720
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-720
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-720
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-720
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-720
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-720/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-720/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-720/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-720/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.7678, 'learning_rate': 8.83884350993236e-05, 'epoch': 0.22}
{'loss': 0.7517, 'learning_rate': 8.835743033110482e-05, 'epoch': 0.22}
{'loss': 0.7654, 'learning_rate': 8.832638967753699e-05, 'epoch': 0.22}
{'loss': 0.7657, 'learning_rate': 8.82953131676602e-05, 'epoch': 0.22}
{'loss': 0.7909, 'learning_rate': 8.826420083054812e-05, 'epoch': 0.22}
{'loss': 0.7161, 'learning_rate': 8.823305269530795e-05, 'epoch': 0.22}
 22%|       | 721/3250 [2:08:30<8:03:34, 11.47s/it]                                                       22%|       | 721/3250 [2:08:30<8:03:34, 11.47s/it] 22%|       | 722/3250 [2:08:41<7:54:15, 11.26s/it]                                                       22%|       | 722/3250 [2:08:41<7:54:15, 11.26s/it] 22%|       | 723/3250 [2:08:52<7:43:02, 10.99s/it]                                                       22%|       | 723/3250 [2:08:52<7:43:02, 10.99s/it] 22%|       | 724/3250 [2:09:02<7:35:48, 10.83s/it]                                                       22%|       | 724/3250 [2:09:02<7:35:48, 10.83s/it] 22%|       | 725/3250 [2:09:12<7:30:09, 10.70s/it]                                                       22%|       | 725/3250 [2:09:12<7:30:09, 10.70s/it] 22%|       | 726/3250 [2:09:23<7:26:04, 10.60s/it]                                                       22%|  {'loss': 0.8246, 'learning_rate': 8.820186879108038e-05, 'epoch': 0.22}
{'loss': 0.7546, 'learning_rate': 8.817064914703954e-05, 'epoch': 0.22}
{'loss': 0.7419, 'learning_rate': 8.813939379239303e-05, 'epoch': 0.22}
{'loss': 0.7537, 'learning_rate': 8.810810275638183e-05, 'epoch': 0.22}
     | 726/3250 [2:09:23<7:26:04, 10.60s/it] 22%|       | 727/3250 [2:09:33<7:22:54, 10.53s/it]                                                       22%|       | 727/3250 [2:09:33<7:22:54, 10.53s/it] 22%|       | 728/3250 [2:09:44<7:20:51, 10.49s/it]                                                       22%|       | 728/3250 [2:09:44<7:20:51, 10.49s/it] 22%|       | 729/3250 [2:09:54<7:19:45, 10.47s/it]                                                       22%|       | 729/3250 [2:09:54<7:19:45, 10.47s/it] 22%|       | 730/3250 [2:10:04<7:18:53, 10.45s/it]                                                       22%|       | 730/3250 [2:10:04<7:18:53, 10.45s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8417118191719055, 'eval_runtime': 2.1469, 'eval_samples_per_second': 5.589, 'eval_steps_per_second': 1.397, 'epoch': 0.22}
                                                       22%|       | 730/3250 [2:10:07<7:18:53, 10.45s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_10/tmp-checkpoint-730
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-730I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-730

I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-730
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-730
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-730/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-730/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-730/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-730/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.7239, 'learning_rate': 8.80767760682803e-05, 'epoch': 0.22}
{'loss': 0.7849, 'learning_rate': 8.804541375739623e-05, 'epoch': 0.23}
{'loss': 0.7435, 'learning_rate': 8.801401585307058e-05, 'epoch': 0.23}
{'loss': 0.7761, 'learning_rate': 8.79825823846778e-05, 'epoch': 0.23}
{'loss': 1.2208, 'learning_rate': 8.795111338162544e-05, 'epoch': 0.23}
{'loss': 0.7502, 'learning_rate': 8.791960887335441e-05, 'epoch': 0.23}
 22%|       | 731/3250 [2:10:17<7:50:58, 11.22s/it]                                                       22%|       | 731/3250 [2:10:17<7:50:58, 11.22s/it] 23%|       | 732/3250 [2:10:28<7:40:21, 10.97s/it]                                                       23%|       | 732/3250 [2:10:28<7:40:21, 10.97s/it] 23%|       | 733/3250 [2:10:38<7:32:55, 10.80s/it]                                                       23%|       | 733/3250 [2:10:38<7:32:55, 10.80s/it] 23%|       | 734/3250 [2:10:49<7:27:38, 10.67s/it]                                                       23%|       | 734/3250 [2:10:49<7:27:38, 10.67s/it] 23%|       | 735/3250 [2:10:59<7:23:37, 10.58s/it]                                                       23%|       | 735/3250 [2:10:59<7:23:37, 10.58s/it] 23%|       | 736/3250 [2:11:09<7:22:42, 10.57s/it]                                                       23%|  {'loss': 0.7509, 'learning_rate': 8.788806888933881e-05, 'epoch': 0.23}
{'loss': 0.7863, 'learning_rate': 8.785649345908588e-05, 'epoch': 0.23}
{'loss': 0.7793, 'learning_rate': 8.782488261213608e-05, 'epoch': 0.23}
{'loss': 0.7266, 'learning_rate': 8.779323637806299e-05, 'epoch': 0.23}
     | 736/3250 [2:11:09<7:22:42, 10.57s/it] 23%|       | 737/3250 [2:11:20<7:20:07, 10.51s/it]                                                       23%|       | 737/3250 [2:11:20<7:20:07, 10.51s/it] 23%|       | 738/3250 [2:11:31<7:23:27, 10.59s/it]                                                       23%|       | 738/3250 [2:11:31<7:23:27, 10.59s/it] 23%|       | 739/3250 [2:11:41<7:20:51, 10.53s/it]                                                       23%|       | 739/3250 [2:11:41<7:20:51, 10.53s/it] 23%|       | 740/3250 [2:11:51<7:18:43, 10.49s/it]                                                       23%|       | 740/3250 [2:11:51<7:18:43, 10.49s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8383493423461914, 'eval_runtime': 2.2825, 'eval_samples_per_second': 5.257, 'eval_steps_per_second': 1.314, 'epoch': 0.23}
                                                       23%|       | 740/3250 [2:11:54<7:18:43, 10.49s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_10/tmp-checkpoint-740
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-740
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-740
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-740
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-740
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-740/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-740/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-740/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-740/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.7393, 'learning_rate': 8.776155478647326e-05, 'epoch': 0.23}
{'loss': 0.8151, 'learning_rate': 8.772983786700668e-05, 'epoch': 0.23}
{'loss': 0.7491, 'learning_rate': 8.769808564933605e-05, 'epoch': 0.23}
{'loss': 0.763, 'learning_rate': 8.766629816316721e-05, 'epoch': 0.23}
{'loss': 0.7111, 'learning_rate': 8.763447543823896e-05, 'epoch': 0.23}
{'loss': 0.7415, 'learning_rate': 8.760261750432313e-05, 'epoch': 0.23}
 23%|       | 741/3250 [2:12:05<8:02:03, 11.53s/it]                                                       23%|       | 741/3250 [2:12:05<8:02:03, 11.53s/it] 23%|       | 742/3250 [2:12:16<7:47:35, 11.19s/it]                                                       23%|       | 742/3250 [2:12:16<7:47:35, 11.19s/it] 23%|       | 743/3250 [2:12:26<7:37:29, 10.95s/it]                                                       23%|       | 743/3250 [2:12:26<7:37:29, 10.95s/it] 23%|       | 744/3250 [2:12:37<7:30:11, 10.78s/it]                                                       23%|       | 744/3250 [2:12:37<7:30:11, 10.78s/it] 23%|       | 745/3250 [2:12:47<7:25:11, 10.66s/it]                                                       23%|       | 745/3250 [2:12:47<7:25:11, 10.66s/it] 23%|       | 746/3250 [2:12:57<7:24:10, 10.64s/it]                                                       23%|  {'loss': 0.7662, 'learning_rate': 8.757072439122444e-05, 'epoch': 0.23}
{'loss': 0.7501, 'learning_rate': 8.753879612878054e-05, 'epoch': 0.23}
{'loss': 0.7386, 'learning_rate': 8.750683274686196e-05, 'epoch': 0.23}
{'loss': 0.7511, 'learning_rate': 8.747483427537209e-05, 'epoch': 0.23}
     | 746/3250 [2:12:58<7:24:10, 10.64s/it] 23%|       | 747/3250 [2:13:08<7:21:02, 10.57s/it]                                                       23%|       | 747/3250 [2:13:08<7:21:02, 10.57s/it] 23%|       | 748/3250 [2:13:18<7:18:22, 10.51s/it]                                                       23%|       | 748/3250 [2:13:18<7:18:22, 10.51s/it] 23%|       | 749/3250 [2:13:29<7:16:24, 10.47s/it]                                                       23%|       | 749/3250 [2:13:29<7:16:24, 10.47s/it] 23%|       | 750/3250 [2:13:39<7:15:11, 10.44s/it]                                                       23%|       | 750/3250 [2:13:39<7:15:11, 10.44s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8416323661804199, 'eval_runtime': 2.279, 'eval_samples_per_second': 5.266, 'eval_steps_per_second': 1.316, 'epoch': 0.23}
                                                       23%|       | 750/3250 [2:13:41<7:15:11, 10.44s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_10/tmp-checkpoint-750
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  I AM HERE AND I AM SAVING THE MODEL/projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-750
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-750

I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-750
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-750
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-750/pytorch_model.bin
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-750/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-750/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-750/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.7661, 'learning_rate': 8.744280074424713e-05, 'epoch': 0.23}
{'loss': 0.771, 'learning_rate': 8.741073218345614e-05, 'epoch': 0.23}
{'loss': 0.7703, 'learning_rate': 8.737862862300085e-05, 'epoch': 0.23}
{'loss': 0.7447, 'learning_rate': 8.734649009291585e-05, 'epoch': 0.23}
{'loss': 0.7899, 'learning_rate': 8.731431662326835e-05, 'epoch': 0.23}
{'loss': 0.7574, 'learning_rate': 8.728210824415827e-05, 'epoch': 0.23}
 23%|       | 751/3250 [2:13:52<7:49:41, 11.28s/it]                                                       23%|       | 751/3250 [2:13:52<7:49:41, 11.28s/it] 23%|       | 752/3250 [2:14:03<7:38:02, 11.00s/it]                                                       23%|       | 752/3250 [2:14:03<7:38:02, 11.00s/it] 23%|       | 753/3250 [2:14:13<7:29:53, 10.81s/it]                                                       23%|       | 753/3250 [2:14:13<7:29:53, 10.81s/it] 23%|       | 754/3250 [2:14:24<7:29:57, 10.82s/it]                                                       23%|       | 754/3250 [2:14:24<7:29:57, 10.82s/it] 23%|       | 755/3250 [2:14:34<7:24:19, 10.69s/it]                                                       23%|       | 755/3250 [2:14:34<7:24:19, 10.69s/it] 23%|       | 756/3250 [2:14:45<7:20:07, 10.59s/it]                                                       23%|  {'loss': 0.7603, 'learning_rate': 8.724986498571828e-05, 'epoch': 0.23}
{'loss': 0.7697, 'learning_rate': 8.721758687811352e-05, 'epoch': 0.23}
{'loss': 0.7351, 'learning_rate': 8.718527395154187e-05, 'epoch': 0.23}
{'loss': 0.7279, 'learning_rate': 8.715292623623373e-05, 'epoch': 0.23}
     | 756/3250 [2:14:45<7:20:07, 10.59s/it] 23%|       | 757/3250 [2:14:55<7:17:16, 10.52s/it]                                                       23%|       | 757/3250 [2:14:55<7:17:16, 10.52s/it] 23%|       | 758/3250 [2:15:05<7:15:04, 10.48s/it]                                                       23%|       | 758/3250 [2:15:05<7:15:04, 10.48s/it] 23%|       | 759/3250 [2:15:16<7:13:50, 10.45s/it]                                                       23%|       | 759/3250 [2:15:16<7:13:50, 10.45s/it] 23%|       | 760/3250 [2:15:26<7:13:09, 10.44s/it]                                                       23%|       | 760/3250 [2:15:26<7:13:09, 10.44s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8379992842674255, 'eval_runtime': 2.6819, 'eval_samples_per_second': 4.474, 'eval_steps_per_second': 1.119, 'epoch': 0.23}
                                                       23%|       | 760/3250 [2:15:29<7:13:09, 10.44s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_10/tmp-checkpoint-760
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-760
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-760
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-760
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-760
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-760/pytorch_model.bin
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-760/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-760/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-760/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.7256, 'learning_rate': 8.712054376245202e-05, 'epoch': 0.23}
{'loss': 0.7789, 'learning_rate': 8.708812656049226e-05, 'epoch': 0.23}
{'loss': 0.7287, 'learning_rate': 8.705567466068237e-05, 'epoch': 0.23}
{'loss': 0.7876, 'learning_rate': 8.702318809338278e-05, 'epoch': 0.24}
{'loss': 1.2494, 'learning_rate': 8.699066688898635e-05, 'epoch': 0.24}
{'loss': 0.6894, 'learning_rate': 8.695811107791836e-05, 'epoch': 0.24}
 23%|       | 761/3250 [2:15:40<7:52:01, 11.38s/it]                                                       23%|       | 761/3250 [2:15:40<7:52:01, 11.38s/it] 23%|       | 762/3250 [2:15:50<7:39:24, 11.08s/it]                                                       23%|       | 762/3250 [2:15:50<7:39:24, 11.08s/it] 23%|       | 763/3250 [2:16:00<7:30:25, 10.87s/it]                                                       23%|       | 763/3250 [2:16:00<7:30:25, 10.87s/it] 24%|       | 764/3250 [2:16:11<7:23:55, 10.71s/it]                                                       24%|       | 764/3250 [2:16:11<7:23:55, 10.71s/it] 24%|       | 765/3250 [2:16:21<7:19:25, 10.61s/it]                                                       24%|       | 765/3250 [2:16:21<7:19:25, 10.61s/it] 24%|       | 766/3250 [2:16:31<7:16:11, 10.54s/it]                                                       24%|  {'loss': 0.73, 'learning_rate': 8.692552069063641e-05, 'epoch': 0.24}
{'loss': 0.763, 'learning_rate': 8.689289575763051e-05, 'epoch': 0.24}
{'loss': 0.7713, 'learning_rate': 8.686023630942292e-05, 'epoch': 0.24}
{'loss': 0.7291, 'learning_rate': 8.68275423765683e-05, 'epoch': 0.24}
     | 766/3250 [2:16:31<7:16:11, 10.54s/it] 24%|       | 767/3250 [2:16:42<7:13:45, 10.48s/it]                                                       24%|       | 767/3250 [2:16:42<7:13:45, 10.48s/it] 24%|       | 768/3250 [2:16:52<7:12:07, 10.45s/it]                                                       24%|       | 768/3250 [2:16:52<7:12:07, 10.45s/it] 24%|       | 769/3250 [2:17:03<7:10:55, 10.42s/it]                                                       24%|       | 769/3250 [2:17:03<7:10:55, 10.42s/it] 24%|       | 770/3250 [2:17:13<7:09:56, 10.40s/it]                                                       24%|       | 770/3250 [2:17:13<7:09:56, 10.40s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8362787961959839, 'eval_runtime': 2.1079, 'eval_samples_per_second': 5.693, 'eval_steps_per_second': 1.423, 'epoch': 0.24}
                                                       24%|       | 770/3250 [2:17:15<7:09:56, 10.40s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_10/tmp-checkpoint-770
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-770
/projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-770
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-770
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-770
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-770/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-770/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-770/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-770/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.7559, 'learning_rate': 8.679481398965347e-05, 'epoch': 0.24}
{'loss': 0.7816, 'learning_rate': 8.676205117929752e-05, 'epoch': 0.24}
{'loss': 0.7628, 'learning_rate': 8.672925397615173e-05, 'epoch': 0.24}
{'loss': 0.7332, 'learning_rate': 8.669642241089959e-05, 'epoch': 0.24}
{'loss': 0.69, 'learning_rate': 8.666355651425672e-05, 'epoch': 0.24}
{'loss': 0.7484, 'learning_rate': 8.663065631697085e-05, 'epoch': 0.24}
 24%|       | 771/3250 [2:17:27<7:51:15, 11.41s/it]                                                       24%|       | 771/3250 [2:17:27<7:51:15, 11.41s/it] 24%|       | 772/3250 [2:17:37<7:37:59, 11.09s/it]                                                       24%|       | 772/3250 [2:17:37<7:37:59, 11.09s/it] 24%|       | 773/3250 [2:17:47<7:28:51, 10.87s/it]                                                       24%|       | 773/3250 [2:17:47<7:28:51, 10.87s/it] 24%|       | 774/3250 [2:17:58<7:22:24, 10.72s/it]                                                       24%|       | 774/3250 [2:17:58<7:22:24, 10.72s/it] 24%|       | 775/3250 [2:18:08<7:17:53, 10.62s/it]                                                       24%|       | 775/3250 [2:18:08<7:17:53, 10.62s/it] 24%|       | 776/3250 [2:18:19<7:14:36, 10.54s/it]                                                       24%|  {'loss': 0.7362, 'learning_rate': 8.65977218498218e-05, 'epoch': 0.24}
{'loss': 0.7456, 'learning_rate': 8.656475314362148e-05, 'epoch': 0.24}
{'loss': 0.739, 'learning_rate': 8.65317502292138e-05, 'epoch': 0.24}
{'loss': 0.7428, 'learning_rate': 8.649871313747466e-05, 'epoch': 0.24}
     | 776/3250 [2:18:19<7:14:36, 10.54s/it] 24%|       | 777/3250 [2:18:29<7:12:08, 10.48s/it]                                                       24%|       | 777/3250 [2:18:29<7:12:08, 10.48s/it] 24%|       | 778/3250 [2:18:39<7:10:27, 10.45s/it]                                                       24%|       | 778/3250 [2:18:39<7:10:27, 10.45s/it] 24%|       | 779/3250 [2:18:50<7:09:13, 10.42s/it]                                                       24%|       | 779/3250 [2:18:50<7:09:13, 10.42s/it] 24%|       | 780/3250 [2:19:00<7:08:13, 10.40s/it]                                                       24%|       | 780/3250 [2:19:00<7:08:13, 10.40s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8353111743927002, 'eval_runtime': 2.1051, 'eval_samples_per_second': 5.7, 'eval_steps_per_second': 1.425, 'epoch': 0.24}
                                                       24%|       | 780/3250 [2:19:02<7:08:13, 10.40s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_10/tmp-checkpoint-780
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-780I AM HERE AND I AM SAVING THE MODEL

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-780
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-780
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-780
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-780/pytorch_model.bin
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-780/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-780/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-780/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.7442, 'learning_rate': 8.646564189931199e-05, 'epoch': 0.24}
{'loss': 0.7562, 'learning_rate': 8.64325365456656e-05, 'epoch': 0.24}
{'loss': 0.7545, 'learning_rate': 8.63993971075073e-05, 'epoch': 0.24}
{'loss': 0.7511, 'learning_rate': 8.636622361584072e-05, 'epoch': 0.24}
{'loss': 0.7392, 'learning_rate': 8.633301610170135e-05, 'epoch': 0.24}
{'loss': 0.771, 'learning_rate': 8.629977459615655e-05, 'epoch': 0.24}
 24%|       | 781/3250 [2:19:13<7:39:43, 11.17s/it]                                                       24%|       | 781/3250 [2:19:13<7:39:43, 11.17s/it] 24%|       | 782/3250 [2:19:23<7:29:24, 10.93s/it]                                                       24%|       | 782/3250 [2:19:23<7:29:24, 10.93s/it] 24%|       | 783/3250 [2:19:34<7:22:09, 10.75s/it]                                                       24%|       | 783/3250 [2:19:34<7:22:09, 10.75s/it] 24%|       | 784/3250 [2:19:44<7:17:07, 10.64s/it]                                                       24%|       | 784/3250 [2:19:44<7:17:07, 10.64s/it] 24%|       | 785/3250 [2:19:54<7:13:29, 10.55s/it]                                                       24%|       | 785/3250 [2:19:54<7:13:29, 10.55s/it] 24%|       | 786/3250 [2:20:05<7:10:54, 10.49s/it]                                                       24%|  {'loss': 0.7117, 'learning_rate': 8.626649913030545e-05, 'epoch': 0.24}
{'loss': 0.8042, 'learning_rate': 8.623318973527893e-05, 'epoch': 0.24}
{'loss': 0.7233, 'learning_rate': 8.619984644223968e-05, 'epoch': 0.24}
{'loss': 0.7295, 'learning_rate': 8.616646928238205e-05, 'epoch': 0.24}
     | 786/3250 [2:20:05<7:10:54, 10.49s/it] 24%|       | 787/3250 [2:20:15<7:13:57, 10.57s/it]                                                       24%|       | 787/3250 [2:20:15<7:13:57, 10.57s/it] 24%|       | 788/3250 [2:20:26<7:13:11, 10.56s/it]                                                       24%|       | 788/3250 [2:20:26<7:13:11, 10.56s/it] 24%|       | 789/3250 [2:20:36<7:10:35, 10.50s/it]                                                       24%|       | 789/3250 [2:20:36<7:10:35, 10.50s/it] 24%|       | 790/3250 [2:20:47<7:08:33, 10.45s/it]                                                       24%|       | 790/3250 [2:20:47<7:08:33, 10.45s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8340762257575989, 'eval_runtime': 2.4829, 'eval_samples_per_second': 4.833, 'eval_steps_per_second': 1.208, 'epoch': 0.24}
                                                       24%|       | 790/3250 [2:20:49<7:08:33, 10.45s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_10/tmp-checkpoint-790
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-790I AM HERE AND I AM SAVING THE MODEL

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-790
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-790
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-790
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-790/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-790/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-790/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-790/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.7306, 'learning_rate': 8.613305828693212e-05, 'epoch': 0.24}
{'loss': 0.7293, 'learning_rate': 8.609961348714756e-05, 'epoch': 0.24}
{'loss': 0.7319, 'learning_rate': 8.60661349143177e-05, 'epoch': 0.24}
{'loss': 0.761, 'learning_rate': 8.603262259976348e-05, 'epoch': 0.24}
{'loss': 0.741, 'learning_rate': 8.59990765748374e-05, 'epoch': 0.24}
{'loss': 1.2025, 'learning_rate': 8.596549687092348e-05, 'epoch': 0.24}
 24%|       | 791/3250 [2:21:00<7:43:29, 11.31s/it]                                                       24%|       | 791/3250 [2:21:00<7:43:29, 11.31s/it] 24%|       | 792/3250 [2:21:10<7:31:25, 11.02s/it]                                                       24%|       | 792/3250 [2:21:10<7:31:25, 11.02s/it] 24%|       | 793/3250 [2:21:21<7:23:08, 10.82s/it]                                                       24%|       | 793/3250 [2:21:21<7:23:08, 10.82s/it] 24%|       | 794/3250 [2:21:31<7:17:19, 10.68s/it]                                                       24%|       | 794/3250 [2:21:31<7:17:19, 10.68s/it] 24%|       | 795/3250 [2:21:41<7:13:12, 10.59s/it]                                                       24%|       | 795/3250 [2:21:41<7:13:12, 10.59s/it] 24%|       | 796/3250 [2:21:52<7:10:04, 10.52s/it]                                                       24%|  {'loss': 0.7357, 'learning_rate': 8.593188351943726e-05, 'epoch': 0.25}
{'loss': 0.7582, 'learning_rate': 8.589823655182576e-05, 'epoch': 0.25}
{'loss': 0.7493, 'learning_rate': 8.586455599956746e-05, 'epoch': 0.25}
{'loss': 0.749, 'learning_rate': 8.583084189417224e-05, 'epoch': 0.25}
     | 796/3250 [2:21:52<7:10:04, 10.52s/it] 25%|       | 797/3250 [2:22:02<7:07:52, 10.47s/it]                                                       25%|       | 797/3250 [2:22:02<7:07:52, 10.47s/it] 25%|       | 798/3250 [2:22:12<7:06:19, 10.43s/it]                                                       25%|       | 798/3250 [2:22:12<7:06:19, 10.43s/it] 25%|       | 799/3250 [2:22:23<7:05:09, 10.41s/it]                                                       25%|       | 799/3250 [2:22:23<7:05:09, 10.41s/it] 25%|       | 800/3250 [2:22:33<7:04:26, 10.39s/it]                                                       25%|       | 800/3250 [2:22:33<7:04:26, 10.39s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8332473039627075, 'eval_runtime': 2.1039, 'eval_samples_per_second': 5.704, 'eval_steps_per_second': 1.426, 'epoch': 0.25}
                                                       25%|       | 800/3250 [2:22:35<7:04:26, 10.39s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_10/tmp-checkpoint-800
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-800
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-800
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-800
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-800
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-800/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-800/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-800/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-800/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.7079, 'learning_rate': 8.579709426718137e-05, 'epoch': 0.25}
{'loss': 0.7812, 'learning_rate': 8.576331315016753e-05, 'epoch': 0.25}
{'loss': 0.7624, 'learning_rate': 8.572949857473462e-05, 'epoch': 0.25}
{'loss': 0.7229, 'learning_rate': 8.569565057251799e-05, 'epoch': 0.25}
{'loss': 0.7198, 'learning_rate': 8.566176917518416e-05, 'epoch': 0.25}
{'loss': 0.7234, 'learning_rate': 8.56278544144309e-05, 'epoch': 0.25}
 25%|       | 801/3250 [2:22:46<7:36:31, 11.18s/it]                                                       25%|       | 801/3250 [2:22:46<7:36:31, 11.18s/it] 25%|       | 802/3250 [2:22:57<7:26:06, 10.93s/it]                                                       25%|       | 802/3250 [2:22:57<7:26:06, 10.93s/it] 25%|       | 803/3250 [2:23:07<7:18:46, 10.76s/it]                                                       25%|       | 803/3250 [2:23:07<7:18:46, 10.76s/it] 25%|       | 804/3250 [2:23:18<7:21:40, 10.83s/it]                                                       25%|       | 804/3250 [2:23:18<7:21:40, 10.83s/it] 25%|       | 805/3250 [2:23:28<7:15:44, 10.69s/it]                                                       25%|       | 805/3250 [2:23:28<7:15:44, 10.69s/it] 25%|       | 806/3250 [2:23:39<7:11:35, 10.60s/it]                                                       25%|  {'loss': 0.7131, 'learning_rate': 8.559390632198723e-05, 'epoch': 0.25}
{'loss': 0.7356, 'learning_rate': 8.555992492961334e-05, 'epoch': 0.25}
{'loss': 0.7362, 'learning_rate': 8.552591026910058e-05, 'epoch': 0.25}
{'loss': 0.7234, 'learning_rate': 8.549186237227138e-05, 'epoch': 0.25}
     | 806/3250 [2:23:39<7:11:35, 10.60s/it] 25%|       | 807/3250 [2:23:49<7:08:37, 10.53s/it]                                                       25%|       | 807/3250 [2:23:49<7:08:37, 10.53s/it] 25%|       | 808/3250 [2:23:59<7:06:29, 10.48s/it]                                                       25%|       | 808/3250 [2:23:59<7:06:29, 10.48s/it] 25%|       | 809/3250 [2:24:10<7:04:54, 10.44s/it]                                                       25%|       | 809/3250 [2:24:10<7:04:54, 10.44s/it] 25%|       | 810/3250 [2:24:20<7:03:38, 10.42s/it]                                                       25%|       | 810/3250 [2:24:20<7:03:38, 10.42s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8302634954452515, 'eval_runtime': 2.1122, 'eval_samples_per_second': 5.681, 'eval_steps_per_second': 1.42, 'epoch': 0.25}
                                                       25%|       | 810/3250 [2:24:22<7:03:38, 10.42s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_10/tmp-checkpoint-810
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-810I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  
/projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-810
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-810
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-810
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-810/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-810/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-810/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-810/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.7565, 'learning_rate': 8.545778127097933e-05, 'epoch': 0.25}
{'loss': 0.7395, 'learning_rate': 8.542366699710905e-05, 'epoch': 0.25}
{'loss': 0.7289, 'learning_rate': 8.538951958257617e-05, 'epoch': 0.25}
{'loss': 0.7473, 'learning_rate': 8.535533905932738e-05, 'epoch': 0.25}
{'loss': 0.74, 'learning_rate': 8.532112545934032e-05, 'epoch': 0.25}
{'loss': 0.7544, 'learning_rate': 8.528687881462357e-05, 'epoch': 0.25}
 25%|       | 811/3250 [2:24:33<7:34:11, 11.17s/it]                                                       25%|       | 811/3250 [2:24:33<7:34:11, 11.17s/it] 25%|       | 812/3250 [2:24:43<7:24:01, 10.93s/it]                                                       25%|       | 812/3250 [2:24:43<7:24:01, 10.93s/it] 25%|       | 813/3250 [2:24:54<7:16:54, 10.76s/it]                                                       25%|       | 813/3250 [2:24:54<7:16:54, 10.76s/it] 25%|       | 814/3250 [2:25:04<7:12:08, 10.64s/it]                                                       25%|       | 814/3250 [2:25:04<7:12:08, 10.64s/it] 25%|       | 815/3250 [2:25:14<7:08:34, 10.56s/it]                                                       25%|       | 815/3250 [2:25:14<7:08:34, 10.56s/it] 25%|       | 816/3250 [2:25:25<7:06:00, 10.50s/it]                                                       25%|  {'loss': 0.6929, 'learning_rate': 8.52525991572166e-05, 'epoch': 0.25}
{'loss': 0.7966, 'learning_rate': 8.521828651918981e-05, 'epoch': 0.25}
{'loss': 0.7357, 'learning_rate': 8.518394093264448e-05, 'epoch': 0.25}
{'loss': 0.7174, 'learning_rate': 8.514956242971262e-05, 'epoch': 0.25}
     | 816/3250 [2:25:25<7:06:00, 10.50s/it] 25%|       | 817/3250 [2:25:35<7:04:11, 10.46s/it]                                                       25%|       | 817/3250 [2:25:35<7:04:11, 10.46s/it] 25%|       | 818/3250 [2:25:46<7:02:46, 10.43s/it]                                                       25%|       | 818/3250 [2:25:46<7:02:46, 10.43s/it] 25%|       | 819/3250 [2:25:56<7:01:37, 10.41s/it]                                                       25%|       | 819/3250 [2:25:56<7:01:37, 10.41s/it] 25%|       | 820/3250 [2:26:07<7:04:05, 10.47s/it]                                                       25%|       | 820/3250 [2:26:07<7:04:05, 10.47s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8315601944923401, 'eval_runtime': 2.1251, 'eval_samples_per_second': 5.647, 'eval_steps_per_second': 1.412, 'epoch': 0.25}
                                                       25%|       | 820/3250 [2:26:09<7:04:05, 10.47s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_10/tmp-checkpoint-820
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-820
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-820
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-820
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-820
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-820/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-820/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-820/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-820/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.7228, 'learning_rate': 8.51151510425571e-05, 'epoch': 0.25}
{'loss': 0.6991, 'learning_rate': 8.508070680337152e-05, 'epoch': 0.25}
{'loss': 0.7527, 'learning_rate': 8.504622974438028e-05, 'epoch': 0.25}
{'loss': 0.7122, 'learning_rate': 8.501171989783845e-05, 'epoch': 0.25}
{'loss': 0.7434, 'learning_rate': 8.497717729603172e-05, 'epoch': 0.25}
{'loss': 1.2008, 'learning_rate': 8.49426019712765e-05, 'epoch': 0.25}
 25%|       | 821/3250 [2:26:20<7:35:00, 11.24s/it]                                                       25%|       | 821/3250 [2:26:20<7:35:00, 11.24s/it] 25%|       | 822/3250 [2:26:30<7:24:17, 10.98s/it]                                                       25%|       | 822/3250 [2:26:30<7:24:17, 10.98s/it] 25%|       | 823/3250 [2:26:40<7:17:02, 10.80s/it]                                                       25%|       | 823/3250 [2:26:40<7:17:02, 10.80s/it] 25%|       | 824/3250 [2:26:51<7:11:41, 10.68s/it]                                                       25%|       | 824/3250 [2:26:51<7:11:41, 10.68s/it] 25%|       | 825/3250 [2:27:01<7:08:07, 10.59s/it]                                                       25%|       | 825/3250 [2:27:01<7:08:07, 10.59s/it] 25%|       | 826/3250 [2:27:11<7:05:07, 10.52s/it]                                                       25%|  {'loss': 0.7248, 'learning_rate': 8.490799395591977e-05, 'epoch': 0.25}
{'loss': 0.7181, 'learning_rate': 8.487335328233912e-05, 'epoch': 0.25}
{'loss': 0.7687, 'learning_rate': 8.483867998294266e-05, 'epoch': 0.26}
{'loss': 0.7471, 'learning_rate': 8.480397409016909e-05, 'epoch': 0.26}
     | 826/3250 [2:27:11<7:05:07, 10.52s/it] 25%|       | 827/3250 [2:27:22<7:03:07, 10.48s/it]                                                       25%|       | 827/3250 [2:27:22<7:03:07, 10.48s/it] 25%|       | 828/3250 [2:27:32<7:01:38, 10.45s/it]                                                       25%|       | 828/3250 [2:27:32<7:01:38, 10.45s/it] 26%|       | 829/3250 [2:27:43<7:00:38, 10.42s/it]                                                       26%|       | 829/3250 [2:27:43<7:00:38, 10.42s/it] 26%|       | 830/3250 [2:27:53<6:59:50, 10.41s/it]                                                       26%|       | 830/3250 [2:27:53<6:59:50, 10.41s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8278388977050781, 'eval_runtime': 2.1105, 'eval_samples_per_second': 5.686, 'eval_steps_per_second': 1.421, 'epoch': 0.26}
                                                       26%|       | 830/3250 [2:27:55<6:59:50, 10.41s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_10/tmp-checkpoint-830
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-830I AM HERE AND I AM SAVING THE MODEL

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-830
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-830
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-830
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-830/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-830/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-830/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-830/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.7024, 'learning_rate': 8.476923563648752e-05, 'epoch': 0.26}
{'loss': 0.7036, 'learning_rate': 8.473446465439758e-05, 'epoch': 0.26}
{'loss': 0.7887, 'learning_rate': 8.469966117642929e-05, 'epoch': 0.26}
{'loss': 0.7179, 'learning_rate': 8.46648252351431e-05, 'epoch': 0.26}
{'loss': 0.7308, 'learning_rate': 8.462995686312985e-05, 'epoch': 0.26}
{'loss': 0.6787, 'learning_rate': 8.459505609301069e-05, 'epoch': 0.26}
 26%|       | 831/3250 [2:28:06<7:30:43, 11.18s/it]                                                       26%|       | 831/3250 [2:28:06<7:30:43, 11.18s/it] 26%|       | 832/3250 [2:28:16<7:20:32, 10.93s/it]                                                       26%|       | 832/3250 [2:28:16<7:20:32, 10.93s/it] 26%|       | 833/3250 [2:28:27<7:13:35, 10.76s/it]                                                       26%|       | 833/3250 [2:28:27<7:13:35, 10.76s/it] 26%|       | 834/3250 [2:28:37<7:08:31, 10.64s/it]                                                       26%|       | 834/3250 [2:28:37<7:08:31, 10.64s/it] 26%|       | 835/3250 [2:28:47<7:05:00, 10.56s/it]                                                       26%|       | 835/3250 [2:28:47<7:05:00, 10.56s/it] 26%|       | 836/3250 [2:28:58<7:02:34, 10.50s/it]                                                       26%|  {'loss': 0.7104, 'learning_rate': 8.456012295743706e-05, 'epoch': 0.26}
{'loss': 0.7497, 'learning_rate': 8.452515748909069e-05, 'epoch': 0.26}
{'loss': 0.722, 'learning_rate': 8.449015972068363e-05, 'epoch': 0.26}
{'loss': 0.7218, 'learning_rate': 8.445512968495806e-05, 'epoch': 0.26}
     | 836/3250 [2:28:58<7:02:34, 10.50s/it] 26%|       | 837/3250 [2:29:09<7:08:25, 10.65s/it]                                                       26%|       | 837/3250 [2:29:09<7:08:25, 10.65s/it] 26%|       | 838/3250 [2:29:19<7:04:44, 10.57s/it]                                                       26%|       | 838/3250 [2:29:19<7:04:44, 10.57s/it] 26%|       | 839/3250 [2:29:30<7:02:09, 10.51s/it]                                                       26%|       | 839/3250 [2:29:30<7:02:09, 10.51s/it] 26%|       | 840/3250 [2:29:40<7:00:17, 10.46s/it]                                                       26%|       | 840/3250 [2:29:40<7:00:17, 10.46s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8246772289276123, 'eval_runtime': 2.1073, 'eval_samples_per_second': 5.694, 'eval_steps_per_second': 1.424, 'epoch': 0.26}
                                                       26%|       | 840/3250 [2:29:42<7:00:17, 10.46s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_10/tmp-checkpoint-840
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-840
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-840
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-840
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-840
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-840/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-840/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-840/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-840/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.7227, 'learning_rate': 8.442006741468639e-05, 'epoch': 0.26}
{'loss': 0.7288, 'learning_rate': 8.438497294267117e-05, 'epoch': 0.26}
{'loss': 0.7346, 'learning_rate': 8.434984630174509e-05, 'epoch': 0.26}
{'loss': 0.7395, 'learning_rate': 8.431468752477091e-05, 'epoch': 0.26}
{'loss': 0.7162, 'learning_rate': 8.427949664464152e-05, 'epoch': 0.26}
{'loss': 0.757, 'learning_rate': 8.424427369427974e-05, 'epoch': 0.26}
 26%|       | 841/3250 [2:29:53<7:30:13, 11.21s/it]                                                       26%|       | 841/3250 [2:29:53<7:30:13, 11.21s/it] 26%|       | 842/3250 [2:30:03<7:19:49, 10.96s/it]                                                       26%|       | 842/3250 [2:30:03<7:19:49, 10.96s/it] 26%|       | 843/3250 [2:30:14<7:12:31, 10.78s/it]                                                       26%|       | 843/3250 [2:30:14<7:12:31, 10.78s/it] 26%|       | 844/3250 [2:30:24<7:07:16, 10.66s/it]                                                       26%|       | 844/3250 [2:30:24<7:07:16, 10.66s/it] 26%|       | 845/3250 [2:30:34<7:03:40, 10.57s/it]                                                       26%|       | 845/3250 [2:30:34<7:03:40, 10.57s/it] 26%|       | 846/3250 [2:30:45<7:01:06, 10.51s/it]                                                       26%|  {'loss': 0.7223, 'learning_rate': 8.42090187066385e-05, 'epoch': 0.26}
{'loss': 0.7337, 'learning_rate': 8.417373171470063e-05, 'epoch': 0.26}
{'loss': 0.7398, 'learning_rate': 8.413841275147892e-05, 'epoch': 0.26}
{'loss': 0.7069, 'learning_rate': 8.410306185001611e-05, 'epoch': 0.26}
     | 846/3250 [2:30:45<7:01:06, 10.51s/it] 26%|       | 847/3250 [2:30:55<6:59:11, 10.47s/it]                                                       26%|       | 847/3250 [2:30:55<6:59:11, 10.47s/it] 26%|       | 848/3250 [2:31:05<6:57:43, 10.43s/it]                                                       26%|       | 848/3250 [2:31:05<6:57:43, 10.43s/it] 26%|       | 849/3250 [2:31:16<6:56:34, 10.41s/it]                                                       26%|       | 849/3250 [2:31:16<6:56:34, 10.41s/it] 26%|       | 850/3250 [2:31:26<6:55:53, 10.40s/it]                                                       26%|       | 850/3250 [2:31:26<6:55:53, 10.40s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8277024626731873, 'eval_runtime': 2.1061, 'eval_samples_per_second': 5.698, 'eval_steps_per_second': 1.424, 'epoch': 0.26}
                                                       26%|       | 850/3250 [2:31:28<6:55:53, 10.40s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_10/tmp-checkpoint-850
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  I AM HERE AND I AM SAVING THE MODEL/projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-850
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-850

I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-850
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-850
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-850/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-850/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-850/pytorch_model.binthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-850/pytorch_model.bin

/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.7044, 'learning_rate': 8.406767904338475e-05, 'epoch': 0.26}
{'loss': 0.6975, 'learning_rate': 8.403226436468728e-05, 'epoch': 0.26}
{'loss': 0.7496, 'learning_rate': 8.399681784705599e-05, 'epoch': 0.26}
{'loss': 0.6967, 'learning_rate': 8.396133952365288e-05, 'epoch': 0.26}
{'loss': 0.7491, 'learning_rate': 8.392582942766976e-05, 'epoch': 0.26}
{'loss': 1.2284, 'learning_rate': 8.389028759232815e-05, 'epoch': 0.26}
 26%|       | 851/3250 [2:31:39<7:25:54, 11.15s/it]                                                       26%|       | 851/3250 [2:31:39<7:25:54, 11.15s/it] 26%|       | 852/3250 [2:31:49<7:16:16, 10.92s/it]                                                       26%|       | 852/3250 [2:31:49<7:16:16, 10.92s/it] 26%|       | 853/3250 [2:32:00<7:12:29, 10.83s/it]                                                       26%|       | 853/3250 [2:32:00<7:12:29, 10.83s/it] 26%|       | 854/3250 [2:32:10<7:06:48, 10.69s/it]                                                       26%|       | 854/3250 [2:32:10<7:06:48, 10.69s/it] 26%|       | 855/3250 [2:32:21<7:02:39, 10.59s/it]                                                       26%|       | 855/3250 [2:32:21<7:02:39, 10.59s/it] 26%|       | 856/3250 [2:32:31<6:59:44, 10.52s/it]                                                       26%|  {'loss': 0.6768, 'learning_rate': 8.385471405087927e-05, 'epoch': 0.26}
{'loss': 0.6972, 'learning_rate': 8.3819108836604e-05, 'epoch': 0.26}
{'loss': 0.737, 'learning_rate': 8.378347198281284e-05, 'epoch': 0.26}
{'loss': 0.7459, 'learning_rate': 8.37478035228459e-05, 'epoch': 0.26}
     | 856/3250 [2:32:31<6:59:44, 10.52s/it] 26%|       | 857/3250 [2:32:41<6:57:38, 10.47s/it]                                                       26%|       | 857/3250 [2:32:41<6:57:38, 10.47s/it] 26%|       | 858/3250 [2:32:52<6:55:55, 10.43s/it]                                                       26%|       | 858/3250 [2:32:52<6:55:55, 10.43s/it] 26%|       | 859/3250 [2:33:02<6:54:53, 10.41s/it]                                                       26%|       | 859/3250 [2:33:02<6:54:53, 10.41s/it] 26%|       | 860/3250 [2:33:13<6:54:06, 10.40s/it]                                                       26%|       | 860/3250 [2:33:13<6:54:06, 10.40s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8265910744667053, 'eval_runtime': 2.1041, 'eval_samples_per_second': 5.703, 'eval_steps_per_second': 1.426, 'epoch': 0.26}
                                                       26%|       | 860/3250 [2:33:15<6:54:06, 10.40s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_10/tmp-checkpoint-860
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-860I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-860

I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-860
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-860
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-860/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-860/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-860/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-860/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.6918, 'learning_rate': 8.371210349007286e-05, 'epoch': 0.26}
{'loss': 0.7212, 'learning_rate': 8.367637191789296e-05, 'epoch': 0.27}
{'loss': 0.7668, 'learning_rate': 8.364060883973489e-05, 'epoch': 0.27}
{'loss': 0.7469, 'learning_rate': 8.360481428905686e-05, 'epoch': 0.27}
{'loss': 0.7103, 'learning_rate': 8.356898829934652e-05, 'epoch': 0.27}
{'loss': 0.6776, 'learning_rate': 8.353313090412093e-05, 'epoch': 0.27}
 26%|       | 861/3250 [2:33:26<7:25:37, 11.19s/it]                                                       26%|       | 861/3250 [2:33:26<7:25:37, 11.19s/it] 27%|       | 862/3250 [2:33:36<7:15:44, 10.95s/it]                                                       27%|       | 862/3250 [2:33:36<7:15:44, 10.95s/it] 27%|       | 863/3250 [2:33:46<7:08:49, 10.78s/it]                                                       27%|       | 863/3250 [2:33:46<7:08:49, 10.78s/it] 27%|       | 864/3250 [2:33:57<7:03:44, 10.66s/it]                                                       27%|       | 864/3250 [2:33:57<7:03:44, 10.66s/it] 27%|       | 865/3250 [2:34:07<7:00:00, 10.57s/it]                                                       27%|       | 865/3250 [2:34:07<7:00:00, 10.57s/it] 27%|       | 866/3250 [2:34:17<6:57:16, 10.50s/it]                                                       27%|  {'loss': 0.7287, 'learning_rate': 8.349724213692651e-05, 'epoch': 0.27}
{'loss': 0.708, 'learning_rate': 8.346132203133906e-05, 'epoch': 0.27}
{'loss': 0.7196, 'learning_rate': 8.34253706209637e-05, 'epoch': 0.27}
{'loss': 0.7134, 'learning_rate': 8.338938793943478e-05, 'epoch': 0.27}
     | 866/3250 [2:34:17<6:57:16, 10.50s/it] 27%|       | 867/3250 [2:34:28<6:55:34, 10.46s/it]                                                       27%|       | 867/3250 [2:34:28<6:55:34, 10.46s/it] 27%|       | 868/3250 [2:34:38<6:54:05, 10.43s/it]                                                       27%|       | 868/3250 [2:34:38<6:54:05, 10.43s/it] 27%|       | 869/3250 [2:34:49<6:57:27, 10.52s/it]                                                       27%|       | 869/3250 [2:34:49<6:57:27, 10.52s/it] 27%|       | 870/3250 [2:34:59<6:55:19, 10.47s/it]                                                       27%|       | 870/3250 [2:34:59<6:55:19, 10.47s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8216409087181091, 'eval_runtime': 2.3429, 'eval_samples_per_second': 5.122, 'eval_steps_per_second': 1.28, 'epoch': 0.27}
                                                       27%|       | 870/3250 [2:35:02<6:55:19, 10.47s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_10/tmp-checkpoint-870
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-870
/projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-870
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-870
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-870
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-870/pytorch_model.bin
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-870/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-870/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-870/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.7119, 'learning_rate': 8.335337402041601e-05, 'epoch': 0.27}
{'loss': 0.7099, 'learning_rate': 8.33173288976002e-05, 'epoch': 0.27}
{'loss': 0.7359, 'learning_rate': 8.328125260470947e-05, 'epoch': 0.27}
{'loss': 0.7375, 'learning_rate': 8.3245145175495e-05, 'epoch': 0.27}
{'loss': 0.7209, 'learning_rate': 8.320900664373719e-05, 'epoch': 0.27}
{'loss': 0.7195, 'learning_rate': 8.317283704324549e-05, 'epoch': 0.27}
 27%|       | 871/3250 [2:35:12<7:27:33, 11.29s/it]                                                       27%|       | 871/3250 [2:35:12<7:27:33, 11.29s/it] 27%|       | 872/3250 [2:35:23<7:16:14, 11.01s/it]                                                       27%|       | 872/3250 [2:35:23<7:16:14, 11.01s/it] 27%|       | 873/3250 [2:35:33<7:08:28, 10.82s/it]                                                       27%|       | 873/3250 [2:35:33<7:08:28, 10.82s/it] 27%|       | 874/3250 [2:35:43<7:02:49, 10.68s/it]                                                       27%|       | 874/3250 [2:35:43<7:02:49, 10.68s/it] 27%|       | 875/3250 [2:35:54<6:58:52, 10.58s/it]                                                       27%|       | 875/3250 [2:35:54<6:58:52, 10.58s/it] 27%|       | 876/3250 [2:36:04<6:56:07, 10.52s/it]                                                       27%|  {'loss': 0.7489, 'learning_rate': 8.313663640785839e-05, 'epoch': 0.27}
{'loss': 0.6876, 'learning_rate': 8.310040477144347e-05, 'epoch': 0.27}
{'loss': 0.7746, 'learning_rate': 8.30641421678973e-05, 'epoch': 0.27}
{'loss': 0.693, 'learning_rate': 8.30278486311454e-05, 'epoch': 0.27}
     | 876/3250 [2:36:04<6:56:07, 10.52s/it] 27%|       | 877/3250 [2:36:15<6:53:55, 10.47s/it]                                                       27%|       | 877/3250 [2:36:15<6:53:55, 10.47s/it] 27%|       | 878/3250 [2:36:25<6:52:48, 10.44s/it]                                                       27%|       | 878/3250 [2:36:25<6:52:48, 10.44s/it] 27%|       | 879/3250 [2:36:35<6:51:42, 10.42s/it]                                                       27%|       | 879/3250 [2:36:35<6:51:42, 10.42s/it] 27%|       | 880/3250 [2:36:46<6:50:59, 10.40s/it]                                                       27%|       | 880/3250 [2:36:46<6:50:59, 10.40s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8253358006477356, 'eval_runtime': 2.1029, 'eval_samples_per_second': 5.706, 'eval_steps_per_second': 1.427, 'epoch': 0.27}
                                                       27%|       | 880/3250 [2:36:48<6:50:59, 10.40s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_10/tmp-checkpoint-880
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-880
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-880
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-880
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-880
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-880/pytorch_model.bin
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-880/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-880/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-880/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.6981, 'learning_rate': 8.29915241951422e-05, 'epoch': 0.27}
{'loss': 0.7038, 'learning_rate': 8.295516889387115e-05, 'epoch': 0.27}
{'loss': 0.7086, 'learning_rate': 8.291878276134447e-05, 'epoch': 0.27}
{'loss': 0.7084, 'learning_rate': 8.288236583160322e-05, 'epoch': 0.27}
{'loss': 0.7448, 'learning_rate': 8.284591813871738e-05, 'epoch': 0.27}
{'loss': 0.716, 'learning_rate': 8.280943971678562e-05, 'epoch': 0.27}
 27%|       | 881/3250 [2:36:59<7:20:44, 11.16s/it]                                                       27%|       | 881/3250 [2:36:59<7:20:44, 11.16s/it] 27%|       | 882/3250 [2:37:09<7:11:16, 10.93s/it]                                                       27%|       | 882/3250 [2:37:09<7:11:16, 10.93s/it] 27%|       | 883/3250 [2:37:19<7:04:40, 10.76s/it]                                                       27%|       | 883/3250 [2:37:19<7:04:40, 10.76s/it] 27%|       | 884/3250 [2:37:30<7:00:00, 10.65s/it]                                                       27%|       | 884/3250 [2:37:30<7:00:00, 10.65s/it] 27%|       | 885/3250 [2:37:40<6:56:45, 10.57s/it]                                                       27%|       | 885/3250 [2:37:40<6:56:45, 10.57s/it] 27%|       | 886/3250 [2:37:51<6:57:29, 10.60s/it]                                                       27%|  {'loss': 1.1832, 'learning_rate': 8.277293059993535e-05, 'epoch': 0.27}
{'loss': 0.7067, 'learning_rate': 8.273639082232276e-05, 'epoch': 0.27}
{'loss': 0.7351, 'learning_rate': 8.269982041813267e-05, 'epoch': 0.27}
{'loss': 0.7234, 'learning_rate': 8.26632194215786e-05, 'epoch': 0.27}
     | 886/3250 [2:37:51<6:57:29, 10.60s/it] 27%|       | 887/3250 [2:38:01<6:54:41, 10.53s/it]                                                       27%|       | 887/3250 [2:38:01<6:54:41, 10.53s/it] 27%|       | 888/3250 [2:38:12<6:52:40, 10.48s/it]                                                       27%|       | 888/3250 [2:38:12<6:52:40, 10.48s/it] 27%|       | 889/3250 [2:38:22<6:51:10, 10.45s/it]                                                       27%|       | 889/3250 [2:38:22<6:51:10, 10.45s/it] 27%|       | 890/3250 [2:38:32<6:50:14, 10.43s/it]                                                       27%|       | 890/3250 [2:38:32<6:50:14, 10.43s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8268338441848755, 'eval_runtime': 2.127, 'eval_samples_per_second': 5.642, 'eval_steps_per_second': 1.41, 'epoch': 0.27}
                                                       27%|       | 890/3250 [2:38:34<6:50:14, 10.43s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_10/tmp-checkpoint-890
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  I AM HERE AND I AM SAVING THE MODEL/projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-890
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-890

I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-890
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-890
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-890/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-890/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-890/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-890/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.7249, 'learning_rate': 8.262658786690262e-05, 'epoch': 0.27}
{'loss': 0.6876, 'learning_rate': 8.258992578837548e-05, 'epoch': 0.27}
{'loss': 0.7689, 'learning_rate': 8.255323322029642e-05, 'epoch': 0.27}
{'loss': 0.7573, 'learning_rate': 8.251651019699322e-05, 'epoch': 0.28}
{'loss': 0.7036, 'learning_rate': 8.247975675282218e-05, 'epoch': 0.28}
{'loss': 0.6857, 'learning_rate': 8.244297292216802e-05, 'epoch': 0.28}
 27%|       | 891/3250 [2:38:45<7:20:38, 11.21s/it]                                                       27%|       | 891/3250 [2:38:45<7:20:38, 11.21s/it] 27%|       | 892/3250 [2:38:56<7:10:50, 10.96s/it]                                                       27%|       | 892/3250 [2:38:56<7:10:50, 10.96s/it] 27%|       | 893/3250 [2:39:06<7:03:42, 10.79s/it]                                                       27%|       | 893/3250 [2:39:06<7:03:42, 10.79s/it] 28%|       | 894/3250 [2:39:16<6:58:34, 10.66s/it]                                                       28%|       | 894/3250 [2:39:16<6:58:34, 10.66s/it] 28%|       | 895/3250 [2:39:27<6:54:59, 10.57s/it]                                                       28%|       | 895/3250 [2:39:27<6:54:59, 10.57s/it] 28%|       | 896/3250 [2:39:37<6:52:34, 10.52s/it]                                                       28%|  {'loss': 0.7179, 'learning_rate': 8.240615873944391e-05, 'epoch': 0.28}
{'loss': 0.7021, 'learning_rate': 8.236931423909138e-05, 'epoch': 0.28}
{'loss': 0.71, 'learning_rate': 8.233243945558042e-05, 'epoch': 0.28}
{'loss': 0.7187, 'learning_rate': 8.229553442340922e-05, 'epoch': 0.28}
     | 896/3250 [2:39:37<6:52:34, 10.52s/it] 28%|       | 897/3250 [2:39:48<6:50:28, 10.47s/it]                                                       28%|       | 897/3250 [2:39:48<6:50:28, 10.47s/it] 28%|       | 898/3250 [2:39:58<6:48:59, 10.43s/it]                                                       28%|       | 898/3250 [2:39:58<6:48:59, 10.43s/it] 28%|       | 899/3250 [2:40:08<6:48:07, 10.42s/it]                                                       28%|       | 899/3250 [2:40:08<6:48:07, 10.42s/it] 28%|       | 900/3250 [2:40:19<6:47:21, 10.40s/it]                                                       28%|       | 900/3250 [2:40:19<6:47:21, 10.40s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8188436627388, 'eval_runtime': 2.1038, 'eval_samples_per_second': 5.704, 'eval_steps_per_second': 1.426, 'epoch': 0.28}
                                                       28%|       | 900/3250 [2:40:21<6:47:21, 10.40s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_10/tmp-checkpoint-900
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-900
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-900
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-900
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-900
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-900/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-900/pytorch_model.binthe pytorch model path is
 the pytorch model path is/projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-900/pytorch_model.bin /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-900/pytorch_model.bin

/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.7016, 'learning_rate': 8.225859917710439e-05, 'epoch': 0.28}
{'loss': 0.7246, 'learning_rate': 8.222163375122072e-05, 'epoch': 0.28}
{'loss': 0.718, 'learning_rate': 8.218463818034128e-05, 'epoch': 0.28}
{'loss': 0.7004, 'learning_rate': 8.214761249907732e-05, 'epoch': 0.28}
{'loss': 0.7136, 'learning_rate': 8.211055674206828e-05, 'epoch': 0.28}
{'loss': 0.7248, 'learning_rate': 8.207347094398173e-05, 'epoch': 0.28}
 28%|       | 901/3250 [2:40:32<7:17:17, 11.17s/it]                                                       28%|       | 901/3250 [2:40:32<7:17:17, 11.17s/it] 28%|       | 902/3250 [2:40:43<7:14:52, 11.11s/it]                                                       28%|       | 902/3250 [2:40:43<7:14:52, 11.11s/it] 28%|       | 903/3250 [2:40:53<7:06:25, 10.90s/it]                                                       28%|       | 903/3250 [2:40:53<7:06:25, 10.90s/it] 28%|       | 904/3250 [2:41:03<7:00:03, 10.74s/it]                                                       28%|       | 904/3250 [2:41:03<7:00:03, 10.74s/it] 28%|       | 905/3250 [2:41:14<6:55:27, 10.63s/it]                                                       28%|       | 905/3250 [2:41:14<6:55:27, 10.63s/it] 28%|       | 906/3250 [2:41:24<6:52:24, 10.56s/it]                                                       28%|  {'loss': 0.73, 'learning_rate': 8.203635513951331e-05, 'epoch': 0.28}
{'loss': 0.6723, 'learning_rate': 8.199920936338681e-05, 'epoch': 0.28}
{'loss': 0.7608, 'learning_rate': 8.1962033650354e-05, 'epoch': 0.28}
{'loss': 0.7108, 'learning_rate': 8.192482803519465e-05, 'epoch': 0.28}
     | 906/3250 [2:41:24<6:52:24, 10.56s/it] 28%|       | 907/3250 [2:41:35<6:50:22, 10.51s/it]                                                       28%|       | 907/3250 [2:41:35<6:50:22, 10.51s/it] 28%|       | 908/3250 [2:41:45<6:48:49, 10.47s/it]                                                       28%|       | 908/3250 [2:41:45<6:48:49, 10.47s/it] 28%|       | 909/3250 [2:41:55<6:47:30, 10.44s/it]                                                       28%|       | 909/3250 [2:41:55<6:47:30, 10.44s/it] 28%|       | 910/3250 [2:42:06<6:46:29, 10.42s/it]                                                       28%|       | 910/3250 [2:42:06<6:46:29, 10.42s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.820094883441925, 'eval_runtime': 2.117, 'eval_samples_per_second': 5.668, 'eval_steps_per_second': 1.417, 'epoch': 0.28}
                                                       28%|       | 910/3250 [2:42:08<6:46:29, 10.42s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_10/tmp-checkpoint-910
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  I AM HERE AND I AM SAVING THE MODEL
/projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-910the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-910

I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-910
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-910
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-910/pytorch_model.bin
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-910/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-910/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-910/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.6882, 'learning_rate': 8.188759255271654e-05, 'epoch': 0.28}
{'loss': 0.6962, 'learning_rate': 8.185032723775539e-05, 'epoch': 0.28}
{'loss': 0.6766, 'learning_rate': 8.181303212517479e-05, 'epoch': 0.28}
{'loss': 0.7249, 'learning_rate': 8.177570724986628e-05, 'epoch': 0.28}
{'loss': 0.694, 'learning_rate': 8.173835264674916e-05, 'epoch': 0.28}
{'loss': 0.7268, 'learning_rate': 8.17009683507706e-05, 'epoch': 0.28}
 28%|       | 911/3250 [2:42:19<7:15:34, 11.17s/it]                                                       28%|       | 911/3250 [2:42:19<7:15:34, 11.17s/it] 28%|       | 912/3250 [2:42:29<7:05:56, 10.93s/it]                                                       28%|       | 912/3250 [2:42:29<7:05:56, 10.93s/it] 28%|       | 913/3250 [2:42:39<6:58:54, 10.76s/it]                                                       28%|       | 913/3250 [2:42:39<6:58:54, 10.76s/it] 28%|       | 914/3250 [2:42:50<6:54:17, 10.64s/it]                                                       28%|       | 914/3250 [2:42:50<6:54:17, 10.64s/it] 28%|       | 915/3250 [2:43:00<6:50:50, 10.56s/it]                                                       28%|       | 915/3250 [2:43:00<6:50:50, 10.56s/it] 28%|       | 916/3250 [2:43:10<6:48:33, 10.50s/it]                                                       28%|  {'loss': 1.1791, 'learning_rate': 8.166355439690553e-05, 'epoch': 0.28}
{'loss': 0.6981, 'learning_rate': 8.162611082015665e-05, 'epoch': 0.28}
{'loss': 0.6995, 'learning_rate': 8.15886376555543e-05, 'epoch': 0.28}
{'loss': 0.7251, 'learning_rate': 8.15511349381566e-05, 'epoch': 0.28}
     | 916/3250 [2:43:10<6:48:33, 10.50s/it] 28%|       | 917/3250 [2:43:21<6:46:42, 10.46s/it]                                                       28%|       | 917/3250 [2:43:21<6:46:42, 10.46s/it] 28%|       | 918/3250 [2:43:31<6:48:19, 10.51s/it]                                                       28%|       | 918/3250 [2:43:31<6:48:19, 10.51s/it] 28%|       | 919/3250 [2:43:42<6:46:24, 10.46s/it]                                                       28%|       | 919/3250 [2:43:42<6:46:24, 10.46s/it] 28%|       | 920/3250 [2:43:52<6:45:10, 10.43s/it]                                                       28%|       | 920/3250 [2:43:52<6:45:10, 10.43s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.827037513256073, 'eval_runtime': 2.1211, 'eval_samples_per_second': 5.657, 'eval_steps_per_second': 1.414, 'epoch': 0.28}
                                                       28%|       | 920/3250 [2:43:54<6:45:10, 10.43s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_10/tmp-checkpoint-920
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-920
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-920
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-920
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-920
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-920/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-920/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-920/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-920/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.7141, 'learning_rate': 8.151360270304927e-05, 'epoch': 0.28}
{'loss': 0.6678, 'learning_rate': 8.14760409853456e-05, 'epoch': 0.28}
{'loss': 0.6911, 'learning_rate': 8.143844982018655e-05, 'epoch': 0.28}
{'loss': 0.7695, 'learning_rate': 8.14008292427406e-05, 'epoch': 0.28}
{'loss': 0.7077, 'learning_rate': 8.13631792882037e-05, 'epoch': 0.28}
{'loss': 0.7163, 'learning_rate': 8.132549999179933e-05, 'epoch': 0.28}
 28%|       | 921/3250 [2:44:05<7:14:14, 11.19s/it]                                                       28%|       | 921/3250 [2:44:05<7:14:14, 11.19s/it] 28%|       | 922/3250 [2:44:15<7:04:31, 10.94s/it]                                                       28%|       | 922/3250 [2:44:15<7:04:31, 10.94s/it] 28%|       | 923/3250 [2:44:26<6:57:38, 10.77s/it]                                                       28%|       | 923/3250 [2:44:26<6:57:38, 10.77s/it] 28%|       | 924/3250 [2:44:36<6:52:49, 10.65s/it]                                                       28%|       | 924/3250 [2:44:36<6:52:49, 10.65s/it] 28%|       | 925/3250 [2:44:47<6:49:20, 10.56s/it]                                                       28%|       | 925/3250 [2:44:47<6:49:20, 10.56s/it] 28%|       | 926/3250 [2:44:57<6:46:47, 10.50s/it]                                                       28%|  {'loss': 0.6547, 'learning_rate': 8.128779138877843e-05, 'epoch': 0.29}
{'loss': 0.7002, 'learning_rate': 8.12500535144193e-05, 'epoch': 0.29}
{'loss': 0.7235, 'learning_rate': 8.12122864040277e-05, 'epoch': 0.29}
{'loss': 0.6914, 'learning_rate': 8.117449009293668e-05, 'epoch': 0.29}
     | 926/3250 [2:44:57<6:46:47, 10.50s/it] 29%|       | 927/3250 [2:45:07<6:45:02, 10.46s/it]                                                       29%|       | 927/3250 [2:45:07<6:45:02, 10.46s/it] 29%|       | 928/3250 [2:45:18<6:43:58, 10.44s/it]                                                       29%|       | 928/3250 [2:45:18<6:43:58, 10.44s/it] 29%|       | 929/3250 [2:45:28<6:42:56, 10.42s/it]                                                       29%|       | 929/3250 [2:45:28<6:42:56, 10.42s/it] 29%|       | 930/3250 [2:45:38<6:42:13, 10.40s/it]                                                       29%|       | 930/3250 [2:45:38<6:42:13, 10.40s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8202720880508423, 'eval_runtime': 2.1155, 'eval_samples_per_second': 5.672, 'eval_steps_per_second': 1.418, 'epoch': 0.29}
                                                       29%|       | 930/3250 [2:45:40<6:42:13, 10.40s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_10/tmp-checkpoint-930
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-930I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-930

I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-930
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-930
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-930/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-930/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-930/pytorch_model.binthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-930/pytorch_model.bin

/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.6774, 'learning_rate': 8.113666461650664e-05, 'epoch': 0.29}
{'loss': 0.7036, 'learning_rate': 8.109881001012527e-05, 'epoch': 0.29}
{'loss': 0.7206, 'learning_rate': 8.106092630920749e-05, 'epoch': 0.29}
{'loss': 0.7186, 'learning_rate': 8.102301354919544e-05, 'epoch': 0.29}
{'loss': 0.7141, 'learning_rate': 8.098507176555849e-05, 'epoch': 0.29}
{'loss': 0.6961, 'learning_rate': 8.09471009937931e-05, 'epoch': 0.29}
 29%|       | 931/3250 [2:45:51<7:11:34, 11.17s/it]                                                       29%|       | 931/3250 [2:45:51<7:11:34, 11.17s/it] 29%|       | 932/3250 [2:46:02<7:02:20, 10.93s/it]                                                       29%|       | 932/3250 [2:46:02<7:02:20, 10.93s/it] 29%|       | 933/3250 [2:46:12<6:55:37, 10.76s/it]                                                       29%|       | 933/3250 [2:46:12<6:55:37, 10.76s/it] 29%|       | 934/3250 [2:46:22<6:50:51, 10.64s/it]                                                       29%|       | 934/3250 [2:46:22<6:50:51, 10.64s/it] 29%|       | 935/3250 [2:46:33<6:54:39, 10.75s/it]                                                       29%|       | 935/3250 [2:46:33<6:54:39, 10.75s/it] 29%|       | 936/3250 [2:46:44<6:50:04, 10.63s/it]                                                       29%|  {'loss': 0.7294, 'learning_rate': 8.090910126942288e-05, 'epoch': 0.29}
{'loss': 0.6996, 'learning_rate': 8.087107262799855e-05, 'epoch': 0.29}
{'loss': 0.716, 'learning_rate': 8.083301510509784e-05, 'epoch': 0.29}
{'loss': 0.7047, 'learning_rate': 8.079492873632554e-05, 'epoch': 0.29}
     | 936/3250 [2:46:44<6:50:04, 10.63s/it] 29%|       | 937/3250 [2:46:54<6:46:48, 10.55s/it]                                                       29%|       | 937/3250 [2:46:54<6:46:48, 10.55s/it] 29%|       | 938/3250 [2:47:05<6:44:32, 10.50s/it]                                                       29%|       | 938/3250 [2:47:05<6:44:32, 10.50s/it] 29%|       | 939/3250 [2:47:15<6:42:50, 10.46s/it]                                                       29%|       | 939/3250 [2:47:15<6:42:50, 10.46s/it] 29%|       | 940/3250 [2:47:25<6:41:35, 10.43s/it]                                                       29%|       | 940/3250 [2:47:25<6:41:35, 10.43s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8169125318527222, 'eval_runtime': 2.12, 'eval_samples_per_second': 5.66, 'eval_steps_per_second': 1.415, 'epoch': 0.29}
                                                       29%|       | 940/3250 [2:47:27<6:41:35, 10.43s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_10/tmp-checkpoint-940
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-940I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in 
 /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-940
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-940
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-940
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-940/pytorch_model.bin
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-940/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-940/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-940/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.6883, 'learning_rate': 8.075681355731338e-05, 'epoch': 0.29}
{'loss': 0.6738, 'learning_rate': 8.07186696037201e-05, 'epoch': 0.29}
{'loss': 0.6724, 'learning_rate': 8.06804969112313e-05, 'epoch': 0.29}
{'loss': 0.7283, 'learning_rate': 8.064229551555951e-05, 'epoch': 0.29}
{'loss': 0.6789, 'learning_rate': 8.060406545244413e-05, 'epoch': 0.29}
{'loss': 0.7418, 'learning_rate': 8.05658067576513e-05, 'epoch': 0.29}
 29%|       | 941/3250 [2:47:38<7:11:34, 11.21s/it]                                                       29%|       | 941/3250 [2:47:38<7:11:34, 11.21s/it] 29%|       | 942/3250 [2:47:49<7:02:25, 10.98s/it]                                                       29%|       | 942/3250 [2:47:49<7:02:25, 10.98s/it] 29%|       | 943/3250 [2:47:59<6:55:56, 10.82s/it]                                                       29%|       | 943/3250 [2:47:59<6:55:56, 10.82s/it] 29%|       | 944/3250 [2:48:10<6:51:02, 10.70s/it]                                                       29%|       | 944/3250 [2:48:10<6:51:02, 10.70s/it] 29%|       | 945/3250 [2:48:20<6:48:01, 10.62s/it]                                                       29%|       | 945/3250 [2:48:20<6:48:01, 10.62s/it] 29%|       | 946/3250 [2:48:30<6:45:40, 10.56s/it]                                                       29%|  {'loss': 1.21, 'learning_rate': 8.052751946697403e-05, 'epoch': 0.29}
{'loss': 0.6451, 'learning_rate': 8.048920361623202e-05, 'epoch': 0.29}
{'loss': 0.6945, 'learning_rate': 8.045085924127177e-05, 'epoch': 0.29}
{'loss': 0.7127, 'learning_rate': 8.041248637796637e-05, 'epoch': 0.29}
     | 946/3250 [2:48:30<6:45:40, 10.56s/it] 29%|       | 947/3250 [2:48:41<6:43:39, 10.52s/it]                                                       29%|       | 947/3250 [2:48:41<6:43:39, 10.52s/it] 29%|       | 948/3250 [2:48:51<6:42:34, 10.49s/it]                                                       29%|       | 948/3250 [2:48:51<6:42:34, 10.49s/it] 29%|       | 949/3250 [2:49:02<6:41:37, 10.47s/it]                                                       29%|       | 949/3250 [2:49:02<6:41:37, 10.47s/it] 29%|       | 950/3250 [2:49:12<6:40:55, 10.46s/it]                                                       29%|       | 950/3250 [2:49:12<6:40:55, 10.46s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8185819983482361, 'eval_runtime': 2.1113, 'eval_samples_per_second': 5.684, 'eval_steps_per_second': 1.421, 'epoch': 0.29}
                                                       29%|       | 950/3250 [2:49:14<6:40:55, 10.46s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_10/tmp-checkpoint-950
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-950
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-950
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-950
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-950
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-950/pytorch_model.bin
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-950/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-950/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-950/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.7225, 'learning_rate': 8.037408506221563e-05, 'epoch': 0.29}
{'loss': 0.685, 'learning_rate': 8.033565532994593e-05, 'epoch': 0.29}
{'loss': 0.6951, 'learning_rate': 8.029719721711031e-05, 'epoch': 0.29}
{'loss': 0.7483, 'learning_rate': 8.025871075968828e-05, 'epoch': 0.29}
{'loss': 0.7201, 'learning_rate': 8.022019599368588e-05, 'epoch': 0.29}
{'loss': 0.6861, 'learning_rate': 8.018165295513569e-05, 'epoch': 0.29}
 29%|       | 951/3250 [2:49:25<7:13:20, 11.31s/it]                                                       29%|       | 951/3250 [2:49:25<7:13:20, 11.31s/it] 29%|       | 952/3250 [2:49:36<7:03:07, 11.05s/it]                                                       29%|       | 952/3250 [2:49:36<7:03:07, 11.05s/it] 29%|       | 953/3250 [2:49:46<6:55:44, 10.86s/it]                                                       29%|       | 953/3250 [2:49:46<6:55:44, 10.86s/it] 29%|       | 954/3250 [2:49:57<6:50:42, 10.73s/it]                                                       29%|       | 954/3250 [2:49:57<6:50:42, 10.73s/it] 29%|       | 955/3250 [2:50:07<6:46:58, 10.64s/it]                                                       29%|       | 955/3250 [2:50:07<6:46:58, 10.64s/it] 29%|       | 956/3250 [2:50:18<6:44:19, 10.58s/it]                                                       29%|  {'loss': 0.6631, 'learning_rate': 8.014308168009668e-05, 'epoch': 0.29}
{'loss': 0.7083, 'learning_rate': 8.01044822046543e-05, 'epoch': 0.29}
{'loss': 0.695, 'learning_rate': 8.006585456492029e-05, 'epoch': 0.3}
{'loss': 0.7116, 'learning_rate': 8.002719879703284e-05, 'epoch': 0.3}
     | 956/3250 [2:50:18<6:44:19, 10.58s/it] 29%|       | 957/3250 [2:50:28<6:42:34, 10.53s/it]                                                       29%|       | 957/3250 [2:50:28<6:42:34, 10.53s/it] 29%|       | 958/3250 [2:50:38<6:41:18, 10.51s/it]                                                       29%|       | 958/3250 [2:50:38<6:41:18, 10.51s/it] 30%|       | 959/3250 [2:50:49<6:40:16, 10.48s/it]                                                       30%|       | 959/3250 [2:50:49<6:40:16, 10.48s/it] 30%|       | 960/3250 [2:50:59<6:39:31, 10.47s/it]                                                       30%|       | 960/3250 [2:50:59<6:39:31, 10.47s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8176356554031372, 'eval_runtime': 2.1166, 'eval_samples_per_second': 5.669, 'eval_steps_per_second': 1.417, 'epoch': 0.3}
                                                       30%|       | 960/3250 [2:51:01<6:39:31, 10.47s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_10/tmp-checkpoint-960
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-960I AM HERE AND I AM SAVING THE MODEL

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-960
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-960
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-960
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-960/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-960/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-960/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-960/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.6889, 'learning_rate': 7.99885149371564e-05, 'epoch': 0.3}
{'loss': 0.7065, 'learning_rate': 7.99498030214817e-05, 'epoch': 0.3}
{'loss': 0.684, 'learning_rate': 7.991106308622572e-05, 'epoch': 0.3}
{'loss': 0.7139, 'learning_rate': 7.987229516763168e-05, 'epoch': 0.3}
{'loss': 0.7039, 'learning_rate': 7.983349930196896e-05, 'epoch': 0.3}
{'loss': 0.693, 'learning_rate': 7.979467552553308e-05, 'epoch': 0.3}
 30%|       | 961/3250 [2:51:12<7:09:11, 11.25s/it]                                                       30%|       | 961/3250 [2:51:12<7:09:11, 11.25s/it] 30%|       | 962/3250 [2:51:23<6:59:38, 11.00s/it]                                                       30%|       | 962/3250 [2:51:23<6:59:38, 11.00s/it] 30%|       | 963/3250 [2:51:33<6:52:33, 10.82s/it]                                                       30%|       | 963/3250 [2:51:33<6:52:33, 10.82s/it] 30%|       | 964/3250 [2:51:44<6:48:00, 10.71s/it]                                                       30%|       | 964/3250 [2:51:44<6:48:00, 10.71s/it] 30%|       | 965/3250 [2:51:54<6:44:35, 10.62s/it]                                                       30%|       | 965/3250 [2:51:54<6:44:35, 10.62s/it] 30%|       | 966/3250 [2:52:05<6:42:01, 10.56s/it]                                                       30%|  {'loss': 0.7037, 'learning_rate': 7.975582387464568e-05, 'epoch': 0.3}
{'loss': 0.7134, 'learning_rate': 7.97169443856545e-05, 'epoch': 0.3}
{'loss': 0.6687, 'learning_rate': 7.967803709493325e-05, 'epoch': 0.3}
{'loss': 0.7444, 'learning_rate': 7.963910203888177e-05, 'epoch': 0.3}
     | 966/3250 [2:52:05<6:42:01, 10.56s/it] 30%|       | 967/3250 [2:52:15<6:40:30, 10.53s/it]                                                       30%|       | 967/3250 [2:52:15<6:40:30, 10.53s/it] 30%|       | 968/3250 [2:52:26<6:43:59, 10.62s/it]                                                       30%|       | 968/3250 [2:52:26<6:43:59, 10.62s/it] 30%|       | 969/3250 [2:52:36<6:41:44, 10.57s/it]                                                       30%|       | 969/3250 [2:52:36<6:41:44, 10.57s/it] 30%|       | 970/3250 [2:52:47<6:39:56, 10.52s/it]                                                       30%|       | 970/3250 [2:52:47<6:39:56, 10.52s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8140274882316589, 'eval_runtime': 2.3553, 'eval_samples_per_second': 5.095, 'eval_steps_per_second': 1.274, 'epoch': 0.3}
                                                       30%|       | 970/3250 [2:52:49<6:39:56, 10.52s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_10/tmp-checkpoint-970
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-970I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-970

I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-970
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-970
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-970/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-970/pytorch_model.bin
the pytorch model path is the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-970/pytorch_model.bin
/projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-970/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.6761, 'learning_rate': 7.960013925392576e-05, 'epoch': 0.3}
{'loss': 0.6793, 'learning_rate': 7.95611487765169e-05, 'epoch': 0.3}
{'loss': 0.6809, 'learning_rate': 7.952213064313283e-05, 'epoch': 0.3}
{'loss': 0.6778, 'learning_rate': 7.9483084890277e-05, 'epoch': 0.3}
{'loss': 0.6863, 'learning_rate': 7.944401155447871e-05, 'epoch': 0.3}
{'loss': 0.713, 'learning_rate': 7.940491067229311e-05, 'epoch': 0.3}
 30%|       | 971/3250 [2:53:00<7:11:58, 11.37s/it]                                                       30%|       | 971/3250 [2:53:00<7:11:58, 11.37s/it] 30%|       | 972/3250 [2:53:10<7:01:05, 11.09s/it]                                                       30%|       | 972/3250 [2:53:10<7:01:05, 11.09s/it] 30%|       | 973/3250 [2:53:21<6:53:27, 10.89s/it]                                                       30%|       | 973/3250 [2:53:21<6:53:27, 10.89s/it] 30%|       | 974/3250 [2:53:31<6:48:07, 10.76s/it]                                                       30%|       | 974/3250 [2:53:31<6:48:07, 10.76s/it] 30%|       | 975/3250 [2:53:42<6:44:23, 10.67s/it]                                                       30%|       | 975/3250 [2:53:42<6:44:23, 10.67s/it] 30%|       | 976/3250 [2:53:52<6:41:33, 10.60s/it]                                                       30%|  {'loss': 0.6979, 'learning_rate': 7.936578228030105e-05, 'epoch': 0.3}
{'loss': 1.1669, 'learning_rate': 7.932662641510915e-05, 'epoch': 0.3}
{'loss': 0.684, 'learning_rate': 7.928744311334977e-05, 'epoch': 0.3}
{'loss': 0.7047, 'learning_rate': 7.92482324116809e-05, 'epoch': 0.3}
     | 976/3250 [2:53:52<6:41:33, 10.60s/it] 30%|       | 977/3250 [2:54:03<6:39:30, 10.55s/it]                                                       30%|       | 977/3250 [2:54:03<6:39:30, 10.55s/it] 30%|       | 978/3250 [2:54:13<6:37:54, 10.51s/it]                                                       30%|       | 978/3250 [2:54:13<6:37:54, 10.51s/it] 30%|       | 979/3250 [2:54:24<6:36:53, 10.49s/it]                                                       30%|       | 979/3250 [2:54:24<6:36:53, 10.49s/it] 30%|       | 980/3250 [2:54:34<6:35:55, 10.46s/it]                                                       30%|       | 980/3250 [2:54:34<6:35:55, 10.46s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8146793842315674, 'eval_runtime': 2.1023, 'eval_samples_per_second': 5.708, 'eval_steps_per_second': 1.427, 'epoch': 0.3}
                                                       30%|       | 980/3250 [2:54:36<6:35:55, 10.46s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_10/tmp-checkpoint-980
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-980
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-980
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-980
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-980
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-980/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-980/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-980/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-980/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.7077, 'learning_rate': 7.920899434678612e-05, 'epoch': 0.3}
{'loss': 0.6974, 'learning_rate': 7.916972895537471e-05, 'epoch': 0.3}
{'loss': 0.6673, 'learning_rate': 7.913043627418144e-05, 'epoch': 0.3}
{'loss': 0.734, 'learning_rate': 7.909111633996664e-05, 'epoch': 0.3}
{'loss': 0.7187, 'learning_rate': 7.905176918951613e-05, 'epoch': 0.3}
{'loss': 0.6818, 'learning_rate': 7.901239485964121e-05, 'epoch': 0.3}
 30%|       | 981/3250 [2:54:47<7:04:20, 11.22s/it]                                                       30%|       | 981/3250 [2:54:47<7:04:20, 11.22s/it] 30%|       | 982/3250 [2:54:57<6:55:01, 10.98s/it]                                                       30%|       | 982/3250 [2:54:57<6:55:01, 10.98s/it] 30%|       | 983/3250 [2:55:08<6:48:06, 10.80s/it]                                                       30%|       | 983/3250 [2:55:08<6:48:06, 10.80s/it] 30%|       | 984/3250 [2:55:18<6:46:00, 10.75s/it]                                                       30%|       | 984/3250 [2:55:18<6:46:00, 10.75s/it] 30%|       | 985/3250 [2:55:29<6:41:31, 10.64s/it]                                                       30%|       | 985/3250 [2:55:29<6:41:31, 10.64s/it] 30%|       | 986/3250 [2:55:39<6:38:32, 10.56s/it]                                                       30%|  {'loss': 0.6613, 'learning_rate': 7.897299338717854e-05, 'epoch': 0.3}
{'loss': 0.7004, 'learning_rate': 7.89335648089903e-05, 'epoch': 0.3}
{'loss': 0.6733, 'learning_rate': 7.889410916196389e-05, 'epoch': 0.3}
{'loss': 0.6882, 'learning_rate': 7.885462648301212e-05, 'epoch': 0.3}
     | 986/3250 [2:55:39<6:38:32, 10.56s/it] 30%|       | 987/3250 [2:55:49<6:36:24, 10.51s/it]                                                       30%|       | 987/3250 [2:55:50<6:36:24, 10.51s/it] 30%|       | 988/3250 [2:56:00<6:35:16, 10.48s/it]                                                       30%|       | 988/3250 [2:56:00<6:35:16, 10.48s/it] 30%|       | 989/3250 [2:56:10<6:34:04, 10.46s/it]                                                       30%|       | 989/3250 [2:56:10<6:34:04, 10.46s/it] 30%|       | 990/3250 [2:56:21<6:33:00, 10.43s/it]                                                       30%|       | 990/3250 [2:56:21<6:33:00, 10.43s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.815831184387207, 'eval_runtime': 2.1031, 'eval_samples_per_second': 5.706, 'eval_steps_per_second': 1.426, 'epoch': 0.3}
                                                       30%|       | 990/3250 [2:56:23<6:33:00, 10.43s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_10/tmp-checkpoint-990
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-990
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-990
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-990
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-990
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-990/pytorch_model.bin
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-990/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-990/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-990/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.6846, 'learning_rate': 7.881511680907307e-05, 'epoch': 0.3}
{'loss': 0.6841, 'learning_rate': 7.877558017711007e-05, 'epoch': 0.31}
{'loss': 0.6905, 'learning_rate': 7.873601662411167e-05, 'epoch': 0.31}
{'loss': 0.6996, 'learning_rate': 7.86964261870916e-05, 'epoch': 0.31}
{'loss': 0.6895, 'learning_rate': 7.865680890308879e-05, 'epoch': 0.31}
{'loss': 0.6849, 'learning_rate': 7.86171648091672e-05, 'epoch': 0.31}
 30%|       | 991/3250 [2:56:34<7:00:59, 11.18s/it]                                                       30%|       | 991/3250 [2:56:34<7:00:59, 11.18s/it] 31%|       | 992/3250 [2:56:44<6:52:02, 10.95s/it]                                                       31%|       | 992/3250 [2:56:44<6:52:02, 10.95s/it] 31%|       | 993/3250 [2:56:54<6:45:25, 10.78s/it]                                                       31%|       | 993/3250 [2:56:54<6:45:25, 10.78s/it] 31%|       | 994/3250 [2:57:05<6:40:47, 10.66s/it]                                                       31%|       | 994/3250 [2:57:05<6:40:47, 10.66s/it] 31%|       | 995/3250 [2:57:15<6:37:29, 10.58s/it]                                                       31%|       | 995/3250 [2:57:15<6:37:29, 10.58s/it] 31%|       | 996/3250 [2:57:26<6:35:00, 10.51s/it]                                                       31%|  {'loss': 0.6994, 'learning_rate': 7.857749394241593e-05, 'epoch': 0.31}
{'loss': 0.7155, 'learning_rate': 7.853779633994913e-05, 'epoch': 0.31}
{'loss': 0.6519, 'learning_rate': 7.849807203890595e-05, 'epoch': 0.31}
{'loss': 0.7309, 'learning_rate': 7.84583210764505e-05, 'epoch': 0.31}
     | 996/3250 [2:57:26<6:35:00, 10.51s/it] 31%|       | 997/3250 [2:57:36<6:33:21, 10.48s/it]                                                       31%|       | 997/3250 [2:57:36<6:33:21, 10.48s/it] 31%|       | 998/3250 [2:57:46<6:32:00, 10.44s/it]                                                       31%|       | 998/3250 [2:57:46<6:32:00, 10.44s/it] 31%|       | 999/3250 [2:57:57<6:31:03, 10.42s/it]                                                       31%|       | 999/3250 [2:57:57<6:31:03, 10.42s/it] 31%|       | 1000/3250 [2:58:07<6:34:52, 10.53s/it]                                                        31%|       | 1000/3250 [2:58:07<6:34:52, 10.53s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8136960864067078, 'eval_runtime': 2.3487, 'eval_samples_per_second': 5.109, 'eval_steps_per_second': 1.277, 'epoch': 0.31}
                                                        31%|       | 1000/3250 [2:58:10<6:34:52, 10.53s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_10/tmp-checkpoint-1000
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1000
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1000
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1000
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1000
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1000/pytorch_model.bin
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1000/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1000/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1000/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.6862, 'learning_rate': 7.841854348977186e-05, 'epoch': 0.31}
{'loss': 0.6716, 'learning_rate': 7.837873931608401e-05, 'epoch': 0.31}
{'loss': 0.6784, 'learning_rate': 7.833890859262579e-05, 'epoch': 0.31}
{'loss': 0.6644, 'learning_rate': 7.829905135666091e-05, 'epoch': 0.31}
{'loss': 0.6886, 'learning_rate': 7.825916764547787e-05, 'epoch': 0.31}
 31%|       | 1001/3250 [2:58:21<7:04:31, 11.33s/it]                                                        31%|       | 1001/3250 [2:58:21<7:04:31, 11.33s/it] 31%|       | 1002/3250 [2:58:31<6:53:39, 11.04s/it]                                                        31%|       | 1002/3250 [2:58:31<6:53:39, 11.04s/it] 31%|       | 1003/3250 [2:58:41<6:46:01, 10.84s/it]                                                        31%|       | 1003/3250 [2:58:41<6:46:01, 10.84s/it] 31%|       | 1004/3250 [2:58:52<6:40:25, 10.70s/it]                                                        31%|       | 1004/3250 [2:58:52<6:40:25, 10.70s/it] 31%|       | 1005/3250 [2:59:02<6:36:36, 10.60s/it]                                                        31%|       | 1005/3250 [2:59:02<6:36:36, 10.60s/it] 31%|       | 1006/3250 [2:59:12<6:33:53, 10.53s/it]                                                       {'loss': 0.6764, 'learning_rate': 7.82192574963899e-05, 'epoch': 0.31}
{'loss': 0.7016, 'learning_rate': 7.817932094673501e-05, 'epoch': 0.31}
{'loss': 1.1592, 'learning_rate': 7.813935803387591e-05, 'epoch': 0.31}
{'loss': 0.6807, 'learning_rate': 7.809936879519994e-05, 'epoch': 0.31}
{'loss': 0.6817, 'learning_rate': 7.805935326811912e-05, 'epoch': 0.31}
 31%|       | 1006/3250 [2:59:12<6:33:53, 10.53s/it] 31%|       | 1007/3250 [2:59:23<6:31:58, 10.49s/it]                                                        31%|       | 1007/3250 [2:59:23<6:31:58, 10.49s/it] 31%|       | 1008/3250 [2:59:33<6:30:20, 10.45s/it]                                                        31%|       | 1008/3250 [2:59:33<6:30:20, 10.45s/it] 31%|       | 1009/3250 [2:59:44<6:29:22, 10.42s/it]                                                        31%|       | 1009/3250 [2:59:44<6:29:22, 10.42s/it] 31%|       | 1010/3250 [2:59:54<6:28:39, 10.41s/it]                                                        31%|       | 1010/3250 [2:59:54<6:28:39, 10.41s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8106088042259216, 'eval_runtime': 2.1156, 'eval_samples_per_second': 5.672, 'eval_steps_per_second': 1.418, 'epoch': 0.31}
                                                        31%|       | 1010/3250 [2:59:56<6:28:39, 10.41s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_10/tmp-checkpoint-1010
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1010
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1010
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1010
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1010
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1010/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1010/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1010/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1010/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.7084, 'learning_rate': 7.801931149007001e-05, 'epoch': 0.31}
{'loss': 0.7043, 'learning_rate': 7.797924349851376e-05, 'epoch': 0.31}
{'loss': 0.6597, 'learning_rate': 7.793914933093604e-05, 'epoch': 0.31}
{'loss': 0.673, 'learning_rate': 7.7899029024847e-05, 'epoch': 0.31}
{'loss': 0.7544, 'learning_rate': 7.785888261778124e-05, 'epoch': 0.31}
 31%|       | 1011/3250 [3:00:07<6:56:42, 11.17s/it]                                                        31%|       | 1011/3250 [3:00:07<6:56:42, 11.17s/it] 31%|       | 1012/3250 [3:00:17<6:47:42, 10.93s/it]                                                        31%|       | 1012/3250 [3:00:17<6:47:42, 10.93s/it] 31%|       | 1013/3250 [3:00:28<6:41:13, 10.76s/it]                                                        31%|       | 1013/3250 [3:00:28<6:41:13, 10.76s/it] 31%|       | 1014/3250 [3:00:38<6:36:42, 10.65s/it]                                                        31%|       | 1014/3250 [3:00:38<6:36:42, 10.65s/it] 31%|       | 1015/3250 [3:00:48<6:33:32, 10.56s/it]                                                        31%|       | 1015/3250 [3:00:48<6:33:32, 10.56s/it] 31%|      | 1016/3250 [3:00:59<6:31:54, 10.53s/it]                                                      {'loss': 0.6901, 'learning_rate': 7.781871014729781e-05, 'epoch': 0.31}
{'loss': 0.6904, 'learning_rate': 7.777851165098012e-05, 'epoch': 0.31}
{'loss': 0.6351, 'learning_rate': 7.773828716643591e-05, 'epoch': 0.31}
{'loss': 0.6901, 'learning_rate': 7.769803673129727e-05, 'epoch': 0.31}
{'loss': 0.6965, 'learning_rate': 7.765776038322057e-05, 'epoch': 0.31}
  31%|      | 1016/3250 [3:00:59<6:31:54, 10.53s/it] 31%|      | 1017/3250 [3:01:10<6:34:15, 10.59s/it]                                                        31%|      | 1017/3250 [3:01:10<6:34:15, 10.59s/it] 31%|      | 1018/3250 [3:01:20<6:31:20, 10.52s/it]                                                        31%|      | 1018/3250 [3:01:20<6:31:20, 10.52s/it] 31%|      | 1019/3250 [3:01:30<6:29:26, 10.47s/it]                                                        31%|      | 1019/3250 [3:01:30<6:29:26, 10.47s/it] 31%|      | 1020/3250 [3:01:41<6:27:55, 10.44s/it]                                                        31%|      | 1020/3250 [3:01:41<6:27:55, 10.44s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8147728443145752, 'eval_runtime': 2.1063, 'eval_samples_per_second': 5.697, 'eval_steps_per_second': 1.424, 'epoch': 0.31}
                                                        31%|      | 1020/3250 [3:01:43<6:27:55, 10.44s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_10/tmp-checkpoint-1020
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1020
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1020
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1020
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1020
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1020/pytorch_model.bin
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1020/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1020/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1020/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.675, 'learning_rate': 7.761745815988637e-05, 'epoch': 0.31}
{'loss': 0.6726, 'learning_rate': 7.757713009899949e-05, 'epoch': 0.31}
{'loss': 0.6708, 'learning_rate': 7.753677623828892e-05, 'epoch': 0.31}
{'loss': 0.6931, 'learning_rate': 7.749639661550775e-05, 'epoch': 0.32}
{'loss': 0.7013, 'learning_rate': 7.745599126843319e-05, 'epoch': 0.32}
 31%|      | 1021/3250 [3:01:54<6:55:24, 11.18s/it]                                                        31%|      | 1021/3250 [3:01:54<6:55:24, 11.18s/it] 31%|      | 1022/3250 [3:02:04<6:46:06, 10.94s/it]                                                        31%|      | 1022/3250 [3:02:04<6:46:06, 10.94s/it] 31%|      | 1023/3250 [3:02:14<6:39:37, 10.77s/it]                                                        31%|      | 1023/3250 [3:02:14<6:39:37, 10.77s/it] 32%|      | 1024/3250 [3:02:25<6:34:55, 10.65s/it]                                                        32%|      | 1024/3250 [3:02:25<6:34:55, 10.65s/it] 32%|      | 1025/3250 [3:02:35<6:31:34, 10.56s/it]                                                        32%|      | 1025/3250 [3:02:35<6:31:34, 10.56s/it] 32%|      | 1026/3250 [3:02:45<6:29:23, 10.51s/it]                                  {'loss': 0.6861, 'learning_rate': 7.741556023486654e-05, 'epoch': 0.32}
{'loss': 0.6654, 'learning_rate': 7.737510355263311e-05, 'epoch': 0.32}
{'loss': 0.7074, 'learning_rate': 7.733462125958219e-05, 'epoch': 0.32}
{'loss': 0.6784, 'learning_rate': 7.729411339358708e-05, 'epoch': 0.32}
{'loss': 0.6948, 'learning_rate': 7.725357999254492e-05, 'epoch': 0.32}
                      32%|      | 1026/3250 [3:02:45<6:29:23, 10.51s/it] 32%|      | 1027/3250 [3:02:56<6:27:41, 10.46s/it]                                                        32%|      | 1027/3250 [3:02:56<6:27:41, 10.46s/it] 32%|      | 1028/3250 [3:03:06<6:26:26, 10.43s/it]                                                        32%|      | 1028/3250 [3:03:06<6:26:26, 10.43s/it] 32%|      | 1029/3250 [3:03:17<6:25:30, 10.41s/it]                                                        32%|      | 1029/3250 [3:03:17<6:25:30, 10.41s/it] 32%|      | 1030/3250 [3:03:27<6:24:53, 10.40s/it]                                                        32%|      | 1030/3250 [3:03:27<6:24:53, 10.40s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8107678890228271, 'eval_runtime': 2.1118, 'eval_samples_per_second': 5.682, 'eval_steps_per_second': 1.421, 'epoch': 0.32}
                                                        32%|      | 1030/3250 [3:03:29<6:24:53, 10.40s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_10/tmp-checkpoint-1030
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1030
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1030
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1030
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1030
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1030/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1030/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1030/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1030/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.6923, 'learning_rate': 7.721302109437685e-05, 'epoch': 0.32}
{'loss': 0.672, 'learning_rate': 7.717243673702777e-05, 'epoch': 0.32}
{'loss': 0.657, 'learning_rate': 7.713182695846643e-05, 'epoch': 0.32}
{'loss': 0.6569, 'learning_rate': 7.709119179668538e-05, 'epoch': 0.32}
{'loss': 0.6984, 'learning_rate': 7.70505312897009e-05, 'epoch': 0.32}
 32%|      | 1031/3250 [3:03:40<6:53:00, 11.17s/it]                                                        32%|      | 1031/3250 [3:03:40<6:53:00, 11.17s/it] 32%|      | 1032/3250 [3:03:50<6:44:02, 10.93s/it]                                                        32%|      | 1032/3250 [3:03:50<6:44:02, 10.93s/it] 32%|      | 1033/3250 [3:04:01<6:43:35, 10.92s/it]                                                        32%|      | 1033/3250 [3:04:01<6:43:35, 10.92s/it] 32%|      | 1034/3250 [3:04:12<6:37:21, 10.76s/it]                                                        32%|      | 1034/3250 [3:04:12<6:37:21, 10.76s/it] 32%|      | 1035/3250 [3:04:22<6:32:56, 10.64s/it]                                                        32%|      | 1035/3250 [3:04:22<6:32:56, 10.64s/it] 32%|      | 1036/3250 [3:04:32<6:29:47, 10.56s/it]                                  {'loss': 0.6437, 'learning_rate': 7.700984547555299e-05, 'epoch': 0.32}
{'loss': 0.707, 'learning_rate': 7.696913439230534e-05, 'epoch': 0.32}
{'loss': 1.18, 'learning_rate': 7.692839807804521e-05, 'epoch': 0.32}
{'loss': 0.6331, 'learning_rate': 7.688763657088358e-05, 'epoch': 0.32}
{'loss': 0.6652, 'learning_rate': 7.68468499089549e-05, 'epoch': 0.32}
                      32%|      | 1036/3250 [3:04:32<6:29:47, 10.56s/it] 32%|      | 1037/3250 [3:04:43<6:27:41, 10.51s/it]                                                        32%|      | 1037/3250 [3:04:43<6:27:41, 10.51s/it] 32%|      | 1038/3250 [3:04:53<6:25:49, 10.47s/it]                                                        32%|      | 1038/3250 [3:04:53<6:25:49, 10.47s/it] 32%|      | 1039/3250 [3:05:03<6:24:47, 10.44s/it]                                                        32%|      | 1039/3250 [3:05:03<6:24:47, 10.44s/it] 32%|      | 1040/3250 [3:05:14<6:23:42, 10.42s/it]                                                        32%|      | 1040/3250 [3:05:14<6:23:42, 10.42s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8082288503646851, 'eval_runtime': 2.1101, 'eval_samples_per_second': 5.687, 'eval_steps_per_second': 1.422, 'epoch': 0.32}
                                                        32%|      | 1040/3250 [3:05:16<6:23:42, 10.42s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_10/tmp-checkpoint-1040
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1040
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1040
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1040
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1040
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1040/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1040/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1040/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1040/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.6919, 'learning_rate': 7.680603813041718e-05, 'epoch': 0.32}
{'loss': 0.7011, 'learning_rate': 7.676520127345197e-05, 'epoch': 0.32}
{'loss': 0.6578, 'learning_rate': 7.672433937626423e-05, 'epoch': 0.32}
{'loss': 0.6714, 'learning_rate': 7.668345247708236e-05, 'epoch': 0.32}
{'loss': 0.7164, 'learning_rate': 7.664254061415818e-05, 'epoch': 0.32}
 32%|      | 1041/3250 [3:05:27<6:51:16, 11.17s/it]                                                        32%|      | 1041/3250 [3:05:27<6:51:16, 11.17s/it] 32%|      | 1042/3250 [3:05:37<6:42:13, 10.93s/it]                                                        32%|      | 1042/3250 [3:05:37<6:42:13, 10.93s/it] 32%|      | 1043/3250 [3:05:47<6:35:47, 10.76s/it]                                                        32%|      | 1043/3250 [3:05:47<6:35:47, 10.76s/it] 32%|      | 1044/3250 [3:05:58<6:31:17, 10.64s/it]                                                        32%|      | 1044/3250 [3:05:58<6:31:17, 10.64s/it] 32%|      | 1045/3250 [3:06:08<6:28:08, 10.56s/it]                                                        32%|      | 1045/3250 [3:06:08<6:28:08, 10.56s/it] 32%|      | 1046/3250 [3:06:19<6:26:08, 10.51s/it]                                  {'loss': 0.6937, 'learning_rate': 7.660160382576683e-05, 'epoch': 0.32}
{'loss': 0.6627, 'learning_rate': 7.65606421502068e-05, 'epoch': 0.32}
{'loss': 0.6341, 'learning_rate': 7.651965562579979e-05, 'epoch': 0.32}
{'loss': 0.6835, 'learning_rate': 7.647864429089087e-05, 'epoch': 0.32}
{'loss': 0.6678, 'learning_rate': 7.64376081838482e-05, 'epoch': 0.32}
                      32%|      | 1046/3250 [3:06:19<6:26:08, 10.51s/it] 32%|      | 1047/3250 [3:06:29<6:24:31, 10.47s/it]                                                        32%|      | 1047/3250 [3:06:29<6:24:31, 10.47s/it] 32%|      | 1048/3250 [3:06:39<6:23:26, 10.45s/it]                                                        32%|      | 1048/3250 [3:06:39<6:23:26, 10.45s/it] 32%|      | 1049/3250 [3:06:50<6:22:34, 10.43s/it]                                                        32%|      | 1049/3250 [3:06:50<6:22:34, 10.43s/it] 32%|      | 1050/3250 [3:07:00<6:25:49, 10.52s/it]                                                        32%|      | 1050/3250 [3:07:00<6:25:49, 10.52s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8119560480117798, 'eval_runtime': 2.1118, 'eval_samples_per_second': 5.682, 'eval_steps_per_second': 1.421, 'epoch': 0.32}
                                                        32%|      | 1050/3250 [3:07:03<6:25:49, 10.52s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_10/tmp-checkpoint-1050
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1050I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1050

I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1050
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1050
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1050/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1050/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1050/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1050/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.6751, 'learning_rate': 7.639654734306321e-05, 'epoch': 0.32}
{'loss': 0.6587, 'learning_rate': 7.635546180695038e-05, 'epoch': 0.32}
{'loss': 0.6817, 'learning_rate': 7.63143516139474e-05, 'epoch': 0.32}
{'loss': 0.674, 'learning_rate': 7.627321680251494e-05, 'epoch': 0.32}
{'loss': 0.6961, 'learning_rate': 7.623205741113673e-05, 'epoch': 0.32}
 32%|      | 1051/3250 [3:07:13<6:52:11, 11.25s/it]                                                        32%|      | 1051/3250 [3:07:13<6:52:11, 11.25s/it] 32%|      | 1052/3250 [3:07:24<6:42:27, 10.99s/it]                                                        32%|      | 1052/3250 [3:07:24<6:42:27, 10.99s/it] 32%|      | 1053/3250 [3:07:34<6:35:41, 10.81s/it]                                                        32%|      | 1053/3250 [3:07:34<6:35:41, 10.81s/it] 32%|      | 1054/3250 [3:07:45<6:30:48, 10.68s/it]                                                        32%|      | 1054/3250 [3:07:45<6:30:48, 10.68s/it] 32%|      | 1055/3250 [3:07:55<6:27:26, 10.59s/it]                                                        32%|      | 1055/3250 [3:07:55<6:27:26, 10.59s/it] 32%|      | 1056/3250 [3:08:05<6:25:01, 10.53s/it]                                  {'loss': 0.6791, 'learning_rate': 7.61908734783195e-05, 'epoch': 0.32}
{'loss': 0.6872, 'learning_rate': 7.614966504259293e-05, 'epoch': 0.33}
{'loss': 0.6786, 'learning_rate': 7.610843214250964e-05, 'epoch': 0.33}
{'loss': 0.6882, 'learning_rate': 7.606717481664514e-05, 'epoch': 0.33}
{'loss': 0.6502, 'learning_rate': 7.602589310359778e-05, 'epoch': 0.33}
                      32%|      | 1056/3250 [3:08:05<6:25:01, 10.53s/it] 33%|      | 1057/3250 [3:08:16<6:23:12, 10.48s/it]                                                        33%|      | 1057/3250 [3:08:16<6:23:12, 10.48s/it] 33%|      | 1058/3250 [3:08:26<6:22:08, 10.46s/it]                                                        33%|      | 1058/3250 [3:08:26<6:22:08, 10.46s/it] 33%|      | 1059/3250 [3:08:36<6:21:05, 10.44s/it]                                                        33%|      | 1059/3250 [3:08:36<6:21:05, 10.44s/it] 33%|      | 1060/3250 [3:08:47<6:20:19, 10.42s/it]                                                        33%|      | 1060/3250 [3:08:47<6:20:19, 10.42s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.810258150100708, 'eval_runtime': 2.1088, 'eval_samples_per_second': 5.691, 'eval_steps_per_second': 1.423, 'epoch': 0.33}
                                                        33%|      | 1060/3250 [3:08:49<6:20:19, 10.42s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_10/tmp-checkpoint-1060
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1060
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1060
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1060
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1060
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1060/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1060/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1060/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1060/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.7223, 'learning_rate': 7.598458704198869e-05, 'epoch': 0.33}
{'loss': 0.6537, 'learning_rate': 7.594325667046186e-05, 'epoch': 0.33}
{'loss': 0.666, 'learning_rate': 7.590190202768394e-05, 'epoch': 0.33}
{'loss': 0.655, 'learning_rate': 7.586052315234437e-05, 'epoch': 0.33}
{'loss': 0.6656, 'learning_rate': 7.58191200831552e-05, 'epoch': 0.33}
 33%|      | 1061/3250 [3:09:00<6:48:21, 11.19s/it]                                                        33%|      | 1061/3250 [3:09:00<6:48:21, 11.19s/it] 33%|      | 1062/3250 [3:09:10<6:39:17, 10.95s/it]                                                        33%|      | 1062/3250 [3:09:10<6:39:17, 10.95s/it] 33%|      | 1063/3250 [3:09:21<6:32:53, 10.78s/it]                                                        33%|      | 1063/3250 [3:09:21<6:32:53, 10.78s/it] 33%|      | 1064/3250 [3:09:31<6:28:28, 10.66s/it]                                                        33%|      | 1064/3250 [3:09:31<6:28:28, 10.66s/it] 33%|      | 1065/3250 [3:09:41<6:25:13, 10.58s/it]                                                        33%|      | 1065/3250 [3:09:41<6:25:13, 10.58s/it] 33%|      | 1066/3250 [3:09:52<6:25:46, 10.60s/it]                                  {'loss': 0.6565, 'learning_rate': 7.577769285885109e-05, 'epoch': 0.33}
{'loss': 0.6872, 'learning_rate': 7.57362415181894e-05, 'epoch': 0.33}
{'loss': 0.6659, 'learning_rate': 7.569476609994994e-05, 'epoch': 0.33}
{'loss': 1.1469, 'learning_rate': 7.565326664293512e-05, 'epoch': 0.33}
{'loss': 0.6534, 'learning_rate': 7.561174318596983e-05, 'epoch': 0.33}
                      33%|      | 1066/3250 [3:09:52<6:25:46, 10.60s/it] 33%|      | 1067/3250 [3:10:02<6:23:02, 10.53s/it]                                                        33%|      | 1067/3250 [3:10:02<6:23:02, 10.53s/it] 33%|      | 1068/3250 [3:10:13<6:21:08, 10.48s/it]                                                        33%|      | 1068/3250 [3:10:13<6:21:08, 10.48s/it] 33%|      | 1069/3250 [3:10:23<6:19:35, 10.44s/it]                                                        33%|      | 1069/3250 [3:10:23<6:19:35, 10.44s/it] 33%|      | 1070/3250 [3:10:33<6:18:43, 10.42s/it]                                                        33%|      | 1070/3250 [3:10:33<6:18:43, 10.42s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8062074780464172, 'eval_runtime': 2.3591, 'eval_samples_per_second': 5.087, 'eval_steps_per_second': 1.272, 'epoch': 0.33}
                                                        33%|      | 1070/3250 [3:10:36<6:18:43, 10.42s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_10/tmp-checkpoint-1070
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1070
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1070
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1070
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1070
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1070/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1070/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1070/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1070/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.6775, 'learning_rate': 7.557019576790138e-05, 'epoch': 0.33}
{'loss': 0.6829, 'learning_rate': 7.552862442759954e-05, 'epoch': 0.33}
{'loss': 0.6744, 'learning_rate': 7.548702920395638e-05, 'epoch': 0.33}
{'loss': 0.6341, 'learning_rate': 7.544541013588645e-05, 'epoch': 0.33}
{'loss': 0.7176, 'learning_rate': 7.540376726232648e-05, 'epoch': 0.33}
 33%|      | 1071/3250 [3:10:47<6:48:35, 11.25s/it]                                                        33%|      | 1071/3250 [3:10:47<6:48:35, 11.25s/it] 33%|      | 1072/3250 [3:10:57<6:39:06, 10.99s/it]                                                        33%|      | 1072/3250 [3:10:57<6:39:06, 10.99s/it] 33%|      | 1073/3250 [3:11:07<6:32:24, 10.81s/it]                                                        33%|      | 1073/3250 [3:11:07<6:32:24, 10.81s/it] 33%|      | 1074/3250 [3:11:18<6:27:23, 10.68s/it]                                                        33%|      | 1074/3250 [3:11:18<6:27:23, 10.68s/it] 33%|      | 1075/3250 [3:11:28<6:23:45, 10.59s/it]                                                        33%|      | 1075/3250 [3:11:28<6:23:45, 10.59s/it] 33%|      | 1076/3250 [3:11:39<6:28:32, 10.72s/it]                                  {'loss': 0.694, 'learning_rate': 7.536210062223552e-05, 'epoch': 0.33}
{'loss': 0.6609, 'learning_rate': 7.532041025459488e-05, 'epoch': 0.33}
{'loss': 0.6419, 'learning_rate': 7.527869619840801e-05, 'epoch': 0.33}
{'loss': 0.6776, 'learning_rate': 7.523695849270061e-05, 'epoch': 0.33}
{'loss': 0.6467, 'learning_rate': 7.519519717652039e-05, 'epoch': 0.33}
                      33%|      | 1076/3250 [3:11:39<6:28:32, 10.72s/it] 33%|      | 1077/3250 [3:11:50<6:24:35, 10.62s/it]                                                        33%|      | 1077/3250 [3:11:50<6:24:35, 10.62s/it] 33%|      | 1078/3250 [3:12:00<6:21:45, 10.55s/it]                                                        33%|      | 1078/3250 [3:12:00<6:21:45, 10.55s/it] 33%|      | 1079/3250 [3:12:10<6:19:27, 10.49s/it]                                                        33%|      | 1079/3250 [3:12:10<6:19:27, 10.49s/it] 33%|      | 1080/3250 [3:12:21<6:17:51, 10.45s/it]                                                        33%|      | 1080/3250 [3:12:21<6:17:51, 10.45s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8086804151535034, 'eval_runtime': 2.3888, 'eval_samples_per_second': 5.023, 'eval_steps_per_second': 1.256, 'epoch': 0.33}
                                                        33%|      | 1080/3250 [3:12:23<6:17:51, 10.45s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_10/tmp-checkpoint-1080
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1080I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1080

I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1080
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1080
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1080/pytorch_model.bin
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1080/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1080/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1080/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.6649, 'learning_rate': 7.515341228893725e-05, 'epoch': 0.33}
{'loss': 0.6549, 'learning_rate': 7.511160386904306e-05, 'epoch': 0.33}
{'loss': 0.6682, 'learning_rate': 7.50697719559518e-05, 'epoch': 0.33}
{'loss': 0.6677, 'learning_rate': 7.502791658879932e-05, 'epoch': 0.33}
{'loss': 0.6833, 'learning_rate': 7.498603780674352e-05, 'epoch': 0.33}
 33%|      | 1081/3250 [3:12:34<6:48:30, 11.30s/it]                                                        33%|      | 1081/3250 [3:12:34<6:48:30, 11.30s/it] 33%|      | 1082/3250 [3:12:45<6:42:15, 11.13s/it]                                                        33%|      | 1082/3250 [3:12:45<6:42:15, 11.13s/it] 33%|      | 1083/3250 [3:12:55<6:33:56, 10.91s/it]                                                        33%|      | 1083/3250 [3:12:55<6:33:56, 10.91s/it] 33%|      | 1084/3250 [3:13:05<6:28:04, 10.75s/it]                                                        33%|      | 1084/3250 [3:13:05<6:28:04, 10.75s/it] 33%|      | 1085/3250 [3:13:16<6:23:48, 10.64s/it]                                                        33%|      | 1085/3250 [3:13:16<6:23:48, 10.64s/it] 33%|      | 1086/3250 [3:13:26<6:20:40, 10.55s/it]                                  {'loss': 0.677, 'learning_rate': 7.494413564896414e-05, 'epoch': 0.33}
{'loss': 0.6646, 'learning_rate': 7.490221015466279e-05, 'epoch': 0.33}
{'loss': 0.6786, 'learning_rate': 7.486026136306293e-05, 'epoch': 0.33}
{'loss': 0.6962, 'learning_rate': 7.481828931340983e-05, 'epoch': 0.34}
{'loss': 0.6191, 'learning_rate': 7.477629404497048e-05, 'epoch': 0.34}
                      33%|      | 1086/3250 [3:13:26<6:20:40, 10.55s/it] 33%|      | 1087/3250 [3:13:37<6:18:32, 10.50s/it]                                                        33%|      | 1087/3250 [3:13:37<6:18:32, 10.50s/it] 33%|      | 1088/3250 [3:13:47<6:17:02, 10.46s/it]                                                        33%|      | 1088/3250 [3:13:47<6:17:02, 10.46s/it] 34%|      | 1089/3250 [3:13:57<6:15:47, 10.43s/it]                                                        34%|      | 1089/3250 [3:13:57<6:15:47, 10.43s/it] 34%|      | 1090/3250 [3:14:08<6:14:57, 10.42s/it]                                                        34%|      | 1090/3250 [3:14:08<6:14:57, 10.42s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8084301352500916, 'eval_runtime': 2.1139, 'eval_samples_per_second': 5.677, 'eval_steps_per_second': 1.419, 'epoch': 0.34}
                                                        34%|      | 1090/3250 [3:14:10<6:14:57, 10.42s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_10/tmp-checkpoint-1090
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1090 /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1090

I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1090
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1090
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1090/pytorch_model.bin
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1090/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1090/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1090/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.708, 'learning_rate': 7.47342755970336e-05, 'epoch': 0.34}
{'loss': 0.6599, 'learning_rate': 7.469223400890966e-05, 'epoch': 0.34}
{'loss': 0.6518, 'learning_rate': 7.465016931993069e-05, 'epoch': 0.34}
{'loss': 0.6498, 'learning_rate': 7.460808156945036e-05, 'epoch': 0.34}
{'loss': 0.6461, 'learning_rate': 7.456597079684397e-05, 'epoch': 0.34}
 34%|      | 1091/3250 [3:14:21<6:41:48, 11.17s/it]                                                        34%|      | 1091/3250 [3:14:21<6:41:48, 11.17s/it] 34%|      | 1092/3250 [3:14:31<6:33:06, 10.93s/it]                                                        34%|      | 1092/3250 [3:14:31<6:33:06, 10.93s/it] 34%|      | 1093/3250 [3:14:41<6:26:58, 10.76s/it]                                                        34%|      | 1093/3250 [3:14:41<6:26:58, 10.76s/it] 34%|      | 1094/3250 [3:14:52<6:22:41, 10.65s/it]                                                        34%|      | 1094/3250 [3:14:52<6:22:41, 10.65s/it] 34%|      | 1095/3250 [3:15:02<6:19:37, 10.57s/it]                                                        34%|      | 1095/3250 [3:15:02<6:19:37, 10.57s/it] 34%|      | 1096/3250 [3:15:13<6:17:31, 10.52s/it]                                  {'loss': 0.6726, 'learning_rate': 7.452383704150828e-05, 'epoch': 0.34}
{'loss': 0.6606, 'learning_rate': 7.44816803428616e-05, 'epoch': 0.34}
{'loss': 0.6826, 'learning_rate': 7.443950074034368e-05, 'epoch': 0.34}
{'loss': 1.1412, 'learning_rate': 7.43972982734157e-05, 'epoch': 0.34}
{'loss': 0.6663, 'learning_rate': 7.435507298156026e-05, 'epoch': 0.34}
                      34%|      | 1096/3250 [3:15:13<6:17:31, 10.52s/it] 34%|      | 1097/3250 [3:15:23<6:15:48, 10.47s/it]                                                        34%|      | 1097/3250 [3:15:23<6:15:48, 10.47s/it] 34%|      | 1098/3250 [3:15:33<6:14:43, 10.45s/it]                                                        34%|      | 1098/3250 [3:15:33<6:14:43, 10.45s/it] 34%|      | 1099/3250 [3:15:44<6:17:52, 10.54s/it]                                                        34%|      | 1099/3250 [3:15:44<6:17:52, 10.54s/it] 34%|      | 1100/3250 [3:15:54<6:16:21, 10.50s/it]                                                        34%|      | 1100/3250 [3:15:54<6:16:21, 10.50s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8033084273338318, 'eval_runtime': 2.675, 'eval_samples_per_second': 4.486, 'eval_steps_per_second': 1.121, 'epoch': 0.34}
                                                        34%|      | 1100/3250 [3:15:57<6:16:21, 10.50s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_10/tmp-checkpoint-1100
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1100I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1100

I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1100
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1100
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1100/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1100/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1100/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1100/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.6586, 'learning_rate': 7.431282490428129e-05, 'epoch': 0.34}
{'loss': 0.683, 'learning_rate': 7.427055408110403e-05, 'epoch': 0.34}
{'loss': 0.6844, 'learning_rate': 7.422826055157501e-05, 'epoch': 0.34}
{'loss': 0.6237, 'learning_rate': 7.4185944355262e-05, 'epoch': 0.34}
{'loss': 0.6555, 'learning_rate': 7.414360553175397e-05, 'epoch': 0.34}
 34%|      | 1101/3250 [3:16:08<6:48:33, 11.41s/it]                                                        34%|      | 1101/3250 [3:16:08<6:48:33, 11.41s/it] 34%|      | 1102/3250 [3:16:18<6:37:21, 11.10s/it]                                                        34%|      | 1102/3250 [3:16:18<6:37:21, 11.10s/it] 34%|      | 1103/3250 [3:16:29<6:29:12, 10.88s/it]                                                        34%|      | 1103/3250 [3:16:29<6:29:12, 10.88s/it] 34%|      | 1104/3250 [3:16:39<6:23:32, 10.72s/it]                                                        34%|      | 1104/3250 [3:16:39<6:23:32, 10.72s/it] 34%|      | 1105/3250 [3:16:49<6:19:30, 10.62s/it]                                                        34%|      | 1105/3250 [3:16:49<6:19:30, 10.62s/it] 34%|      | 1106/3250 [3:17:00<6:22:53, 10.72s/it]                                  {'loss': 0.7287, 'learning_rate': 7.41012441206611e-05, 'epoch': 0.34}
{'loss': 0.6698, 'learning_rate': 7.405886016161465e-05, 'epoch': 0.34}
{'loss': 0.6725, 'learning_rate': 7.401645369426697e-05, 'epoch': 0.34}
{'loss': 0.6208, 'learning_rate': 7.397402475829152e-05, 'epoch': 0.34}
{'loss': 0.6661, 'learning_rate': 7.393157339338276e-05, 'epoch': 0.34}
                      34%|      | 1106/3250 [3:17:00<6:22:53, 10.72s/it] 34%|      | 1107/3250 [3:17:11<6:19:19, 10.62s/it]                                                        34%|      | 1107/3250 [3:17:11<6:19:19, 10.62s/it] 34%|      | 1108/3250 [3:17:21<6:16:34, 10.55s/it]                                                        34%|      | 1108/3250 [3:17:21<6:16:34, 10.55s/it] 34%|      | 1109/3250 [3:17:32<6:14:29, 10.50s/it]                                                        34%|      | 1109/3250 [3:17:32<6:14:29, 10.50s/it] 34%|      | 1110/3250 [3:17:42<6:13:07, 10.46s/it]                                                        34%|      | 1110/3250 [3:17:42<6:13:07, 10.46s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8065986037254333, 'eval_runtime': 2.1635, 'eval_samples_per_second': 5.546, 'eval_steps_per_second': 1.387, 'epoch': 0.34}
                                                        34%|      | 1110/3250 [3:17:44<6:13:07, 10.46s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_10/tmp-checkpoint-1110
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1110
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1110
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1110
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1110
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1110/pytorch_model.bin
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1110/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1110/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1110/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.6834, 'learning_rate': 7.388909963925611e-05, 'epoch': 0.34}
{'loss': 0.6454, 'learning_rate': 7.384660353564794e-05, 'epoch': 0.34}
{'loss': 0.6457, 'learning_rate': 7.380408512231557e-05, 'epoch': 0.34}
{'loss': 0.6589, 'learning_rate': 7.376154443903713e-05, 'epoch': 0.34}
{'loss': 0.6687, 'learning_rate': 7.371898152561166e-05, 'epoch': 0.34}
 34%|      | 1111/3250 [3:17:55<6:40:43, 11.24s/it]                                                        34%|      | 1111/3250 [3:17:55<6:40:43, 11.24s/it] 34%|      | 1112/3250 [3:18:05<6:31:20, 10.98s/it]                                                        34%|      | 1112/3250 [3:18:05<6:31:20, 10.98s/it] 34%|      | 1113/3250 [3:18:16<6:24:31, 10.80s/it]                                                        34%|      | 1113/3250 [3:18:16<6:24:31, 10.80s/it] 34%|      | 1114/3250 [3:18:26<6:20:00, 10.67s/it]                                                        34%|      | 1114/3250 [3:18:26<6:20:00, 10.67s/it] 34%|      | 1115/3250 [3:18:37<6:21:30, 10.72s/it]                                                        34%|      | 1115/3250 [3:18:37<6:21:30, 10.72s/it] 34%|      | 1116/3250 [3:18:47<6:17:56, 10.63s/it]                                  {'loss': 0.6802, 'learning_rate': 7.367639642185891e-05, 'epoch': 0.34}
{'loss': 0.6721, 'learning_rate': 7.363378916761945e-05, 'epoch': 0.34}
{'loss': 0.6465, 'learning_rate': 7.359115980275455e-05, 'epoch': 0.34}
{'loss': 0.6849, 'learning_rate': 7.354850836714621e-05, 'epoch': 0.34}
{'loss': 0.662, 'learning_rate': 7.350583490069701e-05, 'epoch': 0.34}
                      34%|      | 1116/3250 [3:18:47<6:17:56, 10.63s/it] 34%|      | 1117/3250 [3:18:58<6:14:59, 10.55s/it]                                                        34%|      | 1117/3250 [3:18:58<6:14:59, 10.55s/it] 34%|      | 1118/3250 [3:19:08<6:13:10, 10.50s/it]                                                        34%|      | 1118/3250 [3:19:08<6:13:10, 10.50s/it] 34%|      | 1119/3250 [3:19:19<6:12:00, 10.47s/it]                                                        34%|      | 1119/3250 [3:19:19<6:12:00, 10.47s/it] 34%|      | 1120/3250 [3:19:29<6:10:47, 10.44s/it]                                                        34%|      | 1120/3250 [3:19:29<6:10:47, 10.44s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8099262118339539, 'eval_runtime': 2.1252, 'eval_samples_per_second': 5.647, 'eval_steps_per_second': 1.412, 'epoch': 0.34}
                                                        34%|      | 1120/3250 [3:19:31<6:10:47, 10.44s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_10/tmp-checkpoint-1120
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1120
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1120
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1120
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1120
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1120/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1120/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1120/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1120/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.6741, 'learning_rate': 7.346313944333016e-05, 'epoch': 0.34}
{'loss': 0.6679, 'learning_rate': 7.342042203498951e-05, 'epoch': 0.35}
{'loss': 0.6479, 'learning_rate': 7.337768271563935e-05, 'epoch': 0.35}
{'loss': 0.6345, 'learning_rate': 7.333492152526452e-05, 'epoch': 0.35}
{'loss': 0.6401, 'learning_rate': 7.329213850387031e-05, 'epoch': 0.35}
 34%|      | 1121/3250 [3:19:42<6:37:27, 11.20s/it]                                                        34%|      | 1121/3250 [3:19:42<6:37:27, 11.20s/it] 35%|      | 1122/3250 [3:19:52<6:28:18, 10.95s/it]                                                        35%|      | 1122/3250 [3:19:52<6:28:18, 10.95s/it] 35%|      | 1123/3250 [3:20:03<6:22:17, 10.78s/it]                                                        35%|      | 1123/3250 [3:20:03<6:22:17, 10.78s/it] 35%|      | 1124/3250 [3:20:13<6:18:00, 10.67s/it]                                                        35%|      | 1124/3250 [3:20:13<6:18:00, 10.67s/it] 35%|      | 1125/3250 [3:20:23<6:15:00, 10.59s/it]                                                        35%|      | 1125/3250 [3:20:24<6:15:00, 10.59s/it] 35%|      | 1126/3250 [3:20:34<6:13:19, 10.55s/it]                                  {'loss': 0.6792, 'learning_rate': 7.324933369148243e-05, 'epoch': 0.35}
{'loss': 0.6332, 'learning_rate': 7.3206507128147e-05, 'epoch': 0.35}
{'loss': 0.698, 'learning_rate': 7.316365885393048e-05, 'epoch': 0.35}
{'loss': 1.1605, 'learning_rate': 7.312078890891963e-05, 'epoch': 0.35}
{'loss': 0.6164, 'learning_rate': 7.307789733322146e-05, 'epoch': 0.35}
                      35%|      | 1126/3250 [3:20:34<6:13:19, 10.55s/it] 35%|      | 1127/3250 [3:20:44<6:11:26, 10.50s/it]                                                        35%|      | 1127/3250 [3:20:44<6:11:26, 10.50s/it] 35%|      | 1128/3250 [3:20:55<6:10:09, 10.47s/it]                                                        35%|      | 1128/3250 [3:20:55<6:10:09, 10.47s/it] 35%|      | 1129/3250 [3:21:05<6:09:09, 10.44s/it]                                                        35%|      | 1129/3250 [3:21:05<6:09:09, 10.44s/it] 35%|      | 1130/3250 [3:21:15<6:08:29, 10.43s/it]                                                        35%|      | 1130/3250 [3:21:15<6:08:29, 10.43s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8046997785568237, 'eval_runtime': 2.1289, 'eval_samples_per_second': 5.637, 'eval_steps_per_second': 1.409, 'epoch': 0.35}
                                                        35%|      | 1130/3250 [3:21:18<6:08:29, 10.43s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_10/tmp-checkpoint-1130
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1130
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1130
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1130
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1130
the pytorch model path isthe pytorch model path is the pytorch model path is/projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1130/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1130/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1130/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1130/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.6512, 'learning_rate': 7.303498416696328e-05, 'epoch': 0.35}
{'loss': 0.674, 'learning_rate': 7.299204945029254e-05, 'epoch': 0.35}
{'loss': 0.6882, 'learning_rate': 7.294909322337689e-05, 'epoch': 0.35}
{'loss': 0.6343, 'learning_rate': 7.29061155264041e-05, 'epoch': 0.35}
{'loss': 0.6671, 'learning_rate': 7.286311639958197e-05, 'epoch': 0.35}
 35%|      | 1131/3250 [3:21:28<6:35:23, 11.20s/it]                                                        35%|      | 1131/3250 [3:21:28<6:35:23, 11.20s/it] 35%|      | 1132/3250 [3:21:39<6:29:03, 11.02s/it]                                                        35%|      | 1132/3250 [3:21:39<6:29:03, 11.02s/it] 35%|      | 1133/3250 [3:21:49<6:21:57, 10.83s/it]                                                        35%|      | 1133/3250 [3:21:49<6:21:57, 10.83s/it] 35%|      | 1134/3250 [3:22:00<6:17:05, 10.69s/it]                                                        35%|      | 1134/3250 [3:22:00<6:17:05, 10.69s/it] 35%|      | 1135/3250 [3:22:10<6:13:25, 10.59s/it]                                                        35%|      | 1135/3250 [3:22:10<6:13:25, 10.59s/it] 35%|      | 1136/3250 [3:22:21<6:11:06, 10.53s/it]                                  {'loss': 0.7047, 'learning_rate': 7.282009588313845e-05, 'epoch': 0.35}
{'loss': 0.6787, 'learning_rate': 7.277705401732143e-05, 'epoch': 0.35}
{'loss': 0.6449, 'learning_rate': 7.273399084239878e-05, 'epoch': 0.35}
{'loss': 0.6141, 'learning_rate': 7.26909063986583e-05, 'epoch': 0.35}
{'loss': 0.6841, 'learning_rate': 7.264780072640774e-05, 'epoch': 0.35}
                      35%|      | 1136/3250 [3:22:21<6:11:06, 10.53s/it] 35%|      | 1137/3250 [3:22:31<6:13:59, 10.62s/it]                                                        35%|      | 1137/3250 [3:22:31<6:13:59, 10.62s/it] 35%|      | 1138/3250 [3:22:42<6:11:20, 10.55s/it]                                                        35%|      | 1138/3250 [3:22:42<6:11:20, 10.55s/it] 35%|      | 1139/3250 [3:22:52<6:09:34, 10.50s/it]                                                        35%|      | 1139/3250 [3:22:52<6:09:34, 10.50s/it] 35%|      | 1140/3250 [3:23:03<6:10:24, 10.53s/it]                                                        35%|      | 1140/3250 [3:23:03<6:10:24, 10.53s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.804985523223877, 'eval_runtime': 3.303, 'eval_samples_per_second': 3.633, 'eval_steps_per_second': 0.908, 'epoch': 0.35}
                                                        35%|      | 1140/3250 [3:23:06<6:10:24, 10.53s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_10/tmp-checkpoint-1140
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1140
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1140
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1140
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1140
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1140/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1140/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1140/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1140/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.6538, 'learning_rate': 7.260467386597466e-05, 'epoch': 0.35}
{'loss': 0.6634, 'learning_rate': 7.256152585770643e-05, 'epoch': 0.35}
{'loss': 0.6425, 'learning_rate': 7.251835674197029e-05, 'epoch': 0.35}
{'loss': 0.6512, 'learning_rate': 7.24751665591531e-05, 'epoch': 0.35}
{'loss': 0.6568, 'learning_rate': 7.243195534966152e-05, 'epoch': 0.35}
 35%|      | 1141/3250 [3:23:17<6:48:14, 11.61s/it]                                                        35%|      | 1141/3250 [3:23:17<6:48:14, 11.61s/it] 35%|      | 1142/3250 [3:23:27<6:34:56, 11.24s/it]                                                        35%|      | 1142/3250 [3:23:27<6:34:56, 11.24s/it] 35%|      | 1143/3250 [3:23:38<6:25:20, 10.97s/it]                                                        35%|      | 1143/3250 [3:23:38<6:25:20, 10.97s/it] 35%|      | 1144/3250 [3:23:48<6:18:49, 10.79s/it]                                                        35%|      | 1144/3250 [3:23:48<6:18:49, 10.79s/it] 35%|      | 1145/3250 [3:23:58<6:14:10, 10.67s/it]                                                        35%|      | 1145/3250 [3:23:58<6:14:10, 10.67s/it] 35%|      | 1146/3250 [3:24:09<6:10:57, 10.58s/it]                                  {'loss': 0.6767, 'learning_rate': 7.238872315392189e-05, 'epoch': 0.35}
{'loss': 0.6651, 'learning_rate': 7.234547001238012e-05, 'epoch': 0.35}
{'loss': 0.6599, 'learning_rate': 7.230219596550176e-05, 'epoch': 0.35}
{'loss': 0.6612, 'learning_rate': 7.22589010537719e-05, 'epoch': 0.35}
{'loss': 0.6661, 'learning_rate': 7.221558531769519e-05, 'epoch': 0.35}
                      35%|      | 1146/3250 [3:24:09<6:10:57, 10.58s/it] 35%|      | 1147/3250 [3:24:19<6:08:33, 10.52s/it]                                                        35%|      | 1147/3250 [3:24:19<6:08:33, 10.52s/it] 35%|      | 1148/3250 [3:24:30<6:11:19, 10.60s/it]                                                        35%|      | 1148/3250 [3:24:30<6:11:19, 10.60s/it] 35%|      | 1149/3250 [3:24:40<6:08:56, 10.54s/it]                                                        35%|      | 1149/3250 [3:24:40<6:08:56, 10.54s/it] 35%|      | 1150/3250 [3:24:51<6:07:00, 10.49s/it]                                                        35%|      | 1150/3250 [3:24:51<6:07:00, 10.49s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.806388795375824, 'eval_runtime': 2.1147, 'eval_samples_per_second': 5.674, 'eval_steps_per_second': 1.419, 'epoch': 0.35}
                                                        35%|      | 1150/3250 [3:24:53<6:07:00, 10.49s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_10/tmp-checkpoint-1150
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1150
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1150
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1150
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1150
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1150/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1150/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1150/pytorch_model.binthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1150/pytorch_model.bin

/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.632, 'learning_rate': 7.217224879779567e-05, 'epoch': 0.35}
{'loss': 0.691, 'learning_rate': 7.212889153461694e-05, 'epoch': 0.35}
{'loss': 0.6285, 'learning_rate': 7.20855135687219e-05, 'epoch': 0.35}
{'loss': 0.636, 'learning_rate': 7.204211494069292e-05, 'epoch': 0.36}
{'loss': 0.6369, 'learning_rate': 7.199869569113161e-05, 'epoch': 0.36}
 35%|      | 1151/3250 [3:25:04<6:33:33, 11.25s/it]                                                        35%|      | 1151/3250 [3:25:04<6:33:33, 11.25s/it] 35%|      | 1152/3250 [3:25:14<6:24:09, 10.99s/it]                                                        35%|      | 1152/3250 [3:25:14<6:24:09, 10.99s/it] 35%|      | 1153/3250 [3:25:24<6:17:30, 10.80s/it]                                                        35%|      | 1153/3250 [3:25:24<6:17:30, 10.80s/it] 36%|      | 1154/3250 [3:25:35<6:18:00, 10.82s/it]                                                        36%|      | 1154/3250 [3:25:35<6:18:00, 10.82s/it] 36%|      | 1155/3250 [3:25:46<6:13:44, 10.70s/it]                                                        36%|      | 1155/3250 [3:25:46<6:13:44, 10.70s/it] 36%|      | 1156/3250 [3:25:56<6:10:14, 10.61s/it]                                  {'loss': 0.6557, 'learning_rate': 7.195525586065892e-05, 'epoch': 0.36}
{'loss': 0.6372, 'learning_rate': 7.191179548991507e-05, 'epoch': 0.36}
{'loss': 0.6734, 'learning_rate': 7.186831461955943e-05, 'epoch': 0.36}
{'loss': 0.6464, 'learning_rate': 7.182481329027061e-05, 'epoch': 0.36}
{'loss': 1.1289, 'learning_rate': 7.178129154274636e-05, 'epoch': 0.36}
                      36%|      | 1156/3250 [3:25:56<6:10:14, 10.61s/it] 36%|      | 1157/3250 [3:26:06<6:07:41, 10.54s/it]                                                        36%|      | 1157/3250 [3:26:06<6:07:41, 10.54s/it] 36%|      | 1158/3250 [3:26:17<6:05:55, 10.49s/it]                                                        36%|      | 1158/3250 [3:26:17<6:05:55, 10.49s/it] 36%|      | 1159/3250 [3:26:27<6:04:32, 10.46s/it]                                                        36%|      | 1159/3250 [3:26:27<6:04:32, 10.46s/it] 36%|      | 1160/3250 [3:26:38<6:03:25, 10.43s/it]                                                        36%|      | 1160/3250 [3:26:38<6:03:25, 10.43s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8023635149002075, 'eval_runtime': 2.9161, 'eval_samples_per_second': 4.115, 'eval_steps_per_second': 1.029, 'epoch': 0.36}
                                                        36%|      | 1160/3250 [3:26:41<6:03:25, 10.43s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_10/tmp-checkpoint-1160
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1160
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1160
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1160
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1160
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1160/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1160/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1160/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1160/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.6332, 'learning_rate': 7.17377494177035e-05, 'epoch': 0.36}
{'loss': 0.6736, 'learning_rate': 7.169418695587791e-05, 'epoch': 0.36}
{'loss': 0.6652, 'learning_rate': 7.165060419802453e-05, 'epoch': 0.36}
{'loss': 0.6515, 'learning_rate': 7.160700118491728e-05, 'epoch': 0.36}
{'loss': 0.6246, 'learning_rate': 7.1563377957349e-05, 'epoch': 0.36}
 36%|      | 1161/3250 [3:26:51<6:38:31, 11.45s/it]                                                        36%|      | 1161/3250 [3:26:51<6:38:31, 11.45s/it] 36%|      | 1162/3250 [3:27:02<6:27:01, 11.12s/it]                                                        36%|      | 1162/3250 [3:27:02<6:27:01, 11.12s/it] 36%|      | 1163/3250 [3:27:12<6:18:52, 10.89s/it]                                                        36%|      | 1163/3250 [3:27:12<6:18:52, 10.89s/it] 36%|      | 1164/3250 [3:27:23<6:16:39, 10.83s/it]                                                        36%|      | 1164/3250 [3:27:23<6:16:39, 10.83s/it] 36%|      | 1165/3250 [3:27:33<6:11:28, 10.69s/it]                                                        36%|      | 1165/3250 [3:27:33<6:11:28, 10.69s/it] 36%|      | 1166/3250 [3:27:44<6:07:58, 10.59s/it]                                  {'loss': 0.6922, 'learning_rate': 7.15197345561315e-05, 'epoch': 0.36}
{'loss': 0.6923, 'learning_rate': 7.147607102209538e-05, 'epoch': 0.36}
{'loss': 0.635, 'learning_rate': 7.143238739609016e-05, 'epoch': 0.36}
{'loss': 0.6183, 'learning_rate': 7.13886837189841e-05, 'epoch': 0.36}
{'loss': 0.6594, 'learning_rate': 7.134496003166423e-05, 'epoch': 0.36}
                      36%|      | 1166/3250 [3:27:44<6:07:58, 10.59s/it] 36%|      | 1167/3250 [3:27:54<6:05:27, 10.53s/it]                                                        36%|      | 1167/3250 [3:27:54<6:05:27, 10.53s/it] 36%|      | 1168/3250 [3:28:04<6:03:44, 10.48s/it]                                                        36%|      | 1168/3250 [3:28:04<6:03:44, 10.48s/it] 36%|      | 1169/3250 [3:28:15<6:02:26, 10.45s/it]                                                        36%|      | 1169/3250 [3:28:15<6:02:26, 10.45s/it] 36%|      | 1170/3250 [3:28:25<6:01:35, 10.43s/it]                                                        36%|      | 1170/3250 [3:28:25<6:01:35, 10.43s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8023822903633118, 'eval_runtime': 2.4924, 'eval_samples_per_second': 4.815, 'eval_steps_per_second': 1.204, 'epoch': 0.36}
                                                        36%|      | 1170/3250 [3:28:28<6:01:35, 10.43s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_10/tmp-checkpoint-1170
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1170
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1170
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1170
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1170
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1170/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1170/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1170/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1170/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.6443, 'learning_rate': 7.130121637503632e-05, 'epoch': 0.36}
{'loss': 0.6621, 'learning_rate': 7.125745279002482e-05, 'epoch': 0.36}
{'loss': 0.6393, 'learning_rate': 7.121366931757281e-05, 'epoch': 0.36}
{'loss': 0.6465, 'learning_rate': 7.116986599864197e-05, 'epoch': 0.36}
{'loss': 0.6515, 'learning_rate': 7.112604287421256e-05, 'epoch': 0.36}
 36%|      | 1171/3250 [3:28:38<6:31:39, 11.30s/it]                                                        36%|      | 1171/3250 [3:28:38<6:31:39, 11.30s/it] 36%|      | 1172/3250 [3:28:49<6:21:37, 11.02s/it]                                                        36%|      | 1172/3250 [3:28:49<6:21:37, 11.02s/it] 36%|      | 1173/3250 [3:28:59<6:14:49, 10.83s/it]                                                        36%|      | 1173/3250 [3:28:59<6:14:49, 10.83s/it] 36%|      | 1174/3250 [3:29:10<6:09:45, 10.69s/it]                                                        36%|      | 1174/3250 [3:29:10<6:09:45, 10.69s/it] 36%|      | 1175/3250 [3:29:20<6:06:13, 10.59s/it]                                                        36%|      | 1175/3250 [3:29:20<6:06:13, 10.59s/it] 36%|      | 1176/3250 [3:29:30<6:03:46, 10.52s/it]                                  {'loss': 0.671, 'learning_rate': 7.108219998528337e-05, 'epoch': 0.36}
{'loss': 0.6544, 'learning_rate': 7.103833737287168e-05, 'epoch': 0.36}
{'loss': 0.6449, 'learning_rate': 7.099445507801323e-05, 'epoch': 0.36}
{'loss': 0.6606, 'learning_rate': 7.095055314176216e-05, 'epoch': 0.36}
{'loss': 0.6739, 'learning_rate': 7.090663160519095e-05, 'epoch': 0.36}
                      36%|      | 1176/3250 [3:29:30<6:03:46, 10.52s/it] 36%|      | 1177/3250 [3:29:41<6:02:09, 10.48s/it]                                                        36%|      | 1177/3250 [3:29:41<6:02:09, 10.48s/it] 36%|      | 1178/3250 [3:29:51<6:00:50, 10.45s/it]                                                        36%|      | 1178/3250 [3:29:51<6:00:50, 10.45s/it] 36%|      | 1179/3250 [3:30:01<5:59:57, 10.43s/it]                                                        36%|      | 1179/3250 [3:30:01<5:59:57, 10.43s/it] 36%|      | 1180/3250 [3:30:12<5:59:08, 10.41s/it]                                                        36%|      | 1180/3250 [3:30:12<5:59:08, 10.41s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8023719787597656, 'eval_runtime': 2.1202, 'eval_samples_per_second': 5.66, 'eval_steps_per_second': 1.415, 'epoch': 0.36}
                                                        36%|      | 1180/3250 [3:30:14<5:59:08, 10.41s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_10/tmp-checkpoint-1180
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1180
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1180
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1180
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1180
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1180/pytorch_model.bin
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1180/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1180/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1180/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.6064, 'learning_rate': 7.086269050939051e-05, 'epoch': 0.36}
{'loss': 0.7, 'learning_rate': 7.081872989546998e-05, 'epoch': 0.36}
{'loss': 0.6466, 'learning_rate': 7.077474980455678e-05, 'epoch': 0.36}
{'loss': 0.6343, 'learning_rate': 7.073075027779651e-05, 'epoch': 0.36}
{'loss': 0.6197, 'learning_rate': 7.068673135635302e-05, 'epoch': 0.36}
 36%|      | 1181/3250 [3:30:25<6:30:11, 11.32s/it]                                                        36%|      | 1181/3250 [3:30:25<6:30:11, 11.32s/it] 36%|      | 1182/3250 [3:30:36<6:20:56, 11.05s/it]                                                        36%|      | 1182/3250 [3:30:36<6:20:56, 11.05s/it] 36%|      | 1183/3250 [3:30:46<6:14:32, 10.87s/it]                                                        36%|      | 1183/3250 [3:30:46<6:14:32, 10.87s/it] 36%|      | 1184/3250 [3:30:57<6:09:56, 10.74s/it]                                                        36%|      | 1184/3250 [3:30:57<6:09:56, 10.74s/it] 36%|      | 1185/3250 [3:31:07<6:06:40, 10.65s/it]                                                        36%|      | 1185/3250 [3:31:07<6:06:40, 10.65s/it] 36%|      | 1186/3250 [3:31:17<6:04:14, 10.59s/it]                                  {'loss': 0.6309, 'learning_rate': 7.06426930814083e-05, 'epoch': 0.36}
{'loss': 0.6562, 'learning_rate': 7.059863549416237e-05, 'epoch': 0.37}
{'loss': 0.638, 'learning_rate': 7.05545586358334e-05, 'epoch': 0.37}
{'loss': 0.6666, 'learning_rate': 7.051046254765755e-05, 'epoch': 0.37}
{'loss': 1.1291, 'learning_rate': 7.046634727088898e-05, 'epoch': 0.37}
                      36%|      | 1186/3250 [3:31:17<6:04:14, 10.59s/it] 37%|      | 1187/3250 [3:31:28<6:02:39, 10.55s/it]                                                        37%|      | 1187/3250 [3:31:28<6:02:39, 10.55s/it] 37%|      | 1188/3250 [3:31:38<6:01:19, 10.51s/it]                                                        37%|      | 1188/3250 [3:31:38<6:01:19, 10.51s/it] 37%|      | 1189/3250 [3:31:49<6:00:20, 10.49s/it]                                                        37%|      | 1189/3250 [3:31:49<6:00:20, 10.49s/it] 37%|      | 1190/3250 [3:31:59<5:59:07, 10.46s/it]                                                        37%|      | 1190/3250 [3:31:59<5:59:07, 10.46s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.7990702986717224, 'eval_runtime': 2.1124, 'eval_samples_per_second': 5.681, 'eval_steps_per_second': 1.42, 'epoch': 0.37}
                                                        37%|      | 1190/3250 [3:32:01<5:59:07, 10.46s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_10/tmp-checkpoint-1190
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  I AM HERE AND I AM SAVING THE MODEL
/projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1190the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1190

I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1190
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1190
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1190/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1190/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1190/pytorch_model.binthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1190/pytorch_model.bin

/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.6401, 'learning_rate': 7.042221284679982e-05, 'epoch': 0.37}
{'loss': 0.6503, 'learning_rate': 7.037805931668005e-05, 'epoch': 0.37}
{'loss': 0.6586, 'learning_rate': 7.03338867218376e-05, 'epoch': 0.37}
{'loss': 0.6549, 'learning_rate': 7.02896951035982e-05, 'epoch': 0.37}
{'loss': 0.6003, 'learning_rate': 7.02454845033054e-05, 'epoch': 0.37}
 37%|      | 1191/3250 [3:32:12<6:24:47, 11.21s/it]                                                        37%|      | 1191/3250 [3:32:12<6:24:47, 11.21s/it] 37%|      | 1192/3250 [3:32:22<6:16:11, 10.97s/it]                                                        37%|      | 1192/3250 [3:32:22<6:16:11, 10.97s/it] 37%|      | 1193/3250 [3:32:33<6:10:23, 10.80s/it]                                                        37%|      | 1193/3250 [3:32:33<6:10:23, 10.80s/it] 37%|      | 1194/3250 [3:32:43<6:06:22, 10.69s/it]                                                        37%|      | 1194/3250 [3:32:43<6:06:22, 10.69s/it] 37%|      | 1195/3250 [3:32:54<6:03:13, 10.60s/it]                                                        37%|      | 1195/3250 [3:32:54<6:03:13, 10.60s/it] 37%|      | 1196/3250 [3:33:04<6:01:01, 10.55s/it]                                  {'loss': 0.6377, 'learning_rate': 7.020125496232044e-05, 'epoch': 0.37}
{'loss': 0.7016, 'learning_rate': 7.015700652202237e-05, 'epoch': 0.37}
{'loss': 0.6474, 'learning_rate': 7.01127392238079e-05, 'epoch': 0.37}
{'loss': 0.6531, 'learning_rate': 7.006845310909131e-05, 'epoch': 0.37}
{'loss': 0.6365, 'learning_rate': 7.002414821930458e-05, 'epoch': 0.37}
                      37%|      | 1196/3250 [3:33:04<6:01:01, 10.55s/it] 37%|      | 1197/3250 [3:33:15<6:01:42, 10.57s/it]                                                        37%|      | 1197/3250 [3:33:15<6:01:42, 10.57s/it] 37%|      | 1198/3250 [3:33:25<5:59:52, 10.52s/it]                                                        37%|      | 1198/3250 [3:33:25<5:59:52, 10.52s/it] 37%|      | 1199/3250 [3:33:36<5:58:25, 10.49s/it]                                                        37%|      | 1199/3250 [3:33:36<5:58:25, 10.49s/it] 37%|      | 1200/3250 [3:33:46<5:57:20, 10.46s/it]                                                        37%|      | 1200/3250 [3:33:46<5:57:20, 10.46s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.7993177771568298, 'eval_runtime': 2.364, 'eval_samples_per_second': 5.076, 'eval_steps_per_second': 1.269, 'epoch': 0.37}
                                                        37%|      | 1200/3250 [3:33:48<5:57:20, 10.46s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_10/tmp-checkpoint-1200
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1200
/projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1200
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1200
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1200
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1200/pytorch_model.bin
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1200/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1200/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1200/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.6252, 'learning_rate': 6.99798245958972e-05, 'epoch': 0.37}
{'loss': 0.6595, 'learning_rate': 6.993548228033618e-05, 'epoch': 0.37}
{'loss': 0.6327, 'learning_rate': 6.989112131410607e-05, 'epoch': 0.37}
{'loss': 0.6437, 'learning_rate': 6.984674173870882e-05, 'epoch': 0.37}
{'loss': 0.6338, 'learning_rate': 6.98023435956638e-05, 'epoch': 0.37}
 37%|      | 1201/3250 [3:33:59<6:26:00, 11.30s/it]                                                        37%|      | 1201/3250 [3:33:59<6:26:00, 11.30s/it] 37%|      | 1202/3250 [3:34:10<6:16:37, 11.03s/it]                                                        37%|      | 1202/3250 [3:34:10<6:16:37, 11.03s/it] 37%|      | 1203/3250 [3:34:20<6:10:07, 10.85s/it]                                                        37%|      | 1203/3250 [3:34:20<6:10:07, 10.85s/it] 37%|      | 1204/3250 [3:34:30<6:05:28, 10.72s/it]                                                        37%|      | 1204/3250 [3:34:30<6:05:28, 10.72s/it] 37%|      | 1205/3250 [3:34:41<6:02:09, 10.63s/it]                                                        37%|      | 1205/3250 [3:34:41<6:02:09, 10.63s/it] 37%|      | 1206/3250 [3:34:51<5:59:48, 10.56s/it]                                  {'loss': 0.6538, 'learning_rate': 6.975792692650777e-05, 'epoch': 0.37}
{'loss': 0.6587, 'learning_rate': 6.971349177279481e-05, 'epoch': 0.37}
{'loss': 0.6517, 'learning_rate': 6.966903817609629e-05, 'epoch': 0.37}
{'loss': 0.629, 'learning_rate': 6.962456617800081e-05, 'epoch': 0.37}
{'loss': 0.6678, 'learning_rate': 6.958007582011426e-05, 'epoch': 0.37}
                      37%|      | 1206/3250 [3:34:51<5:59:48, 10.56s/it] 37%|      | 1207/3250 [3:35:02<5:58:05, 10.52s/it]                                                        37%|      | 1207/3250 [3:35:02<5:58:05, 10.52s/it] 37%|      | 1208/3250 [3:35:12<5:56:42, 10.48s/it]                                                        37%|      | 1208/3250 [3:35:12<5:56:42, 10.48s/it] 37%|      | 1209/3250 [3:35:23<5:55:50, 10.46s/it]                                                        37%|      | 1209/3250 [3:35:23<5:55:50, 10.46s/it] 37%|      | 1210/3250 [3:35:33<5:55:15, 10.45s/it]                                                        37%|      | 1210/3250 [3:35:33<5:55:15, 10.45s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.803432285785675, 'eval_runtime': 2.1141, 'eval_samples_per_second': 5.676, 'eval_steps_per_second': 1.419, 'epoch': 0.37}
                                                        37%|      | 1210/3250 [3:35:35<5:55:15, 10.45s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_10/tmp-checkpoint-1210
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1210
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1210
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1210
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1210
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1210/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1210/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1210/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1210/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.6336, 'learning_rate': 6.95355671440596e-05, 'epoch': 0.37}
{'loss': 0.6566, 'learning_rate': 6.949104019147703e-05, 'epoch': 0.37}
{'loss': 0.6442, 'learning_rate': 6.94464950040238e-05, 'epoch': 0.37}
{'loss': 0.6392, 'learning_rate': 6.940193162337421e-05, 'epoch': 0.37}
{'loss': 0.6125, 'learning_rate': 6.935735009121958e-05, 'epoch': 0.37}
 37%|      | 1211/3250 [3:35:46<6:20:51, 11.21s/it]                                                        37%|      | 1211/3250 [3:35:46<6:20:51, 11.21s/it] 37%|      | 1212/3250 [3:35:56<6:12:35, 10.97s/it]                                                        37%|      | 1212/3250 [3:35:56<6:12:35, 10.97s/it] 37%|      | 1213/3250 [3:36:07<6:10:38, 10.92s/it]                                                        37%|      | 1213/3250 [3:36:07<6:10:38, 10.92s/it] 37%|      | 1214/3250 [3:36:18<6:05:23, 10.77s/it]                                                        37%|      | 1214/3250 [3:36:18<6:05:23, 10.77s/it] 37%|      | 1215/3250 [3:36:28<6:01:38, 10.66s/it]                                                        37%|      | 1215/3250 [3:36:28<6:01:38, 10.66s/it] 37%|      | 1216/3250 [3:36:38<5:59:03, 10.59s/it]                                  {'loss': 0.6226, 'learning_rate': 6.931275044926828e-05, 'epoch': 0.37}
{'loss': 0.6536, 'learning_rate': 6.926813273924553e-05, 'epoch': 0.37}
{'loss': 0.6155, 'learning_rate': 6.922349700289348e-05, 'epoch': 0.37}
{'loss': 0.6707, 'learning_rate': 6.91788432819712e-05, 'epoch': 0.38}
{'loss': 1.1384, 'learning_rate': 6.91341716182545e-05, 'epoch': 0.38}
                      37%|      | 1216/3250 [3:36:38<5:59:03, 10.59s/it] 37%|      | 1217/3250 [3:36:49<5:57:07, 10.54s/it]                                                        37%|      | 1217/3250 [3:36:49<5:57:07, 10.54s/it] 37%|      | 1218/3250 [3:36:59<5:55:38, 10.50s/it]                                                        37%|      | 1218/3250 [3:36:59<5:55:38, 10.50s/it] 38%|      | 1219/3250 [3:37:10<5:54:37, 10.48s/it]                                                        38%|      | 1219/3250 [3:37:10<5:54:37, 10.48s/it] 38%|      | 1220/3250 [3:37:20<5:53:36, 10.45s/it]                                                        38%|      | 1220/3250 [3:37:20<5:53:36, 10.45s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.7979415655136108, 'eval_runtime': 2.1087, 'eval_samples_per_second': 5.691, 'eval_steps_per_second': 1.423, 'epoch': 0.38}
                                                        38%|      | 1220/3250 [3:37:22<5:53:36, 10.45s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_10/tmp-checkpoint-1220
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1220
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1220
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1220
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1220
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1220/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1220/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1220/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1220/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.6173, 'learning_rate': 6.908948205353603e-05, 'epoch': 0.38}
{'loss': 0.6334, 'learning_rate': 6.904477462962521e-05, 'epoch': 0.38}
{'loss': 0.6607, 'learning_rate': 6.900004938834809e-05, 'epoch': 0.38}
{'loss': 0.6592, 'learning_rate': 6.895530637154745e-05, 'epoch': 0.38}
{'loss': 0.6255, 'learning_rate': 6.891054562108273e-05, 'epoch': 0.38}
 38%|      | 1221/3250 [3:37:33<6:18:56, 11.21s/it]                                                        38%|      | 1221/3250 [3:37:33<6:18:56, 11.21s/it] 38%|      | 1222/3250 [3:37:43<6:10:39, 10.97s/it]                                                        38%|      | 1222/3250 [3:37:43<6:10:39, 10.97s/it] 38%|      | 1223/3250 [3:37:54<6:04:46, 10.80s/it]                                                        38%|      | 1223/3250 [3:37:54<6:04:46, 10.80s/it] 38%|      | 1224/3250 [3:38:04<6:00:51, 10.69s/it]                                                        38%|      | 1224/3250 [3:38:04<6:00:51, 10.69s/it] 38%|      | 1225/3250 [3:38:15<5:57:41, 10.60s/it]                                                        38%|      | 1225/3250 [3:38:15<5:57:41, 10.60s/it] 38%|      | 1226/3250 [3:38:25<5:55:23, 10.54s/it]                                  {'loss': 0.6276, 'learning_rate': 6.886576717882982e-05, 'epoch': 0.38}
{'loss': 0.6872, 'learning_rate': 6.882097108668132e-05, 'epoch': 0.38}
{'loss': 0.6639, 'learning_rate': 6.877615738654628e-05, 'epoch': 0.38}
{'loss': 0.6416, 'learning_rate': 6.87313261203502e-05, 'epoch': 0.38}
{'loss': 0.5971, 'learning_rate': 6.868647733003502e-05, 'epoch': 0.38}
                      38%|      | 1226/3250 [3:38:25<5:55:23, 10.54s/it] 38%|      | 1227/3250 [3:38:35<5:53:57, 10.50s/it]                                                        38%|      | 1227/3250 [3:38:35<5:53:57, 10.50s/it] 38%|      | 1228/3250 [3:38:46<5:52:47, 10.47s/it]                                                        38%|      | 1228/3250 [3:38:46<5:52:47, 10.47s/it] 38%|      | 1229/3250 [3:38:56<5:52:06, 10.45s/it]                                                        38%|      | 1229/3250 [3:38:56<5:52:06, 10.45s/it] 38%|      | 1230/3250 [3:39:07<5:55:13, 10.55s/it]                                                        38%|      | 1230/3250 [3:39:07<5:55:13, 10.55s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.7988281846046448, 'eval_runtime': 2.344, 'eval_samples_per_second': 5.119, 'eval_steps_per_second': 1.28, 'epoch': 0.38}
                                                        38%|      | 1230/3250 [3:39:09<5:55:13, 10.55s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_10/tmp-checkpoint-1230
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1230I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1230

I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1230
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1230
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1230/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1230/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1230/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1230/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.6629, 'learning_rate': 6.864161105755915e-05, 'epoch': 0.38}
{'loss': 0.6383, 'learning_rate': 6.859672734489723e-05, 'epoch': 0.38}
{'loss': 0.6442, 'learning_rate': 6.855182623404033e-05, 'epoch': 0.38}
{'loss': 0.6255, 'learning_rate': 6.850690776699573e-05, 'epoch': 0.38}
{'loss': 0.6396, 'learning_rate': 6.846197198578695e-05, 'epoch': 0.38}
 38%|      | 1231/3250 [3:39:20<6:23:20, 11.39s/it]                                                        38%|      | 1231/3250 [3:39:20<6:23:20, 11.39s/it] 38%|      | 1232/3250 [3:39:31<6:13:06, 11.09s/it]                                                        38%|      | 1232/3250 [3:39:31<6:13:06, 11.09s/it] 38%|      | 1233/3250 [3:39:41<6:06:00, 10.89s/it]                                                        38%|      | 1233/3250 [3:39:41<6:06:00, 10.89s/it] 38%|      | 1234/3250 [3:39:52<6:00:57, 10.74s/it]                                                        38%|      | 1234/3250 [3:39:52<6:00:57, 10.74s/it] 38%|      | 1235/3250 [3:40:02<5:57:34, 10.65s/it]                                                        38%|      | 1235/3250 [3:40:02<5:57:34, 10.65s/it] 38%|      | 1236/3250 [3:40:12<5:54:57, 10.57s/it]                                  {'loss': 0.6264, 'learning_rate': 6.841701893245374e-05, 'epoch': 0.38}
{'loss': 0.6571, 'learning_rate': 6.8372048649052e-05, 'epoch': 0.38}
{'loss': 0.6483, 'learning_rate': 6.832706117765375e-05, 'epoch': 0.38}
{'loss': 0.6495, 'learning_rate': 6.828205656034706e-05, 'epoch': 0.38}
{'loss': 0.6394, 'learning_rate': 6.823703483923607e-05, 'epoch': 0.38}
                      38%|      | 1236/3250 [3:40:12<5:54:57, 10.57s/it] 38%|      | 1237/3250 [3:40:23<5:53:12, 10.53s/it]                                                        38%|      | 1237/3250 [3:40:23<5:53:12, 10.53s/it] 38%|      | 1238/3250 [3:40:33<5:51:50, 10.49s/it]                                                        38%|      | 1238/3250 [3:40:33<5:51:50, 10.49s/it] 38%|      | 1239/3250 [3:40:44<5:50:53, 10.47s/it]                                                        38%|      | 1239/3250 [3:40:44<5:50:53, 10.47s/it] 38%|      | 1240/3250 [3:40:54<5:50:12, 10.45s/it]                                                        38%|      | 1240/3250 [3:40:54<5:50:12, 10.45s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.799858033657074, 'eval_runtime': 2.1219, 'eval_samples_per_second': 5.655, 'eval_steps_per_second': 1.414, 'epoch': 0.38}
                                                        38%|      | 1240/3250 [3:40:56<5:50:12, 10.45s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_10/tmp-checkpoint-1240
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1240I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1240

I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1240
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1240
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1240/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1240/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1240/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1240/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.6422, 'learning_rate': 6.819199605644094e-05, 'epoch': 0.38}
{'loss': 0.6123, 'learning_rate': 6.814694025409773e-05, 'epoch': 0.38}
{'loss': 0.6719, 'learning_rate': 6.810186747435849e-05, 'epoch': 0.38}
{'loss': 0.6133, 'learning_rate': 6.805677775939111e-05, 'epoch': 0.38}
{'loss': 0.6124, 'learning_rate': 6.801167115137934e-05, 'epoch': 0.38}
 38%|      | 1241/3250 [3:41:07<6:15:31, 11.22s/it]                                                        38%|      | 1241/3250 [3:41:07<6:15:31, 11.22s/it] 38%|      | 1242/3250 [3:41:17<6:07:10, 10.97s/it]                                                        38%|      | 1242/3250 [3:41:17<6:07:10, 10.97s/it] 38%|      | 1243/3250 [3:41:28<6:01:07, 10.80s/it]                                                        38%|      | 1243/3250 [3:41:28<6:01:07, 10.80s/it] 38%|      | 1244/3250 [3:41:38<5:57:00, 10.68s/it]                                                        38%|      | 1244/3250 [3:41:38<5:57:00, 10.68s/it] 38%|      | 1245/3250 [3:41:49<5:54:03, 10.60s/it]                                                        38%|      | 1245/3250 [3:41:49<5:54:03, 10.60s/it] 38%|      | 1246/3250 [3:41:59<5:54:30, 10.61s/it]                                  {'loss': 0.6211, 'learning_rate': 6.796654769252274e-05, 'epoch': 0.38}
{'loss': 0.6314, 'learning_rate': 6.792140742503661e-05, 'epoch': 0.38}
{'loss': 0.6232, 'learning_rate': 6.7876250391152e-05, 'epoch': 0.38}
{'loss': 0.6595, 'learning_rate': 6.783107663311565e-05, 'epoch': 0.38}
{'loss': 0.6271, 'learning_rate': 6.778588619318993e-05, 'epoch': 0.38}
                      38%|      | 1246/3250 [3:41:59<5:54:30, 10.61s/it] 38%|      | 1247/3250 [3:42:10<5:52:18, 10.55s/it]                                                        38%|      | 1247/3250 [3:42:10<5:52:18, 10.55s/it] 38%|      | 1248/3250 [3:42:20<5:50:38, 10.51s/it]                                                        38%|      | 1248/3250 [3:42:20<5:50:38, 10.51s/it] 38%|      | 1249/3250 [3:42:31<5:49:33, 10.48s/it]                                                        38%|      | 1249/3250 [3:42:31<5:49:33, 10.48s/it] 38%|      | 1250/3250 [3:42:41<5:48:37, 10.46s/it]                                                        38%|      | 1250/3250 [3:42:41<5:48:37, 10.46s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.7988429069519043, 'eval_runtime': 2.1058, 'eval_samples_per_second': 5.699, 'eval_steps_per_second': 1.425, 'epoch': 0.38}
                                                        38%|      | 1250/3250 [3:42:43<5:48:37, 10.46s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_10/tmp-checkpoint-1250
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1250
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1250
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1250
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1250
the pytorch model path is the pytorch model path is/projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1250/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1250/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1250/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1250/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 1.1264, 'learning_rate': 6.77406791136528e-05, 'epoch': 0.38}
{'loss': 0.6293, 'learning_rate': 6.769545543679785e-05, 'epoch': 0.39}
{'loss': 0.6542, 'learning_rate': 6.76502152049341e-05, 'epoch': 0.39}
{'loss': 0.6444, 'learning_rate': 6.760495846038614e-05, 'epoch': 0.39}
{'loss': 0.6345, 'learning_rate': 6.755968524549402e-05, 'epoch': 0.39}
 38%|      | 1251/3250 [3:42:54<6:14:55, 11.25s/it]                                                        38%|      | 1251/3250 [3:42:54<6:14:55, 11.25s/it] 39%|      | 1252/3250 [3:43:05<6:06:39, 11.01s/it]                                                        39%|      | 1252/3250 [3:43:05<6:06:39, 11.01s/it] 39%|      | 1253/3250 [3:43:15<6:00:21, 10.83s/it]                                                        39%|      | 1253/3250 [3:43:15<6:00:21, 10.83s/it] 39%|      | 1254/3250 [3:43:25<5:56:01, 10.70s/it]                                                        39%|      | 1254/3250 [3:43:25<5:56:01, 10.70s/it] 39%|      | 1255/3250 [3:43:36<5:52:59, 10.62s/it]                                                        39%|      | 1255/3250 [3:43:36<5:52:59, 10.62s/it] 39%|      | 1256/3250 [3:43:46<5:50:57, 10.56s/it]                                  {'loss': 0.6126, 'learning_rate': 6.75143956026131e-05, 'epoch': 0.39}
{'loss': 0.6787, 'learning_rate': 6.746908957411421e-05, 'epoch': 0.39}
{'loss': 0.6769, 'learning_rate': 6.742376720238346e-05, 'epoch': 0.39}
{'loss': 0.615, 'learning_rate': 6.737842852982225e-05, 'epoch': 0.39}
{'loss': 0.6069, 'learning_rate': 6.733307359884725e-05, 'epoch': 0.39}
                      39%|      | 1256/3250 [3:43:46<5:50:57, 10.56s/it] 39%|      | 1257/3250 [3:43:57<5:49:49, 10.53s/it]                                                        39%|      | 1257/3250 [3:43:57<5:49:49, 10.53s/it] 39%|      | 1258/3250 [3:44:07<5:48:25, 10.49s/it]                                                        39%|      | 1258/3250 [3:44:07<5:48:25, 10.49s/it] 39%|      | 1259/3250 [3:44:17<5:47:24, 10.47s/it]                                                        39%|      | 1259/3250 [3:44:17<5:47:24, 10.47s/it] 39%|      | 1260/3250 [3:44:28<5:46:34, 10.45s/it]                                                        39%|      | 1260/3250 [3:44:28<5:46:34, 10.45s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.7986697554588318, 'eval_runtime': 2.112, 'eval_samples_per_second': 5.682, 'eval_steps_per_second': 1.42, 'epoch': 0.39}
                                                        39%|      | 1260/3250 [3:44:30<5:46:34, 10.45s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_10/tmp-checkpoint-1260
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1260
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1260
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1260
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1260
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1260/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1260/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1260/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1260/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.641, 'learning_rate': 6.728770245189032e-05, 'epoch': 0.39}
{'loss': 0.6229, 'learning_rate': 6.724231513139852e-05, 'epoch': 0.39}
{'loss': 0.6421, 'learning_rate': 6.719691167983401e-05, 'epoch': 0.39}
{'loss': 0.6341, 'learning_rate': 6.715149213967407e-05, 'epoch': 0.39}
{'loss': 0.6389, 'learning_rate': 6.7106056553411e-05, 'epoch': 0.39}
 39%|      | 1261/3250 [3:44:41<6:11:29, 11.21s/it]                                                        39%|      | 1261/3250 [3:44:41<6:11:29, 11.21s/it] 39%|      | 1262/3250 [3:44:51<6:03:21, 10.97s/it]                                                        39%|      | 1262/3250 [3:44:51<6:03:21, 10.97s/it] 39%|      | 1263/3250 [3:45:02<6:04:18, 11.00s/it]                                                        39%|      | 1263/3250 [3:45:02<6:04:18, 11.00s/it] 39%|      | 1264/3250 [3:45:13<5:58:18, 10.83s/it]                                                        39%|      | 1264/3250 [3:45:13<5:58:18, 10.83s/it] 39%|      | 1265/3250 [3:45:23<5:53:50, 10.70s/it]                                                        39%|      | 1265/3250 [3:45:23<5:53:50, 10.70s/it] 39%|      | 1266/3250 [3:45:34<5:51:51, 10.64s/it]                                  {'loss': 0.637, 'learning_rate': 6.706060496355212e-05, 'epoch': 0.39}
{'loss': 0.6548, 'learning_rate': 6.701513741261976e-05, 'epoch': 0.39}
{'loss': 0.6371, 'learning_rate': 6.696965394315114e-05, 'epoch': 0.39}
{'loss': 0.6412, 'learning_rate': 6.692415459769836e-05, 'epoch': 0.39}
{'loss': 0.6519, 'learning_rate': 6.687863941882841e-05, 'epoch': 0.39}
                      39%|      | 1266/3250 [3:45:34<5:51:51, 10.64s/it] 39%|      | 1267/3250 [3:45:44<5:49:31, 10.58s/it]                                                        39%|      | 1267/3250 [3:45:44<5:49:31, 10.58s/it] 39%|      | 1268/3250 [3:45:54<5:47:37, 10.52s/it]                                                        39%|      | 1268/3250 [3:45:54<5:47:37, 10.52s/it] 39%|      | 1269/3250 [3:46:05<5:46:13, 10.49s/it]                                                        39%|      | 1269/3250 [3:46:05<5:46:13, 10.49s/it] 39%|      | 1270/3250 [3:46:15<5:45:28, 10.47s/it]                                                        39%|      | 1270/3250 [3:46:15<5:45:28, 10.47s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.7985947132110596, 'eval_runtime': 2.2485, 'eval_samples_per_second': 5.337, 'eval_steps_per_second': 1.334, 'epoch': 0.39}
                                                        39%|      | 1270/3250 [3:46:18<5:45:28, 10.47s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_10/tmp-checkpoint-1270
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1270I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1270

I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1270
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1270
the pytorch model path isthe pytorch model path is  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1270/pytorch_model.bin/projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1270/pytorch_model.bin

the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1270/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1270/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.653, 'learning_rate': 6.683310844912311e-05, 'epoch': 0.39}
{'loss': 0.5938, 'learning_rate': 6.6787561731179e-05, 'epoch': 0.39}
{'loss': 0.6785, 'learning_rate': 6.674199930760738e-05, 'epoch': 0.39}
{'loss': 0.6311, 'learning_rate': 6.669642122103423e-05, 'epoch': 0.39}
{'loss': 0.6326, 'learning_rate': 6.665082751410023e-05, 'epoch': 0.39}
 39%|      | 1271/3250 [3:46:28<6:11:06, 11.25s/it]                                                        39%|      | 1271/3250 [3:46:28<6:11:06, 11.25s/it] 39%|      | 1272/3250 [3:46:39<6:02:20, 10.99s/it]                                                        39%|      | 1272/3250 [3:46:39<6:02:20, 10.99s/it] 39%|      | 1273/3250 [3:46:49<5:56:20, 10.81s/it]                                                        39%|      | 1273/3250 [3:46:49<5:56:20, 10.81s/it] 39%|      | 1274/3250 [3:47:00<5:51:58, 10.69s/it]                                                        39%|      | 1274/3250 [3:47:00<5:51:58, 10.69s/it] 39%|      | 1275/3250 [3:47:10<5:48:57, 10.60s/it]                                                        39%|      | 1275/3250 [3:47:10<5:48:57, 10.60s/it] 39%|      | 1276/3250 [3:47:20<5:46:53, 10.54s/it]                                  {'loss': 0.6103, 'learning_rate': 6.66052182294606e-05, 'epoch': 0.39}
{'loss': 0.614, 'learning_rate': 6.655959340978519e-05, 'epoch': 0.39}
{'loss': 0.6363, 'learning_rate': 6.651395309775837e-05, 'epoch': 0.39}
{'loss': 0.6281, 'learning_rate': 6.646829733607896e-05, 'epoch': 0.39}
{'loss': 0.6421, 'learning_rate': 6.642262616746034e-05, 'epoch': 0.39}
                      39%|      | 1276/3250 [3:47:20<5:46:53, 10.54s/it] 39%|      | 1277/3250 [3:47:31<5:45:19, 10.50s/it]                                                        39%|      | 1277/3250 [3:47:31<5:45:19, 10.50s/it] 39%|      | 1278/3250 [3:47:41<5:44:12, 10.47s/it]                                                        39%|      | 1278/3250 [3:47:41<5:44:12, 10.47s/it] 39%|      | 1279/3250 [3:47:52<5:46:20, 10.54s/it]                                                        39%|      | 1279/3250 [3:47:52<5:46:20, 10.54s/it] 39%|      | 1280/3250 [3:48:02<5:44:49, 10.50s/it]                                                        39%|      | 1280/3250 [3:48:02<5:44:49, 10.50s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.795595109462738, 'eval_runtime': 2.1149, 'eval_samples_per_second': 5.674, 'eval_steps_per_second': 1.418, 'epoch': 0.39}
                                                        39%|      | 1280/3250 [3:48:04<5:44:49, 10.50s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_10/tmp-checkpoint-1280
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1280
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1280
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1280
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1280
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1280/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1280/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1280/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1280/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 1.1101, 'learning_rate': 6.637693963463018e-05, 'epoch': 0.39}
{'loss': 0.6091, 'learning_rate': 6.633123778033061e-05, 'epoch': 0.39}
{'loss': 0.6481, 'learning_rate': 6.628552064731807e-05, 'epoch': 0.39}
{'loss': 0.6523, 'learning_rate': 6.623978827836327e-05, 'epoch': 0.4}
{'loss': 0.6486, 'learning_rate': 6.61940407162512e-05, 'epoch': 0.4}
 39%|      | 1281/3250 [3:48:15<6:08:49, 11.24s/it]                                                        39%|      | 1281/3250 [3:48:15<6:08:49, 11.24s/it] 39%|      | 1282/3250 [3:48:26<6:00:00, 10.98s/it]                                                        39%|      | 1282/3250 [3:48:26<6:00:00, 10.98s/it] 39%|      | 1283/3250 [3:48:36<5:53:52, 10.79s/it]                                                        39%|      | 1283/3250 [3:48:36<5:53:52, 10.79s/it] 40%|      | 1284/3250 [3:48:46<5:49:26, 10.66s/it]                                                        40%|      | 1284/3250 [3:48:46<5:49:26, 10.66s/it] 40%|      | 1285/3250 [3:48:57<5:46:24, 10.58s/it]                                                        40%|      | 1285/3250 [3:48:57<5:46:24, 10.58s/it] 40%|      | 1286/3250 [3:49:07<5:44:13, 10.52s/it]                                  {'loss': 0.5986, 'learning_rate': 6.614827800378108e-05, 'epoch': 0.4}
{'loss': 0.6302, 'learning_rate': 6.610250018376623e-05, 'epoch': 0.4}
{'loss': 0.6808, 'learning_rate': 6.60567072990342e-05, 'epoch': 0.4}
{'loss': 0.6296, 'learning_rate': 6.601089939242657e-05, 'epoch': 0.4}
{'loss': 0.627, 'learning_rate': 6.5965076506799e-05, 'epoch': 0.4}
                      40%|      | 1286/3250 [3:49:07<5:44:13, 10.52s/it] 40%|      | 1287/3250 [3:49:17<5:42:38, 10.47s/it]                                                        40%|      | 1287/3250 [3:49:17<5:42:38, 10.47s/it] 40%|      | 1288/3250 [3:49:28<5:41:32, 10.44s/it]                                                        40%|      | 1288/3250 [3:49:28<5:41:32, 10.44s/it] 40%|      | 1289/3250 [3:49:38<5:40:38, 10.42s/it]                                                        40%|      | 1289/3250 [3:49:38<5:40:38, 10.42s/it] 40%|      | 1290/3250 [3:49:49<5:39:48, 10.40s/it]                                                        40%|      | 1290/3250 [3:49:49<5:39:48, 10.40s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.7976985573768616, 'eval_runtime': 2.113, 'eval_samples_per_second': 5.679, 'eval_steps_per_second': 1.42, 'epoch': 0.4}
                                                        40%|      | 1290/3250 [3:49:51<5:39:48, 10.40s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_10/tmp-checkpoint-1290
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1290I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1290

I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1290
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1290
the pytorch model path isthe pytorch model path is the pytorch model path is/projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1290/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1290/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1290/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1290/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.6326, 'learning_rate': 6.591923868502117e-05, 'epoch': 0.4}
{'loss': 0.6114, 'learning_rate': 6.58733859699767e-05, 'epoch': 0.4}
{'loss': 0.6422, 'learning_rate': 6.582751840456315e-05, 'epoch': 0.4}
{'loss': 0.6142, 'learning_rate': 6.578163603169202e-05, 'epoch': 0.4}
{'loss': 0.6314, 'learning_rate': 6.573573889428862e-05, 'epoch': 0.4}
 40%|      | 1291/3250 [3:50:01<6:04:11, 11.15s/it]                                                        40%|      | 1291/3250 [3:50:01<6:04:11, 11.15s/it] 40%|      | 1292/3250 [3:50:12<5:56:23, 10.92s/it]                                                        40%|      | 1292/3250 [3:50:12<5:56:23, 10.92s/it] 40%|      | 1293/3250 [3:50:22<5:50:37, 10.75s/it]                                                        40%|      | 1293/3250 [3:50:22<5:50:37, 10.75s/it] 40%|      | 1294/3250 [3:50:33<5:46:43, 10.64s/it]                                                        40%|      | 1294/3250 [3:50:33<5:46:43, 10.64s/it] 40%|      | 1295/3250 [3:50:43<5:47:20, 10.66s/it]                                                        40%|      | 1295/3250 [3:50:43<5:47:20, 10.66s/it] 40%|      | 1296/3250 [3:50:54<5:46:59, 10.66s/it]                                  {'loss': 0.6173, 'learning_rate': 6.568982703529206e-05, 'epoch': 0.4}
{'loss': 0.637, 'learning_rate': 6.564390049765528e-05, 'epoch': 0.4}
{'loss': 0.6455, 'learning_rate': 6.55979593243449e-05, 'epoch': 0.4}
{'loss': 0.635, 'learning_rate': 6.555200355834123e-05, 'epoch': 0.4}
{'loss': 0.6091, 'learning_rate': 6.55060332426383e-05, 'epoch': 0.4}
                      40%|      | 1296/3250 [3:50:54<5:46:59, 10.66s/it] 40%|      | 1297/3250 [3:51:04<5:43:59, 10.57s/it]                                                        40%|      | 1297/3250 [3:51:04<5:43:59, 10.57s/it] 40%|      | 1298/3250 [3:51:15<5:41:52, 10.51s/it]                                                        40%|      | 1298/3250 [3:51:15<5:41:52, 10.51s/it] 40%|      | 1299/3250 [3:51:25<5:40:18, 10.47s/it]                                                        40%|      | 1299/3250 [3:51:25<5:40:18, 10.47s/it] 40%|      | 1300/3250 [3:51:35<5:39:08, 10.44s/it]                                                        40%|      | 1300/3250 [3:51:35<5:39:08, 10.44s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.7963075041770935, 'eval_runtime': 2.1162, 'eval_samples_per_second': 5.671, 'eval_steps_per_second': 1.418, 'epoch': 0.4}
                                                        40%|      | 1300/3250 [3:51:38<5:39:08, 10.44s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_10/tmp-checkpoint-1300
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1300
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1300
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1300
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1300
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1300/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1300/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1300/pytorch_model.binthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1300/pytorch_model.bin

/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.6601, 'learning_rate': 6.546004842024369e-05, 'epoch': 0.4}
{'loss': 0.6109, 'learning_rate': 6.541404913417853e-05, 'epoch': 0.4}
{'loss': 0.6434, 'learning_rate': 6.536803542747756e-05, 'epoch': 0.4}
{'loss': 0.6362, 'learning_rate': 6.532200734318896e-05, 'epoch': 0.4}
{'loss': 0.6188, 'learning_rate': 6.527596492437436e-05, 'epoch': 0.4}
 40%|      | 1301/3250 [3:51:48<6:03:17, 11.18s/it]                                                        40%|      | 1301/3250 [3:51:48<6:03:17, 11.18s/it] 40%|      | 1302/3250 [3:51:59<5:55:09, 10.94s/it]                                                        40%|      | 1302/3250 [3:51:59<5:55:09, 10.94s/it] 40%|      | 1303/3250 [3:52:09<5:49:23, 10.77s/it]                                                        40%|      | 1303/3250 [3:52:09<5:49:23, 10.77s/it] 40%|      | 1304/3250 [3:52:19<5:45:14, 10.64s/it]                                                        40%|      | 1304/3250 [3:52:19<5:45:14, 10.64s/it] 40%|      | 1305/3250 [3:52:30<5:42:28, 10.56s/it]                                                        40%|      | 1305/3250 [3:52:30<5:42:28, 10.56s/it] 40%|      | 1306/3250 [3:52:40<5:40:22, 10.51s/it]                                  {'loss': 0.5963, 'learning_rate': 6.52299082141088e-05, 'epoch': 0.4}
{'loss': 0.6031, 'learning_rate': 6.518383725548074e-05, 'epoch': 0.4}
{'loss': 0.6339, 'learning_rate': 6.51377520915919e-05, 'epoch': 0.4}
{'loss': 0.5992, 'learning_rate': 6.509165276555734e-05, 'epoch': 0.4}
{'loss': 0.6496, 'learning_rate': 6.504553932050534e-05, 'epoch': 0.4}
                      40%|      | 1306/3250 [3:52:40<5:40:22, 10.51s/it] 40%|      | 1307/3250 [3:52:51<5:38:58, 10.47s/it]                                                        40%|      | 1307/3250 [3:52:51<5:38:58, 10.47s/it] 40%|      | 1308/3250 [3:53:01<5:37:53, 10.44s/it]                                                        40%|      | 1308/3250 [3:53:01<5:37:53, 10.44s/it] 40%|      | 1309/3250 [3:53:11<5:37:04, 10.42s/it]                                                        40%|      | 1309/3250 [3:53:11<5:37:04, 10.42s/it] 40%|      | 1310/3250 [3:53:22<5:36:24, 10.40s/it]                                                        40%|      | 1310/3250 [3:53:22<5:36:24, 10.40s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.7923115491867065, 'eval_runtime': 2.1104, 'eval_samples_per_second': 5.686, 'eval_steps_per_second': 1.422, 'epoch': 0.4}
                                                        40%|      | 1310/3250 [3:53:24<5:36:24, 10.40s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_10/tmp-checkpoint-1310
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1310
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1310
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1310
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1310
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1310/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1310/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1310/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1310/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 1.1184, 'learning_rate': 6.49994117995774e-05, 'epoch': 0.4}
{'loss': 0.6034, 'learning_rate': 6.495327024592817e-05, 'epoch': 0.4}
{'loss': 0.6087, 'learning_rate': 6.490711470272549e-05, 'epoch': 0.4}
{'loss': 0.6406, 'learning_rate': 6.486094521315022e-05, 'epoch': 0.4}
{'loss': 0.6419, 'learning_rate': 6.481476182039627e-05, 'epoch': 0.4}
 40%|      | 1311/3250 [3:53:35<6:00:46, 11.16s/it]                                                        40%|      | 1311/3250 [3:53:35<6:00:46, 11.16s/it] 40%|      | 1312/3250 [3:53:45<5:55:15, 11.00s/it]                                                        40%|      | 1312/3250 [3:53:45<5:55:15, 11.00s/it] 40%|      | 1313/3250 [3:53:56<5:48:40, 10.80s/it]                                                        40%|      | 1313/3250 [3:53:56<5:48:40, 10.80s/it] 40%|      | 1314/3250 [3:54:06<5:44:13, 10.67s/it]                                                        40%|      | 1314/3250 [3:54:06<5:44:13, 10.67s/it] 40%|      | 1315/3250 [3:54:16<5:41:04, 10.58s/it]                                                        40%|      | 1315/3250 [3:54:16<5:41:04, 10.58s/it] 40%|      | 1316/3250 [3:54:27<5:38:53, 10.51s/it]                                  {'loss': 0.6099, 'learning_rate': 6.476856456767064e-05, 'epoch': 0.4}
{'loss': 0.6128, 'learning_rate': 6.472235349819318e-05, 'epoch': 0.41}
{'loss': 0.6612, 'learning_rate': 6.467612865519674e-05, 'epoch': 0.41}
{'loss': 0.64, 'learning_rate': 6.462989008192706e-05, 'epoch': 0.41}
{'loss': 0.6159, 'learning_rate': 6.458363782164266e-05, 'epoch': 0.41}
                      40%|      | 1316/3250 [3:54:27<5:38:53, 10.51s/it] 41%|      | 1317/3250 [3:54:37<5:37:13, 10.47s/it]                                                        41%|      | 1317/3250 [3:54:37<5:37:13, 10.47s/it] 41%|      | 1318/3250 [3:54:47<5:36:07, 10.44s/it]                                                        41%|      | 1318/3250 [3:54:47<5:36:07, 10.44s/it] 41%|      | 1319/3250 [3:54:58<5:35:08, 10.41s/it]                                                        41%|      | 1319/3250 [3:54:58<5:35:08, 10.41s/it] 41%|      | 1320/3250 [3:55:08<5:34:34, 10.40s/it]                                                        41%|      | 1320/3250 [3:55:08<5:34:34, 10.40s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.7971431016921997, 'eval_runtime': 2.1142, 'eval_samples_per_second': 5.676, 'eval_steps_per_second': 1.419, 'epoch': 0.41}
                                                        41%|      | 1320/3250 [3:55:10<5:34:34, 10.40s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_10/tmp-checkpoint-1320
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1320
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1320
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1320
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1320
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1320/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1320/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1320/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1320/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.5826, 'learning_rate': 6.453737191761493e-05, 'epoch': 0.41}
{'loss': 0.6335, 'learning_rate': 6.449109241312803e-05, 'epoch': 0.41}
{'loss': 0.6138, 'learning_rate': 6.444479935147878e-05, 'epoch': 0.41}
{'loss': 0.6278, 'learning_rate': 6.439849277597671e-05, 'epoch': 0.41}
{'loss': 0.6053, 'learning_rate': 6.435217272994406e-05, 'epoch': 0.41}
 41%|      | 1321/3250 [3:55:21<5:59:11, 11.17s/it]                                                        41%|      | 1321/3250 [3:55:21<5:59:11, 11.17s/it] 41%|      | 1322/3250 [3:55:31<5:51:08, 10.93s/it]                                                        41%|      | 1322/3250 [3:55:31<5:51:08, 10.93s/it] 41%|      | 1323/3250 [3:55:42<5:45:23, 10.75s/it]                                                        41%|      | 1323/3250 [3:55:42<5:45:23, 10.75s/it] 41%|      | 1324/3250 [3:55:52<5:41:23, 10.64s/it]                                                        41%|      | 1324/3250 [3:55:52<5:41:23, 10.64s/it] 41%|      | 1325/3250 [3:56:02<5:38:28, 10.55s/it]                                                        41%|      | 1325/3250 [3:56:02<5:38:28, 10.55s/it] 41%|      | 1326/3250 [3:56:13<5:36:30, 10.49s/it]                                  {'loss': 0.6335, 'learning_rate': 6.430583925671558e-05, 'epoch': 0.41}
{'loss': 0.6192, 'learning_rate': 6.42594923996386e-05, 'epoch': 0.41}
{'loss': 0.654, 'learning_rate': 6.421313220207304e-05, 'epoch': 0.41}
{'loss': 0.6314, 'learning_rate': 6.416675870739118e-05, 'epoch': 0.41}
{'loss': 0.6227, 'learning_rate': 6.412037195897785e-05, 'epoch': 0.41}
                      41%|      | 1326/3250 [3:56:13<5:36:30, 10.49s/it] 41%|      | 1327/3250 [3:56:23<5:35:04, 10.45s/it]                                                        41%|      | 1327/3250 [3:56:23<5:35:04, 10.45s/it] 41%|      | 1328/3250 [3:56:34<5:39:54, 10.61s/it]                                                        41%|      | 1328/3250 [3:56:34<5:39:54, 10.61s/it] 41%|      | 1329/3250 [3:56:45<5:37:26, 10.54s/it]                                                        41%|      | 1329/3250 [3:56:45<5:37:26, 10.54s/it] 41%|      | 1330/3250 [3:56:55<5:35:44, 10.49s/it]                                                        41%|      | 1330/3250 [3:56:55<5:35:44, 10.49s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.7948514223098755, 'eval_runtime': 2.1123, 'eval_samples_per_second': 5.681, 'eval_steps_per_second': 1.42, 'epoch': 0.41}
                                                        41%|      | 1330/3250 [3:56:57<5:35:44, 10.49s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_10/tmp-checkpoint-1330
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1330
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1330
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1330
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1330
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1330/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1330/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1330/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1330/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.6316, 'learning_rate': 6.407397200023027e-05, 'epoch': 0.41}
{'loss': 0.6366, 'learning_rate': 6.402755887455792e-05, 'epoch': 0.41}
{'loss': 0.5993, 'learning_rate': 6.398113262538272e-05, 'epoch': 0.41}
{'loss': 0.6602, 'learning_rate': 6.393469329613879e-05, 'epoch': 0.41}
{'loss': 0.5996, 'learning_rate': 6.388824093027253e-05, 'epoch': 0.41}
 41%|      | 1331/3250 [3:57:08<5:58:55, 11.22s/it]                                                        41%|      | 1331/3250 [3:57:08<5:58:55, 11.22s/it] 41%|      | 1332/3250 [3:57:18<5:50:27, 10.96s/it]                                                        41%|      | 1332/3250 [3:57:18<5:50:27, 10.96s/it] 41%|      | 1333/3250 [3:57:29<5:44:28, 10.78s/it]                                                        41%|      | 1333/3250 [3:57:29<5:44:28, 10.78s/it] 41%|      | 1334/3250 [3:57:39<5:40:25, 10.66s/it]                                                        41%|      | 1334/3250 [3:57:39<5:40:25, 10.66s/it] 41%|      | 1335/3250 [3:57:49<5:37:21, 10.57s/it]                                                        41%|      | 1335/3250 [3:57:49<5:37:21, 10.57s/it] 41%|      | 1336/3250 [3:58:00<5:35:12, 10.51s/it]                                  {'loss': 0.6016, 'learning_rate': 6.384177557124247e-05, 'epoch': 0.41}
{'loss': 0.6093, 'learning_rate': 6.37952972625194e-05, 'epoch': 0.41}
{'loss': 0.6138, 'learning_rate': 6.374880604758615e-05, 'epoch': 0.41}
{'loss': 0.6117, 'learning_rate': 6.370230196993763e-05, 'epoch': 0.41}
{'loss': 0.6323, 'learning_rate': 6.36557850730808e-05, 'epoch': 0.41}
                      41%|      | 1336/3250 [3:58:00<5:35:12, 10.51s/it] 41%|      | 1337/3250 [3:58:10<5:33:46, 10.47s/it]                                                        41%|      | 1337/3250 [3:58:10<5:33:46, 10.47s/it] 41%|      | 1338/3250 [3:58:20<5:32:32, 10.44s/it]                                                        41%|      | 1338/3250 [3:58:20<5:32:32, 10.44s/it] 41%|      | 1339/3250 [3:58:31<5:31:48, 10.42s/it]                                                        41%|      | 1339/3250 [3:58:31<5:31:48, 10.42s/it] 41%|      | 1340/3250 [3:58:41<5:31:07, 10.40s/it]                                                        41%|      | 1340/3250 [3:58:41<5:31:07, 10.40s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.7919687032699585, 'eval_runtime': 2.1162, 'eval_samples_per_second': 5.671, 'eval_steps_per_second': 1.418, 'epoch': 0.41}
                                                        41%|      | 1340/3250 [3:58:43<5:31:07, 10.40s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_10/tmp-checkpoint-1340
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1340
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1340
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1340
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1340
/u/bzd2/.local/lib/python3.9/site-packages/huggingface_hub/repocard.py:105: UserWarning: Repo card metadata block was not found. Setting CardData to empty.
  warnings.warn("Repo card metadata block was not found. Setting CardData to empty.")
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1340/pytorch_model.binthe pytorch model path is
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1340/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1340/pytorch_model.binthe pytorch model path is
 /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1340/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.6191, 'learning_rate': 6.360925540053463e-05, 'epoch': 0.41}
{'loss': 1.1021, 'learning_rate': 6.356271299582999e-05, 'epoch': 0.41}
{'loss': 0.614, 'learning_rate': 6.351615790250973e-05, 'epoch': 0.41}
{'loss': 0.6367, 'learning_rate': 6.346959016412852e-05, 'epoch': 0.41}
{'loss': 0.6283, 'learning_rate': 6.342300982425284e-05, 'epoch': 0.41}
 41%|     | 1341/3250 [3:58:54<5:54:58, 11.16s/it]                                                        41%|     | 1341/3250 [3:58:54<5:54:58, 11.16s/it] 41%|     | 1342/3250 [3:59:04<5:47:00, 10.91s/it]                                                        41%|     | 1342/3250 [3:59:04<5:47:00, 10.91s/it] 41%|     | 1343/3250 [3:59:15<5:41:26, 10.74s/it]                                                        41%|     | 1343/3250 [3:59:15<5:41:26, 10.74s/it] 41%|     | 1344/3250 [3:59:25<5:37:27, 10.62s/it]                                                        41%|     | 1344/3250 [3:59:25<5:37:27, 10.62s/it] 41%|     | 1345/3250 [3:59:36<5:36:59, 10.61s/it]                                                        41%|     | 1345/3250 [3:59:36<5:36:59, 10.61s/it] 41%|     | 1346/3250 [3:59:46<5:34:18, 10.53s/it]            {'loss': 0.6299, 'learning_rate': 6.337641692646106e-05, 'epoch': 0.41}
{'loss': 0.5983, 'learning_rate': 6.332981151434317e-05, 'epoch': 0.41}
{'loss': 0.6593, 'learning_rate': 6.328319363150095e-05, 'epoch': 0.41}
{'loss': 0.6555, 'learning_rate': 6.323656332154786e-05, 'epoch': 0.42}
{'loss': 0.6094, 'learning_rate': 6.318992062810891e-05, 'epoch': 0.42}
                                            41%|     | 1346/3250 [3:59:46<5:34:18, 10.53s/it] 41%|     | 1347/3250 [3:59:56<5:32:25, 10.48s/it]                                                        41%|     | 1347/3250 [3:59:56<5:32:25, 10.48s/it] 41%|     | 1348/3250 [4:00:07<5:31:05, 10.44s/it]                                                        41%|     | 1348/3250 [4:00:07<5:31:05, 10.44s/it] 42%|     | 1349/3250 [4:00:17<5:30:04, 10.42s/it]                                                        42%|     | 1349/3250 [4:00:17<5:30:04, 10.42s/it] 42%|     | 1350/3250 [4:00:27<5:29:24, 10.40s/it]                                                        42%|     | 1350/3250 [4:00:27<5:29:24, 10.40s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.7949402928352356, 'eval_runtime': 2.1137, 'eval_samples_per_second': 5.677, 'eval_steps_per_second': 1.419, 'epoch': 0.42}
                                                        42%|     | 1350/3250 [4:00:30<5:29:24, 10.40s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_10/tmp-checkpoint-1350
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1350
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1350
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1350
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1350
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1350/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1350/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1350/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1350/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.5886, 'learning_rate': 6.314326559482076e-05, 'epoch': 0.42}
{'loss': 0.6215, 'learning_rate': 6.30965982653316e-05, 'epoch': 0.42}
{'loss': 0.615, 'learning_rate': 6.30499186833011e-05, 'epoch': 0.42}
{'loss': 0.6214, 'learning_rate': 6.300322689240041e-05, 'epoch': 0.42}
{'loss': 0.6116, 'learning_rate': 6.295652293631212e-05, 'epoch': 0.42}
 42%|     | 1351/3250 [4:00:40<5:53:23, 11.17s/it]                                                        42%|     | 1351/3250 [4:00:40<5:53:23, 11.17s/it] 42%|     | 1352/3250 [4:00:51<5:45:36, 10.93s/it]                                                        42%|     | 1352/3250 [4:00:51<5:45:36, 10.93s/it] 42%|     | 1353/3250 [4:01:01<5:39:58, 10.75s/it]                                                        42%|     | 1353/3250 [4:01:01<5:39:58, 10.75s/it] 42%|     | 1354/3250 [4:01:11<5:35:53, 10.63s/it]                                                        42%|     | 1354/3250 [4:01:12<5:35:53, 10.63s/it] 42%|     | 1355/3250 [4:01:22<5:32:54, 10.54s/it]                                                        42%|     | 1355/3250 [4:01:22<5:32:54, 10.54s/it] 42%|     | 1356/3250 [4:01:32<5:30:38, 10.47s/it]            {'loss': 0.6204, 'learning_rate': 6.290980685873017e-05, 'epoch': 0.42}
{'loss': 0.6179, 'learning_rate': 6.286307870335984e-05, 'epoch': 0.42}
{'loss': 0.6353, 'learning_rate': 6.281633851391777e-05, 'epoch': 0.42}
{'loss': 0.6265, 'learning_rate': 6.276958633413175e-05, 'epoch': 0.42}
{'loss': 0.6222, 'learning_rate': 6.272282220774091e-05, 'epoch': 0.42}
                                            42%|     | 1356/3250 [4:01:32<5:30:38, 10.47s/it] 42%|     | 1357/3250 [4:01:42<5:29:10, 10.43s/it]                                                        42%|     | 1357/3250 [4:01:42<5:29:10, 10.43s/it] 42%|     | 1358/3250 [4:01:53<5:28:10, 10.41s/it]                                                        42%|     | 1358/3250 [4:01:53<5:28:10, 10.41s/it] 42%|     | 1359/3250 [4:02:03<5:27:18, 10.39s/it]                                                        42%|     | 1359/3250 [4:02:03<5:27:18, 10.39s/it] 42%|     | 1360/3250 [4:02:14<5:26:48, 10.37s/it]                                                        42%|     | 1360/3250 [4:02:14<5:26:48, 10.37s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.793080747127533, 'eval_runtime': 2.1073, 'eval_samples_per_second': 5.694, 'eval_steps_per_second': 1.424, 'epoch': 0.42}
                                                        42%|     | 1360/3250 [4:02:16<5:26:48, 10.37s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_10/tmp-checkpoint-1360
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1360
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1360
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1360
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1360
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1360/pytorch_model.bin
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1360/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1360/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1360/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.6285, 'learning_rate': 6.267604617849544e-05, 'epoch': 0.42}
{'loss': 0.6361, 'learning_rate': 6.262925829015676e-05, 'epoch': 0.42}
{'loss': 0.5716, 'learning_rate': 6.258245858649731e-05, 'epoch': 0.42}
{'loss': 0.6569, 'learning_rate': 6.253564711130067e-05, 'epoch': 0.42}
{'loss': 0.598, 'learning_rate': 6.248882390836135e-05, 'epoch': 0.42}
 42%|     | 1361/3250 [4:02:27<5:54:50, 11.27s/it]                                                        42%|     | 1361/3250 [4:02:27<5:54:50, 11.27s/it] 42%|     | 1362/3250 [4:02:37<5:45:52, 10.99s/it]                                                        42%|     | 1362/3250 [4:02:37<5:45:52, 10.99s/it] 42%|     | 1363/3250 [4:02:48<5:39:35, 10.80s/it]                                                        42%|     | 1363/3250 [4:02:48<5:39:35, 10.80s/it] 42%|     | 1364/3250 [4:02:58<5:35:09, 10.66s/it]                                                        42%|     | 1364/3250 [4:02:58<5:35:09, 10.66s/it] 42%|     | 1365/3250 [4:03:08<5:32:02, 10.57s/it]                                                        42%|     | 1365/3250 [4:03:08<5:32:02, 10.57s/it] 42%|     | 1366/3250 [4:03:19<5:29:41, 10.50s/it]            {'loss': 0.6072, 'learning_rate': 6.244198902148486e-05, 'epoch': 0.42}
{'loss': 0.5912, 'learning_rate': 6.239514249448767e-05, 'epoch': 0.42}
{'loss': 0.6053, 'learning_rate': 6.234828437119709e-05, 'epoch': 0.42}
{'loss': 0.6248, 'learning_rate': 6.230141469545132e-05, 'epoch': 0.42}
{'loss': 0.5989, 'learning_rate': 6.225453351109934e-05, 'epoch': 0.42}
                                            42%|     | 1366/3250 [4:03:19<5:29:41, 10.50s/it] 42%|     | 1367/3250 [4:03:29<5:28:05, 10.45s/it]                                                        42%|     | 1367/3250 [4:03:29<5:28:05, 10.45s/it] 42%|     | 1368/3250 [4:03:39<5:26:51, 10.42s/it]                                                        42%|     | 1368/3250 [4:03:39<5:26:51, 10.42s/it] 42%|     | 1369/3250 [4:03:50<5:25:55, 10.40s/it]                                                        42%|     | 1369/3250 [4:03:50<5:25:55, 10.40s/it] 42%|     | 1370/3250 [4:04:00<5:25:13, 10.38s/it]                                                        42%|     | 1370/3250 [4:04:00<5:25:13, 10.38s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.7893349528312683, 'eval_runtime': 2.3306, 'eval_samples_per_second': 5.149, 'eval_steps_per_second': 1.287, 'epoch': 0.42}
                                                        42%|     | 1370/3250 [4:04:02<5:25:13, 10.38s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_10/tmp-checkpoint-1370
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1370
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1370
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1370
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1370
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1370/pytorch_model.bin
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1370/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1370/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1370/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.6297, 'learning_rate': 6.220764086200094e-05, 'epoch': 0.42}
{'loss': 1.1004, 'learning_rate': 6.216073679202656e-05, 'epoch': 0.42}
{'loss': 0.5895, 'learning_rate': 6.211382134505742e-05, 'epoch': 0.42}
{'loss': 0.6359, 'learning_rate': 6.206689456498529e-05, 'epoch': 0.42}
{'loss': 0.6202, 'learning_rate': 6.20199564957126e-05, 'epoch': 0.42}
 42%|     | 1371/3250 [4:04:13<5:51:09, 11.21s/it]                                                        42%|     | 1371/3250 [4:04:13<5:51:09, 11.21s/it] 42%|     | 1372/3250 [4:04:23<5:42:54, 10.96s/it]                                                        42%|     | 1372/3250 [4:04:23<5:42:54, 10.96s/it] 42%|     | 1373/3250 [4:04:34<5:37:01, 10.77s/it]                                                        42%|     | 1373/3250 [4:04:34<5:37:01, 10.77s/it] 42%|     | 1374/3250 [4:04:44<5:32:51, 10.65s/it]                                                        42%|     | 1374/3250 [4:04:44<5:32:51, 10.65s/it] 42%|     | 1375/3250 [4:04:55<5:30:04, 10.56s/it]                                                        42%|     | 1375/3250 [4:04:55<5:30:04, 10.56s/it] 42%|     | 1376/3250 [4:05:05<5:27:59, 10.50s/it]            {'loss': 0.6376, 'learning_rate': 6.197300718115234e-05, 'epoch': 0.42}
{'loss': 0.5728, 'learning_rate': 6.192604666522801e-05, 'epoch': 0.42}
{'loss': 0.6187, 'learning_rate': 6.187907499187356e-05, 'epoch': 0.42}
{'loss': 0.6747, 'learning_rate': 6.183209220503343e-05, 'epoch': 0.42}
{'loss': 0.6132, 'learning_rate': 6.178509834866244e-05, 'epoch': 0.42}
                                            42%|     | 1376/3250 [4:05:05<5:27:59, 10.50s/it] 42%|     | 1377/3250 [4:05:16<5:28:43, 10.53s/it]                                                        42%|     | 1377/3250 [4:05:16<5:28:43, 10.53s/it] 42%|     | 1378/3250 [4:05:26<5:26:56, 10.48s/it]                                                        42%|     | 1378/3250 [4:05:26<5:26:56, 10.48s/it] 42%|     | 1379/3250 [4:05:36<5:25:35, 10.44s/it]                                                        42%|     | 1379/3250 [4:05:36<5:25:35, 10.44s/it] 42%|     | 1380/3250 [4:05:47<5:24:36, 10.42s/it]                                                        42%|     | 1380/3250 [4:05:47<5:24:36, 10.42s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.7937382459640503, 'eval_runtime': 2.11, 'eval_samples_per_second': 5.687, 'eval_steps_per_second': 1.422, 'epoch': 0.42}
                                                        42%|     | 1380/3250 [4:05:49<5:24:36, 10.42s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_10/tmp-checkpoint-1380
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1380
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1380
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1380
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1380
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1380/pytorch_model.bin
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1380/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1380/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1380/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.6127, 'learning_rate': 6.173809346672574e-05, 'epoch': 0.42}
{'loss': 0.6068, 'learning_rate': 6.169107760319884e-05, 'epoch': 0.43}
{'loss': 0.5947, 'learning_rate': 6.164405080206746e-05, 'epoch': 0.43}
{'loss': 0.6283, 'learning_rate': 6.159701310732763e-05, 'epoch': 0.43}
{'loss': 0.5956, 'learning_rate': 6.154996456298552e-05, 'epoch': 0.43}
 42%|     | 1381/3250 [4:06:00<5:47:55, 11.17s/it]                                                        42%|     | 1381/3250 [4:06:00<5:47:55, 11.17s/it] 43%|     | 1382/3250 [4:06:10<5:39:59, 10.92s/it]                                                        43%|     | 1382/3250 [4:06:10<5:39:59, 10.92s/it] 43%|     | 1383/3250 [4:06:20<5:34:33, 10.75s/it]                                                        43%|     | 1383/3250 [4:06:20<5:34:33, 10.75s/it] 43%|     | 1384/3250 [4:06:31<5:30:31, 10.63s/it]                                                        43%|     | 1384/3250 [4:06:31<5:30:31, 10.63s/it] 43%|     | 1385/3250 [4:06:41<5:27:49, 10.55s/it]                                                        43%|     | 1385/3250 [4:06:41<5:27:49, 10.55s/it] 43%|     | 1386/3250 [4:06:51<5:25:48, 10.49s/it]            {'loss': 0.6127, 'learning_rate': 6.150290521305746e-05, 'epoch': 0.43}
{'loss': 0.6115, 'learning_rate': 6.145583510156989e-05, 'epoch': 0.43}
{'loss': 0.6135, 'learning_rate': 6.14087542725593e-05, 'epoch': 0.43}
{'loss': 0.6277, 'learning_rate': 6.136166277007229e-05, 'epoch': 0.43}
{'loss': 0.6278, 'learning_rate': 6.13145606381653e-05, 'epoch': 0.43}
                                            43%|     | 1386/3250 [4:06:51<5:25:48, 10.49s/it] 43%|     | 1387/3250 [4:07:02<5:24:35, 10.45s/it]                                                        43%|     | 1387/3250 [4:07:02<5:24:35, 10.45s/it] 43%|     | 1388/3250 [4:07:12<5:23:35, 10.43s/it]                                                        43%|     | 1388/3250 [4:07:12<5:23:35, 10.43s/it] 43%|     | 1389/3250 [4:07:22<5:22:41, 10.40s/it]                                                        43%|     | 1389/3250 [4:07:22<5:22:41, 10.40s/it] 43%|     | 1390/3250 [4:07:33<5:22:10, 10.39s/it]                                                        43%|     | 1390/3250 [4:07:33<5:22:10, 10.39s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.7919116616249084, 'eval_runtime': 2.113, 'eval_samples_per_second': 5.679, 'eval_steps_per_second': 1.42, 'epoch': 0.43}
                                                        43%|     | 1390/3250 [4:07:35<5:22:10, 10.39s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_10/tmp-checkpoint-1390
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1390
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1390
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1390
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1390
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1390/pytorch_model.binthe pytorch model path is
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1390/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1390/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1390/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.6056, 'learning_rate': 6.126744792090487e-05, 'epoch': 0.43}
{'loss': 0.6342, 'learning_rate': 6.122032466236733e-05, 'epoch': 0.43}
{'loss': 0.5942, 'learning_rate': 6.117319090663893e-05, 'epoch': 0.43}
{'loss': 0.6193, 'learning_rate': 6.112604669781572e-05, 'epoch': 0.43}
{'loss': 0.6112, 'learning_rate': 6.107889208000354e-05, 'epoch': 0.43}
 43%|     | 1391/3250 [4:07:46<5:45:26, 11.15s/it]                                                        43%|     | 1391/3250 [4:07:46<5:45:26, 11.15s/it] 43%|     | 1392/3250 [4:07:56<5:38:00, 10.92s/it]                                                        43%|     | 1392/3250 [4:07:56<5:38:00, 10.92s/it] 43%|     | 1393/3250 [4:08:06<5:32:41, 10.75s/it]                                                        43%|     | 1393/3250 [4:08:06<5:32:41, 10.75s/it] 43%|     | 1394/3250 [4:08:17<5:34:45, 10.82s/it]                                                        43%|     | 1394/3250 [4:08:17<5:34:45, 10.82s/it] 43%|     | 1395/3250 [4:08:28<5:30:14, 10.68s/it]                                                        43%|     | 1395/3250 [4:08:28<5:30:14, 10.68s/it] 43%|     | 1396/3250 [4:08:38<5:26:59, 10.58s/it]            {'loss': 0.5982, 'learning_rate': 6.103172709731793e-05, 'epoch': 0.43}
{'loss': 0.581, 'learning_rate': 6.098455179388417e-05, 'epoch': 0.43}
{'loss': 0.5908, 'learning_rate': 6.093736621383721e-05, 'epoch': 0.43}
{'loss': 0.6368, 'learning_rate': 6.089017040132155e-05, 'epoch': 0.43}
{'loss': 0.5838, 'learning_rate': 6.084296440049132e-05, 'epoch': 0.43}
                                            43%|     | 1396/3250 [4:08:38<5:26:59, 10.58s/it] 43%|     | 1397/3250 [4:08:48<5:24:52, 10.52s/it]                                                        43%|     | 1397/3250 [4:08:48<5:24:52, 10.52s/it] 43%|     | 1398/3250 [4:08:59<5:23:03, 10.47s/it]                                                        43%|     | 1398/3250 [4:08:59<5:23:03, 10.47s/it] 43%|     | 1399/3250 [4:09:09<5:21:58, 10.44s/it]                                                        43%|     | 1399/3250 [4:09:09<5:21:58, 10.44s/it] 43%|     | 1400/3250 [4:09:20<5:21:21, 10.42s/it]                                                        43%|     | 1400/3250 [4:09:20<5:21:21, 10.42s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.7913881540298462, 'eval_runtime': 2.1081, 'eval_samples_per_second': 5.692, 'eval_steps_per_second': 1.423, 'epoch': 0.43}
                                                        43%|     | 1400/3250 [4:09:22<5:21:21, 10.42s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_10/tmp-checkpoint-1400
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1400
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1400
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1400
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1400
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1400/pytorch_model.bin
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1400/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1400/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1400/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.6352, 'learning_rate': 6.079574825551017e-05, 'epoch': 0.43}
{'loss': 1.0964, 'learning_rate': 6.0748522010551215e-05, 'epoch': 0.43}
{'loss': 0.5892, 'learning_rate': 6.070128570979703e-05, 'epoch': 0.43}
{'loss': 0.6089, 'learning_rate': 6.0654039397439635e-05, 'epoch': 0.43}
{'loss': 0.6239, 'learning_rate': 6.060678311768035e-05, 'epoch': 0.43}
 43%|     | 1401/3250 [4:09:32<5:44:30, 11.18s/it]                                                        43%|     | 1401/3250 [4:09:32<5:44:30, 11.18s/it] 43%|     | 1402/3250 [4:09:43<5:36:33, 10.93s/it]                                                        43%|     | 1402/3250 [4:09:43<5:36:33, 10.93s/it] 43%|     | 1403/3250 [4:09:53<5:31:04, 10.76s/it]                                                        43%|     | 1403/3250 [4:09:53<5:31:04, 10.76s/it] 43%|     | 1404/3250 [4:10:04<5:27:09, 10.63s/it]                                                        43%|     | 1404/3250 [4:10:04<5:27:09, 10.63s/it] 43%|     | 1405/3250 [4:10:14<5:24:30, 10.55s/it]                                                        43%|     | 1405/3250 [4:10:14<5:24:30, 10.55s/it] 43%|     | 1406/3250 [4:10:24<5:22:36, 10.50s/it]            {'loss': 0.6165, 'learning_rate': 6.0559516914729886e-05, 'epoch': 0.43}
{'loss': 0.5895, 'learning_rate': 6.05122408328082e-05, 'epoch': 0.43}
{'loss': 0.6016, 'learning_rate': 6.0464954916144465e-05, 'epoch': 0.43}
{'loss': 0.6555, 'learning_rate': 6.0417659208977127e-05, 'epoch': 0.43}
{'loss': 0.6231, 'learning_rate': 6.0370353755553753e-05, 'epoch': 0.43}
                                            43%|     | 1406/3250 [4:10:24<5:22:36, 10.50s/it] 43%|     | 1407/3250 [4:10:35<5:21:17, 10.46s/it]                                                        43%|     | 1407/3250 [4:10:35<5:21:17, 10.46s/it] 43%|     | 1408/3250 [4:10:45<5:20:12, 10.43s/it]                                                        43%|     | 1408/3250 [4:10:45<5:20:12, 10.43s/it] 43%|     | 1409/3250 [4:10:55<5:19:20, 10.41s/it]                                                        43%|     | 1409/3250 [4:10:55<5:19:20, 10.41s/it] 43%|     | 1410/3250 [4:11:06<5:22:10, 10.51s/it]                                                        43%|     | 1410/3250 [4:11:06<5:22:10, 10.51s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.7947258353233337, 'eval_runtime': 2.1142, 'eval_samples_per_second': 5.676, 'eval_steps_per_second': 1.419, 'epoch': 0.43}
                                                        43%|     | 1410/3250 [4:11:08<5:22:10, 10.51s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_10/tmp-checkpoint-1410
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in I AM HERE AND I AM SAVING THE MODEL
 the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1410
/projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1410
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1410
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1410
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1410/pytorch_model.bin
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1410/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1410/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1410/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.6019, 'learning_rate': 6.0323038600131024e-05, 'epoch': 0.43}
{'loss': 0.5789, 'learning_rate': 6.027571378697468e-05, 'epoch': 0.43}
{'loss': 0.6169, 'learning_rate': 6.022837936035952e-05, 'epoch': 0.43}
{'loss': 0.599, 'learning_rate': 6.018103536456936e-05, 'epoch': 0.44}
{'loss': 0.6072, 'learning_rate': 6.013368184389692e-05, 'epoch': 0.44}
 43%|     | 1411/3250 [4:11:19<5:44:29, 11.24s/it]                                                        43%|     | 1411/3250 [4:11:19<5:44:29, 11.24s/it] 43%|     | 1412/3250 [4:11:29<5:36:19, 10.98s/it]                                                        43%|     | 1412/3250 [4:11:29<5:36:19, 10.98s/it] 43%|     | 1413/3250 [4:11:40<5:30:38, 10.80s/it]                                                        43%|     | 1413/3250 [4:11:40<5:30:38, 10.80s/it] 44%|     | 1414/3250 [4:11:50<5:26:27, 10.67s/it]                                                        44%|     | 1414/3250 [4:11:50<5:26:27, 10.67s/it] 44%|     | 1415/3250 [4:12:01<5:23:34, 10.58s/it]                                                        44%|     | 1415/3250 [4:12:01<5:23:34, 10.58s/it] 44%|     | 1416/3250 [4:12:11<5:21:25, 10.52s/it]            {'loss': 0.5856, 'learning_rate': 6.008631884264388e-05, 'epoch': 0.44}
{'loss': 0.6233, 'learning_rate': 6.003894640512073e-05, 'epoch': 0.44}
{'loss': 0.6011, 'learning_rate': 5.9991564575646855e-05, 'epoch': 0.44}
{'loss': 0.6354, 'learning_rate': 5.994417339855039e-05, 'epoch': 0.44}
{'loss': 0.6148, 'learning_rate': 5.989677291816818e-05, 'epoch': 0.44}
                                            44%|     | 1416/3250 [4:12:11<5:21:25, 10.52s/it] 44%|     | 1417/3250 [4:12:21<5:19:57, 10.47s/it]                                                        44%|     | 1417/3250 [4:12:21<5:19:57, 10.47s/it] 44%|     | 1418/3250 [4:12:32<5:18:57, 10.45s/it]                                                        44%|     | 1418/3250 [4:12:32<5:18:57, 10.45s/it] 44%|     | 1419/3250 [4:12:42<5:18:05, 10.42s/it]                                                        44%|     | 1419/3250 [4:12:42<5:18:05, 10.42s/it] 44%|     | 1420/3250 [4:12:52<5:17:19, 10.40s/it]                                                        44%|     | 1420/3250 [4:12:52<5:17:19, 10.40s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.7918692231178284, 'eval_runtime': 2.1124, 'eval_samples_per_second': 5.681, 'eval_steps_per_second': 1.42, 'epoch': 0.44}
                                                        44%|     | 1420/3250 [4:12:54<5:17:19, 10.40s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_10/tmp-checkpoint-1420
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1420I AM HERE AND I AM SAVING THE MODEL

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1420
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1420
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1420
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1420/pytorch_model.bin
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1420/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1420/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1420/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.61, 'learning_rate': 5.984936317884584e-05, 'epoch': 0.44}
{'loss': 0.6191, 'learning_rate': 5.9801944224937644e-05, 'epoch': 0.44}
{'loss': 0.6132, 'learning_rate': 5.9754516100806423e-05, 'epoch': 0.44}
{'loss': 0.5876, 'learning_rate': 5.970707885082364e-05, 'epoch': 0.44}
{'loss': 0.6516, 'learning_rate': 5.965963251936929e-05, 'epoch': 0.44}
 44%|     | 1421/3250 [4:13:05<5:41:03, 11.19s/it]                                                        44%|     | 1421/3250 [4:13:05<5:41:03, 11.19s/it] 44%|     | 1422/3250 [4:13:16<5:33:31, 10.95s/it]                                                        44%|     | 1422/3250 [4:13:16<5:33:31, 10.95s/it] 44%|     | 1423/3250 [4:13:26<5:27:57, 10.77s/it]                                                        44%|     | 1423/3250 [4:13:26<5:27:57, 10.77s/it] 44%|     | 1424/3250 [4:13:36<5:24:13, 10.65s/it]                                                        44%|     | 1424/3250 [4:13:36<5:24:13, 10.65s/it] 44%|     | 1425/3250 [4:13:47<5:21:22, 10.57s/it]                                                        44%|     | 1425/3250 [4:13:47<5:21:22, 10.57s/it] 44%|     | 1426/3250 [4:13:57<5:19:22, 10.51s/it]            {'loss': 0.5934, 'learning_rate': 5.961217715083185e-05, 'epoch': 0.44}
{'loss': 0.5807, 'learning_rate': 5.9564712789608256e-05, 'epoch': 0.44}
{'loss': 0.594, 'learning_rate': 5.951723948010388e-05, 'epoch': 0.44}
{'loss': 0.6033, 'learning_rate': 5.946975726673241e-05, 'epoch': 0.44}
{'loss': 0.5878, 'learning_rate': 5.9422266193915924e-05, 'epoch': 0.44}
                                            44%|     | 1426/3250 [4:13:57<5:19:22, 10.51s/it] 44%|     | 1427/3250 [4:14:08<5:20:16, 10.54s/it]                                                        44%|     | 1427/3250 [4:14:08<5:20:16, 10.54s/it] 44%|     | 1428/3250 [4:14:18<5:18:31, 10.49s/it]                                                        44%|     | 1428/3250 [4:14:18<5:18:31, 10.49s/it] 44%|     | 1429/3250 [4:14:29<5:17:20, 10.46s/it]                                                        44%|     | 1429/3250 [4:14:29<5:17:20, 10.46s/it] 44%|     | 1430/3250 [4:14:39<5:16:31, 10.44s/it]                                                        44%|     | 1430/3250 [4:14:39<5:16:31, 10.44s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.789584219455719, 'eval_runtime': 2.3396, 'eval_samples_per_second': 5.129, 'eval_steps_per_second': 1.282, 'epoch': 0.44}
                                                        44%|     | 1430/3250 [4:14:41<5:16:31, 10.44s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_10/tmp-checkpoint-1430
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1430
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1430
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1430
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1430
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1430/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1430/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1430/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1430/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.6205, 'learning_rate': 5.937476630608475e-05, 'epoch': 0.44}
{'loss': 0.6073, 'learning_rate': 5.932725764767748e-05, 'epoch': 0.44}
{'loss': 1.0877, 'learning_rate': 5.927974026314091e-05, 'epoch': 0.44}
{'loss': 0.5998, 'learning_rate': 5.9232214196930014e-05, 'epoch': 0.44}
{'loss': 0.6227, 'learning_rate': 5.918467949350784e-05, 'epoch': 0.44}
 44%|     | 1431/3250 [4:14:52<5:41:03, 11.25s/it]                                                        44%|     | 1431/3250 [4:14:52<5:41:03, 11.25s/it] 44%|     | 1432/3250 [4:15:02<5:32:47, 10.98s/it]                                                        44%|     | 1432/3250 [4:15:02<5:32:47, 10.98s/it] 44%|     | 1433/3250 [4:15:13<5:26:52, 10.79s/it]                                                        44%|     | 1433/3250 [4:15:13<5:26:52, 10.79s/it] 44%|     | 1434/3250 [4:15:23<5:22:39, 10.66s/it]                                                        44%|     | 1434/3250 [4:15:23<5:22:39, 10.66s/it] 44%|     | 1435/3250 [4:15:34<5:19:39, 10.57s/it]                                                        44%|     | 1435/3250 [4:15:34<5:19:39, 10.57s/it] 44%|     | 1436/3250 [4:15:44<5:17:44, 10.51s/it]            {'loss': 0.6228, 'learning_rate': 5.913713619734558e-05, 'epoch': 0.44}
{'loss': 0.6139, 'learning_rate': 5.908958435292241e-05, 'epoch': 0.44}
{'loss': 0.5825, 'learning_rate': 5.904202400472553e-05, 'epoch': 0.44}
{'loss': 0.6387, 'learning_rate': 5.899445519725009e-05, 'epoch': 0.44}
{'loss': 0.6459, 'learning_rate': 5.894687797499916e-05, 'epoch': 0.44}
                                            44%|     | 1436/3250 [4:15:44<5:17:44, 10.51s/it] 44%|     | 1437/3250 [4:15:54<5:16:10, 10.46s/it]                                                        44%|     | 1437/3250 [4:15:54<5:16:10, 10.46s/it] 44%|     | 1438/3250 [4:16:05<5:14:57, 10.43s/it]                                                        44%|     | 1438/3250 [4:16:05<5:14:57, 10.43s/it] 44%|     | 1439/3250 [4:16:15<5:14:15, 10.41s/it]                                                        44%|     | 1439/3250 [4:16:15<5:14:15, 10.41s/it] 44%|     | 1440/3250 [4:16:25<5:14:00, 10.41s/it]                                                        44%|     | 1440/3250 [4:16:25<5:14:00, 10.41s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.7912823557853699, 'eval_runtime': 2.1147, 'eval_samples_per_second': 5.675, 'eval_steps_per_second': 1.419, 'epoch': 0.44}
                                                        44%|     | 1440/3250 [4:16:28<5:14:00, 10.41s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_10/tmp-checkpoint-1440
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1440I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in 
 /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1440
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1440
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1440
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1440/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1440/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1440/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1440/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.586, 'learning_rate': 5.889929238248368e-05, 'epoch': 0.44}
{'loss': 0.5759, 'learning_rate': 5.8851698464222416e-05, 'epoch': 0.44}
{'loss': 0.6049, 'learning_rate': 5.880409626474195e-05, 'epoch': 0.44}
{'loss': 0.5964, 'learning_rate': 5.8756485828576544e-05, 'epoch': 0.44}
{'loss': 0.6159, 'learning_rate': 5.870886720026825e-05, 'epoch': 0.44}
 44%|     | 1441/3250 [4:16:38<5:37:29, 11.19s/it]                                                        44%|     | 1441/3250 [4:16:38<5:37:29, 11.19s/it] 44%|     | 1442/3250 [4:16:49<5:30:08, 10.96s/it]                                                        44%|     | 1442/3250 [4:16:49<5:30:08, 10.96s/it] 44%|     | 1443/3250 [4:17:00<5:28:22, 10.90s/it]                                                        44%|     | 1443/3250 [4:17:00<5:28:22, 10.90s/it] 44%|     | 1444/3250 [4:17:10<5:23:23, 10.74s/it]                                                        44%|     | 1444/3250 [4:17:10<5:23:23, 10.74s/it] 44%|     | 1445/3250 [4:17:20<5:20:03, 10.64s/it]                                                        44%|     | 1445/3250 [4:17:20<5:20:03, 10.64s/it] 44%|     | 1446/3250 [4:17:31<5:17:42, 10.57s/it]            {'loss': 0.5874, 'learning_rate': 5.8661240424366735e-05, 'epoch': 0.44}
{'loss': 0.5962, 'learning_rate': 5.861360554542927e-05, 'epoch': 0.45}
{'loss': 0.5993, 'learning_rate': 5.8565962608020765e-05, 'epoch': 0.45}
{'loss': 0.6278, 'learning_rate': 5.851831165671363e-05, 'epoch': 0.45}
{'loss': 0.6142, 'learning_rate': 5.847065273608777e-05, 'epoch': 0.45}
                                            44%|     | 1446/3250 [4:17:31<5:17:42, 10.57s/it] 45%|     | 1447/3250 [4:17:41<5:15:58, 10.52s/it]                                                        45%|     | 1447/3250 [4:17:41<5:15:58, 10.52s/it] 45%|     | 1448/3250 [4:17:52<5:14:43, 10.48s/it]                                                        45%|     | 1448/3250 [4:17:52<5:14:43, 10.48s/it] 45%|     | 1449/3250 [4:18:02<5:13:41, 10.45s/it]                                                        45%|     | 1449/3250 [4:18:02<5:13:41, 10.45s/it] 45%|     | 1450/3250 [4:18:12<5:12:56, 10.43s/it]                                                        45%|     | 1450/3250 [4:18:12<5:12:56, 10.43s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.7902657389640808, 'eval_runtime': 2.1214, 'eval_samples_per_second': 5.657, 'eval_steps_per_second': 1.414, 'epoch': 0.45}
                                                        45%|     | 1450/3250 [4:18:14<5:12:56, 10.43s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_10/tmp-checkpoint-1450
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1450
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1450
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1450
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1450
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1450/pytorch_model.bin
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1450/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1450/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1450/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.5918, 'learning_rate': 5.8422985890730576e-05, 'epoch': 0.45}
{'loss': 0.6148, 'learning_rate': 5.837531116523682e-05, 'epoch': 0.45}
{'loss': 0.6329, 'learning_rate': 5.832762860420868e-05, 'epoch': 0.45}
{'loss': 0.5559, 'learning_rate': 5.827993825225561e-05, 'epoch': 0.45}
{'loss': 0.6383, 'learning_rate': 5.823224015399442e-05, 'epoch': 0.45}
 45%|     | 1451/3250 [4:18:25<5:35:38, 11.19s/it]                                                        45%|     | 1451/3250 [4:18:25<5:35:38, 11.19s/it] 45%|     | 1452/3250 [4:18:36<5:28:21, 10.96s/it]                                                        45%|     | 1452/3250 [4:18:36<5:28:21, 10.96s/it] 45%|     | 1453/3250 [4:18:46<5:23:08, 10.79s/it]                                                        45%|     | 1453/3250 [4:18:46<5:23:08, 10.79s/it] 45%|     | 1454/3250 [4:18:56<5:19:21, 10.67s/it]                                                        45%|     | 1454/3250 [4:18:56<5:19:21, 10.67s/it] 45%|     | 1455/3250 [4:19:07<5:16:41, 10.59s/it]                                                        45%|     | 1455/3250 [4:19:07<5:16:41, 10.59s/it] 45%|     | 1456/3250 [4:19:17<5:14:47, 10.53s/it]            {'loss': 0.5912, 'learning_rate': 5.8184534354049104e-05, 'epoch': 0.45}
{'loss': 0.601, 'learning_rate': 5.813682089705092e-05, 'epoch': 0.45}
{'loss': 0.5856, 'learning_rate': 5.808909982763825e-05, 'epoch': 0.45}
{'loss': 0.5874, 'learning_rate': 5.8041371190456595e-05, 'epoch': 0.45}
{'loss': 0.6108, 'learning_rate': 5.799363503015856e-05, 'epoch': 0.45}
                                            45%|     | 1456/3250 [4:19:17<5:14:47, 10.53s/it] 45%|     | 1457/3250 [4:19:28<5:13:14, 10.48s/it]                                                        45%|     | 1457/3250 [4:19:28<5:13:14, 10.48s/it] 45%|     | 1458/3250 [4:19:38<5:12:13, 10.45s/it]                                                        45%|     | 1458/3250 [4:19:38<5:12:13, 10.45s/it] 45%|     | 1459/3250 [4:19:49<5:15:55, 10.58s/it]                                                        45%|     | 1459/3250 [4:19:49<5:15:55, 10.58s/it] 45%|     | 1460/3250 [4:19:59<5:14:13, 10.53s/it]                                                        45%|     | 1460/3250 [4:19:59<5:14:13, 10.53s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.7902506589889526, 'eval_runtime': 2.1151, 'eval_samples_per_second': 5.673, 'eval_steps_per_second': 1.418, 'epoch': 0.45}
                                                        45%|     | 1460/3250 [4:20:01<5:14:13, 10.53s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_10/tmp-checkpoint-1460
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1460
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1460
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1460
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1460
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1460/pytorch_model.bin
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1460/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1460/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1460/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.5956, 'learning_rate': 5.794589139140381e-05, 'epoch': 0.45}
{'loss': 0.6133, 'learning_rate': 5.789814031885894e-05, 'epoch': 0.45}
{'loss': 1.0876, 'learning_rate': 5.7850381857197525e-05, 'epoch': 0.45}
{'loss': 0.5794, 'learning_rate': 5.7802616051100086e-05, 'epoch': 0.45}
{'loss': 0.6141, 'learning_rate': 5.775484294525399e-05, 'epoch': 0.45}
 45%|     | 1461/3250 [4:20:12<5:35:49, 11.26s/it]                                                        45%|     | 1461/3250 [4:20:12<5:35:49, 11.26s/it] 45%|     | 1462/3250 [4:20:23<5:27:53, 11.00s/it]                                                        45%|     | 1462/3250 [4:20:23<5:27:53, 11.00s/it] 45%|     | 1463/3250 [4:20:33<5:22:02, 10.81s/it]                                                        45%|     | 1463/3250 [4:20:33<5:22:02, 10.81s/it] 45%|     | 1464/3250 [4:20:43<5:18:02, 10.68s/it]                                                        45%|     | 1464/3250 [4:20:43<5:18:02, 10.68s/it] 45%|     | 1465/3250 [4:20:54<5:15:09, 10.59s/it]                                                        45%|     | 1465/3250 [4:20:54<5:15:09, 10.59s/it] 45%|     | 1466/3250 [4:21:04<5:13:16, 10.54s/it]            {'loss': 0.6139, 'learning_rate': 5.770706258435342e-05, 'epoch': 0.45}
{'loss': 0.621, 'learning_rate': 5.765927501309938e-05, 'epoch': 0.45}
{'loss': 0.5593, 'learning_rate': 5.761148027619958e-05, 'epoch': 0.45}
{'loss': 0.5983, 'learning_rate': 5.756367841836847e-05, 'epoch': 0.45}
{'loss': 0.6498, 'learning_rate': 5.7515869484327155e-05, 'epoch': 0.45}
                                            45%|     | 1466/3250 [4:21:04<5:13:16, 10.54s/it] 45%|     | 1467/3250 [4:21:15<5:11:45, 10.49s/it]                                                        45%|     | 1467/3250 [4:21:15<5:11:45, 10.49s/it] 45%|     | 1468/3250 [4:21:25<5:10:39, 10.46s/it]                                                        45%|     | 1468/3250 [4:21:25<5:10:39, 10.46s/it] 45%|     | 1469/3250 [4:21:35<5:09:51, 10.44s/it]                                                        45%|     | 1469/3250 [4:21:35<5:09:51, 10.44s/it] 45%|     | 1470/3250 [4:21:46<5:09:16, 10.42s/it]                                                        45%|     | 1470/3250 [4:21:46<5:09:16, 10.42s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.792096734046936, 'eval_runtime': 2.1185, 'eval_samples_per_second': 5.664, 'eval_steps_per_second': 1.416, 'epoch': 0.45}
                                                        45%|     | 1470/3250 [4:21:48<5:09:16, 10.42s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_10/tmp-checkpoint-1470
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1470
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1470
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1470
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1470
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1470/pytorch_model.bin
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1470/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1470/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1470/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.5987, 'learning_rate': 5.746805351880334e-05, 'epoch': 0.45}
{'loss': 0.5938, 'learning_rate': 5.742023056653131e-05, 'epoch': 0.45}
{'loss': 0.5956, 'learning_rate': 5.73724006722519e-05, 'epoch': 0.45}
{'loss': 0.5844, 'learning_rate': 5.732456388071247e-05, 'epoch': 0.45}
{'loss': 0.6135, 'learning_rate': 5.7276720236666746e-05, 'epoch': 0.45}
 45%|     | 1471/3250 [4:21:59<5:32:11, 11.20s/it]                                                        45%|     | 1471/3250 [4:21:59<5:32:11, 11.20s/it] 45%|     | 1472/3250 [4:22:09<5:24:50, 10.96s/it]                                                        45%|     | 1472/3250 [4:22:09<5:24:50, 10.96s/it] 45%|     | 1473/3250 [4:22:20<5:19:32, 10.79s/it]                                                        45%|     | 1473/3250 [4:22:20<5:19:32, 10.79s/it] 45%|     | 1474/3250 [4:22:30<5:15:45, 10.67s/it]                                                        45%|     | 1474/3250 [4:22:30<5:15:45, 10.67s/it] 45%|     | 1475/3250 [4:22:40<5:13:00, 10.58s/it]                                                        45%|     | 1475/3250 [4:22:40<5:13:00, 10.58s/it] 45%|     | 1476/3250 [4:22:51<5:15:06, 10.66s/it]            {'loss': 0.5755, 'learning_rate': 5.722886978487496e-05, 'epoch': 0.45}
{'loss': 0.5958, 'learning_rate': 5.7181012570103656e-05, 'epoch': 0.45}
{'loss': 0.5987, 'learning_rate': 5.713314863712571e-05, 'epoch': 0.45}
{'loss': 0.598, 'learning_rate': 5.70852780307203e-05, 'epoch': 0.46}
{'loss': 0.624, 'learning_rate': 5.703740079567286e-05, 'epoch': 0.46}
                                            45%|     | 1476/3250 [4:22:51<5:15:06, 10.66s/it] 45%|     | 1477/3250 [4:23:02<5:12:37, 10.58s/it]                                                        45%|     | 1477/3250 [4:23:02<5:12:37, 10.58s/it] 45%|     | 1478/3250 [4:23:12<5:10:37, 10.52s/it]                                                        45%|     | 1478/3250 [4:23:12<5:10:37, 10.52s/it] 46%|     | 1479/3250 [4:23:22<5:09:19, 10.48s/it]                                                        46%|     | 1479/3250 [4:23:22<5:09:19, 10.48s/it] 46%|     | 1480/3250 [4:23:33<5:08:09, 10.45s/it]                                                        46%|     | 1480/3250 [4:23:33<5:08:09, 10.45s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.7883491516113281, 'eval_runtime': 2.1686, 'eval_samples_per_second': 5.534, 'eval_steps_per_second': 1.383, 'epoch': 0.46}
                                                        46%|     | 1480/3250 [4:23:35<5:08:09, 10.45s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_10/tmp-checkpoint-1480
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1480I AM HERE AND I AM SAVING THE MODEL

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1480
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1480
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1480
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1480/pytorch_model.binthe pytorch model path is
 /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1480/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1480/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1480/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.6109, 'learning_rate': 5.698951697677498e-05, 'epoch': 0.46}
{'loss': 0.5899, 'learning_rate': 5.694162661882444e-05, 'epoch': 0.46}
{'loss': 0.6156, 'learning_rate': 5.6893729766625146e-05, 'epoch': 0.46}
{'loss': 0.5773, 'learning_rate': 5.684582646498706e-05, 'epoch': 0.46}
{'loss': 0.6138, 'learning_rate': 5.679791675872619e-05, 'epoch': 0.46}
 46%|     | 1481/3250 [4:23:46<5:31:20, 11.24s/it]                                                        46%|     | 1481/3250 [4:23:46<5:31:20, 11.24s/it] 46%|     | 1482/3250 [4:23:56<5:23:38, 10.98s/it]                                                        46%|     | 1482/3250 [4:23:56<5:23:38, 10.98s/it] 46%|     | 1483/3250 [4:24:07<5:18:19, 10.81s/it]                                                        46%|     | 1483/3250 [4:24:07<5:18:19, 10.81s/it] 46%|     | 1484/3250 [4:24:17<5:14:28, 10.68s/it]                                                        46%|     | 1484/3250 [4:24:17<5:14:28, 10.68s/it] 46%|     | 1485/3250 [4:24:27<5:11:31, 10.59s/it]                                                        46%|     | 1485/3250 [4:24:27<5:11:31, 10.59s/it] 46%|     | 1486/3250 [4:24:38<5:09:34, 10.53s/it]            {'loss': 0.5958, 'learning_rate': 5.675000069266451e-05, 'epoch': 0.46}
{'loss': 0.5766, 'learning_rate': 5.6702078311629995e-05, 'epoch': 0.46}
{'loss': 0.573, 'learning_rate': 5.6654149660456455e-05, 'epoch': 0.46}
{'loss': 0.5739, 'learning_rate': 5.660621478398367e-05, 'epoch': 0.46}
{'loss': 0.6127, 'learning_rate': 5.655827372705712e-05, 'epoch': 0.46}
                                            46%|     | 1486/3250 [4:24:38<5:09:34, 10.53s/it] 46%|     | 1487/3250 [4:24:48<5:08:00, 10.48s/it]                                                        46%|     | 1487/3250 [4:24:48<5:08:00, 10.48s/it] 46%|     | 1488/3250 [4:24:59<5:06:58, 10.45s/it]                                                        46%|     | 1488/3250 [4:24:59<5:06:58, 10.45s/it] 46%|     | 1489/3250 [4:25:09<5:06:11, 10.43s/it]                                                        46%|     | 1489/3250 [4:25:09<5:06:11, 10.43s/it] 46%|     | 1490/3250 [4:25:19<5:05:31, 10.42s/it]                                                        46%|     | 1490/3250 [4:25:19<5:05:31, 10.42s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.787171483039856, 'eval_runtime': 2.1186, 'eval_samples_per_second': 5.664, 'eval_steps_per_second': 1.416, 'epoch': 0.46}
                                                        46%|     | 1490/3250 [4:25:21<5:05:31, 10.42s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_10/tmp-checkpoint-1490
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1490
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1490
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1490
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1490
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1490/pytorch_model.binthe pytorch model path is
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1490/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1490/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1490/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.5756, 'learning_rate': 5.651032653452817e-05, 'epoch': 0.46}
{'loss': 0.6222, 'learning_rate': 5.6462373251253875e-05, 'epoch': 0.46}
{'loss': 1.0874, 'learning_rate': 5.641441392209699e-05, 'epoch': 0.46}
{'loss': 0.5742, 'learning_rate': 5.636644859192594e-05, 'epoch': 0.46}
{'loss': 0.6018, 'learning_rate': 5.6318477305614756e-05, 'epoch': 0.46}
 46%|     | 1491/3250 [4:25:32<5:27:40, 11.18s/it]                                                        46%|     | 1491/3250 [4:25:32<5:27:40, 11.18s/it] 46%|     | 1492/3250 [4:25:43<5:26:59, 11.16s/it]                                                        46%|     | 1492/3250 [4:25:43<5:26:59, 11.16s/it] 46%|     | 1493/3250 [4:25:54<5:20:04, 10.93s/it]                                                        46%|     | 1493/3250 [4:25:54<5:20:04, 10.93s/it] 46%|     | 1494/3250 [4:26:04<5:15:01, 10.76s/it]                                                        46%|     | 1494/3250 [4:26:04<5:15:01, 10.76s/it] 46%|     | 1495/3250 [4:26:14<5:11:22, 10.65s/it]                                                        46%|     | 1495/3250 [4:26:14<5:11:22, 10.65s/it] 46%|     | 1496/3250 [4:26:25<5:08:51, 10.57s/it]            {'loss': 0.6059, 'learning_rate': 5.6270500108043046e-05, 'epoch': 0.46}
{'loss': 0.5994, 'learning_rate': 5.6222517044095945e-05, 'epoch': 0.46}
{'loss': 0.5779, 'learning_rate': 5.6174528158664096e-05, 'epoch': 0.46}
{'loss': 0.5825, 'learning_rate': 5.612653349664353e-05, 'epoch': 0.46}
{'loss': 0.6364, 'learning_rate': 5.6078533102935745e-05, 'epoch': 0.46}
                                            46%|     | 1496/3250 [4:26:25<5:08:51, 10.57s/it] 46%|     | 1497/3250 [4:26:35<5:07:02, 10.51s/it]                                                        46%|     | 1497/3250 [4:26:35<5:07:02, 10.51s/it] 46%|     | 1498/3250 [4:26:46<5:05:43, 10.47s/it]                                                        46%|     | 1498/3250 [4:26:46<5:05:43, 10.47s/it] 46%|     | 1499/3250 [4:26:56<5:04:35, 10.44s/it]                                                        46%|     | 1499/3250 [4:26:56<5:04:35, 10.44s/it] 46%|     | 1500/3250 [4:27:06<5:03:51, 10.42s/it]                                                        46%|     | 1500/3250 [4:27:06<5:03:51, 10.42s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.7895005345344543, 'eval_runtime': 2.1171, 'eval_samples_per_second': 5.668, 'eval_steps_per_second': 1.417, 'epoch': 0.46}
                                                        46%|     | 1500/3250 [4:27:08<5:03:51, 10.42s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_10/tmp-checkpoint-1500
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1500
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1500
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1500
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1500
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1500/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1500/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1500/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1500/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.6136, 'learning_rate': 5.60305270224476e-05, 'epoch': 0.46}
{'loss': 0.6042, 'learning_rate': 5.598251530009121e-05, 'epoch': 0.46}
{'loss': 0.5573, 'learning_rate': 5.5934497980784054e-05, 'epoch': 0.46}
{'loss': 0.611, 'learning_rate': 5.5886475109448765e-05, 'epoch': 0.46}
{'loss': 0.5961, 'learning_rate': 5.583844673101323e-05, 'epoch': 0.46}
 46%|     | 1501/3250 [4:27:19<5:26:12, 11.19s/it]                                                        46%|     | 1501/3250 [4:27:19<5:26:12, 11.19s/it] 46%|     | 1502/3250 [4:27:30<5:19:43, 10.97s/it]                                                        46%|     | 1502/3250 [4:27:30<5:19:43, 10.97s/it] 46%|     | 1503/3250 [4:27:40<5:14:23, 10.80s/it]                                                        46%|     | 1503/3250 [4:27:40<5:14:23, 10.80s/it] 46%|     | 1504/3250 [4:27:51<5:10:42, 10.68s/it]                                                        46%|     | 1504/3250 [4:27:51<5:10:42, 10.68s/it] 46%|     | 1505/3250 [4:28:01<5:07:48, 10.58s/it]                                                        46%|     | 1505/3250 [4:28:01<5:07:48, 10.58s/it] 46%|     | 1506/3250 [4:28:13<5:19:45, 11.00s/it]            {'loss': 0.6008, 'learning_rate': 5.5790412890410446e-05, 'epoch': 0.46}
{'loss': 0.5779, 'learning_rate': 5.574237363257858e-05, 'epoch': 0.46}
{'loss': 0.603, 'learning_rate': 5.56943290024608e-05, 'epoch': 0.46}
{'loss': 0.5872, 'learning_rate': 5.564627904500533e-05, 'epoch': 0.46}
{'loss': 0.6188, 'learning_rate': 5.559822380516539e-05, 'epoch': 0.46}
                                            46%|     | 1506/3250 [4:28:13<5:19:45, 11.00s/it] 46%|     | 1507/3250 [4:28:23<5:15:11, 10.85s/it]                                                        46%|     | 1507/3250 [4:28:23<5:15:11, 10.85s/it] 46%|     | 1508/3250 [4:28:34<5:11:04, 10.71s/it]                                                        46%|     | 1508/3250 [4:28:34<5:11:04, 10.71s/it] 46%|     | 1509/3250 [4:28:45<5:13:47, 10.81s/it]                                                        46%|     | 1509/3250 [4:28:45<5:13:47, 10.81s/it] 46%|     | 1510/3250 [4:28:55<5:09:48, 10.68s/it]                                                        46%|     | 1510/3250 [4:28:55<5:09:48, 10.68s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.7891690731048584, 'eval_runtime': 2.3581, 'eval_samples_per_second': 5.089, 'eval_steps_per_second': 1.272, 'epoch': 0.46}
                                                        46%|     | 1510/3250 [4:28:58<5:09:48, 10.68s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_10/tmp-checkpoint-1510
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1510
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1510
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1510
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1510
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1510/pytorch_model.bin
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1510/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1510/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1510/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.6216, 'learning_rate': 5.5550163327899126e-05, 'epoch': 0.46}
{'loss': 0.572, 'learning_rate': 5.550209765816958e-05, 'epoch': 0.47}
{'loss': 0.6124, 'learning_rate': 5.545402684094467e-05, 'epoch': 0.47}
{'loss': 0.6046, 'learning_rate': 5.540595092119709e-05, 'epoch': 0.47}
{'loss': 0.5672, 'learning_rate': 5.535786994390436e-05, 'epoch': 0.47}
 46%|     | 1511/3250 [4:29:09<5:32:19, 11.47s/it]                                                        46%|     | 1511/3250 [4:29:09<5:32:19, 11.47s/it] 47%|     | 1512/3250 [4:29:19<5:22:48, 11.14s/it]                                                        47%|     | 1512/3250 [4:29:19<5:22:48, 11.14s/it] 47%|     | 1513/3250 [4:29:29<5:16:07, 10.92s/it]                                                        47%|     | 1513/3250 [4:29:29<5:16:07, 10.92s/it] 47%|     | 1514/3250 [4:29:40<5:11:23, 10.76s/it]                                                        47%|     | 1514/3250 [4:29:40<5:11:23, 10.76s/it] 47%|     | 1515/3250 [4:29:50<5:08:07, 10.66s/it]                                                        47%|     | 1515/3250 [4:29:50<5:08:07, 10.66s/it] 47%|     | 1516/3250 [4:30:01<5:05:34, 10.57s/it]            {'loss': 0.6265, 'learning_rate': 5.530978395404872e-05, 'epoch': 0.47}
{'loss': 0.5782, 'learning_rate': 5.526169299661705e-05, 'epoch': 0.47}
{'loss': 0.5706, 'learning_rate': 5.521359711660094e-05, 'epoch': 0.47}
{'loss': 0.5782, 'learning_rate': 5.516549635899655e-05, 'epoch': 0.47}
{'loss': 0.5913, 'learning_rate': 5.511739076880461e-05, 'epoch': 0.47}
                                            47%|     | 1516/3250 [4:30:01<5:05:34, 10.57s/it] 47%|     | 1517/3250 [4:30:11<5:03:52, 10.52s/it]                                                        47%|     | 1517/3250 [4:30:11<5:03:52, 10.52s/it] 47%|     | 1518/3250 [4:30:21<5:02:31, 10.48s/it]                                                        47%|     | 1518/3250 [4:30:21<5:02:31, 10.48s/it] 47%|     | 1519/3250 [4:30:32<5:01:36, 10.45s/it]                                                        47%|     | 1519/3250 [4:30:32<5:01:36, 10.45s/it] 47%|     | 1520/3250 [4:30:42<5:01:13, 10.45s/it]                                                        47%|     | 1520/3250 [4:30:42<5:01:13, 10.45s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.7906097173690796, 'eval_runtime': 2.191, 'eval_samples_per_second': 5.477, 'eval_steps_per_second': 1.369, 'epoch': 0.47}
                                                        47%|     | 1520/3250 [4:30:44<5:01:13, 10.45s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_10/tmp-checkpoint-1520
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1520 /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1520

I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1520
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1520
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1520/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1520/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1520/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1520/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.5942, 'learning_rate': 5.50692803910304e-05, 'epoch': 0.47}
{'loss': 0.614, 'learning_rate': 5.502116527068363e-05, 'epoch': 0.47}
{'loss': 0.6354, 'learning_rate': 5.497304545277846e-05, 'epoch': 0.47}
{'loss': 1.04, 'learning_rate': 5.492492098233346e-05, 'epoch': 0.47}
{'loss': 0.5819, 'learning_rate': 5.487679190437158e-05, 'epoch': 0.47}
 47%|     | 1521/3250 [4:30:55<5:23:49, 11.24s/it]                                                        47%|     | 1521/3250 [4:30:55<5:23:49, 11.24s/it] 47%|     | 1522/3250 [4:31:06<5:16:36, 10.99s/it]                                                        47%|     | 1522/3250 [4:31:06<5:16:36, 10.99s/it] 47%|     | 1523/3250 [4:31:16<5:11:22, 10.82s/it]                                                        47%|     | 1523/3250 [4:31:16<5:11:22, 10.82s/it] 47%|     | 1524/3250 [4:31:26<5:07:17, 10.68s/it]                                                        47%|     | 1524/3250 [4:31:26<5:07:17, 10.68s/it] 47%|     | 1525/3250 [4:31:37<5:06:41, 10.67s/it]                                                        47%|     | 1525/3250 [4:31:37<5:06:41, 10.67s/it] 47%|     | 1526/3250 [4:31:47<5:04:09, 10.59s/it]            {'loss': 0.6018, 'learning_rate': 5.482865826392001e-05, 'epoch': 0.47}
{'loss': 0.6009, 'learning_rate': 5.4780520106010256e-05, 'epoch': 0.47}
{'loss': 0.5768, 'learning_rate': 5.473237747567805e-05, 'epoch': 0.47}
{'loss': 0.5648, 'learning_rate': 5.468423041796331e-05, 'epoch': 0.47}
{'loss': 0.6224, 'learning_rate': 5.463607897791006e-05, 'epoch': 0.47}
                                            47%|     | 1526/3250 [4:31:47<5:04:09, 10.59s/it] 47%|     | 1527/3250 [4:31:58<5:02:20, 10.53s/it]                                                        47%|     | 1527/3250 [4:31:58<5:02:20, 10.53s/it] 47%|     | 1528/3250 [4:32:08<5:01:01, 10.49s/it]                                                        47%|     | 1528/3250 [4:32:08<5:01:01, 10.49s/it] 47%|     | 1529/3250 [4:32:19<4:59:59, 10.46s/it]                                                        47%|     | 1529/3250 [4:32:19<4:59:59, 10.46s/it] 47%|     | 1530/3250 [4:32:29<4:59:11, 10.44s/it]                                                        47%|     | 1530/3250 [4:32:29<4:59:11, 10.44s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.7902666330337524, 'eval_runtime': 2.1221, 'eval_samples_per_second': 5.655, 'eval_steps_per_second': 1.414, 'epoch': 0.47}
                                                        47%|     | 1530/3250 [4:32:31<4:59:11, 10.44s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_10/tmp-checkpoint-1530
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1530
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1530
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1530
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1530
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1530/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1530/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1530/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1530/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.6173, 'learning_rate': 5.458792320056645e-05, 'epoch': 0.47}
{'loss': 0.5758, 'learning_rate': 5.45397631309847e-05, 'epoch': 0.47}
{'loss': 0.5655, 'learning_rate': 5.449159881422101e-05, 'epoch': 0.47}
{'loss': 0.5864, 'learning_rate': 5.444343029533562e-05, 'epoch': 0.47}
{'loss': 0.5799, 'learning_rate': 5.439525761939261e-05, 'epoch': 0.47}
 47%|     | 1531/3250 [4:32:42<5:20:41, 11.19s/it]                                                        47%|     | 1531/3250 [4:32:42<5:20:41, 11.19s/it] 47%|     | 1532/3250 [4:32:52<5:14:43, 10.99s/it]                                                        47%|     | 1532/3250 [4:32:52<5:14:43, 10.99s/it] 47%|     | 1533/3250 [4:33:03<5:09:42, 10.82s/it]                                                        47%|     | 1533/3250 [4:33:03<5:09:42, 10.82s/it] 47%|     | 1534/3250 [4:33:14<5:15:36, 11.03s/it]                                                        47%|     | 1534/3250 [4:33:14<5:15:36, 11.03s/it] 47%|     | 1535/3250 [4:33:25<5:09:41, 10.83s/it]                                                        47%|     | 1535/3250 [4:33:25<5:09:41, 10.83s/it] 47%|     | 1536/3250 [4:33:35<5:05:50, 10.71s/it]            {'loss': 0.6073, 'learning_rate': 5.4347080831460015e-05, 'epoch': 0.47}
{'loss': 0.5678, 'learning_rate': 5.4298899976609717e-05, 'epoch': 0.47}
{'loss': 0.5812, 'learning_rate': 5.425071509991737e-05, 'epoch': 0.47}
{'loss': 0.6042, 'learning_rate': 5.420252624646238e-05, 'epoch': 0.47}
{'loss': 0.6095, 'learning_rate': 5.415433346132793e-05, 'epoch': 0.47}
                                            47%|     | 1536/3250 [4:33:35<5:05:50, 10.71s/it] 47%|     | 1537/3250 [4:33:46<5:02:44, 10.60s/it]                                                        47%|     | 1537/3250 [4:33:46<5:02:44, 10.60s/it] 47%|     | 1538/3250 [4:33:56<5:01:06, 10.55s/it]                                                        47%|     | 1538/3250 [4:33:56<5:01:06, 10.55s/it] 47%|     | 1539/3250 [4:34:06<4:59:28, 10.50s/it]                                                        47%|     | 1539/3250 [4:34:06<4:59:28, 10.50s/it] 47%|     | 1540/3250 [4:34:17<4:58:16, 10.47s/it]                                                        47%|     | 1540/3250 [4:34:17<4:58:16, 10.47s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.786724328994751, 'eval_runtime': 2.1166, 'eval_samples_per_second': 5.669, 'eval_steps_per_second': 1.417, 'epoch': 0.47}
                                                        47%|     | 1540/3250 [4:34:19<4:58:16, 10.47s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_10/tmp-checkpoint-1540
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1540
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1540
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1540
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1540
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1540/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1540/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1540/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1540/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.5921, 'learning_rate': 5.410613678960084e-05, 'epoch': 0.47}
{'loss': 0.5934, 'learning_rate': 5.4057936276371565e-05, 'epoch': 0.47}
{'loss': 0.6016, 'learning_rate': 5.400973196673419e-05, 'epoch': 0.47}
{'loss': 0.5942, 'learning_rate': 5.3961523905786295e-05, 'epoch': 0.48}
{'loss': 0.5435, 'learning_rate': 5.3913312138629014e-05, 'epoch': 0.48}
 47%|     | 1541/3250 [4:34:30<5:23:09, 11.35s/it]                                                        47%|     | 1541/3250 [4:34:30<5:23:09, 11.35s/it] 47%|     | 1542/3250 [4:34:41<5:14:53, 11.06s/it]                                                        47%|     | 1542/3250 [4:34:41<5:14:53, 11.06s/it] 47%|     | 1543/3250 [4:34:51<5:09:01, 10.86s/it]                                                        47%|     | 1543/3250 [4:34:51<5:09:01, 10.86s/it] 48%|     | 1544/3250 [4:35:01<5:04:53, 10.72s/it]                                                        48%|     | 1544/3250 [4:35:01<5:04:53, 10.72s/it] 48%|     | 1545/3250 [4:35:12<5:01:57, 10.63s/it]                                                        48%|     | 1545/3250 [4:35:12<5:01:57, 10.63s/it] 48%|     | 1546/3250 [4:35:22<4:59:48, 10.56s/it]            {'loss': 0.6204, 'learning_rate': 5.386509671036695e-05, 'epoch': 0.48}
{'loss': 0.5767, 'learning_rate': 5.38168776661081e-05, 'epoch': 0.48}
{'loss': 0.5756, 'learning_rate': 5.376865505096385e-05, 'epoch': 0.48}
{'loss': 0.5587, 'learning_rate': 5.372042891004896e-05, 'epoch': 0.48}
{'loss': 0.5786, 'learning_rate': 5.367219928848145e-05, 'epoch': 0.48}
                                            48%|     | 1546/3250 [4:35:22<4:59:48, 10.56s/it] 48%|     | 1547/3250 [4:35:33<4:58:10, 10.51s/it]                                                        48%|     | 1547/3250 [4:35:33<4:58:10, 10.51s/it] 48%|     | 1548/3250 [4:35:43<4:57:12, 10.48s/it]                                                        48%|     | 1548/3250 [4:35:43<4:57:12, 10.48s/it] 48%|     | 1549/3250 [4:35:53<4:56:28, 10.46s/it]                                                        48%|     | 1549/3250 [4:35:53<4:56:28, 10.46s/it] 48%|     | 1550/3250 [4:36:04<4:55:46, 10.44s/it]                                                        48%|     | 1550/3250 [4:36:04<4:55:46, 10.44s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.7880136370658875, 'eval_runtime': 2.1154, 'eval_samples_per_second': 5.673, 'eval_steps_per_second': 1.418, 'epoch': 0.48}
                                                        48%|     | 1550/3250 [4:36:06<4:55:46, 10.44s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_10/tmp-checkpoint-1550
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1550
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1550
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1550
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1550
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1550/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1550/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1550/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1550/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.5978, 'learning_rate': 5.3623966231382615e-05, 'epoch': 0.48}
{'loss': 0.5782, 'learning_rate': 5.357572978387697e-05, 'epoch': 0.48}
{'loss': 0.604, 'learning_rate': 5.3527489991092186e-05, 'epoch': 0.48}
{'loss': 1.0773, 'learning_rate': 5.3479246898159063e-05, 'epoch': 0.48}
{'loss': 0.571, 'learning_rate': 5.3431000550211505e-05, 'epoch': 0.48}
 48%|     | 1551/3250 [4:36:17<5:18:15, 11.24s/it]                                                        48%|     | 1551/3250 [4:36:17<5:18:15, 11.24s/it] 48%|     | 1552/3250 [4:36:27<5:11:06, 10.99s/it]                                                        48%|     | 1552/3250 [4:36:27<5:11:06, 10.99s/it] 48%|     | 1553/3250 [4:36:38<5:05:59, 10.82s/it]                                                        48%|     | 1553/3250 [4:36:38<5:05:59, 10.82s/it] 48%|     | 1554/3250 [4:36:48<5:02:26, 10.70s/it]                                                        48%|     | 1554/3250 [4:36:48<5:02:26, 10.70s/it] 48%|     | 1555/3250 [4:36:59<4:59:40, 10.61s/it]                                                        48%|     | 1555/3250 [4:36:59<4:59:40, 10.61s/it] 48%|     | 1556/3250 [4:37:09<4:57:35, 10.54s/it]            {'loss': 0.6016, 'learning_rate': 5.338275099238647e-05, 'epoch': 0.48}
{'loss': 0.595, 'learning_rate': 5.333449826982385e-05, 'epoch': 0.48}
{'loss': 0.6107, 'learning_rate': 5.328624242766661e-05, 'epoch': 0.48}
{'loss': 0.5488, 'learning_rate': 5.323798351106052e-05, 'epoch': 0.48}
{'loss': 0.5871, 'learning_rate': 5.31897215651543e-05, 'epoch': 0.48}
                                            48%|     | 1556/3250 [4:37:09<4:57:35, 10.54s/it] 48%|     | 1557/3250 [4:37:19<4:56:14, 10.50s/it]                                                        48%|     | 1557/3250 [4:37:19<4:56:14, 10.50s/it] 48%|     | 1558/3250 [4:37:30<4:57:13, 10.54s/it]                                                        48%|     | 1558/3250 [4:37:30<4:57:13, 10.54s/it] 48%|     | 1559/3250 [4:37:40<4:55:58, 10.50s/it]                                                        48%|     | 1559/3250 [4:37:40<4:55:58, 10.50s/it] 48%|     | 1560/3250 [4:37:51<4:54:55, 10.47s/it]                                                        48%|     | 1560/3250 [4:37:51<4:54:55, 10.47s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.7900221943855286, 'eval_runtime': 2.113, 'eval_samples_per_second': 5.679, 'eval_steps_per_second': 1.42, 'epoch': 0.48}
                                                        48%|     | 1560/3250 [4:37:53<4:54:55, 10.47s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_10/tmp-checkpoint-1560
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  I AM HERE AND I AM SAVING THE MODEL
/projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1560the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1560

I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1560
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1560
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1560/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1560/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1560/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1560/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.6352, 'learning_rate': 5.314145663509951e-05, 'epoch': 0.48}
{'loss': 0.5912, 'learning_rate': 5.309318876605043e-05, 'epoch': 0.48}
{'loss': 0.5757, 'learning_rate': 5.3044918003164156e-05, 'epoch': 0.48}
{'loss': 0.5842, 'learning_rate': 5.299664439160047e-05, 'epoch': 0.48}
{'loss': 0.5754, 'learning_rate': 5.294836797652182e-05, 'epoch': 0.48}
 48%|     | 1561/3250 [4:38:04<5:16:39, 11.25s/it]                                                        48%|     | 1561/3250 [4:38:04<5:16:39, 11.25s/it] 48%|     | 1562/3250 [4:38:14<5:09:12, 10.99s/it]                                                        48%|     | 1562/3250 [4:38:14<5:09:12, 10.99s/it] 48%|     | 1563/3250 [4:38:25<5:03:58, 10.81s/it]                                                        48%|     | 1563/3250 [4:38:25<5:03:58, 10.81s/it] 48%|     | 1564/3250 [4:38:35<5:00:21, 10.69s/it]                                                        48%|     | 1564/3250 [4:38:35<5:00:21, 10.69s/it] 48%|     | 1565/3250 [4:38:45<4:57:47, 10.60s/it]                                                        48%|     | 1565/3250 [4:38:45<4:57:47, 10.60s/it] 48%|     | 1566/3250 [4:38:56<4:55:50, 10.54s/it]            {'loss': 0.6035, 'learning_rate': 5.290008880309326e-05, 'epoch': 0.48}
{'loss': 0.5772, 'learning_rate': 5.2851806916482464e-05, 'epoch': 0.48}
{'loss': 0.5922, 'learning_rate': 5.2803522361859594e-05, 'epoch': 0.48}
{'loss': 0.5735, 'learning_rate': 5.275523518439735e-05, 'epoch': 0.48}
{'loss': 0.5924, 'learning_rate': 5.270694542927088e-05, 'epoch': 0.48}
                                            48%|     | 1566/3250 [4:38:56<4:55:50, 10.54s/it] 48%|     | 1567/3250 [4:39:06<4:54:36, 10.50s/it]                                                        48%|     | 1567/3250 [4:39:06<4:54:36, 10.50s/it] 48%|     | 1568/3250 [4:39:17<4:53:37, 10.47s/it]                                                        48%|     | 1568/3250 [4:39:17<4:53:37, 10.47s/it] 48%|     | 1569/3250 [4:39:27<4:52:43, 10.45s/it]                                                        48%|     | 1569/3250 [4:39:27<4:52:43, 10.45s/it] 48%|     | 1570/3250 [4:39:37<4:52:05, 10.43s/it]                                                        48%|     | 1570/3250 [4:39:37<4:52:05, 10.43s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.7856544852256775, 'eval_runtime': 2.3341, 'eval_samples_per_second': 5.141, 'eval_steps_per_second': 1.285, 'epoch': 0.48}
                                                        48%|     | 1570/3250 [4:39:40<4:52:05, 10.43s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_10/tmp-checkpoint-1570
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1570I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1570

I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1570
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1570
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1570/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1570/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1570/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1570/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.6046, 'learning_rate': 5.265865314165771e-05, 'epoch': 0.48}
{'loss': 0.5986, 'learning_rate': 5.261035836673779e-05, 'epoch': 0.48}
{'loss': 0.5728, 'learning_rate': 5.256206114969333e-05, 'epoch': 0.48}
{'loss': 0.6032, 'learning_rate': 5.251376153570889e-05, 'epoch': 0.48}
{'loss': 0.5644, 'learning_rate': 5.2465459569971224e-05, 'epoch': 0.48}
 48%|     | 1571/3250 [4:39:51<5:14:51, 11.25s/it]                                                        48%|     | 1571/3250 [4:39:51<5:14:51, 11.25s/it] 48%|     | 1572/3250 [4:40:01<5:07:29, 11.00s/it]                                                        48%|     | 1572/3250 [4:40:01<5:07:29, 11.00s/it] 48%|     | 1573/3250 [4:40:11<5:02:15, 10.81s/it]                                                        48%|     | 1573/3250 [4:40:11<5:02:15, 10.81s/it] 48%|     | 1574/3250 [4:40:22<5:01:42, 10.80s/it]                                                        48%|     | 1574/3250 [4:40:22<5:01:42, 10.80s/it] 48%|     | 1575/3250 [4:40:33<4:58:09, 10.68s/it]                                                        48%|     | 1575/3250 [4:40:33<4:58:09, 10.68s/it] 48%|     | 1576/3250 [4:40:43<4:55:41, 10.60s/it]            {'loss': 0.5967, 'learning_rate': 5.2417155297669326e-05, 'epoch': 0.48}
{'loss': 0.5933, 'learning_rate': 5.236884876399429e-05, 'epoch': 0.49}
{'loss': 0.5804, 'learning_rate': 5.232054001413941e-05, 'epoch': 0.49}
{'loss': 0.5696, 'learning_rate': 5.2272229093299985e-05, 'epoch': 0.49}
{'loss': 0.5566, 'learning_rate': 5.222391604667336e-05, 'epoch': 0.49}
                                            48%|     | 1576/3250 [4:40:43<4:55:41, 10.60s/it] 49%|     | 1577/3250 [4:40:53<4:53:55, 10.54s/it]                                                        49%|     | 1577/3250 [4:40:53<4:53:55, 10.54s/it] 49%|     | 1578/3250 [4:41:04<4:52:32, 10.50s/it]                                                        49%|     | 1578/3250 [4:41:04<4:52:32, 10.50s/it] 49%|     | 1579/3250 [4:41:14<4:51:42, 10.47s/it]                                                        49%|     | 1579/3250 [4:41:14<4:51:42, 10.47s/it] 49%|     | 1580/3250 [4:41:25<4:51:01, 10.46s/it]                                                        49%|     | 1580/3250 [4:41:25<4:51:01, 10.46s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.7879712581634521, 'eval_runtime': 2.1172, 'eval_samples_per_second': 5.668, 'eval_steps_per_second': 1.417, 'epoch': 0.49}
                                                        49%|     | 1580/3250 [4:41:27<4:51:01, 10.46s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_10/tmp-checkpoint-1580
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1580
/projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1580
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1580
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1580
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1580/pytorch_model.bin
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1580/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1580/pytorch_model.binthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1580/pytorch_model.bin

/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.5947, 'learning_rate': 5.217560091945887e-05, 'epoch': 0.49}
{'loss': 0.574, 'learning_rate': 5.212728375685781e-05, 'epoch': 0.49}
{'loss': 0.6036, 'learning_rate': 5.20789646040734e-05, 'epoch': 0.49}
{'loss': 1.0769, 'learning_rate': 5.203064350631064e-05, 'epoch': 0.49}
{'loss': 0.5778, 'learning_rate': 5.198232050877645e-05, 'epoch': 0.49}
 49%|     | 1581/3250 [4:41:38<5:12:14, 11.22s/it]                                                        49%|     | 1581/3250 [4:41:38<5:12:14, 11.22s/it] 49%|     | 1582/3250 [4:41:48<5:05:08, 10.98s/it]                                                        49%|     | 1582/3250 [4:41:48<5:05:08, 10.98s/it] 49%|     | 1583/3250 [4:41:58<5:00:19, 10.81s/it]                                                        49%|     | 1583/3250 [4:41:58<5:00:19, 10.81s/it] 49%|     | 1584/3250 [4:42:09<4:56:30, 10.68s/it]                                                        49%|     | 1584/3250 [4:42:09<4:56:30, 10.68s/it] 49%|     | 1585/3250 [4:42:19<4:53:57, 10.59s/it]                                                        49%|     | 1585/3250 [4:42:19<4:53:57, 10.59s/it] 49%|     | 1586/3250 [4:42:30<4:52:10, 10.53s/it]            {'loss': 0.596, 'learning_rate': 5.1933995656679444e-05, 'epoch': 0.49}
{'loss': 0.5942, 'learning_rate': 5.188566899523002e-05, 'epoch': 0.49}
{'loss': 0.596, 'learning_rate': 5.183734056964027e-05, 'epoch': 0.49}
{'loss': 0.5616, 'learning_rate': 5.1789010425123894e-05, 'epoch': 0.49}
{'loss': 0.5691, 'learning_rate': 5.174067860689625e-05, 'epoch': 0.49}
                                            49%|     | 1586/3250 [4:42:30<4:52:10, 10.53s/it] 49%|     | 1587/3250 [4:42:40<4:50:59, 10.50s/it]                                                        49%|     | 1587/3250 [4:42:40<4:50:59, 10.50s/it] 49%|     | 1588/3250 [4:42:50<4:49:56, 10.47s/it]                                                        49%|     | 1588/3250 [4:42:50<4:49:56, 10.47s/it] 49%|     | 1589/3250 [4:43:01<4:49:22, 10.45s/it]                                                        49%|     | 1589/3250 [4:43:01<4:49:22, 10.45s/it] 49%|     | 1590/3250 [4:43:11<4:48:48, 10.44s/it]                                                        49%|     | 1590/3250 [4:43:11<4:48:48, 10.44s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.7893646955490112, 'eval_runtime': 2.1061, 'eval_samples_per_second': 5.698, 'eval_steps_per_second': 1.424, 'epoch': 0.49}
                                                        49%|     | 1590/3250 [4:43:13<4:48:48, 10.44s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_10/tmp-checkpoint-1590
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1590
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1590
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1590
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1590
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1590/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1590/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1590/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1590/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.6323, 'learning_rate': 5.1692345160174225e-05, 'epoch': 0.49}
{'loss': 0.6019, 'learning_rate': 5.164401013017627e-05, 'epoch': 0.49}
{'loss': 0.5859, 'learning_rate': 5.159567356212226e-05, 'epoch': 0.49}
{'loss': 0.5466, 'learning_rate': 5.1547335501233565e-05, 'epoch': 0.49}
{'loss': 0.5921, 'learning_rate': 5.149899599273291e-05, 'epoch': 0.49}
 49%|     | 1591/3250 [4:43:24<5:11:31, 11.27s/it]                                                        49%|     | 1591/3250 [4:43:24<5:11:31, 11.27s/it] 49%|     | 1592/3250 [4:43:35<5:04:09, 11.01s/it]                                                        49%|     | 1592/3250 [4:43:35<5:04:09, 11.01s/it] 49%|     | 1593/3250 [4:43:45<4:58:55, 10.82s/it]                                                        49%|     | 1593/3250 [4:43:45<4:58:55, 10.82s/it] 49%|     | 1594/3250 [4:43:56<4:55:18, 10.70s/it]                                                        49%|     | 1594/3250 [4:43:56<4:55:18, 10.70s/it] 49%|     | 1595/3250 [4:44:06<4:52:45, 10.61s/it]                                                        49%|     | 1595/3250 [4:44:06<4:52:45, 10.61s/it] 49%|     | 1596/3250 [4:44:16<4:50:46, 10.55s/it]            {'loss': 0.5813, 'learning_rate': 5.14506550818444e-05, 'epoch': 0.49}
{'loss': 0.5807, 'learning_rate': 5.140231281379345e-05, 'epoch': 0.49}
{'loss': 0.5667, 'learning_rate': 5.135396923380673e-05, 'epoch': 0.49}
{'loss': 0.5934, 'learning_rate': 5.130562438711215e-05, 'epoch': 0.49}
{'loss': 0.5694, 'learning_rate': 5.1257278318938785e-05, 'epoch': 0.49}
                                            49%|     | 1596/3250 [4:44:16<4:50:46, 10.55s/it] 49%|     | 1597/3250 [4:44:27<4:49:24, 10.50s/it]                                                        49%|     | 1597/3250 [4:44:27<4:49:24, 10.50s/it] 49%|     | 1598/3250 [4:44:37<4:48:30, 10.48s/it]                                                        49%|     | 1598/3250 [4:44:37<4:48:30, 10.48s/it] 49%|     | 1599/3250 [4:44:48<4:47:40, 10.45s/it]                                                        49%|     | 1599/3250 [4:44:48<4:47:40, 10.45s/it] 49%|     | 1600/3250 [4:44:58<4:46:49, 10.43s/it]                                                        49%|     | 1600/3250 [4:44:58<4:46:49, 10.43s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.7868446111679077, 'eval_runtime': 2.1144, 'eval_samples_per_second': 5.675, 'eval_steps_per_second': 1.419, 'epoch': 0.49}
                                                        49%|     | 1600/3250 [4:45:00<4:46:49, 10.43s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_10/tmp-checkpoint-1600
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1600
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1600
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1600
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1600
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1600/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1600/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1600/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1600/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.6032, 'learning_rate': 5.1208931074516885e-05, 'epoch': 0.49}
{'loss': 0.5951, 'learning_rate': 5.116058269907779e-05, 'epoch': 0.49}
{'loss': 0.5732, 'learning_rate': 5.111223323785387e-05, 'epoch': 0.49}
{'loss': 0.59, 'learning_rate': 5.106388273607855e-05, 'epoch': 0.49}
{'loss': 0.5945, 'learning_rate': 5.101553123898622e-05, 'epoch': 0.49}
 49%|     | 1601/3250 [4:45:11<5:07:33, 11.19s/it]                                                        49%|     | 1601/3250 [4:45:11<5:07:33, 11.19s/it] 49%|     | 1602/3250 [4:45:21<5:00:42, 10.95s/it]                                                        49%|     | 1602/3250 [4:45:21<5:00:42, 10.95s/it] 49%|     | 1603/3250 [4:45:32<4:56:01, 10.78s/it]                                                        49%|     | 1603/3250 [4:45:32<4:56:01, 10.78s/it] 49%|     | 1604/3250 [4:45:42<4:52:42, 10.67s/it]                                                        49%|     | 1604/3250 [4:45:42<4:52:42, 10.67s/it] 49%|     | 1605/3250 [4:45:53<4:50:15, 10.59s/it]                                                        49%|     | 1605/3250 [4:45:53<4:50:15, 10.59s/it] 49%|     | 1606/3250 [4:46:03<4:48:27, 10.53s/it]            {'loss': 0.555, 'learning_rate': 5.096717879181217e-05, 'epoch': 0.49}
{'loss': 0.6087, 'learning_rate': 5.0918825439792604e-05, 'epoch': 0.49}
{'loss': 0.5739, 'learning_rate': 5.087047122816458e-05, 'epoch': 0.49}
{'loss': 0.5648, 'learning_rate': 5.082211620216595e-05, 'epoch': 0.5}
{'loss': 0.5629, 'learning_rate': 5.077376040703533e-05, 'epoch': 0.5}
                                            49%|     | 1606/3250 [4:46:03<4:48:27, 10.53s/it] 49%|     | 1607/3250 [4:46:14<4:52:09, 10.67s/it]                                                        49%|     | 1607/3250 [4:46:14<4:52:09, 10.67s/it] 49%|     | 1608/3250 [4:46:24<4:49:41, 10.59s/it]                                                        49%|     | 1608/3250 [4:46:24<4:49:41, 10.59s/it] 50%|     | 1609/3250 [4:46:35<4:47:59, 10.53s/it]                                                        50%|     | 1609/3250 [4:46:35<4:47:59, 10.53s/it] 50%|     | 1610/3250 [4:46:45<4:46:46, 10.49s/it]                                                        50%|     | 1610/3250 [4:46:45<4:46:46, 10.49s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.7897972464561462, 'eval_runtime': 2.1205, 'eval_samples_per_second': 5.659, 'eval_steps_per_second': 1.415, 'epoch': 0.5}
                                                        50%|     | 1610/3250 [4:46:47<4:46:46, 10.49s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_10/tmp-checkpoint-1610
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1610
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1610
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1610
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1610
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1610/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1610/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1610/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1610/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.5852, 'learning_rate': 5.072540388801204e-05, 'epoch': 0.5}
{'loss': 0.5714, 'learning_rate': 5.0677046690336096e-05, 'epoch': 0.5}
{'loss': 0.5984, 'learning_rate': 5.0628688859248164e-05, 'epoch': 0.5}
{'loss': 0.6721, 'learning_rate': 5.0580330439989465e-05, 'epoch': 0.5}
{'loss': 0.9745, 'learning_rate': 5.0531971477801776e-05, 'epoch': 0.5}
 50%|     | 1611/3250 [4:46:58<5:07:23, 11.25s/it]                                                        50%|     | 1611/3250 [4:46:58<5:07:23, 11.25s/it] 50%|     | 1612/3250 [4:47:09<5:00:17, 11.00s/it]                                                        50%|     | 1612/3250 [4:47:09<5:00:17, 11.00s/it] 50%|     | 1613/3250 [4:47:19<4:55:14, 10.82s/it]                                                        50%|     | 1613/3250 [4:47:19<4:55:14, 10.82s/it] 50%|     | 1614/3250 [4:47:29<4:51:35, 10.69s/it]                                                        50%|     | 1614/3250 [4:47:29<4:51:35, 10.69s/it] 50%|     | 1615/3250 [4:47:40<4:48:57, 10.60s/it]                                                        50%|     | 1615/3250 [4:47:40<4:48:57, 10.60s/it] 50%|     | 1616/3250 [4:47:50<4:47:01, 10.54s/it]            {'loss': 0.5737, 'learning_rate': 5.048361201792742e-05, 'epoch': 0.5}
{'loss': 0.592, 'learning_rate': 5.043525210560912e-05, 'epoch': 0.5}
{'loss': 0.5934, 'learning_rate': 5.0386891786090105e-05, 'epoch': 0.5}
{'loss': 0.5884, 'learning_rate': 5.0338531104613926e-05, 'epoch': 0.5}
{'loss': 0.5677, 'learning_rate': 5.029017010642447e-05, 'epoch': 0.5}
                                            50%|     | 1616/3250 [4:47:50<4:47:01, 10.54s/it] 50%|     | 1617/3250 [4:48:01<4:45:36, 10.49s/it]                                                        50%|     | 1617/3250 [4:48:01<4:45:36, 10.49s/it] 50%|     | 1618/3250 [4:48:11<4:44:38, 10.46s/it]                                                        50%|     | 1618/3250 [4:48:11<4:44:38, 10.46s/it] 50%|     | 1619/3250 [4:48:21<4:43:57, 10.45s/it]                                                        50%|     | 1619/3250 [4:48:21<4:43:57, 10.45s/it] 50%|     | 1620/3250 [4:48:32<4:43:17, 10.43s/it]                                                        50%|     | 1620/3250 [4:48:32<4:43:17, 10.43s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.7882960438728333, 'eval_runtime': 2.117, 'eval_samples_per_second': 5.669, 'eval_steps_per_second': 1.417, 'epoch': 0.5}
                                                        50%|     | 1620/3250 [4:48:34<4:43:17, 10.43s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_10/tmp-checkpoint-1620
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1620
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1620
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1620
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1620
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1620/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1620/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1620/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1620/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.6143, 'learning_rate': 5.024180883676597e-05, 'epoch': 0.5}
{'loss': 0.6118, 'learning_rate': 5.019344734088287e-05, 'epoch': 0.5}
{'loss': 0.5724, 'learning_rate': 5.014508566401982e-05, 'epoch': 0.5}
{'loss': 0.5596, 'learning_rate': 5.009672385142167e-05, 'epoch': 0.5}
{'loss': 0.5797, 'learning_rate': 5.004836194833339e-05, 'epoch': 0.5}
 50%|     | 1621/3250 [4:48:45<5:05:06, 11.24s/it]                                                        50%|     | 1621/3250 [4:48:45<5:05:06, 11.24s/it] 50%|     | 1622/3250 [4:48:55<4:58:03, 10.98s/it]                                                        50%|     | 1622/3250 [4:48:55<4:58:03, 10.98s/it] 50%|     | 1623/3250 [4:49:06<4:55:59, 10.92s/it]                                                        50%|     | 1623/3250 [4:49:06<4:55:59, 10.92s/it] 50%|     | 1624/3250 [4:49:16<4:51:31, 10.76s/it]                                                        50%|     | 1624/3250 [4:49:16<4:51:31, 10.76s/it] 50%|     | 1625/3250 [4:49:27<4:48:30, 10.65s/it]                                                        50%|     | 1625/3250 [4:49:27<4:48:30, 10.65s/it] 50%|     | 1626/3250 [4:49:37<4:46:11, 10.57s/it]            {'loss': 0.5764, 'learning_rate': 5e-05, 'epoch': 0.5}
{'loss': 0.5944, 'learning_rate': 4.995163805166662e-05, 'epoch': 0.5}
{'loss': 0.5706, 'learning_rate': 4.990327614857834e-05, 'epoch': 0.5}
{'loss': 0.573, 'learning_rate': 4.9854914335980193e-05, 'epoch': 0.5}
{'loss': 0.5741, 'learning_rate': 4.980655265911714e-05, 'epoch': 0.5}
                                            50%|     | 1626/3250 [4:49:37<4:46:11, 10.57s/it] 50%|     | 1627/3250 [4:49:48<4:44:31, 10.52s/it]                                                        50%|     | 1627/3250 [4:49:48<4:44:31, 10.52s/it] 50%|     | 1628/3250 [4:49:58<4:43:15, 10.48s/it]                                                        50%|     | 1628/3250 [4:49:58<4:43:15, 10.48s/it] 50%|     | 1629/3250 [4:50:08<4:42:20, 10.45s/it]                                                        50%|     | 1629/3250 [4:50:08<4:42:20, 10.45s/it] 50%|     | 1630/3250 [4:50:19<4:41:37, 10.43s/it]                                                        50%|     | 1630/3250 [4:50:19<4:41:37, 10.43s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.7838338613510132, 'eval_runtime': 2.3512, 'eval_samples_per_second': 5.104, 'eval_steps_per_second': 1.276, 'epoch': 0.5}
                                                        50%|     | 1630/3250 [4:50:21<4:41:37, 10.43s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_10/tmp-checkpoint-1630
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1630
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1630
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1630
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1630
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1630/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1630/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1630/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1630/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.595, 'learning_rate': 4.975819116323403e-05, 'epoch': 0.5}
{'loss': 0.5872, 'learning_rate': 4.970982989357552e-05, 'epoch': 0.5}
{'loss': 0.5725, 'learning_rate': 4.966146889538608e-05, 'epoch': 0.5}
{'loss': 0.5876, 'learning_rate': 4.96131082139099e-05, 'epoch': 0.5}
{'loss': 0.5967, 'learning_rate': 4.9564747894390903e-05, 'epoch': 0.5}
 50%|     | 1631/3250 [4:50:32<5:05:08, 11.31s/it]                                                        50%|     | 1631/3250 [4:50:32<5:05:08, 11.31s/it] 50%|     | 1632/3250 [4:50:43<4:57:40, 11.04s/it]                                                        50%|     | 1632/3250 [4:50:43<4:57:40, 11.04s/it] 50%|     | 1633/3250 [4:50:53<4:52:16, 10.84s/it]                                                        50%|     | 1633/3250 [4:50:53<4:52:16, 10.84s/it] 50%|     | 1634/3250 [4:51:03<4:48:20, 10.71s/it]                                                        50%|     | 1634/3250 [4:51:03<4:48:20, 10.71s/it] 50%|     | 1635/3250 [4:51:14<4:45:24, 10.60s/it]                                                        50%|     | 1635/3250 [4:51:14<4:45:24, 10.60s/it] 50%|     | 1636/3250 [4:51:24<4:43:20, 10.53s/it]            {'loss': 0.5396, 'learning_rate': 4.951638798207261e-05, 'epoch': 0.5}
{'loss': 0.6179, 'learning_rate': 4.946802852219824e-05, 'epoch': 0.5}
{'loss': 0.5778, 'learning_rate': 4.941966956001056e-05, 'epoch': 0.5}
{'loss': 0.5688, 'learning_rate': 4.9371311140751854e-05, 'epoch': 0.5}
{'loss': 0.5539, 'learning_rate': 4.9322953309663916e-05, 'epoch': 0.5}
                                            50%|     | 1636/3250 [4:51:24<4:43:20, 10.53s/it] 50%|     | 1637/3250 [4:51:34<4:42:04, 10.49s/it]                                                        50%|     | 1637/3250 [4:51:34<4:42:04, 10.49s/it] 50%|     | 1638/3250 [4:51:45<4:40:55, 10.46s/it]                                                        50%|     | 1638/3250 [4:51:45<4:40:55, 10.46s/it] 50%|     | 1639/3250 [4:51:55<4:40:13, 10.44s/it]                                                        50%|     | 1639/3250 [4:51:55<4:40:13, 10.44s/it] 50%|     | 1640/3250 [4:52:06<4:43:06, 10.55s/it]                                                        50%|     | 1640/3250 [4:52:06<4:43:06, 10.55s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.7887817621231079, 'eval_runtime': 2.2145, 'eval_samples_per_second': 5.419, 'eval_steps_per_second': 1.355, 'epoch': 0.5}
                                                        50%|     | 1640/3250 [4:52:08<4:43:06, 10.55s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_10/tmp-checkpoint-1640
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1640
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1640
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1640
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1640
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1640/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1640/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1640/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1640/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.5666, 'learning_rate': 4.9274596111987974e-05, 'epoch': 0.5}
{'loss': 0.5887, 'learning_rate': 4.922623959296468e-05, 'epoch': 0.51}
{'loss': 0.5616, 'learning_rate': 4.9177883797834064e-05, 'epoch': 0.51}
{'loss': 0.5907, 'learning_rate': 4.912952877183543e-05, 'epoch': 0.51}
{'loss': 1.0662, 'learning_rate': 4.908117456020741e-05, 'epoch': 0.51}
 50%|     | 1641/3250 [4:52:19<5:03:51, 11.33s/it]                                                        50%|     | 1641/3250 [4:52:19<5:03:51, 11.33s/it] 51%|     | 1642/3250 [4:52:30<4:56:12, 11.05s/it]                                                        51%|     | 1642/3250 [4:52:30<4:56:12, 11.05s/it] 51%|     | 1643/3250 [4:52:40<4:50:36, 10.85s/it]                                                        51%|     | 1643/3250 [4:52:40<4:50:36, 10.85s/it] 51%|     | 1644/3250 [4:52:50<4:46:47, 10.71s/it]                                                        51%|     | 1644/3250 [4:52:50<4:46:47, 10.71s/it] 51%|     | 1645/3250 [4:53:01<4:43:49, 10.61s/it]                                                        51%|     | 1645/3250 [4:53:01<4:43:49, 10.61s/it] 51%|     | 1646/3250 [4:53:11<4:41:53, 10.54s/it]            {'loss': 0.562, 'learning_rate': 4.903282120818785e-05, 'epoch': 0.51}
{'loss': 0.5833, 'learning_rate': 4.898446876101379e-05, 'epoch': 0.51}
{'loss': 0.5954, 'learning_rate': 4.893611726392145e-05, 'epoch': 0.51}
{'loss': 0.602, 'learning_rate': 4.8887766762146134e-05, 'epoch': 0.51}
{'loss': 0.5335, 'learning_rate': 4.8839417300922216e-05, 'epoch': 0.51}
                                            51%|     | 1646/3250 [4:53:11<4:41:53, 10.54s/it] 51%|     | 1647/3250 [4:53:21<4:40:26, 10.50s/it]                                                        51%|     | 1647/3250 [4:53:21<4:40:26, 10.50s/it] 51%|     | 1648/3250 [4:53:32<4:39:33, 10.47s/it]                                                        51%|     | 1648/3250 [4:53:32<4:39:33, 10.47s/it] 51%|     | 1649/3250 [4:53:42<4:38:37, 10.44s/it]                                                        51%|     | 1649/3250 [4:53:42<4:38:37, 10.44s/it] 51%|     | 1650/3250 [4:53:53<4:37:59, 10.42s/it]                                                        51%|     | 1650/3250 [4:53:53<4:37:59, 10.42s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.7867335677146912, 'eval_runtime': 2.1231, 'eval_samples_per_second': 5.652, 'eval_steps_per_second': 1.413, 'epoch': 0.51}
                                                        51%|     | 1650/3250 [4:53:55<4:37:59, 10.42s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_10/tmp-checkpoint-1650
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1650
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1650
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1650
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1650
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1650/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1650/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1650/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1650/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.5766, 'learning_rate': 4.8791068925483106e-05, 'epoch': 0.51}
{'loss': 0.6239, 'learning_rate': 4.8742721681061226e-05, 'epoch': 0.51}
{'loss': 0.5749, 'learning_rate': 4.869437561288788e-05, 'epoch': 0.51}
{'loss': 0.5659, 'learning_rate': 4.8646030766193285e-05, 'epoch': 0.51}
{'loss': 0.5707, 'learning_rate': 4.859768718620656e-05, 'epoch': 0.51}
 51%|     | 1651/3250 [4:54:06<4:58:41, 11.21s/it]                                                        51%|     | 1651/3250 [4:54:06<4:58:41, 11.21s/it] 51%|     | 1652/3250 [4:54:16<4:51:49, 10.96s/it]                                                        51%|     | 1652/3250 [4:54:16<4:51:49, 10.96s/it] 51%|     | 1653/3250 [4:54:26<4:47:05, 10.79s/it]                                                        51%|     | 1653/3250 [4:54:26<4:47:05, 10.79s/it] 51%|     | 1654/3250 [4:54:37<4:43:44, 10.67s/it]                                                        51%|     | 1654/3250 [4:54:37<4:43:44, 10.67s/it] 51%|     | 1655/3250 [4:54:47<4:41:16, 10.58s/it]                                                        51%|     | 1655/3250 [4:54:47<4:41:16, 10.58s/it] 51%|     | 1656/3250 [4:54:58<4:42:34, 10.64s/it]            {'loss': 0.5585, 'learning_rate': 4.854934491815561e-05, 'epoch': 0.51}
{'loss': 0.5902, 'learning_rate': 4.8501004007267095e-05, 'epoch': 0.51}
{'loss': 0.5566, 'learning_rate': 4.845266449876645e-05, 'epoch': 0.51}
{'loss': 0.5685, 'learning_rate': 4.8404326437877746e-05, 'epoch': 0.51}
{'loss': 0.5789, 'learning_rate': 4.8355989869823737e-05, 'epoch': 0.51}
                                            51%|     | 1656/3250 [4:54:58<4:42:34, 10.64s/it] 51%|     | 1657/3250 [4:55:08<4:40:21, 10.56s/it]                                                        51%|     | 1657/3250 [4:55:08<4:40:21, 10.56s/it] 51%|     | 1658/3250 [4:55:19<4:38:50, 10.51s/it]                                                        51%|     | 1658/3250 [4:55:19<4:38:50, 10.51s/it] 51%|     | 1659/3250 [4:55:29<4:37:49, 10.48s/it]                                                        51%|     | 1659/3250 [4:55:29<4:37:49, 10.48s/it] 51%|     | 1660/3250 [4:55:40<4:36:54, 10.45s/it]                                                        51%|     | 1660/3250 [4:55:40<4:36:54, 10.45s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.7864741683006287, 'eval_runtime': 2.1224, 'eval_samples_per_second': 5.654, 'eval_steps_per_second': 1.413, 'epoch': 0.51}
                                                        51%|     | 1660/3250 [4:55:42<4:36:54, 10.45s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_10/tmp-checkpoint-1660
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1660
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1660
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1660
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1660
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1660/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1660/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1660/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1660/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.5831, 'learning_rate': 4.830765483982578e-05, 'epoch': 0.51}
{'loss': 0.592, 'learning_rate': 4.8259321393103754e-05, 'epoch': 0.51}
{'loss': 0.5975, 'learning_rate': 4.821098957487611e-05, 'epoch': 0.51}
{'loss': 0.5777, 'learning_rate': 4.816265943035975e-05, 'epoch': 0.51}
{'loss': 0.5815, 'learning_rate': 4.811433100476999e-05, 'epoch': 0.51}
 51%|     | 1661/3250 [4:55:53<4:58:40, 11.28s/it]                                                        51%|     | 1661/3250 [4:55:53<4:58:40, 11.28s/it] 51%|     | 1662/3250 [4:56:03<4:51:13, 11.00s/it]                                                        51%|     | 1662/3250 [4:56:03<4:51:13, 11.00s/it] 51%|     | 1663/3250 [4:56:13<4:45:59, 10.81s/it]                                                        51%|     | 1663/3250 [4:56:13<4:45:59, 10.81s/it] 51%|     | 1664/3250 [4:56:24<4:42:28, 10.69s/it]                                                        51%|     | 1664/3250 [4:56:24<4:42:28, 10.69s/it] 51%|     | 1665/3250 [4:56:34<4:39:59, 10.60s/it]                                                        51%|     | 1665/3250 [4:56:34<4:39:59, 10.60s/it] 51%|    | 1666/3250 [4:56:45<4:38:07, 10.53s/it]          {'loss': 0.5541, 'learning_rate': 4.806600434332056e-05, 'epoch': 0.51}
{'loss': 0.5972, 'learning_rate': 4.801767949122356e-05, 'epoch': 0.51}
{'loss': 0.5737, 'learning_rate': 4.796935649368935e-05, 'epoch': 0.51}
{'loss': 0.561, 'learning_rate': 4.7921035395926625e-05, 'epoch': 0.51}
{'loss': 0.5623, 'learning_rate': 4.7872716243142194e-05, 'epoch': 0.51}
                                              51%|    | 1666/3250 [4:56:45<4:38:07, 10.53s/it] 51%|    | 1667/3250 [4:56:55<4:36:37, 10.48s/it]                                                        51%|    | 1667/3250 [4:56:55<4:36:37, 10.48s/it] 51%|    | 1668/3250 [4:57:05<4:35:43, 10.46s/it]                                                        51%|    | 1668/3250 [4:57:05<4:35:43, 10.46s/it] 51%|    | 1669/3250 [4:57:16<4:34:54, 10.43s/it]                                                        51%|    | 1669/3250 [4:57:16<4:34:54, 10.43s/it] 51%|    | 1670/3250 [4:57:26<4:34:20, 10.42s/it]                                                        51%|    | 1670/3250 [4:57:26<4:34:20, 10.42s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.7895388603210449, 'eval_runtime': 2.1128, 'eval_samples_per_second': 5.68, 'eval_steps_per_second': 1.42, 'epoch': 0.51}
                                                        51%|    | 1670/3250 [4:57:28<4:34:20, 10.42s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_10/tmp-checkpoint-1670
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1670
/projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1670
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1670
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1670
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1670/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1670/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1670/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1670/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.5462, 'learning_rate': 4.782439908054115e-05, 'epoch': 0.51}
{'loss': 0.5952, 'learning_rate': 4.777608395332667e-05, 'epoch': 0.51}
{'loss': 0.5579, 'learning_rate': 4.7727770906700034e-05, 'epoch': 0.51}
{'loss': 0.5928, 'learning_rate': 4.76794599858606e-05, 'epoch': 0.52}
{'loss': 1.0711, 'learning_rate': 4.763115123600571e-05, 'epoch': 0.52}
 51%|    | 1671/3250 [4:57:39<4:54:26, 11.19s/it]                                                        51%|    | 1671/3250 [4:57:39<4:54:26, 11.19s/it] 51%|    | 1672/3250 [4:57:50<4:47:51, 10.94s/it]                                                        51%|    | 1672/3250 [4:57:50<4:47:51, 10.94s/it] 51%|    | 1673/3250 [4:58:00<4:45:26, 10.86s/it]                                                        51%|    | 1673/3250 [4:58:00<4:45:26, 10.86s/it] 52%|    | 1674/3250 [4:58:11<4:41:30, 10.72s/it]                                                        52%|    | 1674/3250 [4:58:11<4:41:30, 10.72s/it] 52%|    | 1675/3250 [4:58:21<4:38:31, 10.61s/it]                                                        52%|    | 1675/3250 [4:58:21<4:38:31, 10.61s/it] 52%|    | 1676/3250 [4:58:31<4:36:36, 1{'loss': 0.5652, 'learning_rate': 4.7582844702330685e-05, 'epoch': 0.52}
{'loss': 0.5629, 'learning_rate': 4.753454043002878e-05, 'epoch': 0.52}
{'loss': 0.587, 'learning_rate': 4.748623846429112e-05, 'epoch': 0.52}
{'loss': 0.5813, 'learning_rate': 4.743793885030668e-05, 'epoch': 0.52}
{'loss': 0.5441, 'learning_rate': 4.7389641633262224e-05, 'epoch': 0.52}
0.54s/it]                                                        52%|    | 1676/3250 [4:58:31<4:36:36, 10.54s/it] 52%|    | 1677/3250 [4:58:42<4:35:09, 10.50s/it]                                                        52%|    | 1677/3250 [4:58:42<4:35:09, 10.50s/it] 52%|    | 1678/3250 [4:58:52<4:34:07, 10.46s/it]                                                        52%|    | 1678/3250 [4:58:52<4:34:07, 10.46s/it] 52%|    | 1679/3250 [4:59:02<4:33:19, 10.44s/it]                                                        52%|    | 1679/3250 [4:59:02<4:33:19, 10.44s/it] 52%|    | 1680/3250 [4:59:13<4:32:46, 10.42s/it]                                                        52%|    | 1680/3250 [4:59:13<4:32:46, 10.42s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.788500189781189, 'eval_runtime': 2.1181, 'eval_samples_per_second': 5.666, 'eval_steps_per_second': 1.416, 'epoch': 0.52}
                                                        52%|    | 1680/3250 [4:59:15<4:32:46, 10.42s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_10/tmp-checkpoint-1680
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1680
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1680
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1680
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1680
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1680/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1680/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1680/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1680/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.5578, 'learning_rate': 4.7341346858342287e-05, 'epoch': 0.52}
{'loss': 0.6267, 'learning_rate': 4.729305457072913e-05, 'epoch': 0.52}
{'loss': 0.5883, 'learning_rate': 4.724476481560265e-05, 'epoch': 0.52}
{'loss': 0.5773, 'learning_rate': 4.7196477638140404e-05, 'epoch': 0.52}
{'loss': 0.537, 'learning_rate': 4.714819308351755e-05, 'epoch': 0.52}
 52%|    | 1681/3250 [4:59:26<4:52:30, 11.19s/it]                                                        52%|    | 1681/3250 [4:59:26<4:52:30, 11.19s/it] 52%|    | 1682/3250 [4:59:36<4:46:04, 10.95s/it]                                                        52%|    | 1682/3250 [4:59:36<4:46:04, 10.95s/it] 52%|    | 1683/3250 [4:59:47<4:41:29, 10.78s/it]                                                        52%|    | 1683/3250 [4:59:47<4:41:29, 10.78s/it] 52%|    | 1684/3250 [4:59:57<4:38:20, 10.66s/it]                                                        52%|    | 1684/3250 [4:59:57<4:38:20, 10.66s/it] 52%|    | 1685/3250 [5:00:07<4:35:59, 10.58s/it]                                                        52%|    | 1685/3250 [5:00:07<4:35:59, 10.58s/it] 52%|    | 1686/3250 [5:00:18<4:34:23, 1{'loss': 0.576, 'learning_rate': 4.7099911196906764e-05, 'epoch': 0.52}
{'loss': 0.5732, 'learning_rate': 4.7051632023478204e-05, 'epoch': 0.52}
{'loss': 0.5634, 'learning_rate': 4.700335560839955e-05, 'epoch': 0.52}
{'loss': 0.5515, 'learning_rate': 4.695508199683586e-05, 'epoch': 0.52}
{'loss': 0.5836, 'learning_rate': 4.6906811233949585e-05, 'epoch': 0.52}
0.53s/it]                                                        52%|    | 1686/3250 [5:00:18<4:34:23, 10.53s/it] 52%|    | 1687/3250 [5:00:28<4:33:02, 10.48s/it]                                                        52%|    | 1687/3250 [5:00:28<4:33:02, 10.48s/it] 52%|    | 1688/3250 [5:00:39<4:32:07, 10.45s/it]                                                        52%|    | 1688/3250 [5:00:39<4:32:07, 10.45s/it] 52%|    | 1689/3250 [5:00:49<4:34:20, 10.54s/it]                                                        52%|    | 1689/3250 [5:00:49<4:34:20, 10.54s/it] 52%|    | 1690/3250 [5:01:00<4:32:57, 10.50s/it]                                                        52%|    | 1690/3250 [5:01:00<4:32:57, 10.50s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.7868459224700928, 'eval_runtime': 2.1192, 'eval_samples_per_second': 5.663, 'eval_steps_per_second': 1.416, 'epoch': 0.52}
                                                        52%|    | 1690/3250 [5:01:02<4:32:57, 10.50s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_10/tmp-checkpoint-1690
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1690
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1690
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1690
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1690
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1690/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1690/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1690/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1690/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.5692, 'learning_rate': 4.68585433649005e-05, 'epoch': 0.52}
{'loss': 0.5955, 'learning_rate': 4.68102784348457e-05, 'epoch': 0.52}
{'loss': 0.5869, 'learning_rate': 4.676201648893949e-05, 'epoch': 0.52}
{'loss': 0.5581, 'learning_rate': 4.67137575723334e-05, 'epoch': 0.52}
{'loss': 0.5859, 'learning_rate': 4.666550173017615e-05, 'epoch': 0.52}
 52%|    | 1691/3250 [5:01:13<4:52:55, 11.27s/it]                                                        52%|    | 1691/3250 [5:01:13<4:52:55, 11.27s/it] 52%|    | 1692/3250 [5:01:23<4:45:48, 11.01s/it]                                                        52%|    | 1692/3250 [5:01:23<4:45:48, 11.01s/it] 52%|    | 1693/3250 [5:01:34<4:40:51, 10.82s/it]                                                        52%|    | 1693/3250 [5:01:34<4:40:51, 10.82s/it] 52%|    | 1694/3250 [5:01:44<4:37:27, 10.70s/it]                                                        52%|    | 1694/3250 [5:01:44<4:37:27, 10.70s/it] 52%|    | 1695/3250 [5:01:54<4:35:02, 10.61s/it]                                                        52%|    | 1695/3250 [5:01:54<4:35:02, 10.61s/it] 52%|    | 1696/3250 [5:02:05<4:33:05, 1{'loss': 0.5769, 'learning_rate': 4.6617249007613544e-05, 'epoch': 0.52}
{'loss': 0.5361, 'learning_rate': 4.65689994497885e-05, 'epoch': 0.52}
{'loss': 0.6061, 'learning_rate': 4.652075310184094e-05, 'epoch': 0.52}
{'loss': 0.5576, 'learning_rate': 4.647251000890782e-05, 'epoch': 0.52}
{'loss': 0.5543, 'learning_rate': 4.642427021612304e-05, 'epoch': 0.52}
0.54s/it]                                                        52%|    | 1696/3250 [5:02:05<4:33:05, 10.54s/it] 52%|    | 1697/3250 [5:02:15<4:31:44, 10.50s/it]                                                        52%|    | 1697/3250 [5:02:15<4:31:44, 10.50s/it] 52%|    | 1698/3250 [5:02:26<4:30:47, 10.47s/it]                                                        52%|    | 1698/3250 [5:02:26<4:30:47, 10.47s/it] 52%|    | 1699/3250 [5:02:36<4:30:05, 10.45s/it]                                                        52%|    | 1699/3250 [5:02:36<4:30:05, 10.45s/it] 52%|    | 1700/3250 [5:02:46<4:29:20, 10.43s/it]                                                        52%|    | 1700/3250 [5:02:46<4:29:20, 10.43s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.7935681343078613, 'eval_runtime': 2.5324, 'eval_samples_per_second': 4.739, 'eval_steps_per_second': 1.185, 'epoch': 0.52}
                                                        52%|    | 1700/3250 [5:02:49<4:29:20, 10.43s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_10/tmp-checkpoint-1700
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1700I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in 
 /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1700
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1700
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1700
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1700/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1700/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1700/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1700/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.5508, 'learning_rate': 4.637603376861738e-05, 'epoch': 0.52}
{'loss': 0.5759, 'learning_rate': 4.6327800711518545e-05, 'epoch': 0.52}
{'loss': 0.5626, 'learning_rate': 4.6279571089951054e-05, 'epoch': 0.52}
{'loss': 0.5907, 'learning_rate': 4.623134494903618e-05, 'epoch': 0.52}
{'loss': 0.7591, 'learning_rate': 4.6183122333891926e-05, 'epoch': 0.52}
 52%|    | 1701/3250 [5:03:00<4:52:38, 11.34s/it]                                                        52%|    | 1701/3250 [5:03:00<4:52:38, 11.34s/it] 52%|    | 1702/3250 [5:03:10<4:45:05, 11.05s/it]                                                        52%|    | 1702/3250 [5:03:10<4:45:05, 11.05s/it] 52%|    | 1703/3250 [5:03:21<4:39:49, 10.85s/it]                                                        52%|    | 1703/3250 [5:03:21<4:39:49, 10.85s/it] 52%|    | 1704/3250 [5:03:31<4:36:07, 10.72s/it]                                                        52%|    | 1704/3250 [5:03:31<4:36:07, 10.72s/it] 52%|    | 1705/3250 [5:03:42<4:35:22, 10.69s/it]                                                        52%|    | 1705/3250 [5:03:42<4:35:22, 10.69s/it] 52%|    | 1706/3250 [5:03:52<4:34:01, 1{'loss': 0.8689, 'learning_rate': 4.613490328963307e-05, 'epoch': 0.52}
{'loss': 0.5614, 'learning_rate': 4.6086687861371004e-05, 'epoch': 0.53}
{'loss': 0.5864, 'learning_rate': 4.6038476094213724e-05, 'epoch': 0.53}
{'loss': 0.5825, 'learning_rate': 4.599026803326583e-05, 'epoch': 0.53}
{'loss': 0.5614, 'learning_rate': 4.594206372362845e-05, 'epoch': 0.53}
0.65s/it]                                                        52%|    | 1706/3250 [5:03:52<4:34:01, 10.65s/it] 53%|    | 1707/3250 [5:04:03<4:32:07, 10.58s/it]                                                        53%|    | 1707/3250 [5:04:03<4:32:07, 10.58s/it] 53%|    | 1708/3250 [5:04:13<4:30:39, 10.53s/it]                                                        53%|    | 1708/3250 [5:04:13<4:30:39, 10.53s/it] 53%|    | 1709/3250 [5:04:23<4:29:39, 10.50s/it]                                                        53%|    | 1709/3250 [5:04:23<4:29:39, 10.50s/it] 53%|    | 1710/3250 [5:04:34<4:28:47, 10.47s/it]                                                        53%|    | 1710/3250 [5:04:34<4:28:47, 10.47s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.7904557585716248, 'eval_runtime': 2.2062, 'eval_samples_per_second': 5.439, 'eval_steps_per_second': 1.36, 'epoch': 0.53}
                                                        53%|    | 1710/3250 [5:04:36<4:28:47, 10.47s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_10/tmp-checkpoint-1710
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1710
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1710
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1710
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1710
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1710/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1710/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1710/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1710/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.5511, 'learning_rate': 4.589386321039917e-05, 'epoch': 0.53}
{'loss': 0.6121, 'learning_rate': 4.5845666538672074e-05, 'epoch': 0.53}
{'loss': 0.6012, 'learning_rate': 4.579747375353763e-05, 'epoch': 0.53}
{'loss': 0.5614, 'learning_rate': 4.574928490008264e-05, 'epoch': 0.53}
{'loss': 0.5402, 'learning_rate': 4.570110002339028e-05, 'epoch': 0.53}
 53%|    | 1711/3250 [5:04:47<4:49:18, 11.28s/it]                                                        53%|    | 1711/3250 [5:04:47<4:49:18, 11.28s/it] 53%|    | 1712/3250 [5:04:57<4:42:37, 11.03s/it]                                                        53%|    | 1712/3250 [5:04:57<4:42:37, 11.03s/it] 53%|    | 1713/3250 [5:05:08<4:37:47, 10.84s/it]                                                        53%|    | 1713/3250 [5:05:08<4:37:47, 10.84s/it] 53%|    | 1714/3250 [5:05:18<4:34:20, 10.72s/it]                                                        53%|    | 1714/3250 [5:05:18<4:34:20, 10.72s/it] 53%|    | 1715/3250 [5:05:29<4:32:03, 10.63s/it]                                                        53%|    | 1715/3250 [5:05:29<4:32:03, 10.63s/it] 53%|    | 1716/3250 [5:05:39<4:30:17, 1{'loss': 0.5777, 'learning_rate': 4.5652919168539976e-05, 'epoch': 0.53}
{'loss': 0.5621, 'learning_rate': 4.560474238060739e-05, 'epoch': 0.53}
{'loss': 0.5825, 'learning_rate': 4.5556569704664394e-05, 'epoch': 0.53}
{'loss': 0.554, 'learning_rate': 4.5508401185778986e-05, 'epoch': 0.53}
{'loss': 0.5624, 'learning_rate': 4.546023686901533e-05, 'epoch': 0.53}
0.57s/it]                                                        53%|    | 1716/3250 [5:05:39<4:30:17, 10.57s/it] 53%|    | 1717/3250 [5:05:50<4:29:01, 10.53s/it]                                                        53%|    | 1717/3250 [5:05:50<4:29:01, 10.53s/it] 53%|    | 1718/3250 [5:06:00<4:27:58, 10.50s/it]                                                        53%|    | 1718/3250 [5:06:00<4:27:58, 10.50s/it] 53%|    | 1719/3250 [5:06:10<4:27:19, 10.48s/it]                                                        53%|    | 1719/3250 [5:06:10<4:27:19, 10.48s/it] 53%|    | 1720/3250 [5:06:21<4:26:37, 10.46s/it]                                                        53%|    | 1720/3250 [5:06:21<4:26:37, 10.46s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.7837050557136536, 'eval_runtime': 2.1313, 'eval_samples_per_second': 5.63, 'eval_steps_per_second': 1.408, 'epoch': 0.53}
                                                        53%|    | 1720/3250 [5:06:23<4:26:37, 10.46s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_10/tmp-checkpoint-1720
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1720
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1720
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1720
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1720
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1720/pytorch_model.bin
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1720/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1720/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1720/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.5594, 'learning_rate': 4.541207679943357e-05, 'epoch': 0.53}
{'loss': 0.5839, 'learning_rate': 4.5363921022089974e-05, 'epoch': 0.53}
{'loss': 0.5796, 'learning_rate': 4.531576958203671e-05, 'epoch': 0.53}
{'loss': 0.5552, 'learning_rate': 4.526762252432195e-05, 'epoch': 0.53}
{'loss': 0.5812, 'learning_rate': 4.5219479893989756e-05, 'epoch': 0.53}
 53%|    | 1721/3250 [5:06:34<4:46:35, 11.25s/it]                                                        53%|    | 1721/3250 [5:06:34<4:46:35, 11.25s/it] 53%|    | 1722/3250 [5:06:45<4:42:56, 11.11s/it]                                                        53%|    | 1722/3250 [5:06:45<4:42:56, 11.11s/it] 53%|    | 1723/3250 [5:06:55<4:37:38, 10.91s/it]                                                        53%|    | 1723/3250 [5:06:55<4:37:38, 10.91s/it] 53%|    | 1724/3250 [5:07:06<4:33:54, 10.77s/it]                                                        53%|    | 1724/3250 [5:07:06<4:33:54, 10.77s/it] 53%|    | 1725/3250 [5:07:16<4:31:10, 10.67s/it]                                                        53%|    | 1725/3250 [5:07:16<4:31:10, 10.67s/it] 53%|    | 1726/3250 [5:07:26<4:29:02, 1{'loss': 0.5884, 'learning_rate': 4.5171341736080004e-05, 'epoch': 0.53}
{'loss': 0.5305, 'learning_rate': 4.5123208095628424e-05, 'epoch': 0.53}
{'loss': 0.6067, 'learning_rate': 4.5075079017666547e-05, 'epoch': 0.53}
{'loss': 0.5543, 'learning_rate': 4.502695454722156e-05, 'epoch': 0.53}
{'loss': 0.5543, 'learning_rate': 4.4978834729316384e-05, 'epoch': 0.53}
0.59s/it]                                                        53%|    | 1726/3250 [5:07:26<4:29:02, 10.59s/it] 53%|    | 1727/3250 [5:07:37<4:27:36, 10.54s/it]                                                        53%|    | 1727/3250 [5:07:37<4:27:36, 10.54s/it] 53%|    | 1728/3250 [5:07:47<4:26:35, 10.51s/it]                                                        53%|    | 1728/3250 [5:07:47<4:26:35, 10.51s/it] 53%|    | 1729/3250 [5:07:58<4:25:53, 10.49s/it]                                                        53%|    | 1729/3250 [5:07:58<4:25:53, 10.49s/it] 53%|    | 1730/3250 [5:08:08<4:25:14, 10.47s/it]                                                        53%|    | 1730/3250 [5:08:08<4:25:14, 10.47s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.7931942343711853, 'eval_runtime': 2.3543, 'eval_samples_per_second': 5.097, 'eval_steps_per_second': 1.274, 'epoch': 0.53}
                                                        53%|    | 1730/3250 [5:08:11<4:25:14, 10.47s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_10/tmp-checkpoint-1730
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1730
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1730
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1730
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1730
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1730/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1730/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1730/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1730/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.5429, 'learning_rate': 4.493071960896961e-05, 'epoch': 0.53}
{'loss': 0.5539, 'learning_rate': 4.488260923119538e-05, 'epoch': 0.53}
{'loss': 0.5778, 'learning_rate': 4.483450364100345e-05, 'epoch': 0.53}
{'loss': 0.5604, 'learning_rate': 4.478640288339907e-05, 'epoch': 0.53}
{'loss': 0.5844, 'learning_rate': 4.473830700338295e-05, 'epoch': 0.53}
 53%|    | 1731/3250 [5:08:21<4:46:37, 11.32s/it]                                                        53%|    | 1731/3250 [5:08:21<4:46:37, 11.32s/it] 53%|    | 1732/3250 [5:08:32<4:39:30, 11.05s/it]                                                        53%|    | 1732/3250 [5:08:32<4:39:30, 11.05s/it] 53%|    | 1733/3250 [5:08:42<4:34:27, 10.85s/it]                                                        53%|    | 1733/3250 [5:08:42<4:34:27, 10.85s/it] 53%|    | 1734/3250 [5:08:53<4:30:52, 10.72s/it]                                                        53%|    | 1734/3250 [5:08:53<4:30:52, 10.72s/it] 53%|    | 1735/3250 [5:09:03<4:28:29, 10.63s/it]                                                        53%|    | 1735/3250 [5:09:03<4:28:29, 10.63s/it] 53%|    | 1736/3250 [5:09:13<4:26:25, 1{'loss': 1.0559, 'learning_rate': 4.4690216045951305e-05, 'epoch': 0.53}
{'loss': 0.5493, 'learning_rate': 4.4642130056095644e-05, 'epoch': 0.53}
{'loss': 0.5772, 'learning_rate': 4.4594049078802925e-05, 'epoch': 0.53}
{'loss': 0.5825, 'learning_rate': 4.454597315905535e-05, 'epoch': 0.54}
{'loss': 0.5926, 'learning_rate': 4.449790234183044e-05, 'epoch': 0.54}
0.56s/it]                                                        53%|    | 1736/3250 [5:09:13<4:26:25, 10.56s/it] 53%|    | 1737/3250 [5:09:24<4:24:58, 10.51s/it]                                                        53%|    | 1737/3250 [5:09:24<4:24:58, 10.51s/it] 53%|    | 1738/3250 [5:09:35<4:25:43, 10.54s/it]                                                        53%|    | 1738/3250 [5:09:35<4:25:43, 10.54s/it] 54%|    | 1739/3250 [5:09:45<4:24:30, 10.50s/it]                                                        54%|    | 1739/3250 [5:09:45<4:24:30, 10.50s/it] 54%|    | 1740/3250 [5:09:55<4:23:41, 10.48s/it]                                                        54%|    | 1740/3250 [5:09:55<4:23:41, 10.48s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.788033127784729, 'eval_runtime': 2.1338, 'eval_samples_per_second': 5.624, 'eval_steps_per_second': 1.406, 'epoch': 0.54}
                                                        54%|    | 1740/3250 [5:09:57<4:23:41, 10.48s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_10/tmp-checkpoint-1740
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1740I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in 
 /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1740
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1740
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1740
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1740/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1740/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1740/pytorch_model.binthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1740/pytorch_model.bin

/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.5358, 'learning_rate': 4.4449836672100885e-05, 'epoch': 0.54}
{'loss': 0.5788, 'learning_rate': 4.4401776194834613e-05, 'epoch': 0.54}
{'loss': 0.6261, 'learning_rate': 4.435372095499468e-05, 'epoch': 0.54}
{'loss': 0.564, 'learning_rate': 4.430567099753921e-05, 'epoch': 0.54}
{'loss': 0.5648, 'learning_rate': 4.425762636742143e-05, 'epoch': 0.54}
 54%|    | 1741/3250 [5:10:08<4:43:04, 11.26s/it]                                                        54%|    | 1741/3250 [5:10:08<4:43:04, 11.26s/it] 54%|    | 1742/3250 [5:10:19<4:36:19, 10.99s/it]                                                        54%|    | 1742/3250 [5:10:19<4:36:19, 10.99s/it] 54%|    | 1743/3250 [5:10:29<4:31:34, 10.81s/it]                                                        54%|    | 1743/3250 [5:10:29<4:31:34, 10.81s/it] 54%|    | 1744/3250 [5:10:40<4:28:12, 10.69s/it]                                                        54%|    | 1744/3250 [5:10:40<4:28:12, 10.69s/it] 54%|    | 1745/3250 [5:10:50<4:25:53, 10.60s/it]                                                        54%|    | 1745/3250 [5:10:50<4:25:53, 10.60s/it] 54%|    | 1746/3250 [5:11:00<4:24:23, 1{'loss': 0.5657, 'learning_rate': 4.420958710958956e-05, 'epoch': 0.54}
{'loss': 0.5549, 'learning_rate': 4.416155326898679e-05, 'epoch': 0.54}
{'loss': 0.5838, 'learning_rate': 4.4113524890551246e-05, 'epoch': 0.54}
{'loss': 0.5436, 'learning_rate': 4.4065502019215965e-05, 'epoch': 0.54}
{'loss': 0.5696, 'learning_rate': 4.401748469990879e-05, 'epoch': 0.54}
0.55s/it]                                                        54%|    | 1746/3250 [5:11:00<4:24:23, 10.55s/it] 54%|    | 1747/3250 [5:11:11<4:23:04, 10.50s/it]                                                        54%|    | 1747/3250 [5:11:11<4:23:04, 10.50s/it] 54%|    | 1748/3250 [5:11:21<4:22:03, 10.47s/it]                                                        54%|    | 1748/3250 [5:11:21<4:22:03, 10.47s/it] 54%|    | 1749/3250 [5:11:32<4:21:21, 10.45s/it]                                                        54%|    | 1749/3250 [5:11:32<4:21:21, 10.45s/it] 54%|    | 1750/3250 [5:11:42<4:20:47, 10.43s/it]                                                        54%|    | 1750/3250 [5:11:42<4:20:47, 10.43s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.7872155904769897, 'eval_runtime': 2.1119, 'eval_samples_per_second': 5.682, 'eval_steps_per_second': 1.421, 'epoch': 0.54}
                                                        54%|    | 1750/3250 [5:11:44<4:20:47, 10.43s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_10/tmp-checkpoint-1750
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1750
/projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1750
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1750
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1750
the pytorch model path isthe pytorch model path isthe pytorch model path is  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1750/pytorch_model.bin/projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1750/pytorch_model.bin

 /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1750/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1750/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.5624, 'learning_rate': 4.39694729775524e-05, 'epoch': 0.54}
{'loss': 0.577, 'learning_rate': 4.392146689706425e-05, 'epoch': 0.54}
{'loss': 0.587, 'learning_rate': 4.387346650335649e-05, 'epoch': 0.54}
{'loss': 0.5814, 'learning_rate': 4.382547184133593e-05, 'epoch': 0.54}
{'loss': 0.5671, 'learning_rate': 4.377748295590407e-05, 'epoch': 0.54}
 54%|    | 1751/3250 [5:11:55<4:39:43, 11.20s/it]                                                        54%|    | 1751/3250 [5:11:55<4:39:43, 11.20s/it] 54%|    | 1752/3250 [5:12:05<4:33:48, 10.97s/it]                                                        54%|    | 1752/3250 [5:12:05<4:33:48, 10.97s/it] 54%|    | 1753/3250 [5:12:16<4:29:18, 10.79s/it]                                                        54%|    | 1753/3250 [5:12:16<4:29:18, 10.79s/it] 54%|    | 1754/3250 [5:12:27<4:28:56, 10.79s/it]                                                        54%|    | 1754/3250 [5:12:27<4:28:56, 10.79s/it] 54%|    | 1755/3250 [5:12:37<4:26:10, 10.68s/it]                                                        54%|    | 1755/3250 [5:12:37<4:26:10, 10.68s/it] 54%|    | 1756/3250 [5:12:47<4:24:07, 1{'loss': 0.5713, 'learning_rate': 4.372949989195697e-05, 'epoch': 0.54}
{'loss': 0.5485, 'learning_rate': 4.3681522694385256e-05, 'epoch': 0.54}
{'loss': 0.5747, 'learning_rate': 4.3633551408074075e-05, 'epoch': 0.54}
{'loss': 0.5606, 'learning_rate': 4.358558607790303e-05, 'epoch': 0.54}
{'loss': 0.54, 'learning_rate': 4.3537626748746143e-05, 'epoch': 0.54}
0.61s/it]                                                        54%|    | 1756/3250 [5:12:47<4:24:07, 10.61s/it] 54%|    | 1757/3250 [5:12:58<4:22:36, 10.55s/it]                                                        54%|    | 1757/3250 [5:12:58<4:22:36, 10.55s/it] 54%|    | 1758/3250 [5:13:08<4:21:32, 10.52s/it]                                                        54%|    | 1758/3250 [5:13:08<4:21:32, 10.52s/it] 54%|    | 1759/3250 [5:13:19<4:20:39, 10.49s/it]                                                        54%|    | 1759/3250 [5:13:19<4:20:39, 10.49s/it] 54%|    | 1760/3250 [5:13:29<4:20:07, 10.47s/it]                                                        54%|    | 1760/3250 [5:13:29<4:20:07, 10.47s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.7958576083183289, 'eval_runtime': 2.1168, 'eval_samples_per_second': 5.669, 'eval_steps_per_second': 1.417, 'epoch': 0.54}
                                                        54%|    | 1760/3250 [5:13:31<4:20:07, 10.47s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_10/tmp-checkpoint-1760
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1760
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1760
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1760
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1760
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1760/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1760/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1760/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1760/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.5518, 'learning_rate': 4.348967346547185e-05, 'epoch': 0.54}
{'loss': 0.5355, 'learning_rate': 4.344172627294289e-05, 'epoch': 0.54}
{'loss': 0.5823, 'learning_rate': 4.339378521601635e-05, 'epoch': 0.54}
{'loss': 0.5522, 'learning_rate': 4.334585033954355e-05, 'epoch': 0.54}
{'loss': 0.5867, 'learning_rate': 4.329792168837002e-05, 'epoch': 0.54}
 54%|    | 1761/3250 [5:13:42<4:40:50, 11.32s/it]                                                        54%|    | 1761/3250 [5:13:42<4:40:50, 11.32s/it] 54%|    | 1762/3250 [5:13:53<4:34:01, 11.05s/it]                                                        54%|    | 1762/3250 [5:13:53<4:34:01, 11.05s/it] 54%|    | 1763/3250 [5:14:03<4:29:18, 10.87s/it]                                                        54%|    | 1763/3250 [5:14:03<4:29:18, 10.87s/it] 54%|    | 1764/3250 [5:14:14<4:25:48, 10.73s/it]                                                        54%|    | 1764/3250 [5:14:14<4:25:48, 10.73s/it] 54%|    | 1765/3250 [5:14:24<4:23:27, 10.64s/it]                                                        54%|    | 1765/3250 [5:14:24<4:23:27, 10.64s/it] 54%|    | 1766/3250 [5:14:35<4:21:29, 1{'loss': 1.0574, 'learning_rate': 4.3249999307335495e-05, 'epoch': 0.54}
{'loss': 0.5533, 'learning_rate': 4.320208324127383e-05, 'epoch': 0.54}
{'loss': 0.5708, 'learning_rate': 4.3154173535012946e-05, 'epoch': 0.54}
{'loss': 0.5726, 'learning_rate': 4.3106270233374845e-05, 'epoch': 0.54}
{'loss': 0.57, 'learning_rate': 4.3058373381175574e-05, 'epoch': 0.54}
0.57s/it]                                                        54%|    | 1766/3250 [5:14:35<4:21:29, 10.57s/it] 54%|    | 1767/3250 [5:14:45<4:20:19, 10.53s/it]                                                        54%|    | 1767/3250 [5:14:45<4:20:19, 10.53s/it] 54%|    | 1768/3250 [5:14:55<4:19:18, 10.50s/it]                                                        54%|    | 1768/3250 [5:14:55<4:19:18, 10.50s/it] 54%|    | 1769/3250 [5:15:06<4:18:30, 10.47s/it]                                                        54%|    | 1769/3250 [5:15:06<4:18:30, 10.47s/it] 54%|    | 1770/3250 [5:15:16<4:18:04, 10.46s/it]                                                        54%|    | 1770/3250 [5:15:16<4:18:04, 10.46s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.785905122756958, 'eval_runtime': 2.1197, 'eval_samples_per_second': 5.661, 'eval_steps_per_second': 1.415, 'epoch': 0.54}
                                                        54%|    | 1770/3250 [5:15:18<4:18:04, 10.46s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_10/tmp-checkpoint-1770
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1770
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1770
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1770
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1770
the pytorch model path isthe pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1770/pytorch_model.bin 
/projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1770/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1770/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1770/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.5454, 'learning_rate': 4.3010483023225045e-05, 'epoch': 0.54}
{'loss': 0.5501, 'learning_rate': 4.296259920432716e-05, 'epoch': 0.55}
{'loss': 0.6066, 'learning_rate': 4.2914721969279705e-05, 'epoch': 0.55}
{'loss': 0.5831, 'learning_rate': 4.28668513628743e-05, 'epoch': 0.55}
{'loss': 0.5645, 'learning_rate': 4.281898742989636e-05, 'epoch': 0.55}
 54%|    | 1771/3250 [5:15:30<4:38:57, 11.32s/it]                                                        54%|    | 1771/3250 [5:15:30<4:38:57, 11.32s/it] 55%|    | 1772/3250 [5:15:40<4:32:15, 11.05s/it]                                                        55%|    | 1772/3250 [5:15:40<4:32:15, 11.05s/it] 55%|    | 1773/3250 [5:15:50<4:27:41, 10.87s/it]                                                        55%|    | 1773/3250 [5:15:50<4:27:41, 10.87s/it] 55%|    | 1774/3250 [5:16:01<4:24:17, 10.74s/it]                                                        55%|    | 1774/3250 [5:16:01<4:24:17, 10.74s/it] 55%|    | 1775/3250 [5:16:11<4:21:41, 10.64s/it]                                                        55%|    | 1775/3250 [5:16:11<4:21:41, 10.64s/it] 55%|    | 1776/3250 [5:16:22<4:19:45, 1{'loss': 0.5151, 'learning_rate': 4.277113021512505e-05, 'epoch': 0.55}
{'loss': 0.5756, 'learning_rate': 4.2723279763333265e-05, 'epoch': 0.55}
{'loss': 0.5669, 'learning_rate': 4.267543611928754e-05, 'epoch': 0.55}
{'loss': 0.5661, 'learning_rate': 4.2627599327748105e-05, 'epoch': 0.55}
{'loss': 0.5471, 'learning_rate': 4.2579769433468694e-05, 'epoch': 0.55}
0.57s/it]                                                        55%|    | 1776/3250 [5:16:22<4:19:45, 10.57s/it] 55%|    | 1777/3250 [5:16:32<4:18:33, 10.53s/it]                                                        55%|    | 1777/3250 [5:16:32<4:18:33, 10.53s/it] 55%|    | 1778/3250 [5:16:43<4:17:32, 10.50s/it]                                                        55%|    | 1778/3250 [5:16:43<4:17:32, 10.50s/it] 55%|    | 1779/3250 [5:16:53<4:16:49, 10.48s/it]                                                        55%|    | 1779/3250 [5:16:53<4:16:49, 10.48s/it] 55%|    | 1780/3250 [5:17:03<4:16:25, 10.47s/it]                                                        55%|    | 1780/3250 [5:17:03<4:16:25, 10.47s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.7858047485351562, 'eval_runtime': 2.1222, 'eval_samples_per_second': 5.654, 'eval_steps_per_second': 1.414, 'epoch': 0.55}
                                                        55%|    | 1780/3250 [5:17:06<4:16:25, 10.47s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_10/tmp-checkpoint-1780
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1780I AM HERE AND I AM SAVING THE MODEL

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1780
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1780
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1780
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1780/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1780/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1780/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1780/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.57, 'learning_rate': 4.253194648119667e-05, 'epoch': 0.55}
{'loss': 0.5699, 'learning_rate': 4.2484130515672856e-05, 'epoch': 0.55}
{'loss': 0.5911, 'learning_rate': 4.243632158163152e-05, 'epoch': 0.55}
{'loss': 0.5711, 'learning_rate': 4.2388519723800415e-05, 'epoch': 0.55}
{'loss': 0.5577, 'learning_rate': 4.234072498690062e-05, 'epoch': 0.55}
 55%|    | 1781/3250 [5:17:16<4:34:53, 11.23s/it]                                                        55%|    | 1781/3250 [5:17:16<4:34:53, 11.23s/it] 55%|    | 1782/3250 [5:17:27<4:28:44, 10.98s/it]                                                        55%|    | 1782/3250 [5:17:27<4:28:44, 10.98s/it] 55%|    | 1783/3250 [5:17:37<4:24:16, 10.81s/it]                                                        55%|    | 1783/3250 [5:17:37<4:24:16, 10.81s/it] 55%|    | 1784/3250 [5:17:48<4:21:14, 10.69s/it]                                                        55%|    | 1784/3250 [5:17:48<4:21:14, 10.69s/it] 55%|    | 1785/3250 [5:17:58<4:19:20, 10.62s/it]                                                        55%|    | 1785/3250 [5:17:58<4:19:20, 10.62s/it] 55%|    | 1786/3250 [5:18:09<4:17:43, 1{'loss': 0.5744, 'learning_rate': 4.229293741564658e-05, 'epoch': 0.55}
{'loss': 0.5705, 'learning_rate': 4.224515705474603e-05, 'epoch': 0.55}
{'loss': 0.5391, 'learning_rate': 4.2197383948899925e-05, 'epoch': 0.55}
{'loss': 0.6113, 'learning_rate': 4.2149618142802494e-05, 'epoch': 0.55}
{'loss': 0.5481, 'learning_rate': 4.210185968114109e-05, 'epoch': 0.55}
0.56s/it]                                                        55%|    | 1786/3250 [5:18:09<4:17:43, 10.56s/it] 55%|    | 1787/3250 [5:18:19<4:19:12, 10.63s/it]                                                        55%|    | 1787/3250 [5:18:19<4:19:12, 10.63s/it] 55%|    | 1788/3250 [5:18:30<4:17:24, 10.56s/it]                                                        55%|    | 1788/3250 [5:18:30<4:17:24, 10.56s/it] 55%|    | 1789/3250 [5:18:40<4:16:01, 10.51s/it]                                                        55%|    | 1789/3250 [5:18:40<4:16:01, 10.51s/it] 55%|    | 1790/3250 [5:18:51<4:15:09, 10.49s/it]                                                        55%|    | 1790/3250 [5:18:51<4:15:09, 10.49s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.792622447013855, 'eval_runtime': 2.3507, 'eval_samples_per_second': 5.105, 'eval_steps_per_second': 1.276, 'epoch': 0.55}
                                                        55%|    | 1790/3250 [5:18:53<4:15:09, 10.49s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_10/tmp-checkpoint-1790
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1790
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1790
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1790
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1790
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1790/pytorch_model.bin
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1790/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1790/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1790/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.5421, 'learning_rate': 4.20541086085962e-05, 'epoch': 0.55}
{'loss': 0.5368, 'learning_rate': 4.200636496984144e-05, 'epoch': 0.55}
{'loss': 0.5739, 'learning_rate': 4.1958628809543416e-05, 'epoch': 0.55}
{'loss': 0.5548, 'learning_rate': 4.1910900172361764e-05, 'epoch': 0.55}
{'loss': 0.5675, 'learning_rate': 4.1863179102949094e-05, 'epoch': 0.55}
 55%|    | 1791/3250 [5:19:04<4:34:52, 11.30s/it]                                                        55%|    | 1791/3250 [5:19:04<4:34:52, 11.30s/it] 55%|    | 1792/3250 [5:19:14<4:28:06, 11.03s/it]                                                        55%|    | 1792/3250 [5:19:14<4:28:06, 11.03s/it] 55%|    | 1793/3250 [5:19:25<4:23:24, 10.85s/it]                                                        55%|    | 1793/3250 [5:19:25<4:23:24, 10.85s/it] 55%|    | 1794/3250 [5:19:35<4:19:58, 10.71s/it]                                                        55%|    | 1794/3250 [5:19:35<4:19:58, 10.71s/it] 55%|    | 1795/3250 [5:19:45<4:17:39, 10.63s/it]                                                        55%|    | 1795/3250 [5:19:45<4:17:39, 10.63s/it] 55%|    | 1796/3250 [5:19:56<4:15:43, 1{'loss': 0.7974, 'learning_rate': 4.18154656459509e-05, 'epoch': 0.55}
{'loss': 0.8217, 'learning_rate': 4.1767759846005596e-05, 'epoch': 0.55}
{'loss': 0.5426, 'learning_rate': 4.1720061747744396e-05, 'epoch': 0.55}
{'loss': 0.573, 'learning_rate': 4.1672371395791335e-05, 'epoch': 0.55}
{'loss': 0.5697, 'learning_rate': 4.162468883476319e-05, 'epoch': 0.55}
0.55s/it]                                                        55%|    | 1796/3250 [5:19:56<4:15:43, 10.55s/it] 55%|    | 1797/3250 [5:20:06<4:14:18, 10.50s/it]                                                        55%|    | 1797/3250 [5:20:06<4:14:18, 10.50s/it] 55%|    | 1798/3250 [5:20:17<4:13:19, 10.47s/it]                                                        55%|    | 1798/3250 [5:20:17<4:13:19, 10.47s/it] 55%|    | 1799/3250 [5:20:27<4:12:27, 10.44s/it]                                                        55%|    | 1799/3250 [5:20:27<4:12:27, 10.44s/it] 55%|    | 1800/3250 [5:20:37<4:11:55, 10.42s/it]                                                        55%|    | 1800/3250 [5:20:37<4:11:55, 10.42s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.7831979990005493, 'eval_runtime': 2.1063, 'eval_samples_per_second': 5.697, 'eval_steps_per_second': 1.424, 'epoch': 0.55}
                                                        55%|    | 1800/3250 [5:20:39<4:11:55, 10.42s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_10/tmp-checkpoint-1800
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1800
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1800
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1800
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1800
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1800/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1800/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1800/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1800/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.5447, 'learning_rate': 4.157701410926943e-05, 'epoch': 0.55}
{'loss': 0.5314, 'learning_rate': 4.152934726391223e-05, 'epoch': 0.55}
{'loss': 0.5945, 'learning_rate': 4.148168834328638e-05, 'epoch': 0.55}
{'loss': 0.5858, 'learning_rate': 4.1434037391979266e-05, 'epoch': 0.56}
{'loss': 0.5474, 'learning_rate': 4.1386394454570745e-05, 'epoch': 0.56}
 55%|    | 1801/3250 [5:20:50<4:30:09, 11.19s/it]                                                        55%|    | 1801/3250 [5:20:50<4:30:09, 11.19s/it] 55%|    | 1802/3250 [5:21:01<4:24:23, 10.96s/it]                                                        55%|    | 1802/3250 [5:21:01<4:24:23, 10.96s/it] 55%|    | 1803/3250 [5:21:11<4:20:18, 10.79s/it]                                                        55%|    | 1803/3250 [5:21:11<4:20:18, 10.79s/it] 56%|    | 1804/3250 [5:21:22<4:19:04, 10.75s/it]                                                        56%|    | 1804/3250 [5:21:22<4:19:04, 10.75s/it] 56%|    | 1805/3250 [5:21:32<4:16:31, 10.65s/it]                                                        56%|    | 1805/3250 [5:21:32<4:16:31, 10.65s/it] 56%|    | 1806/3250 [5:21:43<4:14:34, 1{'loss': 0.5297, 'learning_rate': 4.133875957563329e-05, 'epoch': 0.56}
{'loss': 0.5693, 'learning_rate': 4.129113279973177e-05, 'epoch': 0.56}
{'loss': 0.5549, 'learning_rate': 4.124351417142347e-05, 'epoch': 0.56}
{'loss': 0.5718, 'learning_rate': 4.1195903735258064e-05, 'epoch': 0.56}
{'loss': 0.5454, 'learning_rate': 4.114830153577759e-05, 'epoch': 0.56}
0.58s/it]                                                        56%|    | 1806/3250 [5:21:43<4:14:34, 10.58s/it] 56%|    | 1807/3250 [5:21:53<4:13:19, 10.53s/it]                                                        56%|    | 1807/3250 [5:21:53<4:13:19, 10.53s/it] 56%|    | 1808/3250 [5:22:03<4:12:23, 10.50s/it]                                                        56%|    | 1808/3250 [5:22:03<4:12:23, 10.50s/it] 56%|    | 1809/3250 [5:22:14<4:11:31, 10.47s/it]                                                        56%|    | 1809/3250 [5:22:14<4:11:31, 10.47s/it] 56%|    | 1810/3250 [5:22:24<4:10:52, 10.45s/it]                                                        56%|    | 1810/3250 [5:22:24<4:10:52, 10.45s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.784672200679779, 'eval_runtime': 2.1103, 'eval_samples_per_second': 5.686, 'eval_steps_per_second': 1.422, 'epoch': 0.56}
                                                        56%|    | 1810/3250 [5:22:26<4:10:52, 10.45s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_10/tmp-checkpoint-1810
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1810
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1810
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1810
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1810
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1810/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1810/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1810/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1810/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.5566, 'learning_rate': 4.110070761751633e-05, 'epoch': 0.56}
{'loss': 0.5613, 'learning_rate': 4.1053122025000843e-05, 'epoch': 0.56}
{'loss': 0.5786, 'learning_rate': 4.100554480274993e-05, 'epoch': 0.56}
{'loss': 0.5745, 'learning_rate': 4.095797599527449e-05, 'epoch': 0.56}
{'loss': 0.5526, 'learning_rate': 4.09104156470776e-05, 'epoch': 0.56}
 56%|    | 1811/3250 [5:22:37<4:29:24, 11.23s/it]                                                        56%|    | 1811/3250 [5:22:37<4:29:24, 11.23s/it] 56%|    | 1812/3250 [5:22:48<4:23:14, 10.98s/it]                                                        56%|    | 1812/3250 [5:22:48<4:23:14, 10.98s/it] 56%|    | 1813/3250 [5:22:58<4:18:54, 10.81s/it]                                                        56%|    | 1813/3250 [5:22:58<4:18:54, 10.81s/it] 56%|    | 1814/3250 [5:23:09<4:15:44, 10.69s/it]                                                        56%|    | 1814/3250 [5:23:09<4:15:44, 10.69s/it] 56%|    | 1815/3250 [5:23:19<4:13:34, 10.60s/it]                                                        56%|    | 1815/3250 [5:23:19<4:13:34, 10.60s/it] 56%|    | 1816/3250 [5:23:29<4:12:05, 1{'loss': 0.5636, 'learning_rate': 4.086286380265443e-05, 'epoch': 0.56}
{'loss': 0.5784, 'learning_rate': 4.081532050649216e-05, 'epoch': 0.56}
{'loss': 0.5212, 'learning_rate': 4.076778580306999e-05, 'epoch': 0.56}
{'loss': 0.5956, 'learning_rate': 4.072025973685908e-05, 'epoch': 0.56}
{'loss': 0.555, 'learning_rate': 4.067274235232251e-05, 'epoch': 0.56}
0.55s/it]                                                        56%|    | 1816/3250 [5:23:29<4:12:05, 10.55s/it] 56%|    | 1817/3250 [5:23:40<4:11:00, 10.51s/it]                                                        56%|    | 1817/3250 [5:23:40<4:11:00, 10.51s/it] 56%|    | 1818/3250 [5:23:50<4:10:04, 10.48s/it]                                                        56%|    | 1818/3250 [5:23:50<4:10:04, 10.48s/it] 56%|    | 1819/3250 [5:24:01<4:09:39, 10.47s/it]                                                        56%|    | 1819/3250 [5:24:01<4:09:39, 10.47s/it] 56%|    | 1820/3250 [5:24:11<4:12:04, 10.58s/it]                                                        56%|    | 1820/3250 [5:24:11<4:12:04, 10.58s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.7924842238426208, 'eval_runtime': 2.127, 'eval_samples_per_second': 5.642, 'eval_steps_per_second': 1.41, 'epoch': 0.56}
                                                        56%|    | 1820/3250 [5:24:14<4:12:04, 10.58s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_10/tmp-checkpoint-1820
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1820I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in 
 /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1820
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1820
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1820
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1820/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1820/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1820/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1820/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.5512, 'learning_rate': 4.0625233693915264e-05, 'epoch': 0.56}
{'loss': 0.5248, 'learning_rate': 4.057773380608411e-05, 'epoch': 0.56}
{'loss': 0.5496, 'learning_rate': 4.053024273326761e-05, 'epoch': 0.56}
{'loss': 0.5636, 'learning_rate': 4.048276051989614e-05, 'epoch': 0.56}
{'loss': 0.5436, 'learning_rate': 4.0435287210391756e-05, 'epoch': 0.56}
 56%|    | 1821/3250 [5:24:24<4:29:19, 11.31s/it]                                                        56%|    | 1821/3250 [5:24:24<4:29:19, 11.31s/it] 56%|    | 1822/3250 [5:24:35<4:22:51, 11.04s/it]                                                        56%|    | 1822/3250 [5:24:35<4:22:51, 11.04s/it] 56%|    | 1823/3250 [5:24:45<4:18:19, 10.86s/it]                                                        56%|    | 1823/3250 [5:24:45<4:18:19, 10.86s/it] 56%|    | 1824/3250 [5:24:56<4:15:14, 10.74s/it]                                                        56%|    | 1824/3250 [5:24:56<4:15:14, 10.74s/it] 56%|    | 1825/3250 [5:25:06<4:12:59, 10.65s/it]                                                        56%|    | 1825/3250 [5:25:06<4:12:59, 10.65s/it] 56%|    | 1826/3250 [5:25:17<4:11:21, 1{'loss': 0.5738, 'learning_rate': 4.038782284916816e-05, 'epoch': 0.56}
{'loss': 1.0574, 'learning_rate': 4.034036748063072e-05, 'epoch': 0.56}
{'loss': 0.5386, 'learning_rate': 4.029292114917638e-05, 'epoch': 0.56}
{'loss': 0.5716, 'learning_rate': 4.0245483899193595e-05, 'epoch': 0.56}
{'loss': 0.574, 'learning_rate': 4.019805577506237e-05, 'epoch': 0.56}
0.59s/it]                                                        56%|    | 1826/3250 [5:25:17<4:11:21, 10.59s/it] 56%|    | 1827/3250 [5:25:27<4:09:54, 10.54s/it]                                                        56%|    | 1827/3250 [5:25:27<4:09:54, 10.54s/it] 56%|    | 1828/3250 [5:25:38<4:08:56, 10.50s/it]                                                        56%|    | 1828/3250 [5:25:38<4:08:56, 10.50s/it] 56%|    | 1829/3250 [5:25:48<4:08:11, 10.48s/it]                                                        56%|    | 1829/3250 [5:25:48<4:08:11, 10.48s/it] 56%|    | 1830/3250 [5:25:58<4:07:42, 10.47s/it]                                                        56%|    | 1830/3250 [5:25:58<4:07:42, 10.47s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.783315122127533, 'eval_runtime': 2.3486, 'eval_samples_per_second': 5.109, 'eval_steps_per_second': 1.277, 'epoch': 0.56}
                                                        56%|    | 1830/3250 [5:26:01<4:07:42, 10.47s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_10/tmp-checkpoint-1830
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1830
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1830
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1830
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1830
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1830/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1830/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1830/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1830/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.5751, 'learning_rate': 4.0150636821154166e-05, 'epoch': 0.56}
{'loss': 0.5295, 'learning_rate': 4.010322708183183e-05, 'epoch': 0.56}
{'loss': 0.5448, 'learning_rate': 4.005582660144963e-05, 'epoch': 0.56}
{'loss': 0.6337, 'learning_rate': 4.000843542435315e-05, 'epoch': 0.56}
{'loss': 0.5486, 'learning_rate': 3.9961053594879266e-05, 'epoch': 0.56}
 56%|    | 1831/3250 [5:26:12<4:27:22, 11.31s/it]                                                        56%|    | 1831/3250 [5:26:12<4:27:22, 11.31s/it] 56%|    | 1832/3250 [5:26:22<4:20:43, 11.03s/it]                                                        56%|    | 1832/3250 [5:26:22<4:20:43, 11.03s/it] 56%|    | 1833/3250 [5:26:32<4:16:06, 10.84s/it]                                                        56%|    | 1833/3250 [5:26:32<4:16:06, 10.84s/it] 56%|    | 1834/3250 [5:26:43<4:12:43, 10.71s/it]                                                        56%|    | 1834/3250 [5:26:43<4:12:43, 10.71s/it] 56%|    | 1835/3250 [5:26:53<4:10:20, 10.61s/it]                                                        56%|    | 1835/3250 [5:26:53<4:10:20, 10.61s/it] 56%|    | 1836/3250 [5:27:04<4:08:39, 1{'loss': 0.5558, 'learning_rate': 3.991368115735612e-05, 'epoch': 0.56}
{'loss': 0.548, 'learning_rate': 3.986631815610308e-05, 'epoch': 0.57}
{'loss': 0.5516, 'learning_rate': 3.981896463543067e-05, 'epoch': 0.57}
{'loss': 0.5823, 'learning_rate': 3.977162063964049e-05, 'epoch': 0.57}
{'loss': 0.5408, 'learning_rate': 3.972428621302534e-05, 'epoch': 0.57}
0.55s/it]                                                        56%|    | 1836/3250 [5:27:04<4:08:39, 10.55s/it] 57%|    | 1837/3250 [5:27:14<4:10:12, 10.62s/it]                                                        57%|    | 1837/3250 [5:27:14<4:10:12, 10.62s/it] 57%|    | 1838/3250 [5:27:25<4:08:27, 10.56s/it]                                                        57%|    | 1838/3250 [5:27:25<4:08:27, 10.56s/it] 57%|    | 1839/3250 [5:27:35<4:07:02, 10.51s/it]                                                        57%|    | 1839/3250 [5:27:35<4:07:02, 10.51s/it] 57%|    | 1840/3250 [5:27:46<4:06:06, 10.47s/it]                                                        57%|    | 1840/3250 [5:27:46<4:06:06, 10.47s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.7874351739883423, 'eval_runtime': 2.1029, 'eval_samples_per_second': 5.706, 'eval_steps_per_second': 1.427, 'epoch': 0.57}
                                                        57%|    | 1840/3250 [5:27:48<4:06:06, 10.47s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_10/tmp-checkpoint-1840
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1840 /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1840

I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1840
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1840
the pytorch model path isthe pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1840/pytorch_model.bin 
/projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1840/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1840/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1840/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.5571, 'learning_rate': 3.9676961399869e-05, 'epoch': 0.57}
{'loss': 0.5457, 'learning_rate': 3.962964624444625e-05, 'epoch': 0.57}
{'loss': 0.5602, 'learning_rate': 3.958234079102288e-05, 'epoch': 0.57}
{'loss': 0.5849, 'learning_rate': 3.953504508385554e-05, 'epoch': 0.57}
{'loss': 0.5727, 'learning_rate': 3.9487759167191815e-05, 'epoch': 0.57}
 57%|    | 1841/3250 [5:27:59<4:23:24, 11.22s/it]                                                        57%|    | 1841/3250 [5:27:59<4:23:24, 11.22s/it] 57%|    | 1842/3250 [5:28:09<4:17:24, 10.97s/it]                                                        57%|    | 1842/3250 [5:28:09<4:17:24, 10.97s/it] 57%|    | 1843/3250 [5:28:19<4:13:14, 10.80s/it]                                                        57%|    | 1843/3250 [5:28:19<4:13:14, 10.80s/it] 57%|    | 1844/3250 [5:28:30<4:10:08, 10.67s/it]                                                        57%|    | 1844/3250 [5:28:30<4:10:08, 10.67s/it] 57%|    | 1845/3250 [5:28:40<4:08:05, 10.59s/it]                                                        57%|    | 1845/3250 [5:28:40<4:08:05, 10.59s/it] 57%|    | 1846/3250 [5:28:51<4:06:29, 1{'loss': 0.564, 'learning_rate': 3.9440483085270126e-05, 'epoch': 0.57}
{'loss': 0.5594, 'learning_rate': 3.939321688231965e-05, 'epoch': 0.57}
{'loss': 0.5365, 'learning_rate': 3.934596060256037e-05, 'epoch': 0.57}
{'loss': 0.5719, 'learning_rate': 3.9298714290202977e-05, 'epoch': 0.57}
{'loss': 0.5544, 'learning_rate': 3.92514779894488e-05, 'epoch': 0.57}
0.53s/it]                                                        57%|    | 1846/3250 [5:28:51<4:06:29, 10.53s/it] 57%|    | 1847/3250 [5:29:01<4:05:22, 10.49s/it]                                                        57%|    | 1847/3250 [5:29:01<4:05:22, 10.49s/it] 57%|    | 1848/3250 [5:29:11<4:04:36, 10.47s/it]                                                        57%|    | 1848/3250 [5:29:11<4:04:36, 10.47s/it] 57%|    | 1849/3250 [5:29:22<4:03:59, 10.45s/it]                                                        57%|    | 1849/3250 [5:29:22<4:03:59, 10.45s/it] 57%|    | 1850/3250 [5:29:32<4:03:30, 10.44s/it]                                                        57%|    | 1850/3250 [5:29:32<4:03:30, 10.44s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.792710542678833, 'eval_runtime': 2.108, 'eval_samples_per_second': 5.693, 'eval_steps_per_second': 1.423, 'epoch': 0.57}
                                                        57%|    | 1850/3250 [5:29:34<4:03:30, 10.44s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_10/tmp-checkpoint-1850
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1850I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1850

I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1850
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1850
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1850/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1850/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1850/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1850/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.526, 'learning_rate': 3.920425174448984e-05, 'epoch': 0.57}
{'loss': 0.5417, 'learning_rate': 3.9157035599508684e-05, 'epoch': 0.57}
{'loss': 0.5236, 'learning_rate': 3.910982959867845e-05, 'epoch': 0.57}
{'loss': 0.5746, 'learning_rate': 3.906263378616279e-05, 'epoch': 0.57}
{'loss': 0.551, 'learning_rate': 3.901544820611584e-05, 'epoch': 0.57}
 57%|    | 1851/3250 [5:29:45<4:21:06, 11.20s/it]                                                        57%|    | 1851/3250 [5:29:45<4:21:06, 11.20s/it] 57%|    | 1852/3250 [5:29:56<4:15:20, 10.96s/it]                                                        57%|    | 1852/3250 [5:29:56<4:15:20, 10.96s/it] 57%|    | 1853/3250 [5:30:06<4:12:59, 10.87s/it]                                                        57%|    | 1853/3250 [5:30:06<4:12:59, 10.87s/it] 57%|    | 1854/3250 [5:30:17<4:09:38, 10.73s/it]                                                        57%|    | 1854/3250 [5:30:17<4:09:38, 10.73s/it] 57%|    | 1855/3250 [5:30:27<4:07:12, 10.63s/it]                                                        57%|    | 1855/3250 [5:30:27<4:07:12, 10.63s/it] 57%|    | 1856/3250 [5:30:37<4:05:25, 1{'loss': 0.5747, 'learning_rate': 3.89682729026821e-05, 'epoch': 0.57}
{'loss': 1.0479, 'learning_rate': 3.892110791999649e-05, 'epoch': 0.57}
{'loss': 0.5569, 'learning_rate': 3.887395330218429e-05, 'epoch': 0.57}
{'loss': 0.5576, 'learning_rate': 3.882680909336108e-05, 'epoch': 0.57}
{'loss': 0.5708, 'learning_rate': 3.877967533763267e-05, 'epoch': 0.57}
0.56s/it]                                                        57%|    | 1856/3250 [5:30:37<4:05:25, 10.56s/it] 57%|    | 1857/3250 [5:30:48<4:08:15, 10.69s/it]                                                        57%|    | 1857/3250 [5:30:48<4:08:15, 10.69s/it] 57%|    | 1858/3250 [5:30:59<4:06:14, 10.61s/it]                                                        57%|    | 1858/3250 [5:30:59<4:06:14, 10.61s/it] 57%|    | 1859/3250 [5:31:09<4:04:30, 10.55s/it]                                                        57%|    | 1859/3250 [5:31:09<4:04:30, 10.55s/it] 57%|    | 1860/3250 [5:31:20<4:03:21, 10.50s/it]                                                        57%|    | 1860/3250 [5:31:20<4:03:21, 10.50s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.7833266854286194, 'eval_runtime': 2.6981, 'eval_samples_per_second': 4.448, 'eval_steps_per_second': 1.112, 'epoch': 0.57}
                                                        57%|    | 1860/3250 [5:31:22<4:03:21, 10.50s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_10/tmp-checkpoint-1860
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1860I AM HERE AND I AM SAVING THE MODEL

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1860
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1860
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1860
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1860/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1860/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1860/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1860/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.576, 'learning_rate': 3.873255207909514e-05, 'epoch': 0.57}
{'loss': 0.5295, 'learning_rate': 3.86854393618347e-05, 'epoch': 0.57}
{'loss': 0.551, 'learning_rate': 3.863833722992774e-05, 'epoch': 0.57}
{'loss': 0.6112, 'learning_rate': 3.859124572744071e-05, 'epoch': 0.57}
{'loss': 0.5733, 'learning_rate': 3.854416489843014e-05, 'epoch': 0.57}
 57%|    | 1861/3250 [5:31:33<4:24:24, 11.42s/it]                                                        57%|    | 1861/3250 [5:31:33<4:24:24, 11.42s/it] 57%|    | 1862/3250 [5:31:44<4:17:05, 11.11s/it]                                                        57%|    | 1862/3250 [5:31:44<4:17:05, 11.11s/it] 57%|    | 1863/3250 [5:31:54<4:11:52, 10.90s/it]                                                        57%|    | 1863/3250 [5:31:54<4:11:52, 10.90s/it] 57%|    | 1864/3250 [5:32:04<4:08:09, 10.74s/it]                                                        57%|    | 1864/3250 [5:32:04<4:08:09, 10.74s/it] 57%|    | 1865/3250 [5:32:15<4:05:30, 10.64s/it]                                                        57%|    | 1865/3250 [5:32:15<4:05:30, 10.64s/it] 57%|    | 1866/3250 [5:32:25<4:03:39, 1{'loss': 0.5549, 'learning_rate': 3.849709478694256e-05, 'epoch': 0.57}
{'loss': 0.5161, 'learning_rate': 3.8450035437014494e-05, 'epoch': 0.57}
{'loss': 0.5682, 'learning_rate': 3.8402986892672377e-05, 'epoch': 0.57}
{'loss': 0.5579, 'learning_rate': 3.8355949197932535e-05, 'epoch': 0.58}
{'loss': 0.5579, 'learning_rate': 3.830892239680117e-05, 'epoch': 0.58}
0.56s/it]                                                        57%|    | 1866/3250 [5:32:25<4:03:39, 10.56s/it] 57%|    | 1867/3250 [5:32:36<4:03:13, 10.55s/it]                                                        57%|    | 1867/3250 [5:32:36<4:03:13, 10.55s/it] 57%|    | 1868/3250 [5:32:46<4:02:14, 10.52s/it]                                                        57%|    | 1868/3250 [5:32:46<4:02:14, 10.52s/it] 58%|    | 1869/3250 [5:32:57<4:03:59, 10.60s/it]                                                        58%|    | 1869/3250 [5:32:57<4:03:59, 10.60s/it] 58%|    | 1870/3250 [5:33:07<4:02:24, 10.54s/it]                                                        58%|    | 1870/3250 [5:33:07<4:02:24, 10.54s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.7856227159500122, 'eval_runtime': 2.112, 'eval_samples_per_second': 5.682, 'eval_steps_per_second': 1.42, 'epoch': 0.58}
                                                        58%|    | 1870/3250 [5:33:09<4:02:24, 10.54s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_10/tmp-checkpoint-1870
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1870
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1870
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1870
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1870
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1870/pytorch_model.bin
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1870/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1870/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1870/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.554, 'learning_rate': 3.8261906533274254e-05, 'epoch': 0.58}
{'loss': 0.5693, 'learning_rate': 3.8214901651337574e-05, 'epoch': 0.58}
{'loss': 0.5621, 'learning_rate': 3.8167907794966574e-05, 'epoch': 0.58}
{'loss': 0.5723, 'learning_rate': 3.812092500812646e-05, 'epoch': 0.58}
{'loss': 0.5709, 'learning_rate': 3.807395333477201e-05, 'epoch': 0.58}
 58%|    | 1871/3250 [5:33:20<4:19:07, 11.27s/it]                                                        58%|    | 1871/3250 [5:33:20<4:19:07, 11.27s/it] 58%|    | 1872/3250 [5:33:31<4:12:53, 11.01s/it]                                                        58%|    | 1872/3250 [5:33:31<4:12:53, 11.01s/it] 58%|    | 1873/3250 [5:33:41<4:08:30, 10.83s/it]                                                        58%|    | 1873/3250 [5:33:41<4:08:30, 10.83s/it] 58%|    | 1874/3250 [5:33:52<4:05:28, 10.70s/it]                                                        58%|    | 1874/3250 [5:33:52<4:05:28, 10.70s/it] 58%|    | 1875/3250 [5:34:02<4:03:16, 10.62s/it]                                                        58%|    | 1875/3250 [5:34:02<4:03:16, 10.62s/it] 58%|    | 1876/3250 [5:34:12<4:01:41, 1{'loss': 0.5647, 'learning_rate': 3.802699281884767e-05, 'epoch': 0.58}
{'loss': 0.5741, 'learning_rate': 3.798004350428741e-05, 'epoch': 0.58}
{'loss': 0.5617, 'learning_rate': 3.793310543501473e-05, 'epoch': 0.58}
{'loss': 0.5326, 'learning_rate': 3.7886178654942595e-05, 'epoch': 0.58}
{'loss': 0.5941, 'learning_rate': 3.7839263207973444e-05, 'epoch': 0.58}
0.55s/it]                                                        58%|    | 1876/3250 [5:34:12<4:01:41, 10.55s/it] 58%|    | 1877/3250 [5:34:23<4:00:28, 10.51s/it]                                                        58%|    | 1877/3250 [5:34:23<4:00:28, 10.51s/it] 58%|    | 1878/3250 [5:34:33<3:59:27, 10.47s/it]                                                        58%|    | 1878/3250 [5:34:33<3:59:27, 10.47s/it] 58%|    | 1879/3250 [5:34:44<3:58:53, 10.45s/it]                                                        58%|    | 1879/3250 [5:34:44<3:58:53, 10.45s/it] 58%|    | 1880/3250 [5:34:54<3:58:16, 10.44s/it]                                                        58%|    | 1880/3250 [5:34:54<3:58:16, 10.44s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.7868923544883728, 'eval_runtime': 2.1021, 'eval_samples_per_second': 5.708, 'eval_steps_per_second': 1.427, 'epoch': 0.58}
                                                        58%|    | 1880/3250 [5:34:56<3:58:16, 10.44s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_10/tmp-checkpoint-1880
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1880
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1880
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1880
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1880
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1880/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1880/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1880/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1880/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.5506, 'learning_rate': 3.7792359137999064e-05, 'epoch': 0.58}
{'loss': 0.5397, 'learning_rate': 3.774546648890066e-05, 'epoch': 0.58}
{'loss': 0.5367, 'learning_rate': 3.769858530454869e-05, 'epoch': 0.58}
{'loss': 0.5586, 'learning_rate': 3.7651715628802916e-05, 'epoch': 0.58}
{'loss': 0.5555, 'learning_rate': 3.7604857505512345e-05, 'epoch': 0.58}
 58%|    | 1881/3250 [5:35:07<4:15:27, 11.20s/it]                                                        58%|    | 1881/3250 [5:35:07<4:15:27, 11.20s/it] 58%|    | 1882/3250 [5:35:17<4:09:46, 10.96s/it]                                                        58%|    | 1882/3250 [5:35:17<4:09:46, 10.96s/it] 58%|    | 1883/3250 [5:35:28<4:05:47, 10.79s/it]                                                        58%|    | 1883/3250 [5:35:28<4:05:47, 10.79s/it] 58%|    | 1884/3250 [5:35:38<4:03:01, 10.67s/it]                                                        58%|    | 1884/3250 [5:35:38<4:03:01, 10.67s/it] 58%|    | 1885/3250 [5:35:49<4:01:06, 10.60s/it]                                                        58%|    | 1885/3250 [5:35:49<4:01:06, 10.60s/it] 58%|    | 1886/3250 [5:35:59<4:01:22, 1{'loss': 0.5634, 'learning_rate': 3.7558010978515143e-05, 'epoch': 0.58}
{'loss': 0.8874, 'learning_rate': 3.7511176091638653e-05, 'epoch': 0.58}
{'loss': 0.7038, 'learning_rate': 3.7464352888699333e-05, 'epoch': 0.58}
{'loss': 0.5462, 'learning_rate': 3.74175414135027e-05, 'epoch': 0.58}
{'loss': 0.58, 'learning_rate': 3.737074170984326e-05, 'epoch': 0.58}
0.62s/it]                                                        58%|    | 1886/3250 [5:35:59<4:01:22, 10.62s/it] 58%|    | 1887/3250 [5:36:10<3:59:38, 10.55s/it]                                                        58%|    | 1887/3250 [5:36:10<3:59:38, 10.55s/it] 58%|    | 1888/3250 [5:36:20<3:58:24, 10.50s/it]                                                        58%|    | 1888/3250 [5:36:20<3:58:24, 10.50s/it] 58%|    | 1889/3250 [5:36:30<3:57:28, 10.47s/it]                                                        58%|    | 1889/3250 [5:36:30<3:57:28, 10.47s/it] 58%|    | 1890/3250 [5:36:41<3:56:41, 10.44s/it]                                                        58%|    | 1890/3250 [5:36:41<3:56:41, 10.44s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.7803140878677368, 'eval_runtime': 2.3331, 'eval_samples_per_second': 5.143, 'eval_steps_per_second': 1.286, 'epoch': 0.58}
                                                        58%|    | 1890/3250 [5:36:43<3:56:41, 10.44s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_10/tmp-checkpoint-1890
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1890
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1890
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1890
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1890
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1890/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1890/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1890/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1890/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.5687, 'learning_rate': 3.7323953821504576e-05, 'epoch': 0.58}
{'loss': 0.5548, 'learning_rate': 3.7277177792259114e-05, 'epoch': 0.58}
{'loss': 0.5373, 'learning_rate': 3.723041366586826e-05, 'epoch': 0.58}
{'loss': 0.5989, 'learning_rate': 3.7183661486082245e-05, 'epoch': 0.58}
{'loss': 0.5743, 'learning_rate': 3.7136921296640165e-05, 'epoch': 0.58}
 58%|    | 1891/3250 [5:36:54<4:15:29, 11.28s/it]                                                        58%|    | 1891/3250 [5:36:54<4:15:29, 11.28s/it] 58%|    | 1892/3250 [5:37:04<4:09:22, 11.02s/it]                                                        58%|    | 1892/3250 [5:37:04<4:09:22, 11.02s/it] 58%|    | 1893/3250 [5:37:15<4:05:02, 10.83s/it]                                                        58%|    | 1893/3250 [5:37:15<4:05:02, 10.83s/it] 58%|    | 1894/3250 [5:37:25<4:01:55, 10.70s/it]                                                        58%|    | 1894/3250 [5:37:25<4:01:55, 10.70s/it] 58%|    | 1895/3250 [5:37:36<3:59:34, 10.61s/it]                                                        58%|    | 1895/3250 [5:37:36<3:59:34, 10.61s/it] 58%|    | 1896/3250 [5:37:46<3:57:56, 1{'loss': 0.5406, 'learning_rate': 3.709019314126985e-05, 'epoch': 0.58}
{'loss': 0.5197, 'learning_rate': 3.704347706368789e-05, 'epoch': 0.58}
{'loss': 0.5641, 'learning_rate': 3.69967731075996e-05, 'epoch': 0.58}
{'loss': 0.555, 'learning_rate': 3.695008131669891e-05, 'epoch': 0.58}
{'loss': 0.5556, 'learning_rate': 3.690340173466842e-05, 'epoch': 0.58}
0.54s/it]                                                        58%|    | 1896/3250 [5:37:46<3:57:56, 10.54s/it] 58%|    | 1897/3250 [5:37:56<3:56:39, 10.49s/it]                                                        58%|    | 1897/3250 [5:37:56<3:56:39, 10.49s/it] 58%|    | 1898/3250 [5:38:07<3:55:45, 10.46s/it]                                                        58%|    | 1898/3250 [5:38:07<3:55:45, 10.46s/it] 58%|    | 1899/3250 [5:38:18<4:02:56, 10.79s/it]                                                        58%|    | 1899/3250 [5:38:18<4:02:56, 10.79s/it] 58%|    | 1900/3250 [5:38:29<4:00:38, 10.70s/it]                                                        58%|    | 1900/3250 [5:38:29<4:00:38, 10.70s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.7862010598182678, 'eval_runtime': 2.6504, 'eval_samples_per_second': 4.528, 'eval_steps_per_second': 1.132, 'epoch': 0.58}
                                                        58%|    | 1900/3250 [5:38:31<4:00:38, 10.70s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_10/tmp-checkpoint-1900
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1900
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1900
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1900
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1900
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1900/pytorch_model.bin
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1900/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1900/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1900/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.5453, 'learning_rate': 3.685673440517924e-05, 'epoch': 0.58}
{'loss': 0.5517, 'learning_rate': 3.6810079371891094e-05, 'epoch': 0.59}
{'loss': 0.5461, 'learning_rate': 3.6763436678452153e-05, 'epoch': 0.59}
{'loss': 0.573, 'learning_rate': 3.6716806368499045e-05, 'epoch': 0.59}
{'loss': 0.5662, 'learning_rate': 3.667018848565683e-05, 'epoch': 0.59}
 58%|    | 1901/3250 [5:38:42<4:19:48, 11.56s/it]                                                        58%|    | 1901/3250 [5:38:42<4:19:48, 11.56s/it] 59%|    | 1902/3250 [5:38:53<4:14:32, 11.33s/it]                                                        59%|    | 1902/3250 [5:38:53<4:14:32, 11.33s/it] 59%|    | 1903/3250 [5:39:04<4:08:15, 11.06s/it]                                                        59%|    | 1903/3250 [5:39:04<4:08:15, 11.06s/it] 59%|    | 1904/3250 [5:39:14<4:03:44, 10.87s/it]                                                        59%|    | 1904/3250 [5:39:14<4:03:44, 10.87s/it] 59%|    | 1905/3250 [5:39:24<4:00:34, 10.73s/it]                                                        59%|    | 1905/3250 [5:39:24<4:00:34, 10.73s/it] 59%|    | 1906/3250 [5:39:36<4:03:33, 1{'loss': 0.5485, 'learning_rate': 3.6623583073538966e-05, 'epoch': 0.59}
{'loss': 0.5526, 'learning_rate': 3.657699017574717e-05, 'epoch': 0.59}
{'loss': 0.5718, 'learning_rate': 3.653040983587151e-05, 'epoch': 0.59}
{'loss': 0.5107, 'learning_rate': 3.6483842097490287e-05, 'epoch': 0.59}
{'loss': 0.5952, 'learning_rate': 3.643728700417002e-05, 'epoch': 0.59}
0.87s/it]                                                        59%|    | 1906/3250 [5:39:36<4:03:33, 10.87s/it] 59%|    | 1907/3250 [5:39:46<4:00:25, 10.74s/it]                                                        59%|    | 1907/3250 [5:39:46<4:00:25, 10.74s/it] 59%|    | 1908/3250 [5:39:56<3:58:06, 10.65s/it]                                                        59%|    | 1908/3250 [5:39:56<3:58:06, 10.65s/it] 59%|    | 1909/3250 [5:40:07<3:56:23, 10.58s/it]                                                        59%|    | 1909/3250 [5:40:07<3:56:23, 10.58s/it] 59%|    | 1910/3250 [5:40:17<3:55:10, 10.53s/it]                                                        59%|    | 1910/3250 [5:40:17<3:55:10, 10.53s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.789359450340271, 'eval_runtime': 2.5782, 'eval_samples_per_second': 4.654, 'eval_steps_per_second': 1.164, 'epoch': 0.59}
                                                        59%|    | 1910/3250 [5:40:20<3:55:10, 10.53s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_10/tmp-checkpoint-1910
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1910
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1910
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1910
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1910
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1910/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1910/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1910/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1910/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.5415, 'learning_rate': 3.639074459946539e-05, 'epoch': 0.59}
{'loss': 0.542, 'learning_rate': 3.63442149269192e-05, 'epoch': 0.59}
{'loss': 0.5167, 'learning_rate': 3.629769803006239e-05, 'epoch': 0.59}
{'loss': 0.5453, 'learning_rate': 3.6251193952413865e-05, 'epoch': 0.59}
{'loss': 0.555, 'learning_rate': 3.62047027374806e-05, 'epoch': 0.59}
 59%|    | 1911/3250 [5:40:31<4:14:47, 11.42s/it]                                                        59%|    | 1911/3250 [5:40:31<4:14:47, 11.42s/it] 59%|    | 1912/3250 [5:40:41<4:07:52, 11.12s/it]                                                        59%|    | 1912/3250 [5:40:41<4:07:52, 11.12s/it] 59%|    | 1913/3250 [5:40:52<4:03:03, 10.91s/it]                                                        59%|    | 1913/3250 [5:40:52<4:03:03, 10.91s/it] 59%|    | 1914/3250 [5:41:02<3:59:41, 10.76s/it]                                                        59%|    | 1914/3250 [5:41:02<3:59:41, 10.76s/it] 59%|    | 1915/3250 [5:41:12<3:57:05, 10.66s/it]                                                        59%|    | 1915/3250 [5:41:12<3:57:05, 10.66s/it] 59%|    | 1916/3250 [5:41:23<3:55:17, 1{'loss': 0.5329, 'learning_rate': 3.6158224428757535e-05, 'epoch': 0.59}
{'loss': 0.5628, 'learning_rate': 3.611175906972749e-05, 'epoch': 0.59}
{'loss': 1.0461, 'learning_rate': 3.606530670386121e-05, 'epoch': 0.59}
{'loss': 0.5332, 'learning_rate': 3.601886737461729e-05, 'epoch': 0.59}
{'loss': 0.5628, 'learning_rate': 3.597244112544208e-05, 'epoch': 0.59}
0.58s/it]                                                        59%|    | 1916/3250 [5:41:23<3:55:17, 10.58s/it] 59%|    | 1917/3250 [5:41:33<3:53:56, 10.53s/it]                                                        59%|    | 1917/3250 [5:41:33<3:53:56, 10.53s/it] 59%|    | 1918/3250 [5:41:44<3:52:44, 10.48s/it]                                                        59%|    | 1918/3250 [5:41:44<3:52:44, 10.48s/it] 59%|    | 1919/3250 [5:41:54<3:54:00, 10.55s/it]                                                        59%|    | 1919/3250 [5:41:54<3:54:00, 10.55s/it] 59%|    | 1920/3250 [5:42:05<3:52:56, 10.51s/it]                                                        59%|    | 1920/3250 [5:42:05<3:52:56, 10.51s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.7835246920585632, 'eval_runtime': 2.4117, 'eval_samples_per_second': 4.976, 'eval_steps_per_second': 1.244, 'epoch': 0.59}
                                                        59%|    | 1920/3250 [5:42:07<3:52:56, 10.51s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_10/tmp-checkpoint-1920
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1920
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1920
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1920
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1920
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1920/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1920/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1920/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1920/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.5566, 'learning_rate': 3.5926027999769754e-05, 'epoch': 0.59}
{'loss': 0.5709, 'learning_rate': 3.587962804102214e-05, 'epoch': 0.59}
{'loss': 0.5192, 'learning_rate': 3.5833241292608846e-05, 'epoch': 0.59}
{'loss': 0.5456, 'learning_rate': 3.5786867797926996e-05, 'epoch': 0.59}
{'loss': 0.6075, 'learning_rate': 3.5740507600361415e-05, 'epoch': 0.59}
 59%|    | 1921/3250 [5:42:18<4:11:53, 11.37s/it]                                                        59%|    | 1921/3250 [5:42:18<4:11:53, 11.37s/it] 59%|    | 1922/3250 [5:42:29<4:05:19, 11.08s/it]                                                        59%|    | 1922/3250 [5:42:29<4:05:19, 11.08s/it] 59%|    | 1923/3250 [5:42:39<4:00:37, 10.88s/it]                                                        59%|    | 1923/3250 [5:42:39<4:00:37, 10.88s/it] 59%|    | 1924/3250 [5:42:49<3:57:17, 10.74s/it]                                                        59%|    | 1924/3250 [5:42:49<3:57:17, 10.74s/it] 59%|    | 1925/3250 [5:43:00<3:54:53, 10.64s/it]                                                        59%|    | 1925/3250 [5:43:00<3:54:53, 10.64s/it] 59%|    | 1926/3250 [5:43:10<3:53:18, 1{'loss': 0.5296, 'learning_rate': 3.569416074328445e-05, 'epoch': 0.59}
{'loss': 0.5441, 'learning_rate': 3.5647827270055945e-05, 'epoch': 0.59}
{'loss': 0.5373, 'learning_rate': 3.560150722402329e-05, 'epoch': 0.59}
{'loss': 0.5326, 'learning_rate': 3.5555200648521236e-05, 'epoch': 0.59}
{'loss': 0.5701, 'learning_rate': 3.550890758687199e-05, 'epoch': 0.59}
0.57s/it]                                                        59%|    | 1926/3250 [5:43:10<3:53:18, 10.57s/it] 59%|    | 1927/3250 [5:43:21<3:51:57, 10.52s/it]                                                        59%|    | 1927/3250 [5:43:21<3:51:57, 10.52s/it] 59%|    | 1928/3250 [5:43:31<3:52:05, 10.53s/it]                                                        59%|    | 1928/3250 [5:43:31<3:52:05, 10.53s/it] 59%|    | 1929/3250 [5:43:42<3:51:01, 10.49s/it]                                                        59%|    | 1929/3250 [5:43:42<3:51:01, 10.49s/it] 59%|    | 1930/3250 [5:43:52<3:50:12, 10.46s/it]                                                        59%|    | 1930/3250 [5:43:52<3:50:12, 10.46s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.7874338030815125, 'eval_runtime': 2.1134, 'eval_samples_per_second': 5.678, 'eval_steps_per_second': 1.42, 'epoch': 0.59}
                                                        59%|    | 1930/3250 [5:43:54<3:50:12, 10.46s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_10/tmp-checkpoint-1930
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1930
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1930
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1930
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1930
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1930/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1930/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1930/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1930/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.5459, 'learning_rate': 3.546262808238507e-05, 'epoch': 0.59}
{'loss': 0.5452, 'learning_rate': 3.5416362178357354e-05, 'epoch': 0.59}
{'loss': 0.5585, 'learning_rate': 3.537010991807296e-05, 'epoch': 0.59}
{'loss': 0.5582, 'learning_rate': 3.5323871344803263e-05, 'epoch': 0.6}
{'loss': 0.5754, 'learning_rate': 3.527764650180683e-05, 'epoch': 0.6}
 59%|    | 1931/3250 [5:44:05<4:06:37, 11.22s/it]                                                        59%|    | 1931/3250 [5:44:05<4:06:37, 11.22s/it] 59%|    | 1932/3250 [5:44:15<4:01:09, 10.98s/it]                                                        59%|    | 1932/3250 [5:44:15<4:01:09, 10.98s/it] 59%|    | 1933/3250 [5:44:26<3:57:18, 10.81s/it]                                                        59%|    | 1933/3250 [5:44:26<3:57:18, 10.81s/it] 60%|    | 1934/3250 [5:44:36<3:54:23, 10.69s/it]                                                        60%|    | 1934/3250 [5:44:36<3:54:23, 10.69s/it] 60%|    | 1935/3250 [5:44:47<3:54:50, 10.72s/it]                                                        60%|    | 1935/3250 [5:44:47<3:54:50, 10.72s/it] 60%|    | 1936/3250 [5:44:57<3:52:36, 1{'loss': 0.5678, 'learning_rate': 3.523143543232936e-05, 'epoch': 0.6}
{'loss': 0.5522, 'learning_rate': 3.518523817960372e-05, 'epoch': 0.6}
{'loss': 0.5569, 'learning_rate': 3.5139054786849784e-05, 'epoch': 0.6}
{'loss': 0.538, 'learning_rate': 3.5092885297274524e-05, 'epoch': 0.6}
{'loss': 0.5683, 'learning_rate': 3.504672975407184e-05, 'epoch': 0.6}
0.62s/it]                                                        60%|    | 1936/3250 [5:44:57<3:52:36, 10.62s/it] 60%|    | 1937/3250 [5:45:08<3:50:56, 10.55s/it]                                                        60%|    | 1937/3250 [5:45:08<3:50:56, 10.55s/it] 60%|    | 1938/3250 [5:45:18<3:49:55, 10.51s/it]                                                        60%|    | 1938/3250 [5:45:18<3:49:55, 10.51s/it] 60%|    | 1939/3250 [5:45:29<3:49:09, 10.49s/it]                                                        60%|    | 1939/3250 [5:45:29<3:49:09, 10.49s/it] 60%|    | 1940/3250 [5:45:39<3:48:29, 10.47s/it]                                                        60%|    | 1940/3250 [5:45:39<3:48:29, 10.47s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.7846162915229797, 'eval_runtime': 2.1201, 'eval_samples_per_second': 5.66, 'eval_steps_per_second': 1.415, 'epoch': 0.6}
                                                        60%|    | 1940/3250 [5:45:41<3:48:29, 10.47s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_10/tmp-checkpoint-1940
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1940/projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1940

I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1940
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1940
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1940/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1940/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1940/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1940/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.5568, 'learning_rate': 3.500058820042263e-05, 'epoch': 0.6}
{'loss': 0.5277, 'learning_rate': 3.4954460679494685e-05, 'epoch': 0.6}
{'loss': 0.5293, 'learning_rate': 3.490834723444268e-05, 'epoch': 0.6}
{'loss': 0.5163, 'learning_rate': 3.486224790840811e-05, 'epoch': 0.6}
{'loss': 0.5657, 'learning_rate': 3.4816162744519263e-05, 'epoch': 0.6}
 60%|    | 1941/3250 [5:45:52<4:05:17, 11.24s/it]                                                        60%|    | 1941/3250 [5:45:52<4:05:17, 11.24s/it] 60%|    | 1942/3250 [5:46:02<3:59:39, 10.99s/it]                                                        60%|    | 1942/3250 [5:46:02<3:59:39, 10.99s/it] 60%|    | 1943/3250 [5:46:13<3:55:36, 10.82s/it]                                                        60%|    | 1943/3250 [5:46:13<3:55:36, 10.82s/it] 60%|    | 1944/3250 [5:46:23<3:52:42, 10.69s/it]                                                        60%|    | 1944/3250 [5:46:23<3:52:42, 10.69s/it] 60%|    | 1945/3250 [5:46:34<3:50:41, 10.61s/it]                                                        60%|    | 1945/3250 [5:46:34<3:50:41, 10.61s/it] 60%|    | 1946/3250 [5:46:44<3:49:14, 1{'loss': 0.5429, 'learning_rate': 3.4770091785891205e-05, 'epoch': 0.6}
{'loss': 0.5665, 'learning_rate': 3.472403507562566e-05, 'epoch': 0.6}
{'loss': 1.0431, 'learning_rate': 3.467799265681105e-05, 'epoch': 0.6}
{'loss': 0.5368, 'learning_rate': 3.463196457252245e-05, 'epoch': 0.6}
{'loss': 0.5532, 'learning_rate': 3.4585950865821473e-05, 'epoch': 0.6}
0.55s/it]                                                        60%|    | 1946/3250 [5:46:44<3:49:14, 10.55s/it] 60%|    | 1947/3250 [5:46:54<3:48:07, 10.50s/it]                                                        60%|    | 1947/3250 [5:46:54<3:48:07, 10.50s/it] 60%|    | 1948/3250 [5:47:05<3:46:57, 10.46s/it]                                                        60%|    | 1948/3250 [5:47:05<3:46:57, 10.46s/it] 60%|    | 1949/3250 [5:47:15<3:46:23, 10.44s/it]                                                        60%|    | 1949/3250 [5:47:15<3:46:23, 10.44s/it] 60%|    | 1950/3250 [5:47:26<3:45:53, 10.43s/it]                                                        60%|    | 1950/3250 [5:47:26<3:45:53, 10.43s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.7828946113586426, 'eval_runtime': 2.1244, 'eval_samples_per_second': 5.649, 'eval_steps_per_second': 1.412, 'epoch': 0.6}
                                                        60%|    | 1950/3250 [5:47:28<3:45:53, 10.43s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_10/tmp-checkpoint-1950
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1950
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1950
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1950
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1950
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1950/pytorch_model.bin
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1950/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1950/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1950/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.5609, 'learning_rate': 3.4539951579756326e-05, 'epoch': 0.6}
{'loss': 0.5643, 'learning_rate': 3.449396675736171e-05, 'epoch': 0.6}
{'loss': 0.5363, 'learning_rate': 3.444799644165876e-05, 'epoch': 0.6}
{'loss': 0.529, 'learning_rate': 3.440204067565511e-05, 'epoch': 0.6}
{'loss': 0.6106, 'learning_rate': 3.435609950234473e-05, 'epoch': 0.6}
 60%|    | 1951/3250 [5:47:39<4:06:04, 11.37s/it]                                                        60%|    | 1951/3250 [5:47:39<4:06:04, 11.37s/it] 60%|    | 1952/3250 [5:47:50<3:59:43, 11.08s/it]                                                        60%|    | 1952/3250 [5:47:50<3:59:43, 11.08s/it] 60%|    | 1953/3250 [5:48:00<3:55:08, 10.88s/it]                                                        60%|    | 1953/3250 [5:48:00<3:55:08, 10.88s/it] 60%|    | 1954/3250 [5:48:10<3:51:57, 10.74s/it]                                                        60%|    | 1954/3250 [5:48:10<3:51:57, 10.74s/it] 60%|    | 1955/3250 [5:48:21<3:49:31, 10.63s/it]                                                        60%|    | 1955/3250 [5:48:21<3:49:31, 10.63s/it] 60%|    | 1956/3250 [5:48:31<3:47:44, 1{'loss': 0.555, 'learning_rate': 3.431017296470797e-05, 'epoch': 0.6}
{'loss': 0.5484, 'learning_rate': 3.426426110571141e-05, 'epoch': 0.6}
{'loss': 0.5108, 'learning_rate': 3.4218363968308e-05, 'epoch': 0.6}
{'loss': 0.5528, 'learning_rate': 3.417248159543687e-05, 'epoch': 0.6}
{'loss': 0.5523, 'learning_rate': 3.412661403002333e-05, 'epoch': 0.6}
0.56s/it]                                                        60%|    | 1956/3250 [5:48:31<3:47:44, 10.56s/it] 60%|    | 1957/3250 [5:48:42<3:46:33, 10.51s/it]                                                        60%|    | 1957/3250 [5:48:42<3:46:33, 10.51s/it] 60%|    | 1958/3250 [5:48:52<3:45:35, 10.48s/it]                                                        60%|    | 1958/3250 [5:48:52<3:45:35, 10.48s/it] 60%|    | 1959/3250 [5:49:02<3:45:00, 10.46s/it]                                                        60%|    | 1959/3250 [5:49:02<3:45:00, 10.46s/it] 60%|    | 1960/3250 [5:49:13<3:44:22, 10.44s/it]                                                        60%|    | 1960/3250 [5:49:13<3:44:22, 10.44s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.7881298661231995, 'eval_runtime': 2.1166, 'eval_samples_per_second': 5.669, 'eval_steps_per_second': 1.417, 'epoch': 0.6}
                                                        60%|    | 1960/3250 [5:49:15<3:44:22, 10.44s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_10/tmp-checkpoint-1960
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1960I AM HERE AND I AM SAVING THE MODEL

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1960
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1960
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1960
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1960/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1960/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1960/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1960/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.5479, 'learning_rate': 3.408076131497885e-05, 'epoch': 0.6}
{'loss': 0.5384, 'learning_rate': 3.403492349320101e-05, 'epoch': 0.6}
{'loss': 0.5609, 'learning_rate': 3.398910060757344e-05, 'epoch': 0.6}
{'loss': 0.546, 'learning_rate': 3.3943292700965824e-05, 'epoch': 0.6}
{'loss': 0.5612, 'learning_rate': 3.389749981623379e-05, 'epoch': 0.6}
 60%|    | 1961/3250 [5:49:26<4:00:54, 11.21s/it]                                                        60%|    | 1961/3250 [5:49:26<4:00:54, 11.21s/it] 60%|    | 1962/3250 [5:49:36<3:55:28, 10.97s/it]                                                        60%|    | 1962/3250 [5:49:36<3:55:28, 10.97s/it] 60%|    | 1963/3250 [5:49:47<3:51:38, 10.80s/it]                                                        60%|    | 1963/3250 [5:49:47<3:51:38, 10.80s/it] 60%|    | 1964/3250 [5:49:57<3:48:49, 10.68s/it]                                                        60%|    | 1964/3250 [5:49:57<3:48:49, 10.68s/it] 60%|    | 1965/3250 [5:50:07<3:46:54, 10.59s/it]                                                        60%|    | 1965/3250 [5:50:07<3:46:54, 10.59s/it] 60%|    | 1966/3250 [5:50:18<3:45:32, 1{'loss': 0.5558, 'learning_rate': 3.3851721996218947e-05, 'epoch': 0.6}
{'loss': 0.5509, 'learning_rate': 3.380595928374881e-05, 'epoch': 0.61}
{'loss': 0.5584, 'learning_rate': 3.376021172163674e-05, 'epoch': 0.61}
{'loss': 0.5444, 'learning_rate': 3.371447935268194e-05, 'epoch': 0.61}
{'loss': 0.5261, 'learning_rate': 3.366876221966939e-05, 'epoch': 0.61}
0.54s/it]                                                        60%|    | 1966/3250 [5:50:18<3:45:32, 10.54s/it] 61%|    | 1967/3250 [5:50:28<3:44:32, 10.50s/it]                                                        61%|    | 1967/3250 [5:50:28<3:44:32, 10.50s/it] 61%|    | 1968/3250 [5:50:39<3:46:14, 10.59s/it]                                                        61%|    | 1968/3250 [5:50:39<3:46:14, 10.59s/it] 61%|    | 1969/3250 [5:50:49<3:44:51, 10.53s/it]                                                        61%|    | 1969/3250 [5:50:49<3:44:51, 10.53s/it] 61%|    | 1970/3250 [5:51:00<3:43:50, 10.49s/it]                                                        61%|    | 1970/3250 [5:51:00<3:43:50, 10.49s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.78585284948349, 'eval_runtime': 2.1244, 'eval_samples_per_second': 5.649, 'eval_steps_per_second': 1.412, 'epoch': 0.61}
                                                        61%|    | 1970/3250 [5:51:02<3:43:50, 10.49s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_10/tmp-checkpoint-1970
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1970
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1970
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1970
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1970
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1970/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1970/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1970/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1970/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.5743, 'learning_rate': 3.362306036536982e-05, 'epoch': 0.61}
{'loss': 0.5217, 'learning_rate': 3.357737383253966e-05, 'epoch': 0.61}
{'loss': 0.5278, 'learning_rate': 3.353170266392104e-05, 'epoch': 0.61}
{'loss': 0.5295, 'learning_rate': 3.3486046902241664e-05, 'epoch': 0.61}
{'loss': 0.5632, 'learning_rate': 3.344040659021482e-05, 'epoch': 0.61}
 61%|    | 1971/3250 [5:51:13<4:00:22, 11.28s/it]                                                        61%|    | 1971/3250 [5:51:13<4:00:22, 11.28s/it] 61%|    | 1972/3250 [5:51:23<3:54:36, 11.01s/it]                                                        61%|    | 1972/3250 [5:51:23<3:54:36, 11.01s/it] 61%|    | 1973/3250 [5:51:34<3:50:20, 10.82s/it]                                                        61%|    | 1973/3250 [5:51:34<3:50:20, 10.82s/it] 61%|    | 1974/3250 [5:51:44<3:47:23, 10.69s/it]                                                        61%|    | 1974/3250 [5:51:44<3:47:23, 10.69s/it] 61%|    | 1975/3250 [5:51:54<3:45:14, 10.60s/it]                                                        61%|    | 1975/3250 [5:51:54<3:45:14, 10.60s/it] 61%|    | 1976/3250 [5:52:05<3:43:45, 1{'loss': 0.5362, 'learning_rate': 3.339478177053941e-05, 'epoch': 0.61}
{'loss': 0.553, 'learning_rate': 3.3349172485899785e-05, 'epoch': 0.61}
{'loss': 0.9186, 'learning_rate': 3.330357877896577e-05, 'epoch': 0.61}
{'loss': 0.6632, 'learning_rate': 3.325800069239263e-05, 'epoch': 0.61}
{'loss': 0.5324, 'learning_rate': 3.321243826882101e-05, 'epoch': 0.61}
0.54s/it]                                                        61%|    | 1976/3250 [5:52:05<3:43:45, 10.54s/it] 61%|    | 1977/3250 [5:52:15<3:42:43, 10.50s/it]                                                        61%|    | 1977/3250 [5:52:15<3:42:43, 10.50s/it] 61%|    | 1978/3250 [5:52:26<3:41:52, 10.47s/it]                                                        61%|    | 1978/3250 [5:52:26<3:41:52, 10.47s/it] 61%|    | 1979/3250 [5:52:36<3:41:11, 10.44s/it]                                                        61%|    | 1979/3250 [5:52:36<3:41:11, 10.44s/it] 61%|    | 1980/3250 [5:52:46<3:40:38, 10.42s/it]                                                        61%|    | 1980/3250 [5:52:46<3:40:38, 10.42s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.7841955423355103, 'eval_runtime': 2.124, 'eval_samples_per_second': 5.65, 'eval_steps_per_second': 1.412, 'epoch': 0.61}
                                                        61%|    | 1980/3250 [5:52:49<3:40:38, 10.42s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_10/tmp-checkpoint-1980
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1980I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in 
 /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1980
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1980
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1980
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1980/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1980/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1980/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1980/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.5618, 'learning_rate': 3.31668915508769e-05, 'epoch': 0.61}
{'loss': 0.5532, 'learning_rate': 3.3121360581171594e-05, 'epoch': 0.61}
{'loss': 0.5432, 'learning_rate': 3.3075845402301655e-05, 'epoch': 0.61}
{'loss': 0.5312, 'learning_rate': 3.303034605684888e-05, 'epoch': 0.61}
{'loss': 0.5969, 'learning_rate': 3.298486258738025e-05, 'epoch': 0.61}
 61%|    | 1981/3250 [5:53:00<3:57:38, 11.24s/it]                                                        61%|    | 1981/3250 [5:53:00<3:57:38, 11.24s/it] 61%|    | 1982/3250 [5:53:10<3:52:09, 10.99s/it]                                                        61%|    | 1982/3250 [5:53:10<3:52:09, 10.99s/it] 61%|    | 1983/3250 [5:53:20<3:48:13, 10.81s/it]                                                        61%|    | 1983/3250 [5:53:20<3:48:13, 10.81s/it] 61%|    | 1984/3250 [5:53:31<3:46:41, 10.74s/it]                                                        61%|    | 1984/3250 [5:53:31<3:46:41, 10.74s/it] 61%|    | 1985/3250 [5:53:41<3:44:16, 10.64s/it]                                                        61%|    | 1985/3250 [5:53:41<3:44:16, 10.64s/it] 61%|    | 1986/3250 [5:53:52<3:42:33, 1{'loss': 0.5698, 'learning_rate': 3.293939503644788e-05, 'epoch': 0.61}
{'loss': 0.5358, 'learning_rate': 3.2893943446589005e-05, 'epoch': 0.61}
{'loss': 0.5135, 'learning_rate': 3.284850786032593e-05, 'epoch': 0.61}
{'loss': 0.5548, 'learning_rate': 3.2803088320165984e-05, 'epoch': 0.61}
{'loss': 0.5468, 'learning_rate': 3.275768486860149e-05, 'epoch': 0.61}
0.56s/it]                                                        61%|    | 1986/3250 [5:53:52<3:42:33, 10.56s/it] 61%|    | 1987/3250 [5:54:02<3:41:24, 10.52s/it]                                                        61%|    | 1987/3250 [5:54:02<3:41:24, 10.52s/it] 61%|    | 1988/3250 [5:54:13<3:40:27, 10.48s/it]                                                        61%|    | 1988/3250 [5:54:13<3:40:27, 10.48s/it] 61%|    | 1989/3250 [5:54:23<3:39:47, 10.46s/it]                                                        61%|    | 1989/3250 [5:54:23<3:39:47, 10.46s/it] 61%|    | 1990/3250 [5:54:33<3:39:09, 10.44s/it]                                                        61%|    | 1990/3250 [5:54:33<3:39:09, 10.44s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.7894354462623596, 'eval_runtime': 2.1255, 'eval_samples_per_second': 5.646, 'eval_steps_per_second': 1.411, 'epoch': 0.61}
                                                        61%|    | 1990/3250 [5:54:35<3:39:09, 10.44s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_10/tmp-checkpoint-1990
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1990
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1990
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1990
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1990
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1990/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1990/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1990/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-1990/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.5482, 'learning_rate': 3.271229754810969e-05, 'epoch': 0.61}
{'loss': 0.5338, 'learning_rate': 3.266692640115277e-05, 'epoch': 0.61}
{'loss': 0.5516, 'learning_rate': 3.262157147017777e-05, 'epoch': 0.61}
{'loss': 0.5436, 'learning_rate': 3.257623279761656e-05, 'epoch': 0.61}
{'loss': 0.5643, 'learning_rate': 3.2530910425885806e-05, 'epoch': 0.61}
 61%|   | 1991/3250 [5:54:46<3:55:16, 11.21s/it]                                                        61%|   | 1991/3250 [5:54:46<3:55:16, 11.21s/it] 61%|   | 1992/3250 [5:54:57<3:49:50, 10.96s/it]                                                        61%|   | 1992/3250 [5:54:57<3:49:50, 10.96s/it] 61%|   | 1993/3250 [5:55:07<3:46:13, 10.80s/it]                                                        61%|   | 1993/3250 [5:55:07<3:46:13, 10.80s/it] 61%|   | 1994/3250 [5:55:18<3:43:07, 10.66s/it]                                                        61%|   | 1994/3250 [5:55:18<3:43:07, 10.66s/it] 61%|   | 1995/3250 [5:55:28<3:41:02, 10.57s/it]                                                        61%|   | 1995/3250 [5:55:28<3:41:02, 10.57s/it] 61%|   | 1996/32{'loss': 0.5552, 'learning_rate': 3.248560439738691e-05, 'epoch': 0.61}
{'loss': 0.559, 'learning_rate': 3.244031475450599e-05, 'epoch': 0.61}
{'loss': 0.5472, 'learning_rate': 3.2395041539613855e-05, 'epoch': 0.61}
{'loss': 0.5625, 'learning_rate': 3.234978479506591e-05, 'epoch': 0.62}
{'loss': 0.5049, 'learning_rate': 3.2304544563202163e-05, 'epoch': 0.62}
50 [5:55:38<3:39:29, 10.50s/it]                                                        61%|   | 1996/3250 [5:55:38<3:39:29, 10.50s/it] 61%|   | 1997/3250 [5:55:49<3:38:23, 10.46s/it]                                                        61%|   | 1997/3250 [5:55:49<3:38:23, 10.46s/it] 61%|   | 1998/3250 [5:55:59<3:37:50, 10.44s/it]                                                        61%|   | 1998/3250 [5:55:59<3:37:50, 10.44s/it] 62%|   | 1999/3250 [5:56:09<3:37:17, 10.42s/it]                                                        62%|   | 1999/3250 [5:56:09<3:37:17, 10.42s/it] 62%|   | 2000/3250 [5:56:20<3:40:43, 10.60s/it]                                                        62%|   | 2000/3250 [5:56:20<3:40:43, 10.60s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.7843028903007507, 'eval_runtime': 2.1185, 'eval_samples_per_second': 5.664, 'eval_steps_per_second': 1.416, 'epoch': 0.62}
                                                        62%|   | 2000/3250 [5:56:22<3:40:43, 10.60s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_10/tmp-checkpoint-2000
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2000I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in 
 /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2000
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2000
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2000
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2000/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2000/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2000/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2000/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.5872, 'learning_rate': 3.22593208863472e-05, 'epoch': 0.62}
{'loss': 0.5321, 'learning_rate': 3.221411380681007e-05, 'epoch': 0.62}
{'loss': 0.5304, 'learning_rate': 3.216892336688435e-05, 'epoch': 0.62}
{'loss': 0.5161, 'learning_rate': 3.2123749608848e-05, 'epoch': 0.62}
{'loss': 0.5352, 'learning_rate': 3.207859257496339e-05, 'epoch': 0.62}
 62%|   | 2001/3250 [5:56:33<3:55:07, 11.30s/it]                                                        62%|   | 2001/3250 [5:56:33<3:55:07, 11.30s/it] 62%|   | 2002/3250 [5:56:44<3:49:00, 11.01s/it]                                                        62%|   | 2002/3250 [5:56:44<3:49:00, 11.01s/it] 62%|   | 2003/3250 [5:56:54<3:44:39, 10.81s/it]                                                        62%|   | 2003/3250 [5:56:54<3:44:39, 10.81s/it] 62%|   | 2004/3250 [5:57:04<3:41:43, 10.68s/it]                                                        62%|   | 2004/3250 [5:57:04<3:41:43, 10.68s/it] 62%|   | 2005/3250 [5:57:15<3:39:29, 10.58s/it]                                                        62%|   | 2005/3250 [5:57:15<3:39:29, 10.58s/it] 62%|   | 2006/32{'loss': 0.5596, 'learning_rate': 3.2033452307477276e-05, 'epoch': 0.62}
{'loss': 0.5271, 'learning_rate': 3.198832884862068e-05, 'epoch': 0.62}
{'loss': 0.5644, 'learning_rate': 3.194322224060891e-05, 'epoch': 0.62}
{'loss': 1.034, 'learning_rate': 3.189813252564152e-05, 'epoch': 0.62}
{'loss': 0.5264, 'learning_rate': 3.1853059745902285e-05, 'epoch': 0.62}
50 [5:57:25<3:37:58, 10.51s/it]                                                        62%|   | 2006/3250 [5:57:25<3:37:58, 10.51s/it] 62%|   | 2007/3250 [5:57:35<3:36:46, 10.46s/it]                                                        62%|   | 2007/3250 [5:57:35<3:36:46, 10.46s/it] 62%|   | 2008/3250 [5:57:46<3:35:55, 10.43s/it]                                                        62%|   | 2008/3250 [5:57:46<3:35:55, 10.43s/it] 62%|   | 2009/3250 [5:57:56<3:35:28, 10.42s/it]                                                        62%|   | 2009/3250 [5:57:56<3:35:28, 10.42s/it] 62%|   | 2010/3250 [5:58:06<3:34:51, 10.40s/it]                                                        62%|   | 2010/3250 [5:58:06<3:34:51, 10.40s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.7867391705513, 'eval_runtime': 2.1149, 'eval_samples_per_second': 5.674, 'eval_steps_per_second': 1.419, 'epoch': 0.62}
                                                        62%|   | 2010/3250 [5:58:09<3:34:51, 10.40s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_10/tmp-checkpoint-2010
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2010
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2010
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2010
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2010
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2010/pytorch_model.bin
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2010/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2010/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2010/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.5575, 'learning_rate': 3.180800394355908e-05, 'epoch': 0.62}
{'loss': 0.5508, 'learning_rate': 3.176296516076394e-05, 'epoch': 0.62}
{'loss': 0.5526, 'learning_rate': 3.1717943439652954e-05, 'epoch': 0.62}
{'loss': 0.522, 'learning_rate': 3.167293882234626e-05, 'epoch': 0.62}
{'loss': 0.536, 'learning_rate': 3.162795135094799e-05, 'epoch': 0.62}
 62%|   | 2011/3250 [5:58:19<3:50:26, 11.16s/it]                                                        62%|   | 2011/3250 [5:58:19<3:50:26, 11.16s/it] 62%|   | 2012/3250 [5:58:30<3:45:16, 10.92s/it]                                                        62%|   | 2012/3250 [5:58:30<3:45:16, 10.92s/it] 62%|   | 2013/3250 [5:58:40<3:41:41, 10.75s/it]                                                        62%|   | 2013/3250 [5:58:40<3:41:41, 10.75s/it] 62%|   | 2014/3250 [5:58:50<3:39:03, 10.63s/it]                                                        62%|   | 2014/3250 [5:58:50<3:39:03, 10.63s/it] 62%|   | 2015/3250 [5:59:01<3:37:30, 10.57s/it]                                                        62%|   | 2015/3250 [5:59:01<3:37:30, 10.57s/it] 62%|   | 2016/32{'loss': 0.617, 'learning_rate': 3.158298106754626e-05, 'epoch': 0.62}
{'loss': 0.524, 'learning_rate': 3.1538028014213055e-05, 'epoch': 0.62}
{'loss': 0.5405, 'learning_rate': 3.149309223300428e-05, 'epoch': 0.62}
{'loss': 0.5421, 'learning_rate': 3.144817376595968e-05, 'epoch': 0.62}
{'loss': 0.5312, 'learning_rate': 3.1403272655102764e-05, 'epoch': 0.62}
50 [5:59:11<3:36:01, 10.50s/it]                                                        62%|   | 2016/3250 [5:59:11<3:36:01, 10.50s/it] 62%|   | 2017/3250 [5:59:22<3:37:21, 10.58s/it]                                                        62%|   | 2017/3250 [5:59:22<3:37:21, 10.58s/it] 62%|   | 2018/3250 [5:59:32<3:35:48, 10.51s/it]                                                        62%|   | 2018/3250 [5:59:32<3:35:48, 10.51s/it] 62%|   | 2019/3250 [5:59:43<3:34:41, 10.46s/it]                                                        62%|   | 2019/3250 [5:59:43<3:34:41, 10.46s/it] 62%|   | 2020/3250 [5:59:53<3:33:50, 10.43s/it]                                                        62%|   | 2020/3250 [5:59:53<3:33:50, 10.43s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.787995457649231, 'eval_runtime': 2.3436, 'eval_samples_per_second': 5.12, 'eval_steps_per_second': 1.28, 'epoch': 0.62}
                                                        62%|   | 2020/3250 [5:59:55<3:33:50, 10.43s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_10/tmp-checkpoint-2020
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2020
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2020
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2020
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2020
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2020/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2020/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2020/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2020/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.547, 'learning_rate': 3.135838894244086e-05, 'epoch': 0.62}
{'loss': 0.5288, 'learning_rate': 3.1313522669964976e-05, 'epoch': 0.62}
{'loss': 0.5263, 'learning_rate': 3.1268673879649815e-05, 'epoch': 0.62}
{'loss': 0.5484, 'learning_rate': 3.1223842613453745e-05, 'epoch': 0.62}
{'loss': 0.551, 'learning_rate': 3.11790289133187e-05, 'epoch': 0.62}
 62%|   | 2021/3250 [6:00:06<3:51:04, 11.28s/it]                                                        62%|   | 2021/3250 [6:00:06<3:51:04, 11.28s/it] 62%|   | 2022/3250 [6:00:17<3:45:06, 11.00s/it]                                                        62%|   | 2022/3250 [6:00:17<3:45:06, 11.00s/it] 62%|   | 2023/3250 [6:00:27<3:40:58, 10.81s/it]                                                        62%|   | 2023/3250 [6:00:27<3:40:58, 10.81s/it] 62%|   | 2024/3250 [6:00:37<3:38:05, 10.67s/it]                                                        62%|   | 2024/3250 [6:00:37<3:38:05, 10.67s/it] 62%|   | 2025/3250 [6:00:48<3:36:00, 10.58s/it]                                                        62%|   | 2025/3250 [6:00:48<3:36:00, 10.58s/it] 62%|   | 2026/32{'loss': 0.5587, 'learning_rate': 3.11342328211702e-05, 'epoch': 0.62}
{'loss': 0.5624, 'learning_rate': 3.1089454378917304e-05, 'epoch': 0.62}
{'loss': 0.5457, 'learning_rate': 3.1044693628452557e-05, 'epoch': 0.62}
{'loss': 0.551, 'learning_rate': 3.0999950611651915e-05, 'epoch': 0.62}
{'loss': 0.5237, 'learning_rate': 3.0955225370374805e-05, 'epoch': 0.62}
50 [6:00:58<3:34:45, 10.53s/it]                                                        62%|   | 2026/3250 [6:00:58<3:34:45, 10.53s/it] 62%|   | 2027/3250 [6:01:09<3:33:45, 10.49s/it]                                                        62%|   | 2027/3250 [6:01:09<3:33:45, 10.49s/it] 62%|   | 2028/3250 [6:01:19<3:32:53, 10.45s/it]                                                        62%|   | 2028/3250 [6:01:19<3:32:53, 10.45s/it] 62%|   | 2029/3250 [6:01:29<3:32:09, 10.43s/it]                                                        62%|   | 2029/3250 [6:01:29<3:32:09, 10.43s/it] 62%|   | 2030/3250 [6:01:40<3:31:30, 10.40s/it]                                                        62%|   | 2030/3250 [6:01:40<3:31:30, 10.40s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.7832858562469482, 'eval_runtime': 2.114, 'eval_samples_per_second': 5.676, 'eval_steps_per_second': 1.419, 'epoch': 0.62}
                                                        62%|   | 2030/3250 [6:01:42<3:31:30, 10.40s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_10/tmp-checkpoint-2030
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2030
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2030
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2030
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2030
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2030/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2030/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2030/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2030/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.5661, 'learning_rate': 3.091051794646398e-05, 'epoch': 0.62}
{'loss': 0.5501, 'learning_rate': 3.086582838174551e-05, 'epoch': 0.63}
{'loss': 0.5276, 'learning_rate': 3.082115671802882e-05, 'epoch': 0.63}
{'loss': 0.526, 'learning_rate': 3.077650299710653e-05, 'epoch': 0.63}
{'loss': 0.5132, 'learning_rate': 3.073186726075449e-05, 'epoch': 0.63}
 62%|   | 2031/3250 [6:01:53<3:46:51, 11.17s/it]                                                        62%|   | 2031/3250 [6:01:53<3:46:51, 11.17s/it] 63%|   | 2032/3250 [6:02:03<3:41:51, 10.93s/it]                                                        63%|   | 2032/3250 [6:02:03<3:41:51, 10.93s/it] 63%|   | 2033/3250 [6:02:14<3:39:51, 10.84s/it]                                                        63%|   | 2033/3250 [6:02:14<3:39:51, 10.84s/it] 63%|   | 2034/3250 [6:02:24<3:36:44, 10.69s/it]                                                        63%|   | 2034/3250 [6:02:24<3:36:44, 10.69s/it] 63%|   | 2035/3250 [6:02:34<3:34:27, 10.59s/it]                                                        63%|   | 2035/3250 [6:02:34<3:34:27, 10.59s/it] 63%|   | 2036/32{'loss': 0.556, 'learning_rate': 3.068724955073172e-05, 'epoch': 0.63}
{'loss': 0.5295, 'learning_rate': 3.0642649908780413e-05, 'epoch': 0.63}
{'loss': 0.5655, 'learning_rate': 3.05980683766258e-05, 'epoch': 0.63}
{'loss': 1.0368, 'learning_rate': 3.05535049959762e-05, 'epoch': 0.63}
{'loss': 0.5338, 'learning_rate': 3.0508959808522974e-05, 'epoch': 0.63}
50 [6:02:45<3:32:52, 10.52s/it]                                                        63%|   | 2036/3250 [6:02:45<3:32:52, 10.52s/it] 63%|   | 2037/3250 [6:02:55<3:31:47, 10.48s/it]                                                        63%|   | 2037/3250 [6:02:55<3:31:47, 10.48s/it] 63%|   | 2038/3250 [6:03:05<3:30:58, 10.44s/it]                                                        63%|   | 2038/3250 [6:03:05<3:30:58, 10.44s/it] 63%|   | 2039/3250 [6:03:16<3:30:08, 10.41s/it]                                                        63%|   | 2039/3250 [6:03:16<3:30:08, 10.41s/it] 63%|   | 2040/3250 [6:03:26<3:29:42, 10.40s/it]                                                        63%|   | 2040/3250 [6:03:26<3:29:42, 10.40s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.786294162273407, 'eval_runtime': 2.1125, 'eval_samples_per_second': 5.681, 'eval_steps_per_second': 1.42, 'epoch': 0.63}
                                                        63%|   | 2040/3250 [6:03:28<3:29:42, 10.40s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_10/tmp-checkpoint-2040
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2040
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2040
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2040
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2040
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2040/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2040/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2040/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2040/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.5496, 'learning_rate': 3.0464432855940417e-05, 'epoch': 0.63}
{'loss': 0.5575, 'learning_rate': 3.0419924179885768e-05, 'epoch': 0.63}
{'loss': 0.5588, 'learning_rate': 3.03754338219992e-05, 'epoch': 0.63}
{'loss': 0.5271, 'learning_rate': 3.0330961823903735e-05, 'epoch': 0.63}
{'loss': 0.5265, 'learning_rate': 3.02865082272052e-05, 'epoch': 0.63}
 63%|   | 2041/3250 [6:03:39<3:45:20, 11.18s/it]                                                        63%|   | 2041/3250 [6:03:39<3:45:20, 11.18s/it] 63%|   | 2042/3250 [6:03:49<3:40:10, 10.94s/it]                                                        63%|   | 2042/3250 [6:03:49<3:40:10, 10.94s/it] 63%|   | 2043/3250 [6:04:00<3:36:31, 10.76s/it]                                                        63%|   | 2043/3250 [6:04:00<3:36:31, 10.76s/it] 63%|   | 2044/3250 [6:04:10<3:33:52, 10.64s/it]                                                        63%|   | 2044/3250 [6:04:10<3:33:52, 10.64s/it] 63%|   | 2045/3250 [6:04:21<3:32:01, 10.56s/it]                                                        63%|   | 2045/3250 [6:04:21<3:32:01, 10.56s/it] 63%|   | 2046/32{'loss': 0.6025, 'learning_rate': 3.024207307349224e-05, 'epoch': 0.63}
{'loss': 0.547, 'learning_rate': 3.0197656404336206e-05, 'epoch': 0.63}
{'loss': 0.5464, 'learning_rate': 3.0153258261291195e-05, 'epoch': 0.63}
{'loss': 0.5052, 'learning_rate': 3.0108878685893947e-05, 'epoch': 0.63}
{'loss': 0.5499, 'learning_rate': 3.006451771966383e-05, 'epoch': 0.63}
50 [6:04:31<3:30:32, 10.49s/it]                                                        63%|   | 2046/3250 [6:04:31<3:30:32, 10.49s/it] 63%|   | 2047/3250 [6:04:41<3:29:23, 10.44s/it]                                                        63%|   | 2047/3250 [6:04:41<3:29:23, 10.44s/it] 63%|   | 2048/3250 [6:04:52<3:28:43, 10.42s/it]                                                        63%|   | 2048/3250 [6:04:52<3:28:43, 10.42s/it] 63%|   | 2049/3250 [6:05:02<3:28:01, 10.39s/it]                                                        63%|   | 2049/3250 [6:05:02<3:28:01, 10.39s/it] 63%|   | 2050/3250 [6:05:13<3:29:52, 10.49s/it]                                                        63%|   | 2050/3250 [6:05:13<3:29:52, 10.49s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.7887918949127197, 'eval_runtime': 2.5155, 'eval_samples_per_second': 4.77, 'eval_steps_per_second': 1.193, 'epoch': 0.63}
                                                        63%|   | 2050/3250 [6:05:15<3:29:52, 10.49s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_10/tmp-checkpoint-2050
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2050
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2050
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2050
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2050
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2050/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2050/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2050/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2050/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.5439, 'learning_rate': 3.002017540410282e-05, 'epoch': 0.63}
{'loss': 0.5341, 'learning_rate': 2.9975851780695442e-05, 'epoch': 0.63}
{'loss': 0.5264, 'learning_rate': 2.9931546890908697e-05, 'epoch': 0.63}
{'loss': 0.5446, 'learning_rate': 2.988726077619211e-05, 'epoch': 0.63}
{'loss': 0.5445, 'learning_rate': 2.9842993477977622e-05, 'epoch': 0.63}
 63%|   | 2051/3250 [6:05:26<3:46:44, 11.35s/it]                                                        63%|   | 2051/3250 [6:05:26<3:46:44, 11.35s/it] 63%|   | 2052/3250 [6:05:36<3:40:38, 11.05s/it]                                                        63%|   | 2052/3250 [6:05:36<3:40:38, 11.05s/it] 63%|   | 2053/3250 [6:05:47<3:36:22, 10.85s/it]                                                        63%|   | 2053/3250 [6:05:47<3:36:22, 10.85s/it] 63%|   | 2054/3250 [6:05:57<3:33:29, 10.71s/it]                                                        63%|   | 2054/3250 [6:05:57<3:33:29, 10.71s/it] 63%|   | 2055/3250 [6:06:08<3:31:32, 10.62s/it]                                                        63%|   | 2055/3250 [6:06:08<3:31:32, 10.62s/it] 63%|   | 2056/32{'loss': 0.5606, 'learning_rate': 2.9798745037679556e-05, 'epoch': 0.63}
{'loss': 0.5491, 'learning_rate': 2.9754515496694603e-05, 'epoch': 0.63}
{'loss': 0.5393, 'learning_rate': 2.9710304896401802e-05, 'epoch': 0.63}
{'loss': 0.5591, 'learning_rate': 2.966611327816241e-05, 'epoch': 0.63}
{'loss': 0.5447, 'learning_rate': 2.962194068331996e-05, 'epoch': 0.63}
50 [6:06:18<3:29:47, 10.54s/it]                                                        63%|   | 2056/3250 [6:06:18<3:29:47, 10.54s/it] 63%|   | 2057/3250 [6:06:28<3:28:35, 10.49s/it]                                                        63%|   | 2057/3250 [6:06:28<3:28:35, 10.49s/it] 63%|   | 2058/3250 [6:06:39<3:27:47, 10.46s/it]                                                        63%|   | 2058/3250 [6:06:39<3:27:47, 10.46s/it] 63%|   | 2059/3250 [6:06:49<3:27:07, 10.43s/it]                                                        63%|   | 2059/3250 [6:06:49<3:27:07, 10.43s/it] 63%|   | 2060/3250 [6:06:59<3:26:50, 10.43s/it]                                                        63%|   | 2060/3250 [6:06:59<3:26:50, 10.43s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.7831093668937683, 'eval_runtime': 2.1271, 'eval_samples_per_second': 5.641, 'eval_steps_per_second': 1.41, 'epoch': 0.63}
                                                        63%|   | 2060/3250 [6:07:02<3:26:50, 10.43s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_10/tmp-checkpoint-2060
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2060
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2060
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2060
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2060
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2060/pytorch_model.bin
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2060/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2060/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2060/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.5183, 'learning_rate': 2.9577787153200197e-05, 'epoch': 0.63}
{'loss': 0.5694, 'learning_rate': 2.9533652729111026e-05, 'epoch': 0.63}
{'loss': 0.5288, 'learning_rate': 2.948953745234246e-05, 'epoch': 0.63}
{'loss': 0.5245, 'learning_rate': 2.9445441364166616e-05, 'epoch': 0.64}
{'loss': 0.5217, 'learning_rate': 2.940136450583765e-05, 'epoch': 0.64}
 63%|   | 2061/3250 [6:07:13<3:42:34, 11.23s/it]                                                        63%|   | 2061/3250 [6:07:13<3:42:34, 11.23s/it] 63%|   | 2062/3250 [6:07:23<3:37:18, 10.97s/it]                                                        63%|   | 2062/3250 [6:07:23<3:37:18, 10.97s/it] 63%|   | 2063/3250 [6:07:33<3:33:38, 10.80s/it]                                                        63%|   | 2063/3250 [6:07:33<3:33:38, 10.80s/it] 64%|   | 2064/3250 [6:07:44<3:30:59, 10.67s/it]                                                        64%|   | 2064/3250 [6:07:44<3:30:59, 10.67s/it] 64%|   | 2065/3250 [6:07:54<3:29:12, 10.59s/it]                                                        64%|   | 2065/3250 [6:07:54<3:29:12, 10.59s/it] 64%|   | 2066/32{'loss': 0.5463, 'learning_rate': 2.935730691859172e-05, 'epoch': 0.64}
{'loss': 0.5416, 'learning_rate': 2.9313268643646986e-05, 'epoch': 0.64}
{'loss': 0.5481, 'learning_rate': 2.92692497222035e-05, 'epoch': 0.64}
{'loss': 0.9517, 'learning_rate': 2.9225250195443236e-05, 'epoch': 0.64}
{'loss': 0.6148, 'learning_rate': 2.9181270104530018e-05, 'epoch': 0.64}
50 [6:08:05<3:29:42, 10.63s/it]                                                        64%|   | 2066/3250 [6:08:05<3:29:42, 10.63s/it] 64%|   | 2067/3250 [6:08:15<3:28:03, 10.55s/it]                                                        64%|   | 2067/3250 [6:08:15<3:28:03, 10.55s/it] 64%|   | 2068/3250 [6:08:26<3:27:07, 10.51s/it]                                                        64%|   | 2068/3250 [6:08:26<3:27:07, 10.51s/it] 64%|   | 2069/3250 [6:08:36<3:26:07, 10.47s/it]                                                        64%|   | 2069/3250 [6:08:36<3:26:07, 10.47s/it] 64%|   | 2070/3250 [6:08:46<3:25:23, 10.44s/it]                                                        64%|   | 2070/3250 [6:08:46<3:25:23, 10.44s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.7861118912696838, 'eval_runtime': 2.2002, 'eval_samples_per_second': 5.454, 'eval_steps_per_second': 1.364, 'epoch': 0.64}
                                                        64%|   | 2070/3250 [6:08:49<3:25:23, 10.44s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_10/tmp-checkpoint-2070
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2070
/projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2070
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2070
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2070
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2070/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2070/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2070/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2070/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.5292, 'learning_rate': 2.91373094906095e-05, 'epoch': 0.64}
{'loss': 0.5666, 'learning_rate': 2.909336839480905e-05, 'epoch': 0.64}
{'loss': 0.551, 'learning_rate': 2.9049446858237854e-05, 'epoch': 0.64}
{'loss': 0.5379, 'learning_rate': 2.900554492198677e-05, 'epoch': 0.64}
{'loss': 0.511, 'learning_rate': 2.8961662627128327e-05, 'epoch': 0.64}
 64%|   | 2071/3250 [6:08:59<3:41:06, 11.25s/it]                                                        64%|   | 2071/3250 [6:08:59<3:41:06, 11.25s/it] 64%|   | 2072/3250 [6:09:10<3:35:42, 10.99s/it]                                                        64%|   | 2072/3250 [6:09:10<3:35:42, 10.99s/it] 64%|   | 2073/3250 [6:09:20<3:31:49, 10.80s/it]                                                        64%|   | 2073/3250 [6:09:20<3:31:49, 10.80s/it] 64%|   | 2074/3250 [6:09:31<3:29:12, 10.67s/it]                                                        64%|   | 2074/3250 [6:09:31<3:29:12, 10.67s/it] 64%|   | 2075/3250 [6:09:41<3:27:09, 10.58s/it]                                                        64%|   | 2075/3250 [6:09:41<3:27:09, 10.58s/it] 64%|   | 2076/32{'loss': 0.5872, 'learning_rate': 2.8917800014716635e-05, 'epoch': 0.64}
{'loss': 0.555, 'learning_rate': 2.8873957125787443e-05, 'epoch': 0.64}
{'loss': 0.5269, 'learning_rate': 2.8830134001358055e-05, 'epoch': 0.64}
{'loss': 0.5068, 'learning_rate': 2.878633068242721e-05, 'epoch': 0.64}
{'loss': 0.5478, 'learning_rate': 2.8742547209975192e-05, 'epoch': 0.64}
50 [6:09:51<3:25:49, 10.52s/it]                                                        64%|   | 2076/3250 [6:09:51<3:25:49, 10.52s/it] 64%|   | 2077/3250 [6:10:02<3:24:55, 10.48s/it]                                                        64%|   | 2077/3250 [6:10:02<3:24:55, 10.48s/it] 64%|   | 2078/3250 [6:10:12<3:24:08, 10.45s/it]                                                        64%|   | 2078/3250 [6:10:12<3:24:08, 10.45s/it] 64%|   | 2079/3250 [6:10:22<3:23:31, 10.43s/it]                                                        64%|   | 2079/3250 [6:10:22<3:23:31, 10.43s/it] 64%|   | 2080/3250 [6:10:33<3:23:07, 10.42s/it]                                                        64%|   | 2080/3250 [6:10:33<3:23:07, 10.42s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.7862988710403442, 'eval_runtime': 2.1642, 'eval_samples_per_second': 5.545, 'eval_steps_per_second': 1.386, 'epoch': 0.64}
                                                        64%|   | 2080/3250 [6:10:35<3:23:07, 10.42s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_10/tmp-checkpoint-2080
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2080
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2080
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2080
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2080
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2080/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2080/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2080/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2080/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.5407, 'learning_rate': 2.869878362496368e-05, 'epoch': 0.64}
{'loss': 0.5358, 'learning_rate': 2.8655039968335773e-05, 'epoch': 0.64}
{'loss': 0.5221, 'learning_rate': 2.8611316281015908e-05, 'epoch': 0.64}
{'loss': 0.5312, 'learning_rate': 2.8567612603909853e-05, 'epoch': 0.64}
{'loss': 0.5386, 'learning_rate': 2.8523928977904623e-05, 'epoch': 0.64}
 64%|   | 2081/3250 [6:10:46<3:39:46, 11.28s/it]                                                        64%|   | 2081/3250 [6:10:46<3:39:46, 11.28s/it] 64%|   | 2082/3250 [6:10:57<3:39:18, 11.27s/it]                                                        64%|   | 2082/3250 [6:10:57<3:39:18, 11.27s/it] 64%|   | 2083/3250 [6:11:08<3:33:53, 11.00s/it]                                                        64%|   | 2083/3250 [6:11:08<3:33:53, 11.00s/it] 64%|   | 2084/3250 [6:11:18<3:30:04, 10.81s/it]                                                        64%|   | 2084/3250 [6:11:18<3:30:04, 10.81s/it] 64%|   | 2085/3250 [6:11:28<3:27:16, 10.68s/it]                                                        64%|   | 2085/3250 [6:11:28<3:27:16, 10.68s/it] 64%|   | 2086/32{'loss': 0.5615, 'learning_rate': 2.848026544386851e-05, 'epoch': 0.64}
{'loss': 0.5507, 'learning_rate': 2.843662204265099e-05, 'epoch': 0.64}
{'loss': 0.5555, 'learning_rate': 2.8392998815082717e-05, 'epoch': 0.64}
{'loss': 0.5391, 'learning_rate': 2.8349395801975453e-05, 'epoch': 0.64}
{'loss': 0.5481, 'learning_rate': 2.8305813044122097e-05, 'epoch': 0.64}
50 [6:11:39<3:25:24, 10.59s/it]                                                        64%|   | 2086/3250 [6:11:39<3:25:24, 10.59s/it] 64%|   | 2087/3250 [6:11:49<3:24:00, 10.53s/it]                                                        64%|   | 2087/3250 [6:11:49<3:24:00, 10.53s/it] 64%|   | 2088/3250 [6:12:00<3:23:00, 10.48s/it]                                                        64%|   | 2088/3250 [6:12:00<3:23:00, 10.48s/it] 64%|   | 2089/3250 [6:12:10<3:22:16, 10.45s/it]                                                        64%|   | 2089/3250 [6:12:10<3:22:16, 10.45s/it] 64%|   | 2090/3250 [6:12:20<3:21:36, 10.43s/it]                                                        64%|   | 2090/3250 [6:12:20<3:21:36, 10.43s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.7817109227180481, 'eval_runtime': 2.1125, 'eval_samples_per_second': 5.681, 'eval_steps_per_second': 1.42, 'epoch': 0.64}
                                                        64%|   | 2090/3250 [6:12:23<3:21:36, 10.43s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_10/tmp-checkpoint-2090
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in I AM HERE AND I AM SAVING THE MODEL 
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2090
/projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2090
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2090
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2090
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2090/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2090/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2090/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2090/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.4961, 'learning_rate': 2.826225058229651e-05, 'epoch': 0.64}
{'loss': 0.5787, 'learning_rate': 2.821870845725366e-05, 'epoch': 0.64}
{'loss': 0.5146, 'learning_rate': 2.8175186709729397e-05, 'epoch': 0.64}
{'loss': 0.5264, 'learning_rate': 2.8131685380440585e-05, 'epoch': 0.64}
{'loss': 0.5089, 'learning_rate': 2.808820451008495e-05, 'epoch': 0.64}
 64%|   | 2091/3250 [6:12:33<3:35:46, 11.17s/it]                                                        64%|   | 2091/3250 [6:12:33<3:35:46, 11.17s/it] 64%|   | 2092/3250 [6:12:44<3:30:54, 10.93s/it]                                                        64%|   | 2092/3250 [6:12:44<3:30:54, 10.93s/it] 64%|   | 2093/3250 [6:12:54<3:27:27, 10.76s/it]                                                        64%|   | 2093/3250 [6:12:54<3:27:27, 10.76s/it] 64%|   | 2094/3250 [6:13:04<3:24:58, 10.64s/it]                                                        64%|   | 2094/3250 [6:13:04<3:24:58, 10.64s/it] 64%|   | 2095/3250 [6:13:15<3:23:12, 10.56s/it]                                                        64%|   | 2095/3250 [6:13:15<3:23:12, 10.56s/it] 64%|   | 2096/32{'loss': 0.5256, 'learning_rate': 2.8044744139341094e-05, 'epoch': 0.64}
{'loss': 0.5505, 'learning_rate': 2.800130430886841e-05, 'epoch': 0.65}
{'loss': 0.5212, 'learning_rate': 2.79578850593071e-05, 'epoch': 0.65}
{'loss': 0.553, 'learning_rate': 2.7914486431278098e-05, 'epoch': 0.65}
{'loss': 1.0255, 'learning_rate': 2.7871108465383066e-05, 'epoch': 0.65}
50 [6:13:25<3:21:54, 10.50s/it]                                                        64%|   | 2096/3250 [6:13:25<3:21:54, 10.50s/it] 65%|   | 2097/3250 [6:13:35<3:20:58, 10.46s/it]                                                        65%|   | 2097/3250 [6:13:35<3:20:58, 10.46s/it] 65%|   | 2098/3250 [6:13:46<3:20:17, 10.43s/it]                                                        65%|   | 2098/3250 [6:13:46<3:20:17, 10.43s/it] 65%|   | 2099/3250 [6:13:56<3:21:15, 10.49s/it]                                                        65%|   | 2099/3250 [6:13:56<3:21:15, 10.49s/it] 65%|   | 2100/3250 [6:14:07<3:20:16, 10.45s/it]                                                        65%|   | 2100/3250 [6:14:07<3:20:16, 10.45s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.7863284349441528, 'eval_runtime': 2.1192, 'eval_samples_per_second': 5.663, 'eval_steps_per_second': 1.416, 'epoch': 0.65}
                                                        65%|   | 2100/3250 [6:14:09<3:20:16, 10.45s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_10/tmp-checkpoint-2100
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2100
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2100
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2100
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2100
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2100/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2100/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2100/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2100/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.5248, 'learning_rate': 2.7827751202204323e-05, 'epoch': 0.65}
{'loss': 0.5465, 'learning_rate': 2.7784414682304832e-05, 'epoch': 0.65}
{'loss': 0.5419, 'learning_rate': 2.77410989462281e-05, 'epoch': 0.65}
{'loss': 0.5501, 'learning_rate': 2.769780403449824e-05, 'epoch': 0.65}
{'loss': 0.51, 'learning_rate': 2.765452998761988e-05, 'epoch': 0.65}
 65%|   | 2101/3250 [6:14:20<3:34:05, 11.18s/it]                                                        65%|   | 2101/3250 [6:14:20<3:34:05, 11.18s/it] 65%|   | 2102/3250 [6:14:30<3:29:11, 10.93s/it]                                                        65%|   | 2102/3250 [6:14:30<3:29:11, 10.93s/it] 65%|   | 2103/3250 [6:14:40<3:25:45, 10.76s/it]                                                        65%|   | 2103/3250 [6:14:40<3:25:45, 10.76s/it] 65%|   | 2104/3250 [6:14:51<3:23:17, 10.64s/it]                                                        65%|   | 2104/3250 [6:14:51<3:23:17, 10.64s/it] 65%|   | 2105/3250 [6:15:01<3:21:28, 10.56s/it]                                                        65%|   | 2105/3250 [6:15:01<3:21:28, 10.56s/it] 65%|   | 2106/32{'loss': 0.5334, 'learning_rate': 2.761127684607811e-05, 'epoch': 0.65}
{'loss': 0.6054, 'learning_rate': 2.7568044650338464e-05, 'epoch': 0.65}
{'loss': 0.5278, 'learning_rate': 2.752483344084692e-05, 'epoch': 0.65}
{'loss': 0.5335, 'learning_rate': 2.7481643258029748e-05, 'epoch': 0.65}
{'loss': 0.5324, 'learning_rate': 2.743847414229358e-05, 'epoch': 0.65}
50 [6:15:12<3:20:11, 10.50s/it]                                                        65%|   | 2106/3250 [6:15:12<3:20:11, 10.50s/it] 65%|   | 2107/3250 [6:15:22<3:19:15, 10.46s/it]                                                        65%|   | 2107/3250 [6:15:22<3:19:15, 10.46s/it] 65%|   | 2108/3250 [6:15:32<3:18:30, 10.43s/it]                                                        65%|   | 2108/3250 [6:15:32<3:18:30, 10.43s/it] 65%|   | 2109/3250 [6:15:43<3:17:52, 10.41s/it]                                                        65%|   | 2109/3250 [6:15:43<3:17:52, 10.41s/it] 65%|   | 2110/3250 [6:15:53<3:17:28, 10.39s/it]                                                        65%|   | 2110/3250 [6:15:53<3:17:28, 10.39s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.7847752571105957, 'eval_runtime': 2.1261, 'eval_samples_per_second': 5.644, 'eval_steps_per_second': 1.411, 'epoch': 0.65}
                                                        65%|   | 2110/3250 [6:15:55<3:17:28, 10.39s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_10/tmp-checkpoint-2110
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2110
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2110
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2110
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2110
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2110/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2110/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2110/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2110/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.5372, 'learning_rate': 2.7395326134025355e-05, 'epoch': 0.65}
{'loss': 0.546, 'learning_rate': 2.7352199273592265e-05, 'epoch': 0.65}
{'loss': 0.5285, 'learning_rate': 2.7309093601341696e-05, 'epoch': 0.65}
{'loss': 0.5259, 'learning_rate': 2.7266009157601224e-05, 'epoch': 0.65}
{'loss': 0.5273, 'learning_rate': 2.722294598267859e-05, 'epoch': 0.65}
 65%|   | 2111/3250 [6:16:06<3:31:54, 11.16s/it]                                                        65%|   | 2111/3250 [6:16:06<3:31:54, 11.16s/it] 65%|   | 2112/3250 [6:16:16<3:27:10, 10.92s/it]                                                        65%|   | 2112/3250 [6:16:16<3:27:10, 10.92s/it] 65%|   | 2113/3250 [6:16:27<3:23:54, 10.76s/it]                                                        65%|   | 2113/3250 [6:16:27<3:23:54, 10.76s/it] 65%|   | 2114/3250 [6:16:37<3:21:32, 10.64s/it]                                                        65%|   | 2114/3250 [6:16:37<3:21:32, 10.64s/it] 65%|   | 2115/3250 [6:16:48<3:23:14, 10.74s/it]                                                        65%|   | 2115/3250 [6:16:48<3:23:14, 10.74s/it] 65%|   | 2116/32{'loss': 0.5584, 'learning_rate': 2.7179904116861556e-05, 'epoch': 0.65}
{'loss': 0.5575, 'learning_rate': 2.713688360041803e-05, 'epoch': 0.65}
{'loss': 0.5424, 'learning_rate': 2.7093884473595922e-05, 'epoch': 0.65}
{'loss': 0.5471, 'learning_rate': 2.705090677662311e-05, 'epoch': 0.65}
{'loss': 0.5428, 'learning_rate': 2.700795054970748e-05, 'epoch': 0.65}
50 [6:16:58<3:20:58, 10.63s/it]                                                        65%|   | 2116/3250 [6:16:58<3:20:58, 10.63s/it] 65%|   | 2117/3250 [6:17:09<3:19:15, 10.55s/it]                                                        65%|   | 2117/3250 [6:17:09<3:19:15, 10.55s/it] 65%|   | 2118/3250 [6:17:19<3:18:01, 10.50s/it]                                                        65%|   | 2118/3250 [6:17:19<3:18:01, 10.50s/it] 65%|   | 2119/3250 [6:17:29<3:17:06, 10.46s/it]                                                        65%|   | 2119/3250 [6:17:29<3:17:06, 10.46s/it] 65%|   | 2120/3250 [6:17:40<3:16:24, 10.43s/it]                                                        65%|   | 2120/3250 [6:17:40<3:16:24, 10.43s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.7830130457878113, 'eval_runtime': 2.1112, 'eval_samples_per_second': 5.684, 'eval_steps_per_second': 1.421, 'epoch': 0.65}
                                                        65%|   | 2120/3250 [6:17:42<3:16:24, 10.43s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_10/tmp-checkpoint-2120
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2120
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2120
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2120
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2120
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2120/pytorch_model.bin
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2120/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2120/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2120/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.5205, 'learning_rate': 2.696501583303674e-05, 'epoch': 0.65}
{'loss': 0.5487, 'learning_rate': 2.692210266677855e-05, 'epoch': 0.65}
{'loss': 0.5459, 'learning_rate': 2.687921109108038e-05, 'epoch': 0.65}
{'loss': 0.5222, 'learning_rate': 2.6836341146069515e-05, 'epoch': 0.65}
{'loss': 0.5269, 'learning_rate': 2.6793492871852986e-05, 'epoch': 0.65}
 65%|   | 2121/3250 [6:17:53<3:30:34, 11.19s/it]                                                        65%|   | 2121/3250 [6:17:53<3:30:34, 11.19s/it] 65%|   | 2122/3250 [6:18:03<3:25:44, 10.94s/it]                                                        65%|   | 2122/3250 [6:18:03<3:25:44, 10.94s/it] 65%|   | 2123/3250 [6:18:14<3:22:16, 10.77s/it]                                                        65%|   | 2123/3250 [6:18:14<3:22:16, 10.77s/it] 65%|   | 2124/3250 [6:18:24<3:19:54, 10.65s/it]                                                        65%|   | 2124/3250 [6:18:24<3:19:54, 10.65s/it] 65%|   | 2125/3250 [6:18:34<3:18:08, 10.57s/it]                                                        65%|   | 2125/3250 [6:18:34<3:18:08, 10.57s/it] 65%|   | 2126/32{'loss': 0.4993, 'learning_rate': 2.6750666308517573e-05, 'epoch': 0.65}
{'loss': 0.5561, 'learning_rate': 2.670786149612972e-05, 'epoch': 0.65}
{'loss': 0.5378, 'learning_rate': 2.6665078474735505e-05, 'epoch': 0.65}
{'loss': 0.5581, 'learning_rate': 2.6622317284360664e-05, 'epoch': 0.66}
{'loss': 1.0319, 'learning_rate': 2.65795779650105e-05, 'epoch': 0.66}
50 [6:18:45<3:16:51, 10.51s/it]                                                        65%|   | 2126/3250 [6:18:45<3:16:51, 10.51s/it] 65%|   | 2127/3250 [6:18:55<3:16:00, 10.47s/it]                                                        65%|   | 2127/3250 [6:18:55<3:16:00, 10.47s/it] 65%|   | 2128/3250 [6:19:05<3:15:17, 10.44s/it]                                                        65%|   | 2128/3250 [6:19:05<3:15:17, 10.44s/it] 66%|   | 2129/3250 [6:19:16<3:14:42, 10.42s/it]                                                        66%|   | 2129/3250 [6:19:16<3:14:42, 10.42s/it] 66%|   | 2130/3250 [6:19:26<3:14:08, 10.40s/it]                                                        66%|   | 2130/3250 [6:19:26<3:14:08, 10.40s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.7876577377319336, 'eval_runtime': 2.1194, 'eval_samples_per_second': 5.662, 'eval_steps_per_second': 1.415, 'epoch': 0.66}
                                                        66%|   | 2130/3250 [6:19:28<3:14:08, 10.40s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_10/tmp-checkpoint-2130
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2130
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2130
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2130
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2130
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2130/pytorch_model.bin
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2130/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2130/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2130/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.539, 'learning_rate': 2.653686055666983e-05, 'epoch': 0.66}
{'loss': 0.5263, 'learning_rate': 2.649416509930299e-05, 'epoch': 0.66}
{'loss': 0.551, 'learning_rate': 2.645149163285381e-05, 'epoch': 0.66}
{'loss': 0.5404, 'learning_rate': 2.6408840197245455e-05, 'epoch': 0.66}
{'loss': 0.5026, 'learning_rate': 2.6366210832380567e-05, 'epoch': 0.66}
 66%|   | 2131/3250 [6:19:39<3:28:23, 11.17s/it]                                                        66%|   | 2131/3250 [6:19:39<3:28:23, 11.17s/it] 66%|   | 2132/3250 [6:19:50<3:25:06, 11.01s/it]                                                        66%|   | 2132/3250 [6:19:50<3:25:06, 11.01s/it] 66%|   | 2133/3250 [6:20:00<3:21:23, 10.82s/it]                                                        66%|   | 2133/3250 [6:20:00<3:21:23, 10.82s/it] 66%|   | 2134/3250 [6:20:10<3:18:38, 10.68s/it]                                                        66%|   | 2134/3250 [6:20:10<3:18:38, 10.68s/it] 66%|   | 2135/3250 [6:20:21<3:16:44, 10.59s/it]                                                        66%|   | 2135/3250 [6:20:21<3:16:44, 10.59s/it] 66%|   | 2136/32{'loss': 0.5153, 'learning_rate': 2.632360357814111e-05, 'epoch': 0.66}
{'loss': 0.5953, 'learning_rate': 2.628101847438835e-05, 'epoch': 0.66}
{'loss': 0.5438, 'learning_rate': 2.6238455560962884e-05, 'epoch': 0.66}
{'loss': 0.5361, 'learning_rate': 2.619591487768444e-05, 'epoch': 0.66}
{'loss': 0.4984, 'learning_rate': 2.615339646435206e-05, 'epoch': 0.66}
50 [6:20:31<3:15:17, 10.52s/it]                                                        66%|   | 2136/3250 [6:20:31<3:15:17, 10.52s/it] 66%|   | 2137/3250 [6:20:42<3:14:18, 10.47s/it]                                                        66%|   | 2137/3250 [6:20:42<3:14:18, 10.47s/it] 66%|   | 2138/3250 [6:20:52<3:13:27, 10.44s/it]                                                        66%|   | 2138/3250 [6:20:52<3:13:27, 10.44s/it] 66%|   | 2139/3250 [6:21:02<3:12:54, 10.42s/it]                                                        66%|   | 2139/3250 [6:21:02<3:12:54, 10.42s/it] 66%|   | 2140/3250 [6:21:13<3:12:24, 10.40s/it]                                                        66%|   | 2140/3250 [6:21:13<3:12:24, 10.40s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.7862685322761536, 'eval_runtime': 2.1138, 'eval_samples_per_second': 5.677, 'eval_steps_per_second': 1.419, 'epoch': 0.66}
                                                        66%|   | 2140/3250 [6:21:15<3:12:24, 10.40s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_10/tmp-checkpoint-2140
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2140
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2140
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2140
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2140
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2140/pytorch_model.bin
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2140/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2140/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2140/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.5343, 'learning_rate': 2.6110900360743896e-05, 'epoch': 0.66}
{'loss': 0.5392, 'learning_rate': 2.6068426606617252e-05, 'epoch': 0.66}
{'loss': 0.529, 'learning_rate': 2.6025975241708478e-05, 'epoch': 0.66}
{'loss': 0.5041, 'learning_rate': 2.598354630573303e-05, 'epoch': 0.66}
{'loss': 0.5445, 'learning_rate': 2.5941139838385383e-05, 'epoch': 0.66}
 66%|   | 2141/3250 [6:21:26<3:26:42, 11.18s/it]                                                        66%|   | 2141/3250 [6:21:26<3:26:42, 11.18s/it] 66%|   | 2142/3250 [6:21:36<3:21:52, 10.93s/it]                                                        66%|   | 2142/3250 [6:21:36<3:21:52, 10.93s/it] 66%|   | 2143/3250 [6:21:46<3:18:32, 10.76s/it]                                                        66%|   | 2143/3250 [6:21:46<3:18:32, 10.76s/it] 66%|   | 2144/3250 [6:21:57<3:16:08, 10.64s/it]                                                        66%|   | 2144/3250 [6:21:57<3:16:08, 10.64s/it] 66%|   | 2145/3250 [6:22:07<3:14:30, 10.56s/it]                                                        66%|   | 2145/3250 [6:22:07<3:14:30, 10.56s/it] 66%|   | 2146/32{'loss': 0.5565, 'learning_rate': 2.589875587933892e-05, 'epoch': 0.66}
{'loss': 0.5487, 'learning_rate': 2.5856394468246036e-05, 'epoch': 0.66}
{'loss': 0.5451, 'learning_rate': 2.581405564473801e-05, 'epoch': 0.66}
{'loss': 0.5284, 'learning_rate': 2.5771739448425e-05, 'epoch': 0.66}
{'loss': 0.5503, 'learning_rate': 2.572944591889598e-05, 'epoch': 0.66}
50 [6:22:17<3:13:08, 10.50s/it]                                                        66%|   | 2146/3250 [6:22:17<3:13:08, 10.50s/it] 66%|   | 2147/3250 [6:22:28<3:12:15, 10.46s/it]                                                        66%|   | 2147/3250 [6:22:28<3:12:15, 10.46s/it] 66%|   | 2148/3250 [6:22:39<3:14:56, 10.61s/it]                                                        66%|   | 2148/3250 [6:22:39<3:14:56, 10.61s/it] 66%|   | 2149/3250 [6:22:49<3:13:26, 10.54s/it]                                                        66%|   | 2149/3250 [6:22:49<3:13:26, 10.54s/it] 66%|   | 2150/3250 [6:23:00<3:12:24, 10.50s/it]                                                        66%|   | 2150/3250 [6:23:00<3:12:24, 10.50s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.7842490077018738, 'eval_runtime': 2.1205, 'eval_samples_per_second': 5.659, 'eval_steps_per_second': 1.415, 'epoch': 0.66}
                                                        66%|   | 2150/3250 [6:23:02<3:12:24, 10.50s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_10/tmp-checkpoint-2150
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2150
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2150
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2150
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2150
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2150/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2150/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2150/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2150/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.5256, 'learning_rate': 2.5687175095718723e-05, 'epoch': 0.66}
{'loss': 0.5232, 'learning_rate': 2.5644927018439748e-05, 'epoch': 0.66}
{'loss': 0.5545, 'learning_rate': 2.56027017265843e-05, 'epoch': 0.66}
{'loss': 0.5134, 'learning_rate': 2.5560499259656325e-05, 'epoch': 0.66}
{'loss': 0.5063, 'learning_rate': 2.55183196571384e-05, 'epoch': 0.66}
 66%|   | 2151/3250 [6:23:13<3:26:02, 11.25s/it]                                                        66%|   | 2151/3250 [6:23:13<3:26:02, 11.25s/it] 66%|   | 2152/3250 [6:23:23<3:21:02, 10.99s/it]                                                        66%|   | 2152/3250 [6:23:23<3:21:02, 10.99s/it] 66%|   | 2153/3250 [6:23:33<3:17:22, 10.80s/it]                                                        66%|   | 2153/3250 [6:23:33<3:17:22, 10.80s/it] 66%|   | 2154/3250 [6:23:44<3:14:46, 10.66s/it]                                                        66%|   | 2154/3250 [6:23:44<3:14:46, 10.66s/it] 66%|   | 2155/3250 [6:23:54<3:12:53, 10.57s/it]                                                        66%|   | 2155/3250 [6:23:54<3:12:53, 10.57s/it] 66%|   | 2156/32{'loss': 0.5099, 'learning_rate': 2.5476162958491727e-05, 'epoch': 0.66}
{'loss': 0.5527, 'learning_rate': 2.5434029203156035e-05, 'epoch': 0.66}
{'loss': 0.5275, 'learning_rate': 2.539191843054963e-05, 'epoch': 0.66}
{'loss': 0.5441, 'learning_rate': 2.5349830680069338e-05, 'epoch': 0.66}
{'loss': 1.0563, 'learning_rate': 2.530776599109036e-05, 'epoch': 0.66}
50 [6:24:04<3:11:33, 10.51s/it]                                                        66%|   | 2156/3250 [6:24:04<3:11:33, 10.51s/it] 66%|   | 2157/3250 [6:24:15<3:10:35, 10.46s/it]                                                        66%|   | 2157/3250 [6:24:15<3:10:35, 10.46s/it] 66%|   | 2158/3250 [6:24:25<3:09:50, 10.43s/it]                                                        66%|   | 2158/3250 [6:24:25<3:09:50, 10.43s/it] 66%|   | 2159/3250 [6:24:35<3:09:25, 10.42s/it]                                                        66%|   | 2159/3250 [6:24:35<3:09:25, 10.42s/it] 66%|   | 2160/3250 [6:24:46<3:08:56, 10.40s/it]                                                        66%|   | 2160/3250 [6:24:46<3:08:56, 10.40s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.7895339131355286, 'eval_runtime': 2.1134, 'eval_samples_per_second': 5.678, 'eval_steps_per_second': 1.419, 'epoch': 0.66}
                                                        66%|   | 2160/3250 [6:24:48<3:08:56, 10.40s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_10/tmp-checkpoint-2160
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  I AM HERE AND I AM SAVING THE MODEL
/projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2160the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2160

I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2160
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2160
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2160/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2160/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2160/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2160/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.5049, 'learning_rate': 2.5265724402966407e-05, 'epoch': 0.66}
{'loss': 0.5202, 'learning_rate': 2.522370595502954e-05, 'epoch': 0.67}
{'loss': 0.5492, 'learning_rate': 2.51817106865902e-05, 'epoch': 0.67}
{'loss': 0.5446, 'learning_rate': 2.5139738636937084e-05, 'epoch': 0.67}
{'loss': 0.5305, 'learning_rate': 2.5097789845337223e-05, 'epoch': 0.67}
 66%|   | 2161/3250 [6:24:59<3:22:50, 11.18s/it]                                                        66%|   | 2161/3250 [6:24:59<3:22:50, 11.18s/it] 67%|   | 2162/3250 [6:25:09<3:18:11, 10.93s/it]                                                        67%|   | 2162/3250 [6:25:09<3:18:11, 10.93s/it] 67%|   | 2163/3250 [6:25:20<3:14:49, 10.75s/it]                                                        67%|   | 2163/3250 [6:25:20<3:14:49, 10.75s/it] 67%|   | 2164/3250 [6:25:30<3:13:44, 10.70s/it]                                                        67%|   | 2164/3250 [6:25:30<3:13:44, 10.70s/it] 67%|   | 2165/3250 [6:25:40<3:11:36, 10.60s/it]                                                        67%|   | 2165/3250 [6:25:40<3:11:36, 10.60s/it] 67%|   | 2166/32{'loss': 0.5119, 'learning_rate': 2.5055864351035868e-05, 'epoch': 0.67}
{'loss': 0.5813, 'learning_rate': 2.5013962193256473e-05, 'epoch': 0.67}
{'loss': 0.5558, 'learning_rate': 2.497208341120067e-05, 'epoch': 0.67}
{'loss': 0.5247, 'learning_rate': 2.493022804404822e-05, 'epoch': 0.67}
{'loss': 0.5065, 'learning_rate': 2.4888396130956948e-05, 'epoch': 0.67}
50 [6:25:51<3:10:00, 10.52s/it]                                                        67%|   | 2166/3250 [6:25:51<3:10:00, 10.52s/it] 67%|   | 2167/3250 [6:26:01<3:08:57, 10.47s/it]                                                        67%|   | 2167/3250 [6:26:01<3:08:57, 10.47s/it] 67%|   | 2168/3250 [6:26:11<3:08:08, 10.43s/it]                                                        67%|   | 2168/3250 [6:26:11<3:08:08, 10.43s/it] 67%|   | 2169/3250 [6:26:22<3:07:33, 10.41s/it]                                                        67%|   | 2169/3250 [6:26:22<3:07:33, 10.41s/it] 67%|   | 2170/3250 [6:26:32<3:07:06, 10.39s/it]                                                        67%|   | 2170/3250 [6:26:32<3:07:06, 10.39s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.7851252555847168, 'eval_runtime': 2.1193, 'eval_samples_per_second': 5.662, 'eval_steps_per_second': 1.416, 'epoch': 0.67}
                                                        67%|   | 2170/3250 [6:26:34<3:07:06, 10.39s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_10/tmp-checkpoint-2170
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2170
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2170
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2170
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2170
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2170/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2170/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2170/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2170/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.5453, 'learning_rate': 2.4846587711062768e-05, 'epoch': 0.67}
{'loss': 0.5352, 'learning_rate': 2.4804802823479613e-05, 'epoch': 0.67}
{'loss': 0.5397, 'learning_rate': 2.4763041507299388e-05, 'epoch': 0.67}
{'loss': 0.528, 'learning_rate': 2.472130380159199e-05, 'epoch': 0.67}
{'loss': 0.5384, 'learning_rate': 2.4679589745405124e-05, 'epoch': 0.67}
 67%|   | 2171/3250 [6:26:45<3:20:45, 11.16s/it]                                                        67%|   | 2171/3250 [6:26:45<3:20:45, 11.16s/it] 67%|   | 2172/3250 [6:26:56<3:16:16, 10.92s/it]                                                        67%|   | 2172/3250 [6:26:56<3:16:16, 10.92s/it] 67%|   | 2173/3250 [6:27:06<3:12:58, 10.75s/it]                                                        67%|   | 2173/3250 [6:27:06<3:12:58, 10.75s/it] 67%|   | 2174/3250 [6:27:16<3:10:37, 10.63s/it]                                                        67%|   | 2174/3250 [6:27:16<3:10:37, 10.63s/it] 67%|   | 2175/3250 [6:27:27<3:08:56, 10.55s/it]                                                        67%|   | 2175/3250 [6:27:27<3:08:56, 10.55s/it] 67%|   | 2176/32{'loss': 0.5253, 'learning_rate': 2.4637899377764494e-05, 'epoch': 0.67}
{'loss': 0.5529, 'learning_rate': 2.459623273767354e-05, 'epoch': 0.67}
{'loss': 0.5398, 'learning_rate': 2.4554589864113563e-05, 'epoch': 0.67}
{'loss': 0.5396, 'learning_rate': 2.4512970796043616e-05, 'epoch': 0.67}
{'loss': 0.539, 'learning_rate': 2.447137557240048e-05, 'epoch': 0.67}
50 [6:27:37<3:07:42, 10.49s/it]                                                        67%|   | 2176/3250 [6:27:37<3:07:42, 10.49s/it] 67%|   | 2177/3250 [6:27:47<3:06:49, 10.45s/it]                                                        67%|   | 2177/3250 [6:27:47<3:06:49, 10.45s/it] 67%|   | 2178/3250 [6:27:58<3:06:05, 10.42s/it]                                                        67%|   | 2178/3250 [6:27:58<3:06:05, 10.42s/it] 67%|   | 2179/3250 [6:28:08<3:05:37, 10.40s/it]                                                        67%|   | 2179/3250 [6:28:08<3:05:37, 10.40s/it] 67%|   | 2180/3250 [6:28:18<3:05:13, 10.39s/it]                                                        67%|   | 2180/3250 [6:28:18<3:05:13, 10.39s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.7816073894500732, 'eval_runtime': 2.114, 'eval_samples_per_second': 5.676, 'eval_steps_per_second': 1.419, 'epoch': 0.67}
                                                        67%|   | 2180/3250 [6:28:20<3:05:13, 10.39s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_10/tmp-checkpoint-2180
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2180
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2180
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2180
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2180
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2180/pytorch_model.bin
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2180/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2180/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2180/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.5488, 'learning_rate': 2.442980423209864e-05, 'epoch': 0.67}
{'loss': 0.4945, 'learning_rate': 2.4388256814030187e-05, 'epoch': 0.67}
{'loss': 0.5735, 'learning_rate': 2.4346733357064888e-05, 'epoch': 0.67}
{'loss': 0.5255, 'learning_rate': 2.4305233900050074e-05, 'epoch': 0.67}
{'loss': 0.5274, 'learning_rate': 2.4263758481810617e-05, 'epoch': 0.67}
 67%|   | 2181/3250 [6:28:32<3:22:03, 11.34s/it]                                                        67%|   | 2181/3250 [6:28:32<3:22:03, 11.34s/it] 67%|   | 2182/3250 [6:28:42<3:16:37, 11.05s/it]                                                        67%|   | 2182/3250 [6:28:42<3:16:37, 11.05s/it] 67%|   | 2183/3250 [6:28:53<3:12:43, 10.84s/it]                                                        67%|   | 2183/3250 [6:28:53<3:12:43, 10.84s/it] 67%|   | 2184/3250 [6:29:03<3:10:01, 10.70s/it]                                                        67%|   | 2184/3250 [6:29:03<3:10:01, 10.70s/it] 67%|   | 2185/3250 [6:29:13<3:07:56, 10.59s/it]                                                        67%|   | 2185/3250 [6:29:13<3:07:56, 10.59s/it] 67%|   | 2186/32{'loss': 0.5105, 'learning_rate': 2.422230714114891e-05, 'epoch': 0.67}
{'loss': 0.5187, 'learning_rate': 2.418087991684483e-05, 'epoch': 0.67}
{'loss': 0.5383, 'learning_rate': 2.4139476847655634e-05, 'epoch': 0.67}
{'loss': 0.5249, 'learning_rate': 2.4098097972316046e-05, 'epoch': 0.67}
{'loss': 0.5452, 'learning_rate': 2.4056743329538138e-05, 'epoch': 0.67}
50 [6:29:24<3:06:33, 10.52s/it]                                                        67%|   | 2186/3250 [6:29:24<3:06:33, 10.52s/it] 67%|   | 2187/3250 [6:29:34<3:05:29, 10.47s/it]                                                        67%|   | 2187/3250 [6:29:34<3:05:29, 10.47s/it] 67%|   | 2188/3250 [6:29:44<3:04:47, 10.44s/it]                                                        67%|   | 2188/3250 [6:29:44<3:04:47, 10.44s/it] 67%|   | 2189/3250 [6:29:55<3:04:07, 10.41s/it]                                                        67%|   | 2189/3250 [6:29:55<3:04:07, 10.41s/it] 67%|   | 2190/3250 [6:30:05<3:03:39, 10.40s/it]                                                        67%|   | 2190/3250 [6:30:05<3:03:39, 10.40s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.7886126041412354, 'eval_runtime': 2.1121, 'eval_samples_per_second': 5.681, 'eval_steps_per_second': 1.42, 'epoch': 0.67}
                                                        67%|   | 2190/3250 [6:30:07<3:03:39, 10.40s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_10/tmp-checkpoint-2190
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2190I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2190

I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2190
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2190
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2190/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2190/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2190/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2190/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 1.0272, 'learning_rate': 2.4015412958011302e-05, 'epoch': 0.67}
{'loss': 0.5285, 'learning_rate': 2.3974106896402237e-05, 'epoch': 0.67}
{'loss': 0.5495, 'learning_rate': 2.393282518335486e-05, 'epoch': 0.67}
{'loss': 0.5381, 'learning_rate': 2.3891567857490372e-05, 'epoch': 0.68}
{'loss': 0.5432, 'learning_rate': 2.3850334957407087e-05, 'epoch': 0.68}
 67%|   | 2191/3250 [6:30:18<3:17:04, 11.17s/it]                                                        67%|   | 2191/3250 [6:30:18<3:17:04, 11.17s/it] 67%|   | 2192/3250 [6:30:28<3:12:37, 10.92s/it]                                                        67%|   | 2192/3250 [6:30:28<3:12:37, 10.92s/it] 67%|   | 2193/3250 [6:30:39<3:09:21, 10.75s/it]                                                        67%|   | 2193/3250 [6:30:39<3:09:21, 10.75s/it] 68%|   | 2194/3250 [6:30:49<3:07:04, 10.63s/it]                                                        68%|   | 2194/3250 [6:30:49<3:07:04, 10.63s/it] 68%|   | 2195/3250 [6:30:59<3:05:25, 10.55s/it]                                                        68%|   | 2195/3250 [6:30:59<3:05:25, 10.55s/it] 68%|   | 2196/32{'loss': 0.5053, 'learning_rate': 2.3809126521680518e-05, 'epoch': 0.68}
{'loss': 0.5281, 'learning_rate': 2.3767942588863283e-05, 'epoch': 0.68}
{'loss': 0.5965, 'learning_rate': 2.372678319748507e-05, 'epoch': 0.68}
{'loss': 0.5231, 'learning_rate': 2.3685648386052617e-05, 'epoch': 0.68}
{'loss': 0.5192, 'learning_rate': 2.3644538193049625e-05, 'epoch': 0.68}
50 [6:31:10<3:04:11, 10.49s/it]                                                        68%|   | 2196/3250 [6:31:10<3:04:11, 10.49s/it] 68%|   | 2197/3250 [6:31:20<3:04:32, 10.51s/it]                                                        68%|   | 2197/3250 [6:31:20<3:04:32, 10.51s/it] 68%|   | 2198/3250 [6:31:31<3:03:27, 10.46s/it]                                                        68%|   | 2198/3250 [6:31:31<3:03:27, 10.46s/it] 68%|   | 2199/3250 [6:31:41<3:02:48, 10.44s/it]                                                        68%|   | 2199/3250 [6:31:41<3:02:48, 10.44s/it] 68%|   | 2200/3250 [6:31:51<3:02:18, 10.42s/it]                                                        68%|   | 2200/3250 [6:31:51<3:02:18, 10.42s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.785222589969635, 'eval_runtime': 2.1099, 'eval_samples_per_second': 5.687, 'eval_steps_per_second': 1.422, 'epoch': 0.68}
                                                        68%|   | 2200/3250 [6:31:54<3:02:18, 10.42s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_10/tmp-checkpoint-2200
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2200
/projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2200
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2200
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2200
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2200/pytorch_model.bin
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2200/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2200/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2200/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.5319, 'learning_rate': 2.360345265693681e-05, 'epoch': 0.68}
{'loss': 0.5257, 'learning_rate': 2.356239181615181e-05, 'epoch': 0.68}
{'loss': 0.5353, 'learning_rate': 2.3521355709109138e-05, 'epoch': 0.68}
{'loss': 0.5225, 'learning_rate': 2.3480344374200203e-05, 'epoch': 0.68}
{'loss': 0.5282, 'learning_rate': 2.343935784979323e-05, 'epoch': 0.68}
 68%|   | 2201/3250 [6:32:04<3:15:38, 11.19s/it]                                                        68%|   | 2201/3250 [6:32:04<3:15:38, 11.19s/it] 68%|   | 2202/3250 [6:32:15<3:11:18, 10.95s/it]                                                        68%|   | 2202/3250 [6:32:15<3:11:18, 10.95s/it] 68%|   | 2203/3250 [6:32:25<3:08:06, 10.78s/it]                                                        68%|   | 2203/3250 [6:32:25<3:08:06, 10.78s/it] 68%|   | 2204/3250 [6:32:36<3:05:48, 10.66s/it]                                                        68%|   | 2204/3250 [6:32:36<3:05:48, 10.66s/it] 68%|   | 2205/3250 [6:32:46<3:04:09, 10.57s/it]                                                        68%|   | 2205/3250 [6:32:46<3:04:09, 10.57s/it] 68%|   | 2206/32{'loss': 0.5228, 'learning_rate': 2.3398396174233178e-05, 'epoch': 0.68}
{'loss': 0.5443, 'learning_rate': 2.3357459385841823e-05, 'epoch': 0.68}
{'loss': 0.553, 'learning_rate': 2.3316547522917638e-05, 'epoch': 0.68}
{'loss': 0.5315, 'learning_rate': 2.3275660623735772e-05, 'epoch': 0.68}
{'loss': 0.5452, 'learning_rate': 2.3234798726548044e-05, 'epoch': 0.68}
50 [6:32:56<3:03:03, 10.52s/it]                                                        68%|   | 2206/3250 [6:32:56<3:03:03, 10.52s/it] 68%|   | 2207/3250 [6:33:07<3:02:11, 10.48s/it]                                                        68%|   | 2207/3250 [6:33:07<3:02:11, 10.48s/it] 68%|   | 2208/3250 [6:33:17<3:01:34, 10.46s/it]                                                        68%|   | 2208/3250 [6:33:17<3:01:34, 10.46s/it] 68%|   | 2209/3250 [6:33:28<3:01:02, 10.43s/it]                                                        68%|   | 2209/3250 [6:33:28<3:01:02, 10.43s/it] 68%|   | 2210/3250 [6:33:38<3:00:42, 10.43s/it]                                                        68%|   | 2210/3250 [6:33:38<3:00:42, 10.43s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.7825660705566406, 'eval_runtime': 2.1177, 'eval_samples_per_second': 5.667, 'eval_steps_per_second': 1.417, 'epoch': 0.68}
                                                        68%|   | 2210/3250 [6:33:40<3:00:42, 10.43s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_10/tmp-checkpoint-2210
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2210
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2210
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2210
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2210
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2210/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2210/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2210/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2210/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.5305, 'learning_rate': 2.3193961869582826e-05, 'epoch': 0.68}
{'loss': 0.5134, 'learning_rate': 2.3153150091045132e-05, 'epoch': 0.68}
{'loss': 0.5456, 'learning_rate': 2.311236342911644e-05, 'epoch': 0.68}
{'loss': 0.5424, 'learning_rate': 2.3071601921954794e-05, 'epoch': 0.68}
{'loss': 0.5246, 'learning_rate': 2.3030865607694675e-05, 'epoch': 0.68}
 68%|   | 2211/3250 [6:33:51<3:13:53, 11.20s/it]                                                        68%|   | 2211/3250 [6:33:51<3:13:53, 11.20s/it] 68%|   | 2212/3250 [6:34:01<3:09:36, 10.96s/it]                                                        68%|   | 2212/3250 [6:34:01<3:09:36, 10.96s/it] 68%|   | 2213/3250 [6:34:12<3:06:33, 10.79s/it]                                                        68%|   | 2213/3250 [6:34:12<3:06:33, 10.79s/it] 68%|   | 2214/3250 [6:34:23<3:06:30, 10.80s/it]                                                        68%|   | 2214/3250 [6:34:23<3:06:30, 10.80s/it] 68%|   | 2215/3250 [6:34:33<3:04:20, 10.69s/it]                                                        68%|   | 2215/3250 [6:34:33<3:04:20, 10.69s/it] 68%|   | 2216/32{'loss': 0.5162, 'learning_rate': 2.2990154524447005e-05, 'epoch': 0.68}
{'loss': 0.5046, 'learning_rate': 2.2949468710299116e-05, 'epoch': 0.68}
{'loss': 0.5434, 'learning_rate': 2.2908808203314635e-05, 'epoch': 0.68}
{'loss': 0.5222, 'learning_rate': 2.2868173041533585e-05, 'epoch': 0.68}
{'loss': 0.5536, 'learning_rate': 2.2827563262972244e-05, 'epoch': 0.68}
50 [6:34:43<3:02:46, 10.61s/it]                                                        68%|   | 2216/3250 [6:34:43<3:02:46, 10.61s/it] 68%|   | 2217/3250 [6:34:54<3:01:36, 10.55s/it]                                                        68%|   | 2217/3250 [6:34:54<3:01:36, 10.55s/it] 68%|   | 2218/3250 [6:35:04<3:00:44, 10.51s/it]                                                        68%|   | 2218/3250 [6:35:04<3:00:44, 10.51s/it] 68%|   | 2219/3250 [6:35:15<3:00:04, 10.48s/it]                                                        68%|   | 2219/3250 [6:35:15<3:00:04, 10.48s/it] 68%|   | 2220/3250 [6:35:25<2:59:40, 10.47s/it]                                                        68%|   | 2220/3250 [6:35:25<2:59:40, 10.47s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.7878962755203247, 'eval_runtime': 2.34, 'eval_samples_per_second': 5.128, 'eval_steps_per_second': 1.282, 'epoch': 0.68}
                                                        68%|   | 2220/3250 [6:35:27<2:59:40, 10.47s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_10/tmp-checkpoint-2220
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2220
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2220
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2220
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2220
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2220/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2220/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2220/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2220/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 1.0248, 'learning_rate': 2.278697890562316e-05, 'epoch': 0.68}
{'loss': 0.5267, 'learning_rate': 2.274642000745507e-05, 'epoch': 0.68}
{'loss': 0.528, 'learning_rate': 2.270588660641294e-05, 'epoch': 0.68}
{'loss': 0.5493, 'learning_rate': 2.266537874041781e-05, 'epoch': 0.68}
{'loss': 0.545, 'learning_rate': 2.262489644736689e-05, 'epoch': 0.68}
 68%|   | 2221/3250 [6:35:38<3:13:34, 11.29s/it]                                                        68%|   | 2221/3250 [6:35:38<3:13:34, 11.29s/it] 68%|   | 2222/3250 [6:35:50<3:12:54, 11.26s/it]                                                        68%|   | 2222/3250 [6:35:50<3:12:54, 11.26s/it] 68%|   | 2223/3250 [6:36:00<3:09:15, 11.06s/it]                                                        68%|   | 2223/3250 [6:36:00<3:09:15, 11.06s/it] 68%|   | 2224/3250 [6:36:11<3:05:47, 10.86s/it]                                                        68%|   | 2224/3250 [6:36:11<3:05:47, 10.86s/it] 68%|   | 2225/3250 [6:36:21<3:03:19, 10.73s/it]                                                        68%|   | 2225/3250 [6:36:21<3:03:19, 10.73s/it] 68%|   | 2226/32{'loss': 0.5154, 'learning_rate': 2.2584439765133454e-05, 'epoch': 0.68}
{'loss': 0.5147, 'learning_rate': 2.2544008731566817e-05, 'epoch': 0.69}
{'loss': 0.5953, 'learning_rate': 2.250360338449226e-05, 'epoch': 0.69}
{'loss': 0.5385, 'learning_rate': 2.246322376171109e-05, 'epoch': 0.69}
{'loss': 0.5361, 'learning_rate': 2.242286990100052e-05, 'epoch': 0.69}
50 [6:36:35<3:19:15, 11.68s/it]                                                        68%|   | 2226/3250 [6:36:35<3:19:15, 11.68s/it] 69%|   | 2227/3250 [6:36:45<3:13:14, 11.33s/it]                                                        69%|   | 2227/3250 [6:36:45<3:13:14, 11.33s/it] 69%|   | 2228/3250 [6:36:56<3:08:19, 11.06s/it]                                                        69%|   | 2228/3250 [6:36:56<3:08:19, 11.06s/it] 69%|   | 2229/3250 [6:37:06<3:04:49, 10.86s/it]                                                        69%|   | 2229/3250 [6:37:06<3:04:49, 10.86s/it] 69%|   | 2230/3250 [6:37:17<3:04:13, 10.84s/it]                                                        69%|   | 2230/3250 [6:37:17<3:04:13, 10.84s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.7863662242889404, 'eval_runtime': 2.2204, 'eval_samples_per_second': 5.404, 'eval_steps_per_second': 1.351, 'epoch': 0.69}
                                                        69%|   | 2230/3250 [6:37:19<3:04:13, 10.84s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_10/tmp-checkpoint-2230
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2230I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2230

I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2230
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2230
the pytorch model path isthe pytorch model path isthe pytorch model path is  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2230/pytorch_model.bin/projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2230/pytorch_model.bin

 /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2230/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2230/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.4886, 'learning_rate': 2.238254184011364e-05, 'epoch': 0.69}
{'loss': 0.5372, 'learning_rate': 2.2342239616779442e-05, 'epoch': 0.69}
{'loss': 0.5358, 'learning_rate': 2.2301963268702723e-05, 'epoch': 0.69}
{'loss': 0.5292, 'learning_rate': 2.226171283356409e-05, 'epoch': 0.69}
{'loss': 0.5178, 'learning_rate': 2.2221488349019903e-05, 'epoch': 0.69}
 69%|   | 2231/3250 [6:37:30<3:15:52, 11.53s/it]                                                        69%|   | 2231/3250 [6:37:30<3:15:52, 11.53s/it] 69%|   | 2232/3250 [6:37:41<3:10:00, 11.20s/it]                                                        69%|   | 2232/3250 [6:37:41<3:10:00, 11.20s/it] 69%|   | 2233/3250 [6:37:51<3:06:45, 11.02s/it]                                                        69%|   | 2233/3250 [6:37:51<3:06:45, 11.02s/it] 69%|   | 2234/3250 [6:38:02<3:03:35, 10.84s/it]                                                        69%|   | 2234/3250 [6:38:02<3:03:35, 10.84s/it] 69%|   | 2235/3250 [6:38:12<3:01:10, 10.71s/it]                                                        69%|   | 2235/3250 [6:38:12<3:01:10, 10.71s/it] 69%|   | 2236/32{'loss': 0.5318, 'learning_rate': 2.2181289852702204e-05, 'epoch': 0.69}
{'loss': 0.5308, 'learning_rate': 2.214111738221877e-05, 'epoch': 0.69}
{'loss': 0.5488, 'learning_rate': 2.210097097515301e-05, 'epoch': 0.69}
{'loss': 0.5374, 'learning_rate': 2.2060850669063963e-05, 'epoch': 0.69}
{'loss': 0.5165, 'learning_rate': 2.2020756501486233e-05, 'epoch': 0.69}
50 [6:38:22<2:59:27, 10.62s/it]                                                        69%|   | 2236/3250 [6:38:22<2:59:27, 10.62s/it] 69%|   | 2237/3250 [6:38:33<2:58:12, 10.56s/it]                                                        69%|   | 2237/3250 [6:38:33<2:58:12, 10.56s/it] 69%|   | 2238/3250 [6:38:43<2:57:19, 10.51s/it]                                                        69%|   | 2238/3250 [6:38:43<2:57:19, 10.51s/it] 69%|   | 2239/3250 [6:38:54<2:56:35, 10.48s/it]                                                        69%|   | 2239/3250 [6:38:54<2:56:35, 10.48s/it] 69%|   | 2240/3250 [6:39:04<2:56:07, 10.46s/it]                                                        69%|   | 2240/3250 [6:39:04<2:56:07, 10.46s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.7821926474571228, 'eval_runtime': 2.1268, 'eval_samples_per_second': 5.642, 'eval_steps_per_second': 1.411, 'epoch': 0.69}
                                                        69%|   | 2240/3250 [6:39:06<2:56:07, 10.46s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_10/tmp-checkpoint-2240
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2240
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2240
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2240
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2240
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2240/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2240/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2240/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2240/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.5485, 'learning_rate': 2.1980688509929997e-05, 'epoch': 0.69}
{'loss': 0.5252, 'learning_rate': 2.194064673188089e-05, 'epoch': 0.69}
{'loss': 0.514, 'learning_rate': 2.1900631204800054e-05, 'epoch': 0.69}
{'loss': 0.569, 'learning_rate': 2.1860641966124114e-05, 'epoch': 0.69}
{'loss': 0.5226, 'learning_rate': 2.1820679053265e-05, 'epoch': 0.69}
 69%|   | 2241/3250 [6:39:17<3:09:27, 11.27s/it]                                                        69%|   | 2241/3250 [6:39:17<3:09:27, 11.27s/it] 69%|   | 2242/3250 [6:39:28<3:04:51, 11.00s/it]                                                        69%|   | 2242/3250 [6:39:28<3:04:51, 11.00s/it] 69%|   | 2243/3250 [6:39:38<3:01:46, 10.83s/it]                                                        69%|   | 2243/3250 [6:39:38<3:01:46, 10.83s/it] 69%|   | 2244/3250 [6:39:48<2:59:26, 10.70s/it]                                                        69%|   | 2244/3250 [6:39:48<2:59:26, 10.70s/it] 69%|   | 2245/3250 [6:39:59<2:57:49, 10.62s/it]                                                        69%|   | 2245/3250 [6:39:59<2:57:49, 10.62s/it] 69%|   | 2246/32{'loss': 0.5109, 'learning_rate': 2.1780742503610118e-05, 'epoch': 0.69}
{'loss': 0.5127, 'learning_rate': 2.1740832354522145e-05, 'epoch': 0.69}
{'loss': 0.5441, 'learning_rate': 2.1700948643339103e-05, 'epoch': 0.69}
{'loss': 0.5196, 'learning_rate': 2.1661091407374218e-05, 'epoch': 0.69}
{'loss': 0.536, 'learning_rate': 2.1621260683916007e-05, 'epoch': 0.69}
50 [6:40:09<2:57:48, 10.63s/it]                                                        69%|   | 2246/3250 [6:40:09<2:57:48, 10.63s/it] 69%|   | 2247/3250 [6:40:20<2:56:33, 10.56s/it]                                                        69%|   | 2247/3250 [6:40:20<2:56:33, 10.56s/it] 69%|   | 2248/3250 [6:40:30<2:55:29, 10.51s/it]                                                        69%|   | 2248/3250 [6:40:30<2:55:29, 10.51s/it] 69%|   | 2249/3250 [6:40:41<2:54:52, 10.48s/it]                                                        69%|   | 2249/3250 [6:40:41<2:54:52, 10.48s/it] 69%|   | 2250/3250 [6:40:51<2:54:16, 10.46s/it]                                                        69%|   | 2250/3250 [6:40:51<2:54:16, 10.46s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.7911026477813721, 'eval_runtime': 2.777, 'eval_samples_per_second': 4.321, 'eval_steps_per_second': 1.08, 'epoch': 0.69}
                                                        69%|   | 2250/3250 [6:40:54<2:54:16, 10.46s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_10/tmp-checkpoint-2250
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2250
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2250
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2250
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2250
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2250/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2250/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2250/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2250/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 1.0483, 'learning_rate': 2.1581456510228147e-05, 'epoch': 0.69}
{'loss': 0.497, 'learning_rate': 2.1541678923549507e-05, 'epoch': 0.69}
{'loss': 0.5105, 'learning_rate': 2.1501927961094054e-05, 'epoch': 0.69}
{'loss': 0.5559, 'learning_rate': 2.1462203660050882e-05, 'epoch': 0.69}
{'loss': 0.539, 'learning_rate': 2.1422506057584078e-05, 'epoch': 0.69}
 69%|   | 2251/3250 [6:41:05<3:09:51, 11.40s/it]                                                        69%|   | 2251/3250 [6:41:05<3:09:51, 11.40s/it] 69%|   | 2252/3250 [6:41:15<3:04:33, 11.10s/it]                                                        69%|   | 2252/3250 [6:41:15<3:04:33, 11.10s/it] 69%|   | 2253/3250 [6:41:25<3:00:46, 10.88s/it]                                                        69%|   | 2253/3250 [6:41:25<3:00:46, 10.88s/it] 69%|   | 2254/3250 [6:41:36<2:58:07, 10.73s/it]                                                        69%|   | 2254/3250 [6:41:36<2:58:07, 10.73s/it] 69%|   | 2255/3250 [6:41:46<2:56:14, 10.63s/it]                                                        69%|   | 2255/3250 [6:41:46<2:56:14, 10.63s/it] 69%|   | 2256/32{'loss': 0.5253, 'learning_rate': 2.1382835190832813e-05, 'epoch': 0.69}
{'loss': 0.5094, 'learning_rate': 2.134319109691122e-05, 'epoch': 0.69}
{'loss': 0.5724, 'learning_rate': 2.1303573812908385e-05, 'epoch': 0.69}
{'loss': 0.5425, 'learning_rate': 2.126398337588834e-05, 'epoch': 0.7}
{'loss': 0.5161, 'learning_rate': 2.122441982288994e-05, 'epoch': 0.7}
50 [6:41:57<2:54:52, 10.56s/it]                                                        69%|   | 2256/3250 [6:41:57<2:54:52, 10.56s/it] 69%|   | 2257/3250 [6:42:07<2:53:47, 10.50s/it]                                                        69%|   | 2257/3250 [6:42:07<2:53:47, 10.50s/it] 69%|   | 2258/3250 [6:42:17<2:53:03, 10.47s/it]                                                        69%|   | 2258/3250 [6:42:17<2:53:03, 10.47s/it] 70%|   | 2259/3250 [6:42:28<2:52:26, 10.44s/it]                                                        70%|   | 2259/3250 [6:42:28<2:52:26, 10.44s/it] 70%|   | 2260/3250 [6:42:38<2:52:02, 10.43s/it]                                                        70%|   | 2260/3250 [6:42:38<2:52:02, 10.43s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.785519540309906, 'eval_runtime': 2.1098, 'eval_samples_per_second': 5.688, 'eval_steps_per_second': 1.422, 'epoch': 0.7}
                                                        70%|   | 2260/3250 [6:42:40<2:52:02, 10.43s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_10/tmp-checkpoint-2260
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2260
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2260
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2260
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2260
the pytorch model path isthe pytorch model path is the pytorch model path is/projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2260/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2260/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2260/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2260/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.4995, 'learning_rate': 2.1184883190926954e-05, 'epoch': 0.7}
{'loss': 0.5415, 'learning_rate': 2.11453735169879e-05, 'epoch': 0.7}
{'loss': 0.5252, 'learning_rate': 2.110589083803613e-05, 'epoch': 0.7}
{'loss': 0.5312, 'learning_rate': 2.1066435191009715e-05, 'epoch': 0.7}
{'loss': 0.511, 'learning_rate': 2.1027006612821453e-05, 'epoch': 0.7}
 70%|   | 2261/3250 [6:42:51<3:04:34, 11.20s/it]                                                        70%|   | 2261/3250 [6:42:51<3:04:34, 11.20s/it] 70%|   | 2262/3250 [6:43:01<3:00:26, 10.96s/it]                                                        70%|   | 2262/3250 [6:43:01<3:00:26, 10.96s/it] 70%|   | 2263/3250 [6:43:12<2:59:28, 10.91s/it]                                                        70%|   | 2263/3250 [6:43:12<2:59:28, 10.91s/it] 70%|   | 2264/3250 [6:43:23<2:56:43, 10.75s/it]                                                        70%|   | 2264/3250 [6:43:23<2:56:43, 10.75s/it] 70%|   | 2265/3250 [6:43:33<2:54:49, 10.65s/it]                                                        70%|   | 2265/3250 [6:43:33<2:54:49, 10.65s/it] 70%|   | 2266/32{'loss': 0.5271, 'learning_rate': 2.0987605140358824e-05, 'epoch': 0.7}
{'loss': 0.5358, 'learning_rate': 2.0948230810483888e-05, 'epoch': 0.7}
{'loss': 0.5505, 'learning_rate': 2.0908883660033374e-05, 'epoch': 0.7}
{'loss': 0.5328, 'learning_rate': 2.0869563725818575e-05, 'epoch': 0.7}
{'loss': 0.5425, 'learning_rate': 2.08302710446253e-05, 'epoch': 0.7}
50 [6:43:43<2:53:27, 10.58s/it]                                                        70%|   | 2266/3250 [6:43:43<2:53:27, 10.58s/it] 70%|   | 2267/3250 [6:43:54<2:52:20, 10.52s/it]                                                        70%|   | 2267/3250 [6:43:54<2:52:20, 10.52s/it] 70%|   | 2268/3250 [6:44:04<2:51:43, 10.49s/it]                                                        70%|   | 2268/3250 [6:44:04<2:51:43, 10.49s/it] 70%|   | 2269/3250 [6:44:15<2:51:04, 10.46s/it]                                                        70%|   | 2269/3250 [6:44:15<2:51:04, 10.46s/it] 70%|   | 2270/3250 [6:44:25<2:50:36, 10.45s/it]                                                        70%|   | 2270/3250 [6:44:25<2:50:36, 10.45s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.7824348211288452, 'eval_runtime': 2.1054, 'eval_samples_per_second': 5.7, 'eval_steps_per_second': 1.425, 'epoch': 0.7}
                                                        70%|   | 2270/3250 [6:44:27<2:50:36, 10.45s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_10/tmp-checkpoint-2270
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  I AM HERE AND I AM SAVING THE MODEL/projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2270
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2270

I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2270
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2270
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2270/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2270/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2270/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2270/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.5273, 'learning_rate': 2.0791005653213884e-05, 'epoch': 0.7}
{'loss': 0.5449, 'learning_rate': 2.075176758831913e-05, 'epoch': 0.7}
{'loss': 0.486, 'learning_rate': 2.0712556886650235e-05, 'epoch': 0.7}
{'loss': 0.579, 'learning_rate': 2.067337358489085e-05, 'epoch': 0.7}
{'loss': 0.5198, 'learning_rate': 2.0634217719698955e-05, 'epoch': 0.7}
 70%|   | 2271/3250 [6:44:38<3:02:39, 11.19s/it]                                                        70%|   | 2271/3250 [6:44:38<3:02:39, 11.19s/it] 70%|   | 2272/3250 [6:44:48<2:58:26, 10.95s/it]                                                        70%|   | 2272/3250 [6:44:48<2:58:26, 10.95s/it] 70%|   | 2273/3250 [6:44:59<2:55:47, 10.80s/it]                                                        70%|   | 2273/3250 [6:44:59<2:55:47, 10.80s/it] 70%|   | 2274/3250 [6:45:09<2:53:32, 10.67s/it]                                                        70%|   | 2274/3250 [6:45:09<2:53:32, 10.67s/it] 70%|   | 2275/3250 [6:45:20<2:51:55, 10.58s/it]                                                        70%|   | 2275/3250 [6:45:20<2:51:55, 10.58s/it] 70%|   | 2276/32{'loss': 0.5302, 'learning_rate': 2.059508932770689e-05, 'epoch': 0.7}
{'loss': 0.5021, 'learning_rate': 2.055598844552129e-05, 'epoch': 0.7}
{'loss': 0.5204, 'learning_rate': 2.0516915109723e-05, 'epoch': 0.7}
{'loss': 0.5345, 'learning_rate': 2.0477869356867186e-05, 'epoch': 0.7}
{'loss': 0.5257, 'learning_rate': 2.043885122348311e-05, 'epoch': 0.7}
50 [6:45:30<2:50:40, 10.51s/it]                                                        70%|   | 2276/3250 [6:45:30<2:50:40, 10.51s/it] 70%|   | 2277/3250 [6:45:40<2:49:49, 10.47s/it]                                                        70%|   | 2277/3250 [6:45:40<2:49:49, 10.47s/it] 70%|   | 2278/3250 [6:45:51<2:49:13, 10.45s/it]                                                        70%|   | 2278/3250 [6:45:51<2:49:13, 10.45s/it] 70%|   | 2279/3250 [6:46:02<2:51:40, 10.61s/it]                                                        70%|   | 2279/3250 [6:46:02<2:51:40, 10.61s/it] 70%|   | 2280/3250 [6:46:12<2:50:33, 10.55s/it]                                                        70%|   | 2280/3250 [6:46:12<2:50:33, 10.55s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.7871253490447998, 'eval_runtime': 2.1069, 'eval_samples_per_second': 5.696, 'eval_steps_per_second': 1.424, 'epoch': 0.7}
                                                        70%|   | 2280/3250 [6:46:14<2:50:33, 10.55s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_10/tmp-checkpoint-2280
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2280I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2280

I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2280
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2280
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2280/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2280/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2280/pytorch_model.binthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2280/pytorch_model.bin

/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.5381, 'learning_rate': 2.0399860746074262e-05, 'epoch': 0.7}
{'loss': 1.024, 'learning_rate': 2.0360897961118248e-05, 'epoch': 0.7}
{'loss': 0.5112, 'learning_rate': 2.0321962905066748e-05, 'epoch': 0.7}
{'loss': 0.54, 'learning_rate': 2.0283055614345532e-05, 'epoch': 0.7}
{'loss': 0.5243, 'learning_rate': 2.024417612535433e-05, 'epoch': 0.7}
 70%|   | 2281/3250 [6:46:25<3:02:08, 11.28s/it]                                                        70%|   | 2281/3250 [6:46:25<3:02:08, 11.28s/it] 70%|   | 2282/3250 [6:46:35<2:57:34, 11.01s/it]                                                        70%|   | 2282/3250 [6:46:35<2:57:34, 11.01s/it] 70%|   | 2283/3250 [6:46:46<2:54:22, 10.82s/it]                                                        70%|   | 2283/3250 [6:46:46<2:54:22, 10.82s/it] 70%|   | 2284/3250 [6:46:56<2:52:05, 10.69s/it]                                                        70%|   | 2284/3250 [6:46:56<2:52:05, 10.69s/it] 70%|   | 2285/3250 [6:47:07<2:53:02, 10.76s/it]                                                        70%|   | 2285/3250 [6:47:07<2:53:02, 10.76s/it] 70%|   | 2286/32{'loss': 0.5341, 'learning_rate': 2.020532447446693e-05, 'epoch': 0.7}
{'loss': 0.4955, 'learning_rate': 2.016650069803105e-05, 'epoch': 0.7}
{'loss': 0.5733, 'learning_rate': 2.012770483236832e-05, 'epoch': 0.7}
{'loss': 0.5489, 'learning_rate': 2.008893691377428e-05, 'epoch': 0.7}
{'loss': 0.5216, 'learning_rate': 2.005019697851832e-05, 'epoch': 0.7}
50 [6:47:18<2:51:24, 10.67s/it]                                                        70%|   | 2286/3250 [6:47:18<2:51:24, 10.67s/it] 70%|   | 2287/3250 [6:47:28<2:49:58, 10.59s/it]                                                        70%|   | 2287/3250 [6:47:28<2:49:58, 10.59s/it] 70%|   | 2288/3250 [6:47:38<2:48:54, 10.54s/it]                                                        70%|   | 2288/3250 [6:47:38<2:48:54, 10.54s/it] 70%|   | 2289/3250 [6:47:49<2:48:05, 10.50s/it]                                                        70%|   | 2289/3250 [6:47:49<2:48:05, 10.50s/it] 70%|   | 2290/3250 [6:47:59<2:47:30, 10.47s/it]                                                        70%|   | 2290/3250 [6:47:59<2:47:30, 10.47s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.7861899733543396, 'eval_runtime': 2.406, 'eval_samples_per_second': 4.988, 'eval_steps_per_second': 1.247, 'epoch': 0.7}
                                                        70%|   | 2290/3250 [6:48:02<2:47:30, 10.47s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_10/tmp-checkpoint-2290
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2290
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2290
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2290
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2290
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2290/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2290/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2290/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2290/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.5213, 'learning_rate': 2.001148506284361e-05, 'epoch': 0.7}
{'loss': 0.5245, 'learning_rate': 1.9972801202967162e-05, 'epoch': 0.71}
{'loss': 0.5167, 'learning_rate': 1.9934145435079702e-05, 'epoch': 0.71}
{'loss': 0.523, 'learning_rate': 1.989551779534571e-05, 'epoch': 0.71}
{'loss': 0.5166, 'learning_rate': 1.985691831990333e-05, 'epoch': 0.71}
 70%|   | 2291/3250 [6:48:13<3:02:09, 11.40s/it]                                                        70%|   | 2291/3250 [6:48:13<3:02:09, 11.40s/it] 71%|   | 2292/3250 [6:48:23<2:57:30, 11.12s/it]                                                        71%|   | 2292/3250 [6:48:23<2:57:30, 11.12s/it] 71%|   | 2293/3250 [6:48:34<2:53:59, 10.91s/it]                                                        71%|   | 2293/3250 [6:48:34<2:53:59, 10.91s/it] 71%|   | 2294/3250 [6:48:44<2:51:26, 10.76s/it]                                                        71%|   | 2294/3250 [6:48:44<2:51:26, 10.76s/it] 71%|   | 2295/3250 [6:48:55<2:49:30, 10.65s/it]                                                        71%|   | 2295/3250 [6:48:55<2:49:30, 10.65s/it] 71%|   | 2296/32{'loss': 0.5127, 'learning_rate': 1.9818347044864328e-05, 'epoch': 0.71}
{'loss': 0.5297, 'learning_rate': 1.9779804006314147e-05, 'epoch': 0.71}
{'loss': 0.5431, 'learning_rate': 1.9741289240311755e-05, 'epoch': 0.71}
{'loss': 0.5364, 'learning_rate': 1.9702802782889706e-05, 'epoch': 0.71}
{'loss': 0.5348, 'learning_rate': 1.9664344670054067e-05, 'epoch': 0.71}
50 [6:49:05<2:49:39, 10.67s/it]                                                        71%|   | 2296/3250 [6:49:05<2:49:39, 10.67s/it] 71%|   | 2297/3250 [6:49:16<2:47:56, 10.57s/it]                                                        71%|   | 2297/3250 [6:49:16<2:47:56, 10.57s/it] 71%|   | 2298/3250 [6:49:26<2:46:45, 10.51s/it]                                                        71%|   | 2298/3250 [6:49:26<2:46:45, 10.51s/it] 71%|   | 2299/3250 [6:49:36<2:45:52, 10.47s/it]                                                        71%|   | 2299/3250 [6:49:36<2:45:52, 10.47s/it] 71%|   | 2300/3250 [6:49:48<2:51:00, 10.80s/it]                                                        71%|   | 2300/3250 [6:49:48<2:51:00, 10.80s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.7834281325340271, 'eval_runtime': 2.3652, 'eval_samples_per_second': 5.074, 'eval_steps_per_second': 1.268, 'epoch': 0.71}
                                                        71%|   | 2300/3250 [6:49:50<2:51:00, 10.80s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_10/tmp-checkpoint-2300
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2300
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2300
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2300
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2300
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2300/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2300/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2300/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2300/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.5257, 'learning_rate': 1.962591493778438e-05, 'epoch': 0.71}
{'loss': 0.5322, 'learning_rate': 1.9587513622033648e-05, 'epoch': 0.71}
{'loss': 0.5054, 'learning_rate': 1.9549140758728246e-05, 'epoch': 0.71}
{'loss': 0.546, 'learning_rate': 1.951079638376798e-05, 'epoch': 0.71}
{'loss': 0.5262, 'learning_rate': 1.9472480533025984e-05, 'epoch': 0.71}
 71%|   | 2301/3250 [6:50:01<3:02:47, 11.56s/it]                                                        71%|   | 2301/3250 [6:50:01<3:02:47, 11.56s/it] 71%|   | 2302/3250 [6:50:12<2:56:50, 11.19s/it]                                                        71%|   | 2302/3250 [6:50:12<2:56:50, 11.19s/it] 71%|   | 2303/3250 [6:50:22<2:52:37, 10.94s/it]                                                        71%|   | 2303/3250 [6:50:22<2:52:37, 10.94s/it] 71%|   | 2304/3250 [6:50:32<2:49:35, 10.76s/it]                                                        71%|   | 2304/3250 [6:50:32<2:49:35, 10.76s/it] 71%|   | 2305/3250 [6:50:43<2:47:25, 10.63s/it]                                                        71%|   | 2305/3250 [6:50:43<2:47:25, 10.63s/it] 71%|   | 2306/32{'loss': 0.5147, 'learning_rate': 1.9434193242348708e-05, 'epoch': 0.71}
{'loss': 0.5124, 'learning_rate': 1.9395934547555878e-05, 'epoch': 0.71}
{'loss': 0.4924, 'learning_rate': 1.9357704484440498e-05, 'epoch': 0.71}
{'loss': 0.5366, 'learning_rate': 1.931950308876871e-05, 'epoch': 0.71}
{'loss': 0.5266, 'learning_rate': 1.9281330396279912e-05, 'epoch': 0.71}
50 [6:50:53<2:45:50, 10.54s/it]                                                        71%|   | 2306/3250 [6:50:53<2:45:50, 10.54s/it] 71%|   | 2307/3250 [6:51:03<2:44:44, 10.48s/it]                                                        71%|   | 2307/3250 [6:51:03<2:44:44, 10.48s/it] 71%|   | 2308/3250 [6:51:14<2:43:53, 10.44s/it]                                                        71%|   | 2308/3250 [6:51:14<2:43:53, 10.44s/it] 71%|   | 2309/3250 [6:51:24<2:43:19, 10.41s/it]                                                        71%|   | 2309/3250 [6:51:24<2:43:19, 10.41s/it] 71%|   | 2310/3250 [6:51:34<2:42:58, 10.40s/it]                                                        71%|   | 2310/3250 [6:51:34<2:42:58, 10.40s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.7888793349266052, 'eval_runtime': 2.1446, 'eval_samples_per_second': 5.596, 'eval_steps_per_second': 1.399, 'epoch': 0.71}
                                                        71%|   | 2310/3250 [6:51:36<2:42:58, 10.40s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_10/tmp-checkpoint-2310
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2310I AM HERE AND I AM SAVING THE MODEL

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2310
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2310
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2310
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2310/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2310/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2310/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2310/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.5407, 'learning_rate': 1.9243186442686615e-05, 'epoch': 0.71}
{'loss': 1.0275, 'learning_rate': 1.920507126367448e-05, 'epoch': 0.71}
{'loss': 0.5262, 'learning_rate': 1.916698489490216e-05, 'epoch': 0.71}
{'loss': 0.5149, 'learning_rate': 1.9128927372001454e-05, 'epoch': 0.71}
{'loss': 0.5485, 'learning_rate': 1.9090898730577133e-05, 'epoch': 0.71}
 71%|   | 2311/3250 [6:51:47<2:55:03, 11.19s/it]                                                        71%|   | 2311/3250 [6:51:47<2:55:03, 11.19s/it] 71%|   | 2312/3250 [6:51:59<2:55:42, 11.24s/it]                                                        71%|   | 2312/3250 [6:51:59<2:55:42, 11.24s/it] 71%|   | 2313/3250 [6:52:09<2:51:28, 10.98s/it]                                                        71%|   | 2313/3250 [6:52:09<2:51:28, 10.98s/it] 71%|   | 2314/3250 [6:52:19<2:48:23, 10.79s/it]                                                        71%|   | 2314/3250 [6:52:19<2:48:23, 10.79s/it] 71%|   | 2315/3250 [6:52:30<2:46:10, 10.66s/it]                                                        71%|   | 2315/3250 [6:52:30<2:46:10, 10.66s/it] 71%|  | 2316/{'loss': 0.5418, 'learning_rate': 1.905289900620692e-05, 'epoch': 0.71}
{'loss': 0.5034, 'learning_rate': 1.9014928234441525e-05, 'epoch': 0.71}
{'loss': 0.5051, 'learning_rate': 1.897698645080456e-05, 'epoch': 0.71}
{'loss': 0.5885, 'learning_rate': 1.893907369079252e-05, 'epoch': 0.71}
{'loss': 0.5416, 'learning_rate': 1.8901189989874745e-05, 'epoch': 0.71}
3250 [6:52:40<2:44:28, 10.57s/it]                                                        71%|  | 2316/3250 [6:52:40<2:44:28, 10.57s/it] 71%|  | 2317/3250 [6:52:50<2:43:21, 10.51s/it]                                                        71%|  | 2317/3250 [6:52:50<2:43:21, 10.51s/it] 71%|  | 2318/3250 [6:53:01<2:42:35, 10.47s/it]                                                        71%|  | 2318/3250 [6:53:01<2:42:35, 10.47s/it] 71%|  | 2319/3250 [6:53:11<2:42:01, 10.44s/it]                                                        71%|  | 2319/3250 [6:53:11<2:42:01, 10.44s/it] 71%|  | 2320/3250 [6:53:22<2:41:25, 10.41s/it]                                                        71%|  | 2320/3250 [6:53:22<2:41:25, 10.41s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.7849981188774109, 'eval_runtime': 2.1095, 'eval_samples_per_second': 5.688, 'eval_steps_per_second': 1.422, 'epoch': 0.71}
                                                        71%|  | 2320/3250 [6:53:24<2:41:25, 10.41s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_10/tmp-checkpoint-2320
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2320
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2320
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2320
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2320
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2320/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2320/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2320/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2320/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.5272, 'learning_rate': 1.886333538349337e-05, 'epoch': 0.71}
{'loss': 0.4801, 'learning_rate': 1.8825509907063327e-05, 'epoch': 0.71}
{'loss': 0.5342, 'learning_rate': 1.8787713595972306e-05, 'epoch': 0.71}
{'loss': 0.5342, 'learning_rate': 1.8749946485580693e-05, 'epoch': 0.72}
{'loss': 0.5198, 'learning_rate': 1.8712208611221572e-05, 'epoch': 0.72}
 71%|  | 2321/3250 [6:53:35<2:53:13, 11.19s/it]                                                        71%|  | 2321/3250 [6:53:35<2:53:13, 11.19s/it] 71%|  | 2322/3250 [6:53:45<2:49:31, 10.96s/it]                                                        71%|  | 2322/3250 [6:53:45<2:49:31, 10.96s/it] 71%|  | 2323/3250 [6:53:55<2:46:53, 10.80s/it]                                                        71%|  | 2323/3250 [6:53:55<2:46:53, 10.80s/it] 72%|  | 2324/3250 [6:54:06<2:44:55, 10.69s/it]                                                        72%|  | 2324/3250 [6:54:06<2:44:55, 10.69s/it] 72%|  | 2325/3250 [6:54:16<2:43:29, 10.60s/it]                                                        72%|  | 2325/3250 [6:54:16<2:43:29, 10.60s/it] 72%|{'loss': 0.5086, 'learning_rate': 1.8674500008200674e-05, 'epoch': 0.72}
{'loss': 0.5213, 'learning_rate': 1.8636820711796306e-05, 'epoch': 0.72}
{'loss': 0.5322, 'learning_rate': 1.8599170757259406e-05, 'epoch': 0.72}
{'loss': 0.544, 'learning_rate': 1.856155017981345e-05, 'epoch': 0.72}
{'loss': 0.5286, 'learning_rate': 1.8523959014654407e-05, 'epoch': 0.72}
  | 2326/3250 [6:54:27<2:42:28, 10.55s/it]                                                        72%|  | 2326/3250 [6:54:27<2:42:28, 10.55s/it] 72%|  | 2327/3250 [6:54:37<2:41:41, 10.51s/it]                                                        72%|  | 2327/3250 [6:54:37<2:41:41, 10.51s/it] 72%|  | 2328/3250 [6:54:48<2:42:54, 10.60s/it]                                                        72%|  | 2328/3250 [6:54:48<2:42:54, 10.60s/it] 72%|  | 2329/3250 [6:54:58<2:41:55, 10.55s/it]                                                        72%|  | 2329/3250 [6:54:58<2:41:55, 10.55s/it] 72%|  | 2330/3250 [6:55:09<2:41:09, 10.51s/it]                                                        72%|  | 2330/3250 [6:55:09<2:41:09, 10.51s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.7842061519622803, 'eval_runtime': 2.1199, 'eval_samples_per_second': 5.661, 'eval_steps_per_second': 1.415, 'epoch': 0.72}
                                                        72%|  | 2330/3250 [6:55:11<2:41:09, 10.51s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_10/tmp-checkpoint-2330
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2330
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2330
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2330
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2330
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2330/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2330/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2330/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2330/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.512, 'learning_rate': 1.8486397296950745e-05, 'epoch': 0.72}
{'loss': 0.5482, 'learning_rate': 1.8448865061843397e-05, 'epoch': 0.72}
{'loss': 0.523, 'learning_rate': 1.8411362344445708e-05, 'epoch': 0.72}
{'loss': 0.5219, 'learning_rate': 1.8373889179843374e-05, 'epoch': 0.72}
{'loss': 0.5465, 'learning_rate': 1.833644560309447e-05, 'epoch': 0.72}
 72%|  | 2331/3250 [6:55:22<2:52:28, 11.26s/it]                                                        72%|  | 2331/3250 [6:55:22<2:52:28, 11.26s/it] 72%|  | 2332/3250 [6:55:32<2:48:28, 11.01s/it]                                                        72%|  | 2332/3250 [6:55:32<2:48:28, 11.01s/it] 72%|  | 2333/3250 [6:55:43<2:45:39, 10.84s/it]                                                        72%|  | 2333/3250 [6:55:43<2:45:39, 10.84s/it] 72%|  | 2334/3250 [6:55:53<2:43:34, 10.71s/it]                                                        72%|  | 2334/3250 [6:55:53<2:43:34, 10.71s/it] 72%|  | 2335/3250 [6:56:03<2:42:01, 10.62s/it]                                                        72%|  | 2335/3250 [6:56:03<2:42:01, 10.62s/it] 72%|{'loss': 0.5092, 'learning_rate': 1.8299031649229402e-05, 'epoch': 0.72}
{'loss': 0.501, 'learning_rate': 1.8261647353250842e-05, 'epoch': 0.72}
{'loss': 0.5022, 'learning_rate': 1.8224292750133743e-05, 'epoch': 0.72}
{'loss': 0.5307, 'learning_rate': 1.8186967874825217e-05, 'epoch': 0.72}
{'loss': 0.5262, 'learning_rate': 1.8149672762244624e-05, 'epoch': 0.72}
  | 2336/3250 [6:56:14<2:40:59, 10.57s/it]                                                        72%|  | 2336/3250 [6:56:14<2:40:59, 10.57s/it] 72%|  | 2337/3250 [6:56:24<2:40:11, 10.53s/it]                                                        72%|  | 2337/3250 [6:56:24<2:40:11, 10.53s/it] 72%|  | 2338/3250 [6:56:35<2:39:35, 10.50s/it]                                                        72%|  | 2338/3250 [6:56:35<2:39:35, 10.50s/it] 72%|  | 2339/3250 [6:56:45<2:39:06, 10.48s/it]                                                        72%|  | 2339/3250 [6:56:45<2:39:06, 10.48s/it] 72%|  | 2340/3250 [6:56:56<2:38:50, 10.47s/it]                                                        72%|  | 2340/3250 [6:56:56<2:38:50, 10.47s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.7895132899284363, 'eval_runtime': 2.1267, 'eval_samples_per_second': 5.642, 'eval_steps_per_second': 1.411, 'epoch': 0.72}
                                                        72%|  | 2340/3250 [6:56:58<2:38:50, 10.47s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_10/tmp-checkpoint-2340
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2340
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2340
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2340
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2340
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2340/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2340/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2340/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2340/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.5393, 'learning_rate': 1.8112407447283465e-05, 'epoch': 0.72}
{'loss': 1.0399, 'learning_rate': 1.8075171964805354e-05, 'epoch': 0.72}
{'loss': 0.4881, 'learning_rate': 1.8037966349646e-05, 'epoch': 0.72}
{'loss': 0.515, 'learning_rate': 1.8000790636613197e-05, 'epoch': 0.72}
{'loss': 0.5474, 'learning_rate': 1.7963644860486706e-05, 'epoch': 0.72}
 72%|  | 2341/3250 [6:57:09<2:50:40, 11.27s/it]                                                        72%|  | 2341/3250 [6:57:09<2:50:40, 11.27s/it] 72%|  | 2342/3250 [6:57:19<2:46:37, 11.01s/it]                                                        72%|  | 2342/3250 [6:57:19<2:46:37, 11.01s/it] 72%|  | 2343/3250 [6:57:30<2:43:48, 10.84s/it]                                                        72%|  | 2343/3250 [6:57:30<2:43:48, 10.84s/it] 72%|  | 2344/3250 [6:57:40<2:41:45, 10.71s/it]                                                        72%|  | 2344/3250 [6:57:40<2:41:45, 10.71s/it] 72%|  | 2345/3250 [6:57:51<2:41:15, 10.69s/it]                                                        72%|  | 2345/3250 [6:57:51<2:41:15, 10.69s/it] 72%|{'loss': 0.5471, 'learning_rate': 1.7926529056018298e-05, 'epoch': 0.72}
{'loss': 0.5142, 'learning_rate': 1.7889443257931737e-05, 'epoch': 0.72}
{'loss': 0.5238, 'learning_rate': 1.785238750092269e-05, 'epoch': 0.72}
{'loss': 0.5748, 'learning_rate': 1.7815361819658732e-05, 'epoch': 0.72}
{'loss': 0.5489, 'learning_rate': 1.777836624877929e-05, 'epoch': 0.72}
  | 2346/3250 [6:58:01<2:39:51, 10.61s/it]                                                        72%|  | 2346/3250 [6:58:01<2:39:51, 10.61s/it] 72%|  | 2347/3250 [6:58:12<2:38:44, 10.55s/it]                                                        72%|  | 2347/3250 [6:58:12<2:38:44, 10.55s/it] 72%|  | 2348/3250 [6:58:22<2:37:51, 10.50s/it]                                                        72%|  | 2348/3250 [6:58:22<2:37:51, 10.50s/it] 72%|  | 2349/3250 [6:58:32<2:37:10, 10.47s/it]                                                        72%|  | 2349/3250 [6:58:32<2:37:10, 10.47s/it] 72%|  | 2350/3250 [6:58:43<2:36:37, 10.44s/it]                                                        72%|  | 2350/3250 [6:58:43<2:36:37, 10.44s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.7842386364936829, 'eval_runtime': 2.3477, 'eval_samples_per_second': 5.111, 'eval_steps_per_second': 1.278, 'epoch': 0.72}
                                                        72%|  | 2350/3250 [6:58:45<2:36:37, 10.44s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_10/tmp-checkpoint-2350
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2350
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2350
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2350
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2350
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2350/pytorch_model.bin
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2350/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2350/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2350/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.5164, 'learning_rate': 1.774140082289563e-05, 'epoch': 0.72}
{'loss': 0.4958, 'learning_rate': 1.770446557659079e-05, 'epoch': 0.72}
{'loss': 0.5433, 'learning_rate': 1.76675605444196e-05, 'epoch': 0.72}
{'loss': 0.5276, 'learning_rate': 1.7630685760908622e-05, 'epoch': 0.72}
{'loss': 0.5326, 'learning_rate': 1.7593841260556103e-05, 'epoch': 0.72}
 72%|  | 2351/3250 [6:58:56<2:49:10, 11.29s/it]                                                        72%|  | 2351/3250 [6:58:56<2:49:10, 11.29s/it] 72%|  | 2352/3250 [6:59:06<2:44:58, 11.02s/it]                                                        72%|  | 2352/3250 [6:59:06<2:44:58, 11.02s/it] 72%|  | 2353/3250 [6:59:17<2:41:59, 10.84s/it]                                                        72%|  | 2353/3250 [6:59:17<2:41:59, 10.84s/it] 72%|  | 2354/3250 [6:59:27<2:39:49, 10.70s/it]                                                        72%|  | 2354/3250 [6:59:27<2:39:49, 10.70s/it] 72%|  | 2355/3250 [6:59:38<2:38:12, 10.61s/it]                                                        72%|  | 2355/3250 [6:59:38<2:38:12, 10.61s/it] 72%|{'loss': 0.5181, 'learning_rate': 1.7557027077832e-05, 'epoch': 0.72}
{'loss': 0.5206, 'learning_rate': 1.7520243247177824e-05, 'epoch': 0.73}
{'loss': 0.529, 'learning_rate': 1.7483489803006776e-05, 'epoch': 0.73}
{'loss': 0.5483, 'learning_rate': 1.7446766779703576e-05, 'epoch': 0.73}
{'loss': 0.5372, 'learning_rate': 1.7410074211624518e-05, 'epoch': 0.73}
  | 2356/3250 [6:59:48<2:37:06, 10.54s/it]                                                        72%|  | 2356/3250 [6:59:48<2:37:06, 10.54s/it] 73%|  | 2357/3250 [6:59:58<2:36:29, 10.51s/it]                                                        73%|  | 2357/3250 [6:59:58<2:36:29, 10.51s/it] 73%|  | 2358/3250 [7:00:09<2:35:46, 10.48s/it]                                                        73%|  | 2358/3250 [7:00:09<2:35:46, 10.48s/it] 73%|  | 2359/3250 [7:00:19<2:35:16, 10.46s/it]                                                        73%|  | 2359/3250 [7:00:19<2:35:16, 10.46s/it] 73%|  | 2360/3250 [7:00:30<2:34:51, 10.44s/it]                                                        73%|  | 2360/3250 [7:00:30<2:34:51, 10.44s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.784135103225708, 'eval_runtime': 2.1123, 'eval_samples_per_second': 5.681, 'eval_steps_per_second': 1.42, 'epoch': 0.73}
                                                        73%|  | 2360/3250 [7:00:32<2:34:51, 10.44s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_10/tmp-checkpoint-2360
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2360
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2360
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2360
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2360
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2360/pytorch_model.bin
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2360/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2360/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2360/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.5401, 'learning_rate': 1.7373412133097372e-05, 'epoch': 0.73}
{'loss': 0.5213, 'learning_rate': 1.733678057842142e-05, 'epoch': 0.73}
{'loss': 0.5391, 'learning_rate': 1.730017958186735e-05, 'epoch': 0.73}
{'loss': 0.4852, 'learning_rate': 1.726360917767726e-05, 'epoch': 0.73}
{'loss': 0.5618, 'learning_rate': 1.722706940006466e-05, 'epoch': 0.73}
 73%|  | 2361/3250 [7:00:43<2:48:00, 11.34s/it]                                                        73%|  | 2361/3250 [7:00:43<2:48:00, 11.34s/it] 73%|  | 2362/3250 [7:00:53<2:43:41, 11.06s/it]                                                        73%|  | 2362/3250 [7:00:53<2:43:41, 11.06s/it] 73%|  | 2363/3250 [7:01:04<2:40:53, 10.88s/it]                                                        73%|  | 2363/3250 [7:01:04<2:40:53, 10.88s/it] 73%|  | 2364/3250 [7:01:14<2:38:38, 10.74s/it]                                                        73%|  | 2364/3250 [7:01:14<2:38:38, 10.74s/it] 73%|  | 2365/3250 [7:01:25<2:36:55, 10.64s/it]                                                        73%|  | 2365/3250 [7:01:25<2:36:55, 10.64s/it] 73%|{'loss': 0.5015, 'learning_rate': 1.7190560283214395e-05, 'epoch': 0.73}
{'loss': 0.5185, 'learning_rate': 1.7154081861282617e-05, 'epoch': 0.73}
{'loss': 0.4958, 'learning_rate': 1.7117634168396774e-05, 'epoch': 0.73}
{'loss': 0.5077, 'learning_rate': 1.7081217238655563e-05, 'epoch': 0.73}
{'loss': 0.5299, 'learning_rate': 1.7044831106128866e-05, 'epoch': 0.73}
  | 2366/3250 [7:01:35<2:35:40, 10.57s/it]                                                        73%|  | 2366/3250 [7:01:35<2:35:40, 10.57s/it] 73%|  | 2367/3250 [7:01:45<2:34:43, 10.51s/it]                                                        73%|  | 2367/3250 [7:01:45<2:34:43, 10.51s/it] 73%|  | 2368/3250 [7:01:56<2:34:02, 10.48s/it]                                                        73%|  | 2368/3250 [7:01:56<2:34:02, 10.48s/it] 73%|  | 2369/3250 [7:02:06<2:33:30, 10.45s/it]                                                        73%|  | 2369/3250 [7:02:06<2:33:30, 10.45s/it] 73%|  | 2370/3250 [7:02:17<2:33:07, 10.44s/it]                                                        73%|  | 2370/3250 [7:02:17<2:33:07, 10.44s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.7882070541381836, 'eval_runtime': 2.1045, 'eval_samples_per_second': 5.702, 'eval_steps_per_second': 1.426, 'epoch': 0.73}
                                                        73%|  | 2370/3250 [7:02:19<2:33:07, 10.44s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_10/tmp-checkpoint-2370
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2370
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2370
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2370
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2370
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2370/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2370/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2370/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2370/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.5139, 'learning_rate': 1.7008475804857804e-05, 'epoch': 0.73}
{'loss': 0.5342, 'learning_rate': 1.697215136885462e-05, 'epoch': 0.73}
{'loss': 1.0144, 'learning_rate': 1.69358578321027e-05, 'epoch': 0.73}
{'loss': 0.5131, 'learning_rate': 1.689959522855652e-05, 'epoch': 0.73}
{'loss': 0.5468, 'learning_rate': 1.6863363592141618e-05, 'epoch': 0.73}
 73%|  | 2371/3250 [7:02:30<2:44:04, 11.20s/it]                                                        73%|  | 2371/3250 [7:02:30<2:44:04, 11.20s/it] 73%|  | 2372/3250 [7:02:40<2:40:25, 10.96s/it]                                                        73%|  | 2372/3250 [7:02:40<2:40:25, 10.96s/it] 73%|  | 2373/3250 [7:02:50<2:37:42, 10.79s/it]                                                        73%|  | 2373/3250 [7:02:50<2:37:42, 10.79s/it] 73%|  | 2374/3250 [7:03:01<2:35:55, 10.68s/it]                                                        73%|  | 2374/3250 [7:03:01<2:35:55, 10.68s/it] 73%|  | 2375/3250 [7:03:11<2:34:29, 10.59s/it]                                                        73%|  | 2375/3250 [7:03:11<2:34:29, 10.59s/it] 73%|{'loss': 0.5152, 'learning_rate': 1.6827162956754522e-05, 'epoch': 0.73}
{'loss': 0.5338, 'learning_rate': 1.6790993356262803e-05, 'epoch': 0.73}
{'loss': 0.5009, 'learning_rate': 1.675485482450499e-05, 'epoch': 0.73}
{'loss': 0.5704, 'learning_rate': 1.6718747395290552e-05, 'epoch': 0.73}
{'loss': 0.5492, 'learning_rate': 1.6682671102399805e-05, 'epoch': 0.73}
  | 2376/3250 [7:03:22<2:33:33, 10.54s/it]                                                        73%|  | 2376/3250 [7:03:22<2:33:33, 10.54s/it] 73%|  | 2377/3250 [7:03:32<2:32:43, 10.50s/it]                                                        73%|  | 2377/3250 [7:03:32<2:32:43, 10.50s/it] 73%|  | 2378/3250 [7:03:43<2:32:55, 10.52s/it]                                                        73%|  | 2378/3250 [7:03:43<2:32:55, 10.52s/it] 73%|  | 2379/3250 [7:03:53<2:32:02, 10.47s/it]                                                        73%|  | 2379/3250 [7:03:53<2:32:02, 10.47s/it] 73%|  | 2380/3250 [7:04:03<2:31:32, 10.45s/it]                                                        73%|  | 2380/3250 [7:04:03<2:31:32, 10.45s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.7845125198364258, 'eval_runtime': 2.3455, 'eval_samples_per_second': 5.116, 'eval_steps_per_second': 1.279, 'epoch': 0.73}
                                                        73%|  | 2380/3250 [7:04:06<2:31:32, 10.45s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_10/tmp-checkpoint-2380
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2380
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2380
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2380
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2380
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2380/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2380/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2380/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2380/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.5155, 'learning_rate': 1.6646625979584024e-05, 'epoch': 0.73}
{'loss': 0.5127, 'learning_rate': 1.6610612060565234e-05, 'epoch': 0.73}
{'loss': 0.5182, 'learning_rate': 1.6574629379036322e-05, 'epoch': 0.73}
{'loss': 0.5135, 'learning_rate': 1.6538677968660948e-05, 'epoch': 0.73}
{'loss': 0.5327, 'learning_rate': 1.6502757863073498e-05, 'epoch': 0.73}
 73%|  | 2381/3250 [7:04:17<2:43:14, 11.27s/it]                                                        73%|  | 2381/3250 [7:04:17<2:43:14, 11.27s/it] 73%|  | 2382/3250 [7:04:27<2:39:03, 11.00s/it]                                                        73%|  | 2382/3250 [7:04:27<2:39:03, 11.00s/it] 73%|  | 2383/3250 [7:04:37<2:36:05, 10.80s/it]                                                        73%|  | 2383/3250 [7:04:37<2:36:05, 10.80s/it] 73%|  | 2384/3250 [7:04:48<2:34:02, 10.67s/it]                                                        73%|  | 2384/3250 [7:04:48<2:34:02, 10.67s/it] 73%|  | 2385/3250 [7:04:58<2:32:34, 10.58s/it]                                                        73%|  | 2385/3250 [7:04:58<2:32:34, 10.58s/it] 73%|{'loss': 0.5152, 'learning_rate': 1.646686909587908e-05, 'epoch': 0.73}
{'loss': 0.5059, 'learning_rate': 1.6431011700653493e-05, 'epoch': 0.73}
{'loss': 0.5249, 'learning_rate': 1.639518571094315e-05, 'epoch': 0.73}
{'loss': 0.5423, 'learning_rate': 1.6359391160265125e-05, 'epoch': 0.74}
{'loss': 0.5327, 'learning_rate': 1.632362808210705e-05, 'epoch': 0.74}
  | 2386/3250 [7:05:08<2:31:27, 10.52s/it]                                                        73%|  | 2386/3250 [7:05:08<2:31:27, 10.52s/it] 73%|  | 2387/3250 [7:05:19<2:30:37, 10.47s/it]                                                        73%|  | 2387/3250 [7:05:19<2:30:37, 10.47s/it] 73%|  | 2388/3250 [7:05:29<2:29:58, 10.44s/it]                                                        73%|  | 2388/3250 [7:05:29<2:29:58, 10.44s/it] 74%|  | 2389/3250 [7:05:40<2:29:29, 10.42s/it]                                                        74%|  | 2389/3250 [7:05:40<2:29:29, 10.42s/it] 74%|  | 2390/3250 [7:05:50<2:29:01, 10.40s/it]                                                        74%|  | 2390/3250 [7:05:50<2:29:01, 10.40s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.7875146269798279, 'eval_runtime': 2.1034, 'eval_samples_per_second': 5.705, 'eval_steps_per_second': 1.426, 'epoch': 0.74}
                                                        74%|  | 2390/3250 [7:05:52<2:29:01, 10.40s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_10/tmp-checkpoint-2390
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2390
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2390
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2390
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2390
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2390/pytorch_model.bin
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2390/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2390/pytorch_model.binthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2390/pytorch_model.bin

/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.5297, 'learning_rate': 1.6287896509927135e-05, 'epoch': 0.74}
{'loss': 0.5274, 'learning_rate': 1.6252196477154095e-05, 'epoch': 0.74}
{'loss': 0.5305, 'learning_rate': 1.6216528017187176e-05, 'epoch': 0.74}
{'loss': 0.5043, 'learning_rate': 1.618089116339601e-05, 'epoch': 0.74}
{'loss': 0.5609, 'learning_rate': 1.6145285949120738e-05, 'epoch': 0.74}
 74%|  | 2391/3250 [7:06:03<2:40:02, 11.18s/it]                                                        74%|  | 2391/3250 [7:06:03<2:40:02, 11.18s/it] 74%|  | 2392/3250 [7:06:13<2:36:19, 10.93s/it]                                                        74%|  | 2392/3250 [7:06:13<2:36:19, 10.93s/it] 74%|  | 2393/3250 [7:06:24<2:33:39, 10.76s/it]                                                        74%|  | 2393/3250 [7:06:24<2:33:39, 10.76s/it] 74%|  | 2394/3250 [7:06:34<2:33:30, 10.76s/it]                                                        74%|  | 2394/3250 [7:06:34<2:33:30, 10.76s/it] 74%|  | 2395/3250 [7:06:45<2:31:36, 10.64s/it]                                                        74%|  | 2395/3250 [7:06:45<2:31:36, 10.64s/it] 74%|{'loss': 0.5327, 'learning_rate': 1.6109712407671867e-05, 'epoch': 0.74}
{'loss': 0.5078, 'learning_rate': 1.6074170572330256e-05, 'epoch': 0.74}
{'loss': 0.5028, 'learning_rate': 1.6038660476347135e-05, 'epoch': 0.74}
{'loss': 0.4934, 'learning_rate': 1.600318215294402e-05, 'epoch': 0.74}
{'loss': 0.5411, 'learning_rate': 1.596773563531273e-05, 'epoch': 0.74}
  | 2396/3250 [7:06:55<2:30:15, 10.56s/it]                                                        74%|  | 2396/3250 [7:06:55<2:30:15, 10.56s/it] 74%|  | 2397/3250 [7:07:05<2:29:13, 10.50s/it]                                                        74%|  | 2397/3250 [7:07:05<2:29:13, 10.50s/it] 74%|  | 2398/3250 [7:07:16<2:28:25, 10.45s/it]                                                        74%|  | 2398/3250 [7:07:16<2:28:25, 10.45s/it] 74%|  | 2399/3250 [7:07:26<2:27:50, 10.42s/it]                                                        74%|  | 2399/3250 [7:07:26<2:27:50, 10.42s/it] 74%|  | 2400/3250 [7:07:37<2:27:27, 10.41s/it]                                                        74%|  | 2400/3250 [7:07:37<2:27:27, 10.41s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.7843892574310303, 'eval_runtime': 2.1017, 'eval_samples_per_second': 5.71, 'eval_steps_per_second': 1.427, 'epoch': 0.74}
                                                        74%|  | 2400/3250 [7:07:39<2:27:27, 10.41s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_10/tmp-checkpoint-2400
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2400
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2400
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2400
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2400
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2400/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2400/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2400/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2400/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.5136, 'learning_rate': 1.5932320956615265e-05, 'epoch': 0.74}
{'loss': 0.537, 'learning_rate': 1.5896938149983908e-05, 'epoch': 0.74}
{'loss': 1.0263, 'learning_rate': 1.586158724852108e-05, 'epoch': 0.74}
{'loss': 0.5246, 'learning_rate': 1.582626828529938e-05, 'epoch': 0.74}
{'loss': 0.5141, 'learning_rate': 1.5790981293361516e-05, 'epoch': 0.74}
 74%|  | 2401/3250 [7:07:49<2:37:53, 11.16s/it]                                                        74%|  | 2401/3250 [7:07:49<2:37:53, 11.16s/it] 74%|  | 2402/3250 [7:08:00<2:34:20, 10.92s/it]                                                        74%|  | 2402/3250 [7:08:00<2:34:20, 10.92s/it] 74%|  | 2403/3250 [7:08:10<2:31:44, 10.75s/it]                                                        74%|  | 2403/3250 [7:08:10<2:31:44, 10.75s/it] 74%|  | 2404/3250 [7:08:20<2:29:55, 10.63s/it]                                                        74%|  | 2404/3250 [7:08:20<2:29:55, 10.63s/it] 74%|  | 2405/3250 [7:08:31<2:28:37, 10.55s/it]                                                        74%|  | 2405/3250 [7:08:31<2:28:37, 10.55s/it] 74%|{'loss': 0.5411, 'learning_rate': 1.5755726305720266e-05, 'epoch': 0.74}
{'loss': 0.5315, 'learning_rate': 1.5720503355358495e-05, 'epoch': 0.74}
{'loss': 0.4921, 'learning_rate': 1.5685312475229085e-05, 'epoch': 0.74}
{'loss': 0.4986, 'learning_rate': 1.5650153698254916e-05, 'epoch': 0.74}
{'loss': 0.582, 'learning_rate': 1.561502705732883e-05, 'epoch': 0.74}
  | 2406/3250 [7:08:41<2:27:37, 10.49s/it]                                                        74%|  | 2406/3250 [7:08:41<2:27:37, 10.49s/it] 74%|  | 2407/3250 [7:08:52<2:26:52, 10.45s/it]                                                        74%|  | 2407/3250 [7:08:52<2:26:52, 10.45s/it] 74%|  | 2408/3250 [7:09:02<2:26:21, 10.43s/it]                                                        74%|  | 2408/3250 [7:09:02<2:26:21, 10.43s/it] 74%|  | 2409/3250 [7:09:12<2:25:53, 10.41s/it]                                                        74%|  | 2409/3250 [7:09:12<2:25:53, 10.41s/it] 74%|  | 2410/3250 [7:09:23<2:26:26, 10.46s/it]                                                        74%|  | 2410/3250 [7:09:23<2:26:26, 10.46s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.7850056290626526, 'eval_runtime': 2.112, 'eval_samples_per_second': 5.682, 'eval_steps_per_second': 1.42, 'epoch': 0.74}
                                                        74%|  | 2410/3250 [7:09:25<2:26:26, 10.46s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_10/tmp-checkpoint-2410
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2410
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2410
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2410
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2410
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2410/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2410/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2410/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2410/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.5283, 'learning_rate': 1.557993258531362e-05, 'epoch': 0.74}
{'loss': 0.5186, 'learning_rate': 1.5544870315041943e-05, 'epoch': 0.74}
{'loss': 0.4856, 'learning_rate': 1.550984027931639e-05, 'epoch': 0.74}
{'loss': 0.5264, 'learning_rate': 1.547484251090932e-05, 'epoch': 0.74}
{'loss': 0.5291, 'learning_rate': 1.5439877042562973e-05, 'epoch': 0.74}
 74%|  | 2411/3250 [7:09:36<2:36:47, 11.21s/it]                                                        74%|  | 2411/3250 [7:09:36<2:36:47, 11.21s/it] 74%|  | 2412/3250 [7:09:46<2:33:02, 10.96s/it]                                                        74%|  | 2412/3250 [7:09:46<2:33:02, 10.96s/it] 74%|  | 2413/3250 [7:09:57<2:30:15, 10.77s/it]                                                        74%|  | 2413/3250 [7:09:57<2:30:15, 10.77s/it] 74%|  | 2414/3250 [7:10:07<2:28:18, 10.64s/it]                                                        74%|  | 2414/3250 [7:10:07<2:28:18, 10.64s/it] 74%|  | 2415/3250 [7:10:17<2:27:13, 10.58s/it]                                                        74%|  | 2415/3250 [7:10:17<2:27:13, 10.58s/it] 74%|{'loss': 0.5185, 'learning_rate': 1.5404943906989334e-05, 'epoch': 0.74}
{'loss': 0.5105, 'learning_rate': 1.5370043136870148e-05, 'epoch': 0.74}
{'loss': 0.5222, 'learning_rate': 1.5335174764856908e-05, 'epoch': 0.74}
{'loss': 0.5271, 'learning_rate': 1.5300338823570725e-05, 'epoch': 0.74}
{'loss': 0.5361, 'learning_rate': 1.526553534560244e-05, 'epoch': 0.74}
  | 2416/3250 [7:10:28<2:26:10, 10.52s/it]                                                        74%|  | 2416/3250 [7:10:28<2:26:10, 10.52s/it] 74%|  | 2417/3250 [7:10:38<2:25:20, 10.47s/it]                                                        74%|  | 2417/3250 [7:10:38<2:25:20, 10.47s/it] 74%|  | 2418/3250 [7:10:48<2:24:41, 10.43s/it]                                                        74%|  | 2418/3250 [7:10:48<2:24:41, 10.43s/it] 74%|  | 2419/3250 [7:10:59<2:24:07, 10.41s/it]                                                        74%|  | 2419/3250 [7:10:59<2:24:07, 10.41s/it] 74%|  | 2420/3250 [7:11:09<2:23:41, 10.39s/it]                                                        74%|  | 2420/3250 [7:11:09<2:23:41, 10.39s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.7855271100997925, 'eval_runtime': 2.6247, 'eval_samples_per_second': 4.572, 'eval_steps_per_second': 1.143, 'epoch': 0.74}
                                                        74%|  | 2420/3250 [7:11:12<2:23:41, 10.39s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_10/tmp-checkpoint-2420
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2420
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2420
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2420
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2420
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2420/pytorch_model.bin
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2420/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2420/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2420/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.523, 'learning_rate': 1.5230764363512485e-05, 'epoch': 0.74}
{'loss': 0.5105, 'learning_rate': 1.5196025909830913e-05, 'epoch': 0.75}
{'loss': 0.5427, 'learning_rate': 1.5161320017057346e-05, 'epoch': 0.75}
{'loss': 0.5174, 'learning_rate': 1.5126646717660897e-05, 'epoch': 0.75}
{'loss': 0.5315, 'learning_rate': 1.5092006044080242e-05, 'epoch': 0.75}
 74%|  | 2421/3250 [7:11:23<2:36:29, 11.33s/it]                                                        74%|  | 2421/3250 [7:11:23<2:36:29, 11.33s/it] 75%|  | 2422/3250 [7:11:33<2:32:17, 11.04s/it]                                                        75%|  | 2422/3250 [7:11:33<2:32:17, 11.04s/it] 75%|  | 2423/3250 [7:11:43<2:29:18, 10.83s/it]                                                        75%|  | 2423/3250 [7:11:43<2:29:18, 10.83s/it] 75%|  | 2424/3250 [7:11:54<2:27:05, 10.68s/it]                                                        75%|  | 2424/3250 [7:11:54<2:27:05, 10.68s/it] 75%|  | 2425/3250 [7:12:04<2:25:32, 10.59s/it]                                                        75%|  | 2425/3250 [7:12:04<2:25:32, 10.59s/it] 75%|{'loss': 0.5255, 'learning_rate': 1.5057398028723513e-05, 'epoch': 0.75}
{'loss': 0.5084, 'learning_rate': 1.5022822703968281e-05, 'epoch': 0.75}
{'loss': 0.4977, 'learning_rate': 1.498828010216155e-05, 'epoch': 0.75}
{'loss': 0.5025, 'learning_rate': 1.4953770255619714e-05, 'epoch': 0.75}
{'loss': 0.5213, 'learning_rate': 1.4919293196628492e-05, 'epoch': 0.75}
  | 2426/3250 [7:12:14<2:24:23, 10.51s/it]                                                        75%|  | 2426/3250 [7:12:14<2:24:23, 10.51s/it] 75%|  | 2427/3250 [7:12:25<2:25:22, 10.60s/it]                                                        75%|  | 2427/3250 [7:12:25<2:25:22, 10.60s/it] 75%|  | 2428/3250 [7:12:36<2:24:12, 10.53s/it]                                                        75%|  | 2428/3250 [7:12:36<2:24:12, 10.53s/it] 75%|  | 2429/3250 [7:12:46<2:23:17, 10.47s/it]                                                        75%|  | 2429/3250 [7:12:46<2:23:17, 10.47s/it] 75%|  | 2430/3250 [7:12:56<2:22:35, 10.43s/it]                                                        75%|  | 2430/3250 [7:12:56<2:22:35, 10.43s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.7862793207168579, 'eval_runtime': 2.1047, 'eval_samples_per_second': 5.702, 'eval_steps_per_second': 1.425, 'epoch': 0.75}
                                                        75%|  | 2430/3250 [7:12:58<2:22:35, 10.43s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_10/tmp-checkpoint-2430
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2430
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2430
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2430
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2430
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2430/pytorch_model.binthe pytorch model path is
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2430/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2430/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2430/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.5217, 'learning_rate': 1.4884848957442931e-05, 'epoch': 0.75}
{'loss': 0.5356, 'learning_rate': 1.4850437570287406e-05, 'epoch': 0.75}
{'loss': 1.0385, 'learning_rate': 1.4816059067355536e-05, 'epoch': 0.75}
{'loss': 0.4943, 'learning_rate': 1.4781713480810184e-05, 'epoch': 0.75}
{'loss': 0.4962, 'learning_rate': 1.4747400842783404e-05, 'epoch': 0.75}
 75%|  | 2431/3250 [7:13:09<2:32:41, 11.19s/it]                                                        75%|  | 2431/3250 [7:13:09<2:32:41, 11.19s/it] 75%|  | 2432/3250 [7:13:20<2:29:06, 10.94s/it]                                                        75%|  | 2432/3250 [7:13:20<2:29:06, 10.94s/it] 75%|  | 2433/3250 [7:13:30<2:26:30, 10.76s/it]                                                        75%|  | 2433/3250 [7:13:30<2:26:30, 10.76s/it] 75%|  | 2434/3250 [7:13:40<2:24:41, 10.64s/it]                                                        75%|  | 2434/3250 [7:13:40<2:24:41, 10.64s/it] 75%|  | 2435/3250 [7:13:51<2:23:19, 10.55s/it]                                                        75%|  | 2435/3250 [7:13:51<2:23:19, 10.55s/it] 75%|{'loss': 0.5438, 'learning_rate': 1.4713121185376461e-05, 'epoch': 0.75}
{'loss': 0.5475, 'learning_rate': 1.4678874540659694e-05, 'epoch': 0.75}
{'loss': 0.5178, 'learning_rate': 1.4644660940672627e-05, 'epoch': 0.75}
{'loss': 0.5113, 'learning_rate': 1.4610480417423839e-05, 'epoch': 0.75}
{'loss': 0.5555, 'learning_rate': 1.4576333002890969e-05, 'epoch': 0.75}
  | 2436/3250 [7:14:01<2:22:20, 10.49s/it]                                                        75%|  | 2436/3250 [7:14:01<2:22:20, 10.49s/it] 75%|  | 2437/3250 [7:14:11<2:21:38, 10.45s/it]                                                        75%|  | 2437/3250 [7:14:11<2:21:38, 10.45s/it] 75%|  | 2438/3250 [7:14:22<2:21:06, 10.43s/it]                                                        75%|  | 2438/3250 [7:14:22<2:21:06, 10.43s/it] 75%|  | 2439/3250 [7:14:32<2:20:38, 10.41s/it]                                                        75%|  | 2439/3250 [7:14:32<2:20:38, 10.41s/it] 75%|  | 2440/3250 [7:14:42<2:20:19, 10.39s/it]                                                        75%|  | 2440/3250 [7:14:42<2:20:19, 10.39s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.7850590944290161, 'eval_runtime': 2.1036, 'eval_samples_per_second': 5.705, 'eval_steps_per_second': 1.426, 'epoch': 0.75}
                                                        75%|  | 2440/3250 [7:14:44<2:20:19, 10.39s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_10/tmp-checkpoint-2440
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2440
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2440
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2440
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2440
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2440/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2440/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2440/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2440/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.5501, 'learning_rate': 1.454221872902069e-05, 'epoch': 0.75}
{'loss': 0.5103, 'learning_rate': 1.450813762772863e-05, 'epoch': 0.75}
{'loss': 0.4925, 'learning_rate': 1.4474089730899437e-05, 'epoch': 0.75}
{'loss': 0.5383, 'learning_rate': 1.444007507038666e-05, 'epoch': 0.75}
{'loss': 0.5269, 'learning_rate': 1.4406093678012766e-05, 'epoch': 0.75}
 75%|  | 2441/3250 [7:14:55<2:30:56, 11.19s/it]                                                        75%|  | 2441/3250 [7:14:55<2:30:56, 11.19s/it] 75%|  | 2442/3250 [7:15:06<2:27:22, 10.94s/it]                                                        75%|  | 2442/3250 [7:15:06<2:27:22, 10.94s/it] 75%|  | 2443/3250 [7:15:17<2:27:20, 10.95s/it]                                                        75%|  | 2443/3250 [7:15:17<2:27:20, 10.95s/it] 75%|  | 2444/3250 [7:15:27<2:24:38, 10.77s/it]                                                        75%|  | 2444/3250 [7:15:27<2:24:38, 10.77s/it] 75%|  | 2445/3250 [7:15:37<2:22:43, 10.64s/it]                                                        75%|  | 2445/3250 [7:15:37<2:22:43, 10.64s/it] 75%|{'loss': 0.5302, 'learning_rate': 1.4372145585569097e-05, 'epoch': 0.75}
{'loss': 0.5096, 'learning_rate': 1.4338230824815852e-05, 'epoch': 0.75}
{'loss': 0.5117, 'learning_rate': 1.4304349427482028e-05, 'epoch': 0.75}
{'loss': 0.5124, 'learning_rate': 1.4270501425265386e-05, 'epoch': 0.75}
{'loss': 0.5371, 'learning_rate': 1.4236686849832498e-05, 'epoch': 0.75}
  | 2446/3250 [7:15:48<2:21:22, 10.55s/it]                                                        75%|  | 2446/3250 [7:15:48<2:21:22, 10.55s/it] 75%|  | 2447/3250 [7:15:58<2:20:22, 10.49s/it]                                                        75%|  | 2447/3250 [7:15:58<2:20:22, 10.49s/it] 75%|  | 2448/3250 [7:16:09<2:20:12, 10.49s/it]                                                        75%|  | 2448/3250 [7:16:09<2:20:12, 10.49s/it] 75%|  | 2449/3250 [7:16:19<2:19:25, 10.44s/it]                                                        75%|  | 2449/3250 [7:16:19<2:19:25, 10.44s/it] 75%|  | 2450/3250 [7:16:29<2:18:52, 10.42s/it]                                                        75%|  | 2450/3250 [7:16:29<2:18:52, 10.42s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.7856765389442444, 'eval_runtime': 2.1019, 'eval_samples_per_second': 5.709, 'eval_steps_per_second': 1.427, 'epoch': 0.75}
                                                        75%|  | 2450/3250 [7:16:31<2:18:52, 10.42s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_10/tmp-checkpoint-2450
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2450
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2450
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2450
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2450
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2450/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2450/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2450/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2450/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.5377, 'learning_rate': 1.4202905732818633e-05, 'epoch': 0.75}
{'loss': 0.5371, 'learning_rate': 1.4169158105827768e-05, 'epoch': 0.75}
{'loss': 0.5179, 'learning_rate': 1.4135444000432541e-05, 'epoch': 0.75}
{'loss': 0.5333, 'learning_rate': 1.410176344817425e-05, 'epoch': 0.76}
{'loss': 0.485, 'learning_rate': 1.4068116480562754e-05, 'epoch': 0.76}
 75%|  | 2451/3250 [7:16:42<2:29:07, 11.20s/it]                                                        75%|  | 2451/3250 [7:16:42<2:29:07, 11.20s/it] 75%|  | 2452/3250 [7:16:53<2:25:36, 10.95s/it]                                                        75%|  | 2452/3250 [7:16:53<2:25:36, 10.95s/it] 75%|  | 2453/3250 [7:17:03<2:23:01, 10.77s/it]                                                        75%|  | 2453/3250 [7:17:03<2:23:01, 10.77s/it] 76%|  | 2454/3250 [7:17:13<2:21:04, 10.63s/it]                                                        76%|  | 2454/3250 [7:17:13<2:21:04, 10.63s/it] 76%|  | 2455/3250 [7:17:24<2:19:38, 10.54s/it]                                                        76%|  | 2455/3250 [7:17:24<2:19:38, 10.54s/it] 76%|{'loss': 0.5518, 'learning_rate': 1.4034503129076531e-05, 'epoch': 0.76}
{'loss': 0.4993, 'learning_rate': 1.4000923425162604e-05, 'epoch': 0.76}
{'loss': 0.5091, 'learning_rate': 1.3967377400236515e-05, 'epoch': 0.76}
{'loss': 0.4903, 'learning_rate': 1.3933865085682312e-05, 'epoch': 0.76}
{'loss': 0.5046, 'learning_rate': 1.3900386512852454e-05, 'epoch': 0.76}
  | 2456/3250 [7:17:34<2:18:38, 10.48s/it]                                                        76%|  | 2456/3250 [7:17:34<2:18:38, 10.48s/it] 76%|  | 2457/3250 [7:17:44<2:17:57, 10.44s/it]                                                        76%|  | 2457/3250 [7:17:44<2:17:57, 10.44s/it] 76%|  | 2458/3250 [7:17:55<2:17:27, 10.41s/it]                                                        76%|  | 2458/3250 [7:17:55<2:17:27, 10.41s/it] 76%|  | 2459/3250 [7:18:05<2:18:04, 10.47s/it]                                                        76%|  | 2459/3250 [7:18:05<2:18:04, 10.47s/it] 76%|  | 2460/3250 [7:18:16<2:17:24, 10.44s/it]                                                        76%|  | 2460/3250 [7:18:16<2:17:24, 10.44s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.7854621410369873, 'eval_runtime': 2.104, 'eval_samples_per_second': 5.703, 'eval_steps_per_second': 1.426, 'epoch': 0.76}
                                                        76%|  | 2460/3250 [7:18:18<2:17:24, 10.44s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_10/tmp-checkpoint-2460
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2460
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2460
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2460
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2460
the pytorch model path isthe pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2460/pytorch_model.bin /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2460/pytorch_model.bin

 /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2460/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2460/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.5285, 'learning_rate': 1.3866941713067888e-05, 'epoch': 0.76}
{'loss': 0.5282, 'learning_rate': 1.3833530717617937e-05, 'epoch': 0.76}
{'loss': 0.5246, 'learning_rate': 1.3800153557760315e-05, 'epoch': 0.76}
{'loss': 1.0205, 'learning_rate': 1.376681026472108e-05, 'epoch': 0.76}
{'loss': 0.5144, 'learning_rate': 1.3733500869694572e-05, 'epoch': 0.76}
 76%|  | 2461/3250 [7:18:29<2:27:25, 11.21s/it]                                                        76%|  | 2461/3250 [7:18:29<2:27:25, 11.21s/it] 76%|  | 2462/3250 [7:18:39<2:23:51, 10.95s/it]                                                        76%|  | 2462/3250 [7:18:39<2:23:51, 10.95s/it] 76%|  | 2463/3250 [7:18:49<2:21:19, 10.77s/it]                                                        76%|  | 2463/3250 [7:18:49<2:21:19, 10.77s/it] 76%|  | 2464/3250 [7:19:00<2:19:17, 10.63s/it]                                                        76%|  | 2464/3250 [7:19:00<2:19:17, 10.63s/it] 76%|  | 2465/3250 [7:19:10<2:17:55, 10.54s/it]                                                        76%|  | 2465/3250 [7:19:10<2:17:55, 10.54s/it] 76%|{'loss': 0.5437, 'learning_rate': 1.3700225403843469e-05, 'epoch': 0.76}
{'loss': 0.5277, 'learning_rate': 1.3666983898298657e-05, 'epoch': 0.76}
{'loss': 0.5266, 'learning_rate': 1.3633776384159285e-05, 'epoch': 0.76}
{'loss': 0.4949, 'learning_rate': 1.3600602892492693e-05, 'epoch': 0.76}
{'loss': 0.5721, 'learning_rate': 1.3567463454334389e-05, 'epoch': 0.76}
  | 2466/3250 [7:19:20<2:16:51, 10.47s/it]                                                        76%|  | 2466/3250 [7:19:20<2:16:51, 10.47s/it] 76%|  | 2467/3250 [7:19:31<2:16:05, 10.43s/it]                                                        76%|  | 2467/3250 [7:19:31<2:16:05, 10.43s/it] 76%|  | 2468/3250 [7:19:41<2:15:33, 10.40s/it]                                                        76%|  | 2468/3250 [7:19:41<2:15:33, 10.40s/it] 76%|  | 2469/3250 [7:19:51<2:15:09, 10.38s/it]                                                        76%|  | 2469/3250 [7:19:51<2:15:09, 10.38s/it] 76%|  | 2470/3250 [7:20:02<2:14:47, 10.37s/it]                                                        76%|  | 2470/3250 [7:20:02<2:14:47, 10.37s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.7851859331130981, 'eval_runtime': 2.0965, 'eval_samples_per_second': 5.724, 'eval_steps_per_second': 1.431, 'epoch': 0.76}
                                                        76%|  | 2470/3250 [7:20:04<2:14:47, 10.37s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_10/tmp-checkpoint-2470
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2470
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2470
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2470
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2470
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2470/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2470/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2470/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2470/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.5485, 'learning_rate': 1.3534358100688016e-05, 'epoch': 0.76}
{'loss': 0.5096, 'learning_rate': 1.3501286862525358e-05, 'epoch': 0.76}
{'loss': 0.5136, 'learning_rate': 1.3468249770786223e-05, 'epoch': 0.76}
{'loss': 0.514, 'learning_rate': 1.3435246856378525e-05, 'epoch': 0.76}
{'loss': 0.5148, 'learning_rate': 1.34022781501782e-05, 'epoch': 0.76}
 76%|  | 2471/3250 [7:20:15<2:24:41, 11.14s/it]                                                        76%|  | 2471/3250 [7:20:15<2:24:41, 11.14s/it] 76%|  | 2472/3250 [7:20:25<2:21:19, 10.90s/it]                                                        76%|  | 2472/3250 [7:20:25<2:21:19, 10.90s/it] 76%|  | 2473/3250 [7:20:35<2:18:55, 10.73s/it]                                                        76%|  | 2473/3250 [7:20:35<2:18:55, 10.73s/it] 76%|  | 2474/3250 [7:20:46<2:17:09, 10.61s/it]                                                        76%|  | 2474/3250 [7:20:46<2:17:09, 10.61s/it] 76%|  | 2475/3250 [7:20:56<2:15:59, 10.53s/it]                                                        76%|  | 2475/3250 [7:20:56<2:15:59, 10.53s/it] 76%|{'loss': 0.5215, 'learning_rate': 1.3369343683029151e-05, 'epoch': 0.76}
{'loss': 0.5229, 'learning_rate': 1.3336443485743294e-05, 'epoch': 0.76}
{'loss': 0.5201, 'learning_rate': 1.3303577589100418e-05, 'epoch': 0.76}
{'loss': 0.5258, 'learning_rate': 1.327074602384828e-05, 'epoch': 0.76}
{'loss': 0.5549, 'learning_rate': 1.3237948820702495e-05, 'epoch': 0.76}
  | 2476/3250 [7:21:07<2:16:35, 10.59s/it]                                                        76%|  | 2476/3250 [7:21:07<2:16:35, 10.59s/it] 76%|  | 2477/3250 [7:21:17<2:15:26, 10.51s/it]                                                        76%|  | 2477/3250 [7:21:17<2:15:26, 10.51s/it] 76%|  | 2478/3250 [7:21:27<2:14:37, 10.46s/it]                                                        76%|  | 2478/3250 [7:21:27<2:14:37, 10.46s/it] 76%|  | 2479/3250 [7:21:38<2:13:57, 10.42s/it]                                                        76%|  | 2479/3250 [7:21:38<2:13:57, 10.42s/it] 76%|  | 2480/3250 [7:21:48<2:13:28, 10.40s/it]                                                        76%|  | 2480/3250 [7:21:48<2:13:28, 10.40s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.787562906742096, 'eval_runtime': 2.5147, 'eval_samples_per_second': 4.772, 'eval_steps_per_second': 1.193, 'epoch': 0.76}
                                                        76%|  | 2480/3250 [7:21:51<2:13:28, 10.40s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_10/tmp-checkpoint-2480
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2480
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2480
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2480
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2480
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2480/pytorch_model.bin
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2480/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2480/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2480/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.5156, 'learning_rate': 1.3205186010346548e-05, 'epoch': 0.76}
{'loss': 0.5406, 'learning_rate': 1.3172457623431706e-05, 'epoch': 0.76}
{'loss': 0.5384, 'learning_rate': 1.3139763690577073e-05, 'epoch': 0.76}
{'loss': 0.5298, 'learning_rate': 1.3107104242369517e-05, 'epoch': 0.76}
{'loss': 0.5003, 'learning_rate': 1.3074479309363608e-05, 'epoch': 0.76}
 76%|  | 2481/3250 [7:22:02<2:25:37, 11.36s/it]                                                        76%|  | 2481/3250 [7:22:02<2:25:37, 11.36s/it] 76%|  | 2482/3250 [7:22:12<2:21:31, 11.06s/it]                                                        76%|  | 2482/3250 [7:22:12<2:21:31, 11.06s/it] 76%|  | 2483/3250 [7:22:22<2:18:40, 10.85s/it]                                                        76%|  | 2483/3250 [7:22:22<2:18:40, 10.85s/it] 76%|  | 2484/3250 [7:22:33<2:16:37, 10.70s/it]                                                        76%|  | 2484/3250 [7:22:33<2:16:37, 10.70s/it] 76%|  | 2485/3250 [7:22:43<2:15:07, 10.60s/it]                                                        76%|  | 2485/3250 [7:22:43<2:15:07, 10.60s/it] 76%|{'loss': 0.5601, 'learning_rate': 1.3041888922081657e-05, 'epoch': 0.76}
{'loss': 0.5273, 'learning_rate': 1.300933311101365e-05, 'epoch': 0.77}
{'loss': 0.5088, 'learning_rate': 1.2976811906617225e-05, 'epoch': 0.77}
{'loss': 0.5048, 'learning_rate': 1.2944325339317637e-05, 'epoch': 0.77}
{'loss': 0.4902, 'learning_rate': 1.2911873439507765e-05, 'epoch': 0.77}
  | 2486/3250 [7:22:53<2:13:59, 10.52s/it]                                                        76%|  | 2486/3250 [7:22:53<2:13:59, 10.52s/it] 77%|  | 2487/3250 [7:23:04<2:13:09, 10.47s/it]                                                        77%|  | 2487/3250 [7:23:04<2:13:09, 10.47s/it] 77%|  | 2488/3250 [7:23:14<2:12:32, 10.44s/it]                                                        77%|  | 2488/3250 [7:23:14<2:12:32, 10.44s/it] 77%|  | 2489/3250 [7:23:24<2:12:03, 10.41s/it]                                                        77%|  | 2489/3250 [7:23:24<2:12:03, 10.41s/it] 77%|  | 2490/3250 [7:23:35<2:11:40, 10.40s/it]                                                        77%|  | 2490/3250 [7:23:35<2:11:40, 10.40s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.7840279936790466, 'eval_runtime': 2.1029, 'eval_samples_per_second': 5.706, 'eval_steps_per_second': 1.427, 'epoch': 0.77}
                                                        77%|  | 2490/3250 [7:23:37<2:11:40, 10.40s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_10/tmp-checkpoint-2490
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2490
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2490
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2490
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2490
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2490/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2490/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2490/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2490/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.5406, 'learning_rate': 1.2879456237547988e-05, 'epoch': 0.77}
{'loss': 0.5094, 'learning_rate': 1.2847073763766287e-05, 'epoch': 0.77}
{'loss': 0.5274, 'learning_rate': 1.2814726048458137e-05, 'epoch': 0.77}
{'loss': 1.0216, 'learning_rate': 1.2782413121886483e-05, 'epoch': 0.77}
{'loss': 0.5112, 'learning_rate': 1.2750135014281729e-05, 'epoch': 0.77}
 77%|  | 2491/3250 [7:23:48<2:21:14, 11.17s/it]                                                        77%|  | 2491/3250 [7:23:48<2:21:14, 11.17s/it] 77%|  | 2492/3250 [7:23:58<2:19:04, 11.01s/it]                                                        77%|  | 2492/3250 [7:23:58<2:19:04, 11.01s/it] 77%|  | 2493/3250 [7:24:09<2:16:29, 10.82s/it]                                                        77%|  | 2493/3250 [7:24:09<2:16:29, 10.82s/it] 77%|  | 2494/3250 [7:24:19<2:14:31, 10.68s/it]                                                        77%|  | 2494/3250 [7:24:19<2:14:31, 10.68s/it] 77%|  | 2495/3250 [7:24:30<2:13:09, 10.58s/it]                                                        77%|  | 2495/3250 [7:24:30<2:13:09, 10.58s/it] 77%|{'loss': 0.5232, 'learning_rate': 1.2717891755841722e-05, 'epoch': 0.77}
{'loss': 0.5423, 'learning_rate': 1.268568337673166e-05, 'epoch': 0.77}
{'loss': 0.5275, 'learning_rate': 1.2653509907084171e-05, 'epoch': 0.77}
{'loss': 0.5114, 'learning_rate': 1.2621371376999152e-05, 'epoch': 0.77}
{'loss': 0.5001, 'learning_rate': 1.2589267816543876e-05, 'epoch': 0.77}
  | 2496/3250 [7:24:40<2:12:04, 10.51s/it]                                                        77%|  | 2496/3250 [7:24:40<2:12:04, 10.51s/it] 77%|  | 2497/3250 [7:24:50<2:11:21, 10.47s/it]                                                        77%|  | 2497/3250 [7:24:50<2:11:21, 10.47s/it] 77%|  | 2498/3250 [7:25:01<2:10:45, 10.43s/it]                                                        77%|  | 2498/3250 [7:25:01<2:10:45, 10.43s/it] 77%|  | 2499/3250 [7:25:11<2:10:17, 10.41s/it]                                                        77%|  | 2499/3250 [7:25:11<2:10:17, 10.41s/it] 77%|  | 2500/3250 [7:25:21<2:09:56, 10.40s/it]                                                        77%|  | 2500/3250 [7:25:21<2:09:56, 10.40s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.7863829731941223, 'eval_runtime': 2.1155, 'eval_samples_per_second': 5.672, 'eval_steps_per_second': 1.418, 'epoch': 0.77}
                                                        77%|  | 2500/3250 [7:25:23<2:09:56, 10.40s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_10/tmp-checkpoint-2500
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2500
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2500
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2500
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2500
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2500/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2500/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2500/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2500/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.5831, 'learning_rate': 1.2557199255752867e-05, 'epoch': 0.77}
{'loss': 0.5287, 'learning_rate': 1.2525165724627936e-05, 'epoch': 0.77}
{'loss': 0.5108, 'learning_rate': 1.249316725313806e-05, 'epoch': 0.77}
{'loss': 0.4776, 'learning_rate': 1.2461203871219473e-05, 'epoch': 0.77}
{'loss': 0.5405, 'learning_rate': 1.242927560877557e-05, 'epoch': 0.77}
 77%|  | 2501/3250 [7:25:34<2:19:46, 11.20s/it]                                                        77%|  | 2501/3250 [7:25:34<2:19:46, 11.20s/it] 77%|  | 2502/3250 [7:25:45<2:16:28, 10.95s/it]                                                        77%|  | 2502/3250 [7:25:45<2:16:28, 10.95s/it] 77%|  | 2503/3250 [7:25:55<2:14:07, 10.77s/it]                                                        77%|  | 2503/3250 [7:25:55<2:14:07, 10.77s/it] 77%|  | 2504/3250 [7:26:05<2:12:25, 10.65s/it]                                                        77%|  | 2504/3250 [7:26:05<2:12:25, 10.65s/it] 77%|  | 2505/3250 [7:26:16<2:11:10, 10.56s/it]                                                        77%|  | 2505/3250 [7:26:16<2:11:10, 10.56s/it] 77%|{'loss': 0.5311, 'learning_rate': 1.2397382495676874e-05, 'epoch': 0.77}
{'loss': 0.5114, 'learning_rate': 1.2365524561761039e-05, 'epoch': 0.77}
{'loss': 0.5075, 'learning_rate': 1.2333701836832812e-05, 'epoch': 0.77}
{'loss': 0.514, 'learning_rate': 1.2301914350663957e-05, 'epoch': 0.77}
{'loss': 0.5235, 'learning_rate': 1.2270162132993323e-05, 'epoch': 0.77}
  | 2506/3250 [7:26:26<2:10:09, 10.50s/it]                                                        77%|  | 2506/3250 [7:26:26<2:10:09, 10.50s/it] 77%|  | 2507/3250 [7:26:37<2:09:29, 10.46s/it]                                                        77%|  | 2507/3250 [7:26:37<2:09:29, 10.46s/it] 77%|  | 2508/3250 [7:26:47<2:08:58, 10.43s/it]                                                        77%|  | 2508/3250 [7:26:47<2:08:58, 10.43s/it] 77%|  | 2509/3250 [7:26:58<2:10:49, 10.59s/it]                                                        77%|  | 2509/3250 [7:26:58<2:10:49, 10.59s/it] 77%|  | 2510/3250 [7:27:08<2:09:43, 10.52s/it]                                                        77%|  | 2510/3250 [7:27:08<2:09:43, 10.52s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.7868553400039673, 'eval_runtime': 2.1155, 'eval_samples_per_second': 5.672, 'eval_steps_per_second': 1.418, 'epoch': 0.77}
                                                        77%|  | 2510/3250 [7:27:10<2:09:43, 10.52s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_10/tmp-checkpoint-2510
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2510
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2510
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2510
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2510
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2510/pytorch_model.bin
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2510/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2510/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2510/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.5397, 'learning_rate': 1.223844521352674e-05, 'epoch': 0.77}
{'loss': 0.5219, 'learning_rate': 1.2206763621937023e-05, 'epoch': 0.77}
{'loss': 0.5111, 'learning_rate': 1.2175117387863916e-05, 'epoch': 0.77}
{'loss': 0.5495, 'learning_rate': 1.2143506540914128e-05, 'epoch': 0.77}
{'loss': 0.5149, 'learning_rate': 1.2111931110661212e-05, 'epoch': 0.77}
 77%|  | 2511/3250 [7:27:21<2:18:57, 11.28s/it]                                                        77%|  | 2511/3250 [7:27:21<2:18:57, 11.28s/it] 77%|  | 2512/3250 [7:27:32<2:15:21, 11.00s/it]                                                        77%|  | 2512/3250 [7:27:32<2:15:21, 11.00s/it] 77%|  | 2513/3250 [7:27:42<2:12:47, 10.81s/it]                                                        77%|  | 2513/3250 [7:27:42<2:12:47, 10.81s/it] 77%|  | 2514/3250 [7:27:52<2:10:57, 10.68s/it]                                                        77%|  | 2514/3250 [7:27:52<2:10:57, 10.68s/it] 77%|  | 2515/3250 [7:28:03<2:09:36, 10.58s/it]                                                        77%|  | 2515/3250 [7:28:03<2:09:36, 10.58s/it] 77%|{'loss': 0.5367, 'learning_rate': 1.2080391126645596e-05, 'epoch': 0.77}
{'loss': 0.5265, 'learning_rate': 1.2048886618374566e-05, 'epoch': 0.77}
{'loss': 0.5078, 'learning_rate': 1.2017417615322219e-05, 'epoch': 0.77}
{'loss': 0.4959, 'learning_rate': 1.1985984146929413e-05, 'epoch': 0.78}
{'loss': 0.5004, 'learning_rate': 1.1954586242603783e-05, 'epoch': 0.78}
  | 2516/3250 [7:28:13<2:08:36, 10.51s/it]                                                        77%|  | 2516/3250 [7:28:13<2:08:36, 10.51s/it] 77%|  | 2517/3250 [7:28:23<2:07:53, 10.47s/it]                                                        77%|  | 2517/3250 [7:28:23<2:07:53, 10.47s/it] 77%|  | 2518/3250 [7:28:34<2:07:20, 10.44s/it]                                                        77%|  | 2518/3250 [7:28:34<2:07:20, 10.44s/it] 78%|  | 2519/3250 [7:28:44<2:06:50, 10.41s/it]                                                        78%|  | 2519/3250 [7:28:44<2:06:50, 10.41s/it] 78%|  | 2520/3250 [7:28:55<2:06:29, 10.40s/it]                                                        78%|  | 2520/3250 [7:28:55<2:06:29, 10.40s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.7848255038261414, 'eval_runtime': 2.1151, 'eval_samples_per_second': 5.673, 'eval_steps_per_second': 1.418, 'epoch': 0.78}
                                                        78%|  | 2520/3250 [7:28:57<2:06:29, 10.40s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_10/tmp-checkpoint-2520
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2520
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2520
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2520
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2520
the pytorch model path isthe pytorch model path is  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2520/pytorch_model.bin
/projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2520/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2520/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2520/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.516, 'learning_rate': 1.1923223931719695e-05, 'epoch': 0.78}
{'loss': 0.5179, 'learning_rate': 1.1891897243618182e-05, 'epoch': 0.78}
{'loss': 0.5321, 'learning_rate': 1.1860606207606978e-05, 'epoch': 0.78}
{'loss': 1.0365, 'learning_rate': 1.1829350852960464e-05, 'epoch': 0.78}
{'loss': 0.4914, 'learning_rate': 1.1798131208919627e-05, 'epoch': 0.78}
 78%|  | 2521/3250 [7:29:07<2:15:41, 11.17s/it]                                                        78%|  | 2521/3250 [7:29:07<2:15:41, 11.17s/it] 78%|  | 2522/3250 [7:29:18<2:12:31, 10.92s/it]                                                        78%|  | 2522/3250 [7:29:18<2:12:31, 10.92s/it] 78%|  | 2523/3250 [7:29:28<2:10:18, 10.75s/it]                                                        78%|  | 2523/3250 [7:29:28<2:10:18, 10.75s/it] 78%|  | 2524/3250 [7:29:39<2:08:40, 10.63s/it]                                                        78%|  | 2524/3250 [7:29:39<2:08:40, 10.63s/it] 78%|  | 2525/3250 [7:29:49<2:08:22, 10.62s/it]                                                        78%|  | 2525/3250 [7:29:49<2:08:22, 10.62s/it] 78%|{'loss': 0.4913, 'learning_rate': 1.176694730469206e-05, 'epoch': 0.78}
{'loss': 0.5374, 'learning_rate': 1.1735799169451888e-05, 'epoch': 0.78}
{'loss': 0.5305, 'learning_rate': 1.1704686832339812e-05, 'epoch': 0.78}
{'loss': 0.5142, 'learning_rate': 1.1673610322463014e-05, 'epoch': 0.78}
{'loss': 0.5201, 'learning_rate': 1.164256966889517e-05, 'epoch': 0.78}
  | 2526/3250 [7:30:00<2:07:13, 10.54s/it]                                                        78%|  | 2526/3250 [7:30:00<2:07:13, 10.54s/it] 78%|  | 2527/3250 [7:30:10<2:06:19, 10.48s/it]                                                        78%|  | 2527/3250 [7:30:10<2:06:19, 10.48s/it] 78%|  | 2528/3250 [7:30:20<2:05:38, 10.44s/it]                                                        78%|  | 2528/3250 [7:30:20<2:05:38, 10.44s/it] 78%|  | 2529/3250 [7:30:31<2:05:07, 10.41s/it]                                                        78%|  | 2529/3250 [7:30:31<2:05:07, 10.41s/it] 78%|  | 2530/3250 [7:30:41<2:04:44, 10.40s/it]                                                        78%|  | 2530/3250 [7:30:41<2:04:44, 10.40s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.786067545413971, 'eval_runtime': 2.1051, 'eval_samples_per_second': 5.701, 'eval_steps_per_second': 1.425, 'epoch': 0.78}
                                                        78%|  | 2530/3250 [7:30:43<2:04:44, 10.40s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_10/tmp-checkpoint-2530
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2530
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2530
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2530
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2530
the pytorch model path isthe pytorch model path is  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2530/pytorch_model.bin/projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2530/pytorch_model.bin

the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2530/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2530/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.5507, 'learning_rate': 1.1611564900676408e-05, 'epoch': 0.78}
{'loss': 0.5299, 'learning_rate': 1.1580596046813302e-05, 'epoch': 0.78}
{'loss': 0.5034, 'learning_rate': 1.1549663136278788e-05, 'epoch': 0.78}
{'loss': 0.4812, 'learning_rate': 1.1518766198012187e-05, 'epoch': 0.78}
{'loss': 0.5266, 'learning_rate': 1.148790526091918e-05, 'epoch': 0.78}
 78%|  | 2531/3250 [7:30:54<2:13:49, 11.17s/it]                                                        78%|  | 2531/3250 [7:30:54<2:13:49, 11.17s/it] 78%|  | 2532/3250 [7:31:04<2:10:38, 10.92s/it]                                                        78%|  | 2532/3250 [7:31:04<2:10:38, 10.92s/it] 78%|  | 2533/3250 [7:31:15<2:08:26, 10.75s/it]                                                        78%|  | 2533/3250 [7:31:15<2:08:26, 10.75s/it] 78%|  | 2534/3250 [7:31:25<2:06:48, 10.63s/it]                                                        78%|  | 2534/3250 [7:31:25<2:06:48, 10.63s/it] 78%|  | 2535/3250 [7:31:35<2:05:40, 10.55s/it]                                                        78%|  | 2535/3250 [7:31:35<2:05:40, 10.55s/it] 78%|{'loss': 0.5237, 'learning_rate': 1.1457080353871769e-05, 'epoch': 0.78}
{'loss': 0.5223, 'learning_rate': 1.1426291505708236e-05, 'epoch': 0.78}
{'loss': 0.5051, 'learning_rate': 1.1395538745233131e-05, 'epoch': 0.78}
{'loss': 0.5208, 'learning_rate': 1.136482210121726e-05, 'epoch': 0.78}
{'loss': 0.5257, 'learning_rate': 1.1334141602397597e-05, 'epoch': 0.78}
  | 2536/3250 [7:31:46<2:04:50, 10.49s/it]                                                        78%|  | 2536/3250 [7:31:46<2:04:50, 10.49s/it] 78%|  | 2537/3250 [7:31:56<2:04:11, 10.45s/it]                                                        78%|  | 2537/3250 [7:31:56<2:04:11, 10.45s/it] 78%|  | 2538/3250 [7:32:06<2:03:43, 10.43s/it]                                                        78%|  | 2538/3250 [7:32:06<2:03:43, 10.43s/it] 78%|  | 2539/3250 [7:32:17<2:03:24, 10.41s/it]                                                        78%|  | 2539/3250 [7:32:17<2:03:24, 10.41s/it] 78%|  | 2540/3250 [7:32:27<2:03:03, 10.40s/it]                                                        78%|  | 2540/3250 [7:32:27<2:03:03, 10.40s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.7857598662376404, 'eval_runtime': 2.1106, 'eval_samples_per_second': 5.686, 'eval_steps_per_second': 1.421, 'epoch': 0.78}
                                                        78%|  | 2540/3250 [7:32:29<2:03:03, 10.40s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_10/tmp-checkpoint-2540
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2540I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2540

I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2540
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2540
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2540/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2540/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2540/pytorch_model.binthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2540/pytorch_model.bin

/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.5428, 'learning_rate': 1.1303497277477337e-05, 'epoch': 0.78}
{'loss': 0.5274, 'learning_rate': 1.127288915512582e-05, 'epoch': 0.78}
{'loss': 0.5305, 'learning_rate': 1.1242317263978525e-05, 'epoch': 0.78}
{'loss': 0.5182, 'learning_rate': 1.1211781632637041e-05, 'epoch': 0.78}
{'loss': 0.5396, 'learning_rate': 1.1181282289668993e-05, 'epoch': 0.78}
 78%|  | 2541/3250 [7:32:40<2:13:14, 11.28s/it]                                                        78%|  | 2541/3250 [7:32:40<2:13:14, 11.28s/it] 78%|  | 2542/3250 [7:32:51<2:09:49, 11.00s/it]                                                        78%|  | 2542/3250 [7:32:51<2:09:49, 11.00s/it] 78%|  | 2543/3250 [7:33:01<2:07:24, 10.81s/it]                                                        78%|  | 2543/3250 [7:33:01<2:07:24, 10.81s/it] 78%|  | 2544/3250 [7:33:12<2:05:41, 10.68s/it]                                                        78%|  | 2544/3250 [7:33:12<2:05:41, 10.68s/it] 78%|  | 2545/3250 [7:33:22<2:04:23, 10.59s/it]                                                        78%|  | 2545/3250 [7:33:22<2:04:23, 10.59s/it] 78%|{'loss': 0.4921, 'learning_rate': 1.1150819263608097e-05, 'epoch': 0.78}
{'loss': 0.5589, 'learning_rate': 1.112039258295408e-05, 'epoch': 0.78}
{'loss': 0.5092, 'learning_rate': 1.109000227617269e-05, 'epoch': 0.78}
{'loss': 0.51, 'learning_rate': 1.1059648371695585e-05, 'epoch': 0.78}
{'loss': 0.4996, 'learning_rate': 1.102933089792042e-05, 'epoch': 0.78}
  | 2546/3250 [7:33:32<2:03:27, 10.52s/it]                                                        78%|  | 2546/3250 [7:33:32<2:03:27, 10.52s/it] 78%|  | 2547/3250 [7:33:43<2:02:47, 10.48s/it]                                                        78%|  | 2547/3250 [7:33:43<2:02:47, 10.48s/it] 78%|  | 2548/3250 [7:33:53<2:02:14, 10.45s/it]                                                        78%|  | 2548/3250 [7:33:53<2:02:14, 10.45s/it] 78%|  | 2549/3250 [7:34:03<2:01:43, 10.42s/it]                                                        78%|  | 2549/3250 [7:34:03<2:01:43, 10.42s/it] 78%|  | 2550/3250 [7:34:14<2:01:23, 10.40s/it]                                                        78%|  | 2550/3250 [7:34:14<2:01:23, 10.40s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.7843903303146362, 'eval_runtime': 2.3667, 'eval_samples_per_second': 5.07, 'eval_steps_per_second': 1.268, 'epoch': 0.78}
                                                        78%|  | 2550/3250 [7:34:16<2:01:23, 10.40s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_10/tmp-checkpoint-2550
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2550
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2550
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2550
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2550
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2550/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2550/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2550/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2550/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.5014, 'learning_rate': 1.0999049883210771e-05, 'epoch': 0.78}
{'loss': 0.5314, 'learning_rate': 1.0968805355896044e-05, 'epoch': 0.79}
{'loss': 0.5202, 'learning_rate': 1.0938597344271579e-05, 'epoch': 0.79}
{'loss': 0.529, 'learning_rate': 1.090842587659851e-05, 'epoch': 0.79}
{'loss': 1.011, 'learning_rate': 1.087829098110381e-05, 'epoch': 0.79}
 78%|  | 2551/3250 [7:34:27<2:10:59, 11.24s/it]                                                        78%|  | 2551/3250 [7:34:27<2:10:59, 11.24s/it] 79%|  | 2552/3250 [7:34:37<2:07:47, 10.99s/it]                                                        79%|  | 2552/3250 [7:34:37<2:07:47, 10.99s/it] 79%|  | 2553/3250 [7:34:48<2:05:29, 10.80s/it]                                                        79%|  | 2553/3250 [7:34:48<2:05:29, 10.80s/it] 79%|  | 2554/3250 [7:34:58<2:03:52, 10.68s/it]                                                        79%|  | 2554/3250 [7:34:58<2:03:52, 10.68s/it] 79%|  | 2555/3250 [7:35:08<2:02:35, 10.58s/it]                                                        79%|  | 2555/3250 [7:35:08<2:02:35, 10.58s/it] 79%|{'loss': 0.5066, 'learning_rate': 1.0848192685980219e-05, 'epoch': 0.79}
{'loss': 0.5425, 'learning_rate': 1.0818131019386252e-05, 'epoch': 0.79}
{'loss': 0.5207, 'learning_rate': 1.0788106009446119e-05, 'epoch': 0.79}
{'loss': 0.5278, 'learning_rate': 1.0758117684249769e-05, 'epoch': 0.79}
{'loss': 0.5016, 'learning_rate': 1.0728166071852835e-05, 'epoch': 0.79}
  | 2556/3250 [7:35:19<2:01:42, 10.52s/it]                                                        79%|  | 2556/3250 [7:35:19<2:01:42, 10.52s/it] 79%|  | 2557/3250 [7:35:29<2:00:59, 10.48s/it]                                                        79%|  | 2557/3250 [7:35:29<2:00:59, 10.48s/it] 79%|  | 2558/3250 [7:35:40<2:01:16, 10.52s/it]                                                        79%|  | 2558/3250 [7:35:40<2:01:16, 10.52s/it] 79%|  | 2559/3250 [7:35:50<2:00:32, 10.47s/it]                                                        79%|  | 2559/3250 [7:35:50<2:00:32, 10.47s/it] 79%|  | 2560/3250 [7:36:01<1:59:58, 10.43s/it]                                                        79%|  | 2560/3250 [7:36:01<1:59:58, 10.43s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.7848466038703918, 'eval_runtime': 2.1161, 'eval_samples_per_second': 5.671, 'eval_steps_per_second': 1.418, 'epoch': 0.79}
                                                        79%|  | 2560/3250 [7:36:03<1:59:58, 10.43s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_10/tmp-checkpoint-2560
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2560
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2560
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2560
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2560
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2560/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2560/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2560/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2560/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.5642, 'learning_rate': 1.0698251200276583e-05, 'epoch': 0.79}
{'loss': 0.5441, 'learning_rate': 1.0668373097507923e-05, 'epoch': 0.79}
{'loss': 0.5073, 'learning_rate': 1.063853179149934e-05, 'epoch': 0.79}
{'loss': 0.5111, 'learning_rate': 1.060872731016892e-05, 'epoch': 0.79}
{'loss': 0.5159, 'learning_rate': 1.0578959681400297e-05, 'epoch': 0.79}
 79%|  | 2561/3250 [7:36:14<2:08:41, 11.21s/it]                                                        79%|  | 2561/3250 [7:36:14<2:08:41, 11.21s/it] 79%|  | 2562/3250 [7:36:24<2:05:36, 10.95s/it]                                                        79%|  | 2562/3250 [7:36:24<2:05:36, 10.95s/it] 79%|  | 2563/3250 [7:36:34<2:03:26, 10.78s/it]                                                        79%|  | 2563/3250 [7:36:34<2:03:26, 10.78s/it] 79%|  | 2564/3250 [7:36:45<2:01:51, 10.66s/it]                                                        79%|  | 2564/3250 [7:36:45<2:01:51, 10.66s/it] 79%|  | 2565/3250 [7:36:55<2:00:43, 10.57s/it]                                                        79%|  | 2565/3250 [7:36:55<2:00:43, 10.57s/it] 79%|{'loss': 0.513, 'learning_rate': 1.05492289330426e-05, 'epoch': 0.79}
{'loss': 0.5207, 'learning_rate': 1.0519535092910482e-05, 'epoch': 0.79}
{'loss': 0.5151, 'learning_rate': 1.0489878188784063e-05, 'epoch': 0.79}
{'loss': 0.5102, 'learning_rate': 1.0460258248408911e-05, 'epoch': 0.79}
{'loss': 0.5247, 'learning_rate': 1.0430675299495973e-05, 'epoch': 0.79}
  | 2566/3250 [7:37:05<1:59:49, 10.51s/it]                                                        79%|  | 2566/3250 [7:37:05<1:59:49, 10.51s/it] 79%|  | 2567/3250 [7:37:16<1:59:08, 10.47s/it]                                                        79%|  | 2567/3250 [7:37:16<1:59:08, 10.47s/it] 79%|  | 2568/3250 [7:37:26<1:58:38, 10.44s/it]                                                        79%|  | 2568/3250 [7:37:26<1:58:38, 10.44s/it] 79%|  | 2569/3250 [7:37:36<1:58:12, 10.41s/it]                                                        79%|  | 2569/3250 [7:37:36<1:58:12, 10.41s/it] 79%|  | 2570/3250 [7:37:47<1:57:50, 10.40s/it]                                                        79%|  | 2570/3250 [7:37:47<1:57:50, 10.40s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.7869430184364319, 'eval_runtime': 2.1118, 'eval_samples_per_second': 5.682, 'eval_steps_per_second': 1.421, 'epoch': 0.79}
                                                        79%|  | 2570/3250 [7:37:49<1:57:50, 10.40s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_10/tmp-checkpoint-2570
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2570
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2570
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2570
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2570
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2570/pytorch_model.bin
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2570/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2570/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2570/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.5463, 'learning_rate': 1.040112936972164e-05, 'epoch': 0.79}
{'loss': 0.5116, 'learning_rate': 1.0371620486727651e-05, 'epoch': 0.79}
{'loss': 0.527, 'learning_rate': 1.0342148678121073e-05, 'epoch': 0.79}
{'loss': 0.5264, 'learning_rate': 1.0312713971474307e-05, 'epoch': 0.79}
{'loss': 0.5191, 'learning_rate': 1.0283316394325054e-05, 'epoch': 0.79}
 79%|  | 2571/3250 [7:38:00<2:06:41, 11.19s/it]                                                        79%|  | 2571/3250 [7:38:00<2:06:41, 11.19s/it] 79%|  | 2572/3250 [7:38:10<2:03:37, 10.94s/it]                                                        79%|  | 2572/3250 [7:38:10<2:03:37, 10.94s/it] 79%|  | 2573/3250 [7:38:21<2:01:26, 10.76s/it]                                                        79%|  | 2573/3250 [7:38:21<2:01:26, 10.76s/it] 79%|  | 2574/3250 [7:38:31<2:01:13, 10.76s/it]                                                        79%|  | 2574/3250 [7:38:31<2:01:13, 10.76s/it] 79%|  | 2575/3250 [7:38:42<1:59:39, 10.64s/it]                                                        79%|  | 2575/3250 [7:38:42<1:59:39, 10.64s/it] 79%|{'loss': 0.4937, 'learning_rate': 1.0253955974176215e-05, 'epoch': 0.79}
{'loss': 0.5471, 'learning_rate': 1.0224632738496003e-05, 'epoch': 0.79}
{'loss': 0.5076, 'learning_rate': 1.0195346714717813e-05, 'epoch': 0.79}
{'loss': 0.4972, 'learning_rate': 1.0166097930240215e-05, 'epoch': 0.79}
{'loss': 0.4932, 'learning_rate': 1.0136886412426972e-05, 'epoch': 0.79}
  | 2576/3250 [7:38:52<1:58:32, 10.55s/it]                                                        79%|  | 2576/3250 [7:38:52<1:58:32, 10.55s/it] 79%|  | 2577/3250 [7:39:02<1:57:42, 10.49s/it]                                                        79%|  | 2577/3250 [7:39:02<1:57:42, 10.49s/it] 79%|  | 2578/3250 [7:39:13<1:56:58, 10.44s/it]                                                        79%|  | 2578/3250 [7:39:13<1:56:58, 10.44s/it] 79%|  | 2579/3250 [7:39:23<1:56:29, 10.42s/it]                                                        79%|  | 2579/3250 [7:39:23<1:56:29, 10.42s/it] 79%|  | 2580/3250 [7:39:33<1:56:07, 10.40s/it]                                                        79%|  | 2580/3250 [7:39:33<1:56:07, 10.40s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.7856624722480774, 'eval_runtime': 2.1095, 'eval_samples_per_second': 5.689, 'eval_steps_per_second': 1.422, 'epoch': 0.79}
                                                        79%|  | 2580/3250 [7:39:36<1:56:07, 10.40s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_10/tmp-checkpoint-2580
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2580
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2580
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2580
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2580
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2580/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2580/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2580/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2580/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.4982, 'learning_rate': 1.0107712188606933e-05, 'epoch': 0.79}
{'loss': 0.5329, 'learning_rate': 1.0078575286074116e-05, 'epoch': 0.79}
{'loss': 0.5013, 'learning_rate': 1.004947573208756e-05, 'epoch': 0.79}
{'loss': 0.523, 'learning_rate': 1.002041355387141e-05, 'epoch': 0.8}
{'loss': 1.0189, 'learning_rate': 9.991388778614824e-06, 'epoch': 0.8}
 79%|  | 2581/3250 [7:39:46<2:04:28, 11.16s/it]                                                        79%|  | 2581/3250 [7:39:46<2:04:28, 11.16s/it] 79%|  | 2582/3250 [7:39:57<2:01:37, 10.93s/it]                                                        79%|  | 2582/3250 [7:39:57<2:01:37, 10.93s/it] 79%|  | 2583/3250 [7:40:07<1:59:33, 10.75s/it]                                                        79%|  | 2583/3250 [7:40:07<1:59:33, 10.75s/it] 80%|  | 2584/3250 [7:40:17<1:58:01, 10.63s/it]                                                        80%|  | 2584/3250 [7:40:17<1:58:01, 10.63s/it] 80%|  | 2585/3250 [7:40:28<1:56:51, 10.54s/it]                                                        80%|  | 2585/3250 [7:40:28<1:56:51, 10.54s/it] 80%|{'loss': 0.5066, 'learning_rate': 9.962401433471985e-06, 'epoch': 0.8}
{'loss': 0.5241, 'learning_rate': 9.933451545562044e-06, 'epoch': 0.8}
{'loss': 0.5344, 'learning_rate': 9.904539141969093e-06, 'epoch': 0.8}
{'loss': 0.5224, 'learning_rate': 9.875664249742183e-06, 'epoch': 0.8}
{'loss': 0.5003, 'learning_rate': 9.84682689589526e-06, 'epoch': 0.8}
  | 2586/3250 [7:40:38<1:56:05, 10.49s/it]                                                        80%|  | 2586/3250 [7:40:38<1:56:05, 10.49s/it] 80%|  | 2587/3250 [7:40:49<1:55:27, 10.45s/it]                                                        80%|  | 2587/3250 [7:40:49<1:55:27, 10.45s/it] 80%|  | 2588/3250 [7:40:59<1:55:12, 10.44s/it]                                                        80%|  | 2588/3250 [7:40:59<1:55:12, 10.44s/it] 80%|  | 2589/3250 [7:41:09<1:54:44, 10.42s/it]                                                        80%|  | 2589/3250 [7:41:09<1:54:44, 10.42s/it] 80%|  | 2590/3250 [7:41:20<1:54:22, 10.40s/it]                                                        80%|  | 2590/3250 [7:41:20<1:54:22, 10.40s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.7860448360443115, 'eval_runtime': 2.3244, 'eval_samples_per_second': 5.163, 'eval_steps_per_second': 1.291, 'epoch': 0.8}
                                                        80%|  | 2590/3250 [7:41:22<1:54:22, 10.40s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_10/tmp-checkpoint-2590
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2590
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2590
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2590
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2590
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2590/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2590/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2590/pytorch_model.binthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2590/pytorch_model.bin

/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.5057, 'learning_rate': 9.818027107407151e-06, 'epoch': 0.8}
{'loss': 0.5768, 'learning_rate': 9.789264911221546e-06, 'epoch': 0.8}
{'loss': 0.5247, 'learning_rate': 9.760540334246965e-06, 'epoch': 0.8}
{'loss': 0.5151, 'learning_rate': 9.731853403356705e-06, 'epoch': 0.8}
{'loss': 0.4736, 'learning_rate': 9.703204145388879e-06, 'epoch': 0.8}
 80%|  | 2591/3250 [7:41:33<2:04:07, 11.30s/it]                                                        80%|  | 2591/3250 [7:41:33<2:04:07, 11.30s/it] 80%|  | 2592/3250 [7:41:43<2:00:52, 11.02s/it]                                                        80%|  | 2592/3250 [7:41:43<2:00:52, 11.02s/it] 80%|  | 2593/3250 [7:41:54<1:58:29, 10.82s/it]                                                        80%|  | 2593/3250 [7:41:54<1:58:29, 10.82s/it] 80%|  | 2594/3250 [7:42:04<1:56:53, 10.69s/it]                                                        80%|  | 2594/3250 [7:42:04<1:56:53, 10.69s/it] 80%|  | 2595/3250 [7:42:15<1:55:32, 10.58s/it]                                                        80%|  | 2595/3250 [7:42:15<1:55:32, 10.58s/it] 80%|{'loss': 0.5255, 'learning_rate': 9.674592587146336e-06, 'epoch': 0.8}
{'loss': 0.5365, 'learning_rate': 9.646018755396664e-06, 'epoch': 0.8}
{'loss': 0.5087, 'learning_rate': 9.617482676872164e-06, 'epoch': 0.8}
{'loss': 0.5056, 'learning_rate': 9.588984378269783e-06, 'epoch': 0.8}
{'loss': 0.5172, 'learning_rate': 9.560523886251171e-06, 'epoch': 0.8}
  | 2596/3250 [7:42:25<1:54:35, 10.51s/it]                                                        80%|  | 2596/3250 [7:42:25<1:54:35, 10.51s/it] 80%|  | 2597/3250 [7:42:35<1:53:52, 10.46s/it]                                                        80%|  | 2597/3250 [7:42:35<1:53:52, 10.46s/it] 80%|  | 2598/3250 [7:42:46<1:53:19, 10.43s/it]                                                        80%|  | 2598/3250 [7:42:46<1:53:19, 10.43s/it] 80%|  | 2599/3250 [7:42:56<1:53:03, 10.42s/it]                                                        80%|  | 2599/3250 [7:42:56<1:53:03, 10.42s/it] 80%|  | 2600/3250 [7:43:06<1:52:39, 10.40s/it]                                                        80%|  | 2600/3250 [7:43:06<1:52:39, 10.40s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.7880762219429016, 'eval_runtime': 2.1026, 'eval_samples_per_second': 5.707, 'eval_steps_per_second': 1.427, 'epoch': 0.8}
                                                        80%|  | 2600/3250 [7:43:08<1:52:39, 10.40s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_10/tmp-checkpoint-2600
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2600I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2600

I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2600
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2600
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2600/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2600/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2600/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2600/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.5191, 'learning_rate': 9.532101227442552e-06, 'epoch': 0.8}
{'loss': 0.5393, 'learning_rate': 9.5037164284348e-06, 'epoch': 0.8}
{'loss': 0.5116, 'learning_rate': 9.475369515783355e-06, 'epoch': 0.8}
{'loss': 0.528, 'learning_rate': 9.447060516008211e-06, 'epoch': 0.8}
{'loss': 0.5379, 'learning_rate': 9.418789455593906e-06, 'epoch': 0.8}
 80%|  | 2601/3250 [7:43:19<2:00:48, 11.17s/it]                                                        80%|  | 2601/3250 [7:43:19<2:00:48, 11.17s/it] 80%|  | 2602/3250 [7:43:30<1:57:58, 10.92s/it]                                                        80%|  | 2602/3250 [7:43:30<1:57:58, 10.92s/it] 80%|  | 2603/3250 [7:43:40<1:55:54, 10.75s/it]                                                        80%|  | 2603/3250 [7:43:40<1:55:54, 10.75s/it] 80%|  | 2604/3250 [7:43:50<1:54:23, 10.63s/it]                                                        80%|  | 2604/3250 [7:43:50<1:54:23, 10.63s/it] 80%|  | 2605/3250 [7:44:01<1:53:25, 10.55s/it]                                                        80%|  | 2605/3250 [7:44:01<1:53:25, 10.55s/it] 80%|{'loss': 0.5074, 'learning_rate': 9.39055636098945e-06, 'epoch': 0.8}
{'loss': 0.5271, 'learning_rate': 9.362361258608365e-06, 'epoch': 0.8}
{'loss': 0.5209, 'learning_rate': 9.334204174828614e-06, 'epoch': 0.8}
{'loss': 0.499, 'learning_rate': 9.30608513599261e-06, 'epoch': 0.8}
{'loss': 0.498, 'learning_rate': 9.27800416840715e-06, 'epoch': 0.8}
  | 2606/3250 [7:44:11<1:53:07, 10.54s/it]                                                        80%|  | 2606/3250 [7:44:11<1:53:07, 10.54s/it] 80%|  | 2607/3250 [7:44:22<1:53:55, 10.63s/it]                                                        80%|  | 2607/3250 [7:44:22<1:53:55, 10.63s/it] 80%|  | 2608/3250 [7:44:32<1:52:54, 10.55s/it]                                                        80%|  | 2608/3250 [7:44:32<1:52:54, 10.55s/it] 80%|  | 2609/3250 [7:44:43<1:52:07, 10.50s/it]                                                        80%|  | 2609/3250 [7:44:43<1:52:07, 10.50s/it] 80%|  | 2610/3250 [7:44:53<1:51:29, 10.45s/it]                                                        80%|  | 2610/3250 [7:44:53<1:51:29, 10.45s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.7847064137458801, 'eval_runtime': 2.1357, 'eval_samples_per_second': 5.619, 'eval_steps_per_second': 1.405, 'epoch': 0.8}
                                                        80%|  | 2610/3250 [7:44:55<1:51:29, 10.45s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_10/tmp-checkpoint-2610
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2610
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2610
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2610
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2610
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2610/pytorch_model.bin
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2610/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2610/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2610/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.4994, 'learning_rate': 9.249961298343435e-06, 'epoch': 0.8}
{'loss': 0.528, 'learning_rate': 9.221956552036992e-06, 'epoch': 0.8}
{'loss': 0.5197, 'learning_rate': 9.193989955687715e-06, 'epoch': 0.8}
{'loss': 0.5356, 'learning_rate': 9.166061535459798e-06, 'epoch': 0.8}
{'loss': 1.0344, 'learning_rate': 9.138171317481697e-06, 'epoch': 0.8}
 80%|  | 2611/3250 [7:45:06<1:59:40, 11.24s/it]                                                        80%|  | 2611/3250 [7:45:06<1:59:40, 11.24s/it] 80%|  | 2612/3250 [7:45:17<1:56:40, 10.97s/it]                                                        80%|  | 2612/3250 [7:45:17<1:56:40, 10.97s/it] 80%|  | 2613/3250 [7:45:27<1:54:30, 10.79s/it]                                                        80%|  | 2613/3250 [7:45:27<1:54:30, 10.79s/it] 80%|  | 2614/3250 [7:45:37<1:52:58, 10.66s/it]                                                        80%|  | 2614/3250 [7:45:37<1:52:58, 10.66s/it] 80%|  | 2615/3250 [7:45:48<1:51:45, 10.56s/it]                                                        80%|  | 2615/3250 [7:45:48<1:51:45, 10.56s/it] 80%|{'loss': 0.4816, 'learning_rate': 9.11031932784618e-06, 'epoch': 0.8}
{'loss': 0.4975, 'learning_rate': 9.082505592610174e-06, 'epoch': 0.81}
{'loss': 0.5291, 'learning_rate': 9.054730137794886e-06, 'epoch': 0.81}
{'loss': 0.531, 'learning_rate': 9.026992989385669e-06, 'epoch': 0.81}
{'loss': 0.5092, 'learning_rate': 8.999294173332058e-06, 'epoch': 0.81}
  | 2616/3250 [7:46:01<2:01:56, 11.54s/it]                                                        80%|  | 2616/3250 [7:46:01<2:01:56, 11.54s/it] 81%|  | 2617/3250 [7:46:12<1:58:27, 11.23s/it]                                                        81%|  | 2617/3250 [7:46:12<1:58:27, 11.23s/it] 81%|  | 2618/3250 [7:46:22<1:55:29, 10.96s/it]                                                        81%|  | 2618/3250 [7:46:22<1:55:29, 10.96s/it] 81%|  | 2619/3250 [7:46:33<1:53:20, 10.78s/it]                                                        81%|  | 2619/3250 [7:46:33<1:53:20, 10.78s/it] 81%|  | 2620/3250 [7:46:43<1:51:48, 10.65s/it]                                                        81%|  | 2620/3250 [7:46:43<1:51:48, 10.65s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.785173237323761, 'eval_runtime': 3.2676, 'eval_samples_per_second': 3.672, 'eval_steps_per_second': 0.918, 'epoch': 0.81}
                                                        81%|  | 2620/3250 [7:46:46<1:51:48, 10.65s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_10/tmp-checkpoint-2620
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2620
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2620
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2620
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2620
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2620/pytorch_model.bin
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2620/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2620/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2620/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.5129, 'learning_rate': 8.971633715547717e-06, 'epoch': 0.81}
{'loss': 0.5466, 'learning_rate': 8.944011641910432e-06, 'epoch': 0.81}
{'loss': 0.5433, 'learning_rate': 8.916427978262082e-06, 'epoch': 0.81}
{'loss': 0.4993, 'learning_rate': 8.888882750408578e-06, 'epoch': 0.81}
{'loss': 0.481, 'learning_rate': 8.861375984119918e-06, 'epoch': 0.81}
 81%|  | 2621/3250 [7:46:57<2:02:56, 11.73s/it]                                                        81%|  | 2621/3250 [7:46:57<2:02:56, 11.73s/it] 81%|  | 2622/3250 [7:47:08<1:58:27, 11.32s/it]                                                        81%|  | 2622/3250 [7:47:08<1:58:27, 11.32s/it] 81%|  | 2623/3250 [7:47:18<1:56:29, 11.15s/it]                                                        81%|  | 2623/3250 [7:47:18<1:56:29, 11.15s/it] 81%|  | 2624/3250 [7:47:29<1:53:49, 10.91s/it]                                                        81%|  | 2624/3250 [7:47:29<1:53:49, 10.91s/it] 81%|  | 2625/3250 [7:47:39<1:51:54, 10.74s/it]                                                        81%|  | 2625/3250 [7:47:39<1:51:54, 10.74s/it] 81%|{'loss': 0.5363, 'learning_rate': 8.83390770513009e-06, 'epoch': 0.81}
{'loss': 0.514, 'learning_rate': 8.80647793913708e-06, 'epoch': 0.81}
{'loss': 0.5188, 'learning_rate': 8.779086711802847e-06, 'epoch': 0.81}
{'loss': 0.5011, 'learning_rate': 8.751734048753306e-06, 'epoch': 0.81}
{'loss': 0.5165, 'learning_rate': 8.724419975578257e-06, 'epoch': 0.81}
  | 2626/3250 [7:47:49<1:50:31, 10.63s/it]                                                        81%|  | 2626/3250 [7:47:49<1:50:31, 10.63s/it] 81%|  | 2627/3250 [7:48:00<1:49:33, 10.55s/it]                                                        81%|  | 2627/3250 [7:48:00<1:49:33, 10.55s/it] 81%|  | 2628/3250 [7:48:10<1:48:49, 10.50s/it]                                                        81%|  | 2628/3250 [7:48:10<1:48:49, 10.50s/it] 81%|  | 2629/3250 [7:48:20<1:48:15, 10.46s/it]                                                        81%|  | 2629/3250 [7:48:20<1:48:15, 10.46s/it] 81%|  | 2630/3250 [7:48:31<1:47:48, 10.43s/it]                                                        81%|  | 2630/3250 [7:48:31<1:47:48, 10.43s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.786454439163208, 'eval_runtime': 2.1232, 'eval_samples_per_second': 5.652, 'eval_steps_per_second': 1.413, 'epoch': 0.81}
                                                        81%|  | 2630/3250 [7:48:33<1:47:48, 10.43s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_10/tmp-checkpoint-2630
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2630I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2630

I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2630
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2630
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2630/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2630/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2630/pytorch_model.binthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2630/pytorch_model.bin

/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.5197, 'learning_rate': 8.697144517831435e-06, 'epoch': 0.81}
{'loss': 0.5416, 'learning_rate': 8.669907701030428e-06, 'epoch': 0.81}
{'loss': 0.5181, 'learning_rate': 8.64270955065669e-06, 'epoch': 0.81}
{'loss': 0.5376, 'learning_rate': 8.615550092155478e-06, 'epoch': 0.81}
{'loss': 0.5142, 'learning_rate': 8.588429350935857e-06, 'epoch': 0.81}
 81%|  | 2631/3250 [7:48:44<1:55:24, 11.19s/it]                                                        81%|  | 2631/3250 [7:48:44<1:55:24, 11.19s/it] 81%|  | 2632/3250 [7:48:54<1:52:41, 10.94s/it]                                                        81%|  | 2632/3250 [7:48:54<1:52:41, 10.94s/it] 81%|  | 2633/3250 [7:49:05<1:50:41, 10.76s/it]                                                        81%|  | 2633/3250 [7:49:05<1:50:41, 10.76s/it] 81%|  | 2634/3250 [7:49:15<1:49:16, 10.64s/it]                                                        81%|  | 2634/3250 [7:49:15<1:49:16, 10.64s/it] 81%|  | 2635/3250 [7:49:25<1:48:15, 10.56s/it]                                                        81%|  | 2635/3250 [7:49:25<1:48:15, 10.56s/it] 81%|{'loss': 0.5335, 'learning_rate': 8.561347352370703e-06, 'epoch': 0.81}
{'loss': 0.488, 'learning_rate': 8.534304121796582e-06, 'epoch': 0.81}
{'loss': 0.5542, 'learning_rate': 8.507299684513848e-06, 'epoch': 0.81}
{'loss': 0.5067, 'learning_rate': 8.480334065786532e-06, 'epoch': 0.81}
{'loss': 0.5109, 'learning_rate': 8.45340729084237e-06, 'epoch': 0.81}
  | 2636/3250 [7:49:36<1:47:31, 10.51s/it]                                                        81%|  | 2636/3250 [7:49:36<1:47:31, 10.51s/it] 81%|  | 2637/3250 [7:49:46<1:46:55, 10.46s/it]                                                        81%|  | 2637/3250 [7:49:46<1:46:55, 10.46s/it] 81%|  | 2638/3250 [7:49:56<1:46:30, 10.44s/it]                                                        81%|  | 2638/3250 [7:49:56<1:46:30, 10.44s/it] 81%|  | 2639/3250 [7:50:07<1:46:03, 10.41s/it]                                                        81%|  | 2639/3250 [7:50:07<1:46:03, 10.41s/it] 81%|  | 2640/3250 [7:50:17<1:46:33, 10.48s/it]                                                        81%|  | 2640/3250 [7:50:17<1:46:33, 10.48s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.7839841842651367, 'eval_runtime': 2.1175, 'eval_samples_per_second': 5.667, 'eval_steps_per_second': 1.417, 'epoch': 0.81}
                                                        81%|  | 2640/3250 [7:50:19<1:46:33, 10.48s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_10/tmp-checkpoint-2640
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2640I AM HERE AND I AM SAVING THE MODEL

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2640
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2640
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2640
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2640/pytorch_model.bin
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2640/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2640/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2640/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.4975, 'learning_rate': 8.426519384872733e-06, 'epoch': 0.81}
{'loss': 0.5005, 'learning_rate': 8.399670373032664e-06, 'epoch': 0.81}
{'loss': 0.5157, 'learning_rate': 8.372860280440759e-06, 'epoch': 0.81}
{'loss': 0.5256, 'learning_rate': 8.346089132179257e-06, 'epoch': 0.81}
{'loss': 0.5178, 'learning_rate': 8.319356953293944e-06, 'epoch': 0.81}
 81%| | 2641/3250 [7:50:30<1:54:22, 11.27s/it]                                                        81%| | 2641/3250 [7:50:30<1:54:22, 11.27s/it] 81%| | 2642/3250 [7:50:41<1:51:26, 11.00s/it]                                                        81%| | 2642/3250 [7:50:41<1:51:26, 11.00s/it] 81%| | 2643/3250 [7:50:51<1:49:26, 10.82s/it]                                                        81%| | 2643/3250 [7:50:51<1:49:26, 10.82s/it] 81%| | 2644/3250 [7:51:02<1:47:54, 10.68s/it]                                                        81%| | 2644/3250 [7:51:02<1:47:54, 10.68s/it] 81%| | 2645/3250 [7:51:12<1:46:46, 10.59s/it]                                                        81%| | 2645/3250 [7:51:12<1:46:46, 10.59s/it{'loss': 1.0049, 'learning_rate': 8.292663768794145e-06, 'epoch': 0.81}
{'loss': 0.5119, 'learning_rate': 8.266009603652724e-06, 'epoch': 0.81}
{'loss': 0.5383, 'learning_rate': 8.239394482805996e-06, 'epoch': 0.81}
{'loss': 0.5198, 'learning_rate': 8.21281843115379e-06, 'epoch': 0.82}
{'loss': 0.5217, 'learning_rate': 8.186281473559381e-06, 'epoch': 0.82}
] 81%| | 2646/3250 [7:51:22<1:45:53, 10.52s/it]                                                        81%| | 2646/3250 [7:51:22<1:45:53, 10.52s/it] 81%| | 2647/3250 [7:51:33<1:45:15, 10.47s/it]                                                        81%| | 2647/3250 [7:51:33<1:45:15, 10.47s/it] 81%| | 2648/3250 [7:51:43<1:44:44, 10.44s/it]                                                        81%| | 2648/3250 [7:51:43<1:44:44, 10.44s/it] 82%| | 2649/3250 [7:51:53<1:44:21, 10.42s/it]                                                        82%| | 2649/3250 [7:51:53<1:44:21, 10.42s/it] 82%| | 2650/3250 [7:52:04<1:44:01, 10.40s/it]                                                        82%| | 2650/3250 [7:52:04<1:44:01, 10.40s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.7839217782020569, 'eval_runtime': 2.336, 'eval_samples_per_second': 5.137, 'eval_steps_per_second': 1.284, 'epoch': 0.82}
                                                        82%| | 2650/3250 [7:52:06<1:44:01, 10.40s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_10/tmp-checkpoint-2650
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2650
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2650
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2650
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2650
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2650/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2650/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2650/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2650/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.5053, 'learning_rate': 8.159783634849427e-06, 'epoch': 0.82}
{'loss': 0.5563, 'learning_rate': 8.13332493981404e-06, 'epoch': 0.82}
{'loss': 0.5433, 'learning_rate': 8.106905413206689e-06, 'epoch': 0.82}
{'loss': 0.5019, 'learning_rate': 8.080525079744212e-06, 'epoch': 0.82}
{'loss': 0.5133, 'learning_rate': 8.054183964106738e-06, 'epoch': 0.82}
 82%| | 2651/3250 [7:52:17<1:52:16, 11.25s/it]                                                        82%| | 2651/3250 [7:52:17<1:52:16, 11.25s/it] 82%| | 2652/3250 [7:52:27<1:49:25, 10.98s/it]                                                        82%| | 2652/3250 [7:52:27<1:49:25, 10.98s/it] 82%| | 2653/3250 [7:52:38<1:47:22, 10.79s/it]                                                        82%| | 2653/3250 [7:52:38<1:47:22, 10.79s/it] 82%| | 2654/3250 [7:52:48<1:45:57, 10.67s/it]                                                        82%| | 2654/3250 [7:52:48<1:45:57, 10.67s/it] 82%| | 2655/3250 [7:52:58<1:44:52, 10.57s/it]                                                        82%| | 2655/3250 [7:52:58<1:44:52, 10.57s/it{'loss': 0.5155, 'learning_rate': 8.02788209093775e-06, 'epoch': 0.82}
{'loss': 0.5054, 'learning_rate': 8.001619484844009e-06, 'epoch': 0.82}
{'loss': 0.5242, 'learning_rate': 7.975396170395521e-06, 'epoch': 0.82}
{'loss': 0.497, 'learning_rate': 7.949212172125565e-06, 'epoch': 0.82}
{'loss': 0.5027, 'learning_rate': 7.923067514530613e-06, 'epoch': 0.82}
] 82%| | 2656/3250 [7:53:09<1:45:12, 10.63s/it]                                                        82%| | 2656/3250 [7:53:09<1:45:12, 10.63s/it] 82%| | 2657/3250 [7:53:20<1:44:18, 10.55s/it]                                                        82%| | 2657/3250 [7:53:20<1:44:18, 10.55s/it] 82%| | 2658/3250 [7:53:30<1:43:35, 10.50s/it]                                                        82%| | 2658/3250 [7:53:30<1:43:35, 10.50s/it] 82%| | 2659/3250 [7:53:40<1:43:00, 10.46s/it]                                                        82%| | 2659/3250 [7:53:40<1:43:00, 10.46s/it] 82%| | 2660/3250 [7:53:51<1:42:36, 10.43s/it]                                                        82%| | 2660/3250 [7:53:51<1:42:36, 10.43s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.7861101627349854, 'eval_runtime': 2.1056, 'eval_samples_per_second': 5.699, 'eval_steps_per_second': 1.425, 'epoch': 0.82}
                                                        82%| | 2660/3250 [7:53:53<1:42:36, 10.43s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_10/tmp-checkpoint-2660
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2660
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2660
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2660
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2660
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2660/pytorch_model.bin
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2660/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2660/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2660/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.5137, 'learning_rate': 7.89696222207032e-06, 'epoch': 0.82}
{'loss': 0.5506, 'learning_rate': 7.870896319167548e-06, 'epoch': 0.82}
{'loss': 0.5141, 'learning_rate': 7.844869830208273e-06, 'epoch': 0.82}
{'loss': 0.5152, 'learning_rate': 7.818882779541631e-06, 'epoch': 0.82}
{'loss': 0.5302, 'learning_rate': 7.79293519147985e-06, 'epoch': 0.82}
 82%| | 2661/3250 [7:54:04<1:50:03, 11.21s/it]                                                        82%| | 2661/3250 [7:54:04<1:50:03, 11.21s/it] 82%| | 2662/3250 [7:54:14<1:47:25, 10.96s/it]                                                        82%| | 2662/3250 [7:54:14<1:47:25, 10.96s/it] 82%| | 2663/3250 [7:54:24<1:45:30, 10.78s/it]                                                        82%| | 2663/3250 [7:54:24<1:45:30, 10.78s/it] 82%| | 2664/3250 [7:54:35<1:44:08, 10.66s/it]                                                        82%| | 2664/3250 [7:54:35<1:44:08, 10.66s/it] 82%| | 2665/3250 [7:54:45<1:43:10, 10.58s/it]                                                        82%| | 2665/3250 [7:54:45<1:43:10, 10.58s/it{'loss': 0.5245, 'learning_rate': 7.767027090298207e-06, 'epoch': 0.82}
{'loss': 0.491, 'learning_rate': 7.74115850023509e-06, 'epoch': 0.82}
{'loss': 0.5501, 'learning_rate': 7.715329445491876e-06, 'epoch': 0.82}
{'loss': 0.5093, 'learning_rate': 7.689539950232977e-06, 'epoch': 0.82}
{'loss': 0.4928, 'learning_rate': 7.663790038585793e-06, 'epoch': 0.82}
] 82%| | 2666/3250 [7:54:56<1:42:23, 10.52s/it]                                                        82%| | 2666/3250 [7:54:56<1:42:23, 10.52s/it] 82%| | 2667/3250 [7:55:06<1:41:48, 10.48s/it]                                                        82%| | 2667/3250 [7:55:06<1:41:48, 10.48s/it] 82%| | 2668/3250 [7:55:16<1:41:18, 10.44s/it]                                                        82%| | 2668/3250 [7:55:16<1:41:18, 10.44s/it] 82%| | 2669/3250 [7:55:27<1:40:53, 10.42s/it]                                                        82%| | 2669/3250 [7:55:27<1:40:53, 10.42s/it] 82%| | 2670/3250 [7:55:37<1:40:34, 10.40s/it]                                                        82%| | 2670/3250 [7:55:37<1:40:34, 10.40s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.7863113880157471, 'eval_runtime': 2.1122, 'eval_samples_per_second': 5.681, 'eval_steps_per_second': 1.42, 'epoch': 0.82}
                                                        82%| | 2670/3250 [7:55:39<1:40:34, 10.40s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_10/tmp-checkpoint-2670
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2670
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2670
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2670
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2670
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2670/pytorch_model.bin
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2670/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2670/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2670/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.5075, 'learning_rate': 7.638079734640703e-06, 'epoch': 0.82}
{'loss': 0.4991, 'learning_rate': 7.612409062451015e-06, 'epoch': 0.82}
{'loss': 0.5254, 'learning_rate': 7.58677804603295e-06, 'epoch': 0.82}
{'loss': 0.5051, 'learning_rate': 7.561186709365653e-06, 'epoch': 0.82}
{'loss': 0.5278, 'learning_rate': 7.5356350763911345e-06, 'epoch': 0.82}
 82%| | 2671/3250 [7:55:50<1:47:59, 11.19s/it]                                                        82%| | 2671/3250 [7:55:50<1:47:59, 11.19s/it] 82%| | 2672/3250 [7:56:00<1:45:25, 10.94s/it]                                                        82%| | 2672/3250 [7:56:00<1:45:25, 10.94s/it] 82%| | 2673/3250 [7:56:12<1:45:39, 10.99s/it]                                                        82%| | 2673/3250 [7:56:12<1:45:39, 10.99s/it] 82%| | 2674/3250 [7:56:22<1:43:42, 10.80s/it]                                                        82%| | 2674/3250 [7:56:22<1:43:42, 10.80s/it] 82%| | 2675/3250 [7:56:32<1:42:18, 10.68s/it]                                                        82%| | 2675/3250 [7:56:32<1:42:18, 10.68s/it{'loss': 1.019, 'learning_rate': 7.510123171014255e-06, 'epoch': 0.82}
{'loss': 0.5077, 'learning_rate': 7.484651017102728e-06, 'epoch': 0.82}
{'loss': 0.5133, 'learning_rate': 7.459218638487064e-06, 'epoch': 0.82}
{'loss': 0.5415, 'learning_rate': 7.4338260589605415e-06, 'epoch': 0.82}
{'loss': 0.5226, 'learning_rate': 7.408473302279234e-06, 'epoch': 0.82}
] 82%| | 2676/3250 [7:56:43<1:41:09, 10.57s/it]                                                        82%| | 2676/3250 [7:56:43<1:41:09, 10.57s/it] 82%| | 2677/3250 [7:56:56<1:48:43, 11.38s/it]                                                        82%| | 2677/3250 [7:56:56<1:48:43, 11.38s/it] 82%| | 2678/3250 [7:57:06<1:45:48, 11.10s/it]                                                        82%| | 2678/3250 [7:57:06<1:45:48, 11.10s/it] 82%| | 2679/3250 [7:57:17<1:43:32, 10.88s/it]                                                        82%| | 2679/3250 [7:57:17<1:43:32, 10.88s/it] 82%| | 2680/3250 [7:57:27<1:41:54, 10.73s/it]                                                        82%| | 2680/3250 [7:57:27<1:41:54, 10.73s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.7848346829414368, 'eval_runtime': 2.576, 'eval_samples_per_second': 4.658, 'eval_steps_per_second': 1.165, 'epoch': 0.82}
                                                        82%| | 2680/3250 [7:57:30<1:41:54, 10.73s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_10/tmp-checkpoint-2680
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2680
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2680
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2680
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2680
the pytorch model path is the pytorch model path is/projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2680/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2680/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2680/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2680/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.5013, 'learning_rate': 7.383160392161953e-06, 'epoch': 0.82}
{'loss': 0.4977, 'learning_rate': 7.357887352290227e-06, 'epoch': 0.83}
{'loss': 0.5662, 'learning_rate': 7.332654206308298e-06, 'epoch': 0.83}
{'loss': 0.5186, 'learning_rate': 7.307460977823044e-06, 'epoch': 0.83}
{'loss': 0.513, 'learning_rate': 7.282307690404055e-06, 'epoch': 0.83}
 82%| | 2681/3250 [7:57:41<1:49:47, 11.58s/it]                                                        82%| | 2681/3250 [7:57:41<1:49:47, 11.58s/it] 83%| | 2682/3250 [7:57:51<1:46:08, 11.21s/it]                                                        83%| | 2682/3250 [7:57:51<1:46:08, 11.21s/it] 83%| | 2683/3250 [7:58:01<1:43:32, 10.96s/it]                                                        83%| | 2683/3250 [7:58:01<1:43:32, 10.96s/it] 83%| | 2684/3250 [7:58:12<1:41:39, 10.78s/it]                                                        83%| | 2684/3250 [7:58:12<1:41:39, 10.78s/it] 83%| | 2685/3250 [7:58:22<1:40:20, 10.66s/it]                                                        83%| | 2685/3250 [7:58:22<1:40:20, 10.66s/it{'loss': 0.4772, 'learning_rate': 7.257194367583503e-06, 'epoch': 0.83}
{'loss': 0.5286, 'learning_rate': 7.232121032856193e-06, 'epoch': 0.83}
{'loss': 0.5387, 'learning_rate': 7.207087709679533e-06, 'epoch': 0.83}
{'loss': 0.4956, 'learning_rate': 7.182094421473479e-06, 'epoch': 0.83}
{'loss': 0.4938, 'learning_rate': 7.157141191620548e-06, 'epoch': 0.83}
] 83%| | 2686/3250 [7:58:32<1:39:16, 10.56s/it]                                                        83%| | 2686/3250 [7:58:32<1:39:16, 10.56s/it] 83%| | 2687/3250 [7:58:43<1:38:33, 10.50s/it]                                                        83%| | 2687/3250 [7:58:43<1:38:33, 10.50s/it] 83%| | 2688/3250 [7:58:53<1:37:58, 10.46s/it]                                                        83%| | 2688/3250 [7:58:53<1:37:58, 10.46s/it] 83%| | 2689/3250 [7:59:04<1:38:48, 10.57s/it]                                                        83%| | 2689/3250 [7:59:04<1:38:48, 10.57s/it] 83%| | 2690/3250 [7:59:14<1:38:06, 10.51s/it]                                                        83%| | 2690/3250 [7:59:14<1:38:06, 10.51s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.7855554223060608, 'eval_runtime': 2.1119, 'eval_samples_per_second': 5.682, 'eval_steps_per_second': 1.421, 'epoch': 0.83}
                                                        83%| | 2690/3250 [7:59:17<1:38:06, 10.51s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_10/tmp-checkpoint-2690
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2690
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2690
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2690
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2690
the pytorch model path isthe pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2690/pytorch_model.bin 
/projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2690/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2690/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2690/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.5248, 'learning_rate': 7.13222804346575e-06, 'epoch': 0.83}
{'loss': 0.512, 'learning_rate': 7.107355000316624e-06, 'epoch': 0.83}
{'loss': 0.5386, 'learning_rate': 7.082522085443183e-06, 'epoch': 0.83}
{'loss': 0.522, 'learning_rate': 7.0577293220778996e-06, 'epoch': 0.83}
{'loss': 0.5227, 'learning_rate': 7.032976733415675e-06, 'epoch': 0.83}
 83%| | 2691/3250 [7:59:27<1:44:43, 11.24s/it]                                                        83%| | 2691/3250 [7:59:27<1:44:43, 11.24s/it] 83%| | 2692/3250 [7:59:38<1:42:02, 10.97s/it]                                                        83%| | 2692/3250 [7:59:38<1:42:02, 10.97s/it] 83%| | 2693/3250 [7:59:48<1:40:06, 10.78s/it]                                                        83%| | 2693/3250 [7:59:48<1:40:06, 10.78s/it] 83%| | 2694/3250 [7:59:58<1:38:41, 10.65s/it]                                                        83%| | 2694/3250 [7:59:58<1:38:41, 10.65s/it] 83%| | 2695/3250 [8:00:09<1:37:42, 10.56s/it]                                                        83%| | 2695/3250 [8:00:09<1:37:42, 10.56s/it{'loss': 0.5286, 'learning_rate': 7.0082643426138405e-06, 'epoch': 0.83}
{'loss': 0.4989, 'learning_rate': 6.983592172792086e-06, 'epoch': 0.83}
{'loss': 0.5226, 'learning_rate': 6.958960247032514e-06, 'epoch': 0.83}
{'loss': 0.5172, 'learning_rate': 6.934368588379553e-06, 'epoch': 0.83}
{'loss': 0.4852, 'learning_rate': 6.909817219839959e-06, 'epoch': 0.83}
] 83%| | 2696/3250 [8:00:19<1:36:57, 10.50s/it]                                                        83%| | 2696/3250 [8:00:19<1:36:57, 10.50s/it] 83%| | 2697/3250 [8:00:29<1:36:22, 10.46s/it]                                                        83%| | 2697/3250 [8:00:29<1:36:22, 10.46s/it] 83%| | 2698/3250 [8:00:40<1:35:56, 10.43s/it]                                                        83%| | 2698/3250 [8:00:40<1:35:56, 10.43s/it] 83%| | 2699/3250 [8:00:50<1:35:31, 10.40s/it]                                                        83%| | 2699/3250 [8:00:50<1:35:31, 10.40s/it] 83%| | 2700/3250 [8:01:00<1:35:11, 10.39s/it]                                                        83%| | 2700/3250 [8:01:00<1:35:11, 10.39s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.7853763699531555, 'eval_runtime': 2.1103, 'eval_samples_per_second': 5.686, 'eval_steps_per_second': 1.422, 'epoch': 0.83}
                                                        83%| | 2700/3250 [8:01:03<1:35:11, 10.39s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_10/tmp-checkpoint-2700
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  I AM HERE AND I AM SAVING THE MODEL
/projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2700the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2700

I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2700
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2700
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2700/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2700/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2700/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2700/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.4927, 'learning_rate': 6.8853061643828156e-06, 'epoch': 0.83}
{'loss': 0.4939, 'learning_rate': 6.860835444939456e-06, 'epoch': 0.83}
{'loss': 0.5204, 'learning_rate': 6.8364050844035245e-06, 'epoch': 0.83}
{'loss': 0.5117, 'learning_rate': 6.812015105630842e-06, 'epoch': 0.83}
{'loss': 0.5323, 'learning_rate': 6.787665531439513e-06, 'epoch': 0.83}
 83%| | 2701/3250 [8:01:13<1:42:05, 11.16s/it]                                                        83%| | 2701/3250 [8:01:13<1:42:05, 11.16s/it] 83%| | 2702/3250 [8:01:24<1:39:41, 10.92s/it]                                                        83%| | 2702/3250 [8:01:24<1:39:41, 10.92s/it] 83%| | 2703/3250 [8:01:34<1:37:57, 10.75s/it]                                                        83%| | 2703/3250 [8:01:34<1:37:57, 10.75s/it] 83%| | 2704/3250 [8:01:45<1:36:44, 10.63s/it]                                                        83%| | 2704/3250 [8:01:45<1:36:44, 10.63s/it] 83%| | 2705/3250 [8:01:55<1:36:34, 10.63s/it]                                                        83%| | 2705/3250 [8:01:55<1:36:34, 10.63s/it{'loss': 1.0296, 'learning_rate': 6.763356384609809e-06, 'epoch': 0.83}
{'loss': 0.4815, 'learning_rate': 6.739087687884188e-06, 'epoch': 0.83}
{'loss': 0.4966, 'learning_rate': 6.714859463967283e-06, 'epoch': 0.83}
{'loss': 0.5272, 'learning_rate': 6.690671735525811e-06, 'epoch': 0.83}
{'loss': 0.5279, 'learning_rate': 6.666524525188656e-06, 'epoch': 0.83}
] 83%| | 2706/3250 [8:02:05<1:35:36, 10.54s/it]                                                        83%| | 2706/3250 [8:02:05<1:35:36, 10.54s/it] 83%| | 2707/3250 [8:02:16<1:34:53, 10.49s/it]                                                        83%| | 2707/3250 [8:02:16<1:34:53, 10.49s/it] 83%| | 2708/3250 [8:02:26<1:34:20, 10.44s/it]                                                        83%| | 2708/3250 [8:02:26<1:34:20, 10.44s/it] 83%| | 2709/3250 [8:02:37<1:33:53, 10.41s/it]                                                        83%| | 2709/3250 [8:02:37<1:33:53, 10.41s/it] 83%| | 2710/3250 [8:02:47<1:33:34, 10.40s/it]                                                        83%| | 2710/3250 [8:02:47<1:33:34, 10.40s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.7862748503684998, 'eval_runtime': 2.1154, 'eval_samples_per_second': 5.673, 'eval_steps_per_second': 1.418, 'epoch': 0.83}
                                                        83%| | 2710/3250 [8:02:49<1:33:34, 10.40s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_10/tmp-checkpoint-2710
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2710I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2710

I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2710
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2710
the pytorch model path is the pytorch model path is/projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2710/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2710/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2710/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2710/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.5041, 'learning_rate': 6.642417855546768e-06, 'epoch': 0.83}
{'loss': 0.5095, 'learning_rate': 6.6183517491531844e-06, 'epoch': 0.83}
{'loss': 0.5476, 'learning_rate': 6.594326228522979e-06, 'epoch': 0.83}
{'loss': 0.5358, 'learning_rate': 6.570341316133271e-06, 'epoch': 0.84}
{'loss': 0.5116, 'learning_rate': 6.546397034423163e-06, 'epoch': 0.84}
 83%| | 2711/3250 [8:03:00<1:40:30, 11.19s/it]                                                        83%| | 2711/3250 [8:03:00<1:40:30, 11.19s/it] 83%| | 2712/3250 [8:03:10<1:38:03, 10.94s/it]                                                        83%| | 2712/3250 [8:03:10<1:38:03, 10.94s/it] 83%| | 2713/3250 [8:03:21<1:36:21, 10.77s/it]                                                        83%| | 2713/3250 [8:03:21<1:36:21, 10.77s/it] 84%| | 2714/3250 [8:03:31<1:35:02, 10.64s/it]                                                        84%| | 2714/3250 [8:03:31<1:35:02, 10.64s/it] 84%| | 2715/3250 [8:03:41<1:34:09, 10.56s/it]                                                        84%| | 2715/3250 [8:03:41<1:34:09, 10.56s/it{'loss': 0.4724, 'learning_rate': 6.522493405793778e-06, 'epoch': 0.84}
{'loss': 0.5329, 'learning_rate': 6.498630452608179e-06, 'epoch': 0.84}
{'loss': 0.5217, 'learning_rate': 6.474808197191401e-06, 'epoch': 0.84}
{'loss': 0.5213, 'learning_rate': 6.4510266618303724e-06, 'epoch': 0.84}
{'loss': 0.4986, 'learning_rate': 6.427285868773947e-06, 'epoch': 0.84}
] 84%| | 2716/3250 [8:03:52<1:33:26, 10.50s/it]                                                        84%| | 2716/3250 [8:03:52<1:33:26, 10.50s/it] 84%| | 2717/3250 [8:04:02<1:32:54, 10.46s/it]                                                        84%| | 2717/3250 [8:04:02<1:32:54, 10.46s/it] 84%| | 2718/3250 [8:04:12<1:32:27, 10.43s/it]                                                        84%| | 2718/3250 [8:04:12<1:32:27, 10.43s/it] 84%| | 2719/3250 [8:04:23<1:32:08, 10.41s/it]                                                        84%| | 2719/3250 [8:04:23<1:32:08, 10.41s/it] 84%| | 2720/3250 [8:04:33<1:31:50, 10.40s/it]                                                        84%| | 2720/3250 [8:04:33<1:31:50, 10.40s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.7854612469673157, 'eval_runtime': 2.7503, 'eval_samples_per_second': 4.363, 'eval_steps_per_second': 1.091, 'epoch': 0.84}
                                                        84%| | 2720/3250 [8:04:36<1:31:50, 10.40s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_10/tmp-checkpoint-2720
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2720
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2720
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2720
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2720
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2720/pytorch_model.bin
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2720/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2720/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2720/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.5108, 'learning_rate': 6.403585840232873e-06, 'epoch': 0.84}
{'loss': 0.5114, 'learning_rate': 6.379926598379726e-06, 'epoch': 0.84}
{'loss': 0.5494, 'learning_rate': 6.356308165348951e-06, 'epoch': 0.84}
{'loss': 0.531, 'learning_rate': 6.332730563236805e-06, 'epoch': 0.84}
{'loss': 0.5104, 'learning_rate': 6.3091938141013495e-06, 'epoch': 0.84}
 84%| | 2721/3250 [8:04:47<1:40:16, 11.37s/it]                                                        84%| | 2721/3250 [8:04:47<1:40:16, 11.37s/it] 84%| | 2722/3250 [8:04:58<1:38:26, 11.19s/it]                                                        84%| | 2722/3250 [8:04:58<1:38:26, 11.19s/it] 84%| | 2723/3250 [8:05:08<1:36:07, 10.94s/it]                                                        84%| | 2723/3250 [8:05:08<1:36:07, 10.94s/it] 84%| | 2724/3250 [8:05:18<1:34:26, 10.77s/it]                                                        84%| | 2724/3250 [8:05:18<1:34:26, 10.77s/it] 84%| | 2725/3250 [8:05:29<1:33:12, 10.65s/it]                                                        84%| | 2725/3250 [8:05:29<1:33:12, 10.65s/it{'loss': 0.5125, 'learning_rate': 6.285697939962437e-06, 'epoch': 0.84}
{'loss': 0.5287, 'learning_rate': 6.262242962801645e-06, 'epoch': 0.84}
{'loss': 0.4833, 'learning_rate': 6.238828904562316e-06, 'epoch': 0.84}
{'loss': 0.5435, 'learning_rate': 6.2154557871495156e-06, 'epoch': 0.84}
{'loss': 0.5038, 'learning_rate': 6.192123632429986e-06, 'epoch': 0.84}
] 84%| | 2726/3250 [8:05:39<1:32:18, 10.57s/it]                                                        84%| | 2726/3250 [8:05:39<1:32:18, 10.57s/it] 84%| | 2727/3250 [8:05:49<1:31:34, 10.51s/it]                                                        84%| | 2727/3250 [8:05:49<1:31:34, 10.51s/it] 84%| | 2728/3250 [8:06:00<1:31:05, 10.47s/it]                                                        84%| | 2728/3250 [8:06:00<1:31:05, 10.47s/it] 84%| | 2729/3250 [8:06:10<1:30:42, 10.45s/it]                                                        84%| | 2729/3250 [8:06:10<1:30:42, 10.45s/it] 84%| | 2730/3250 [8:06:21<1:30:21, 10.43s/it]                                                        84%| | 2730/3250 [8:06:21<1:30:21, 10.43s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.7859115600585938, 'eval_runtime': 2.1185, 'eval_samples_per_second': 5.664, 'eval_steps_per_second': 1.416, 'epoch': 0.84}
                                                        84%| | 2730/3250 [8:06:23<1:30:21, 10.43s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_10/tmp-checkpoint-2730
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2730
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2730
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2730
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2730
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2730/pytorch_model.bin
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2730/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2730/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2730/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.5085, 'learning_rate': 6.168832462232171e-06, 'epoch': 0.84}
{'loss': 0.4905, 'learning_rate': 6.145582298346153e-06, 'epoch': 0.84}
{'loss': 0.505, 'learning_rate': 6.1223731625236534e-06, 'epoch': 0.84}
{'loss': 0.5231, 'learning_rate': 6.099205076478004e-06, 'epoch': 0.84}
{'loss': 0.5404, 'learning_rate': 6.076078061884166e-06, 'epoch': 0.84}
 84%| | 2731/3250 [8:06:34<1:36:55, 11.21s/it]                                                        84%| | 2731/3250 [8:06:34<1:36:55, 11.21s/it] 84%| | 2732/3250 [8:06:44<1:34:33, 10.95s/it]                                                        84%| | 2732/3250 [8:06:44<1:34:33, 10.95s/it] 84%| | 2733/3250 [8:06:54<1:32:53, 10.78s/it]                                                        84%| | 2733/3250 [8:06:54<1:32:53, 10.78s/it] 84%| | 2734/3250 [8:07:05<1:31:42, 10.66s/it]                                                        84%| | 2734/3250 [8:07:05<1:31:42, 10.66s/it] 84%| | 2735/3250 [8:07:15<1:30:47, 10.58s/it]                                                        84%| | 2735/3250 [8:07:15<1:30:47, 10.58s/it{'loss': 0.5123, 'learning_rate': 6.052992140378627e-06, 'epoch': 0.84}
{'loss': 1.0041, 'learning_rate': 6.02994733355946e-06, 'epoch': 0.84}
{'loss': 0.5083, 'learning_rate': 6.006943662986275e-06, 'epoch': 0.84}
{'loss': 0.5327, 'learning_rate': 5.98398115018019e-06, 'epoch': 0.84}
{'loss': 0.5156, 'learning_rate': 5.961059816623799e-06, 'epoch': 0.84}
] 84%| | 2736/3250 [8:07:25<1:30:03, 10.51s/it]                                                        84%| | 2736/3250 [8:07:25<1:30:03, 10.51s/it] 84%| | 2737/3250 [8:07:36<1:29:28, 10.46s/it]                                                        84%| | 2737/3250 [8:07:36<1:29:28, 10.46s/it] 84%| | 2738/3250 [8:07:46<1:29:39, 10.51s/it]                                                        84%| | 2738/3250 [8:07:46<1:29:39, 10.51s/it] 84%| | 2739/3250 [8:07:57<1:29:10, 10.47s/it]                                                        84%| | 2739/3250 [8:07:57<1:29:10, 10.47s/it] 84%| | 2740/3250 [8:08:07<1:28:44, 10.44s/it]                                                        84%| | 2740/3250 [8:08:07<1:28:44, 10.44s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.7864505648612976, 'eval_runtime': 2.1121, 'eval_samples_per_second': 5.682, 'eval_steps_per_second': 1.42, 'epoch': 0.84}
                                                        84%| | 2740/3250 [8:08:09<1:28:44, 10.44s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_10/tmp-checkpoint-2740
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2740
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2740
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2740
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2740
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2740/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2740/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2740/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2740/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.5141, 'learning_rate': 5.9381796837612025e-06, 'epoch': 0.84}
{'loss': 0.4828, 'learning_rate': 5.91534077299794e-06, 'epoch': 0.84}
{'loss': 0.5559, 'learning_rate': 5.892543105700987e-06, 'epoch': 0.84}
{'loss': 0.533, 'learning_rate': 5.8697867031987485e-06, 'epoch': 0.84}
{'loss': 0.5069, 'learning_rate': 5.8470715867809774e-06, 'epoch': 0.84}
 84%| | 2741/3250 [8:08:20<1:35:07, 11.21s/it]                                                        84%| | 2741/3250 [8:08:20<1:35:07, 11.21s/it] 84%| | 2742/3250 [8:08:31<1:32:45, 10.96s/it]                                                        84%| | 2742/3250 [8:08:31<1:32:45, 10.96s/it] 84%| | 2743/3250 [8:08:41<1:31:04, 10.78s/it]                                                        84%| | 2743/3250 [8:08:41<1:31:04, 10.78s/it] 84%| | 2744/3250 [8:08:51<1:29:50, 10.65s/it]                                                        84%| | 2744/3250 [8:08:51<1:29:50, 10.65s/it] 84%| | 2745/3250 [8:09:02<1:28:55, 10.56s/it]                                                        84%| | 2745/3250 [8:09:02<1:28:55, 10.56s/it{'loss': 0.5084, 'learning_rate': 5.824397777698859e-06, 'epoch': 0.84}
{'loss': 0.5065, 'learning_rate': 5.801765297164891e-06, 'epoch': 0.85}
{'loss': 0.5005, 'learning_rate': 5.779174166352935e-06, 'epoch': 0.85}
{'loss': 0.5231, 'learning_rate': 5.756624406398159e-06, 'epoch': 0.85}
{'loss': 0.5028, 'learning_rate': 5.734116038397019e-06, 'epoch': 0.85}
] 84%| | 2746/3250 [8:09:12<1:28:14, 10.50s/it]                                                        84%| | 2746/3250 [8:09:12<1:28:14, 10.50s/it] 85%| | 2747/3250 [8:09:22<1:27:42, 10.46s/it]                                                        85%| | 2747/3250 [8:09:22<1:27:42, 10.46s/it] 85%| | 2748/3250 [8:09:33<1:27:17, 10.43s/it]                                                        85%| | 2748/3250 [8:09:33<1:27:17, 10.43s/it] 85%| | 2749/3250 [8:09:43<1:26:58, 10.42s/it]                                                        85%| | 2749/3250 [8:09:43<1:26:58, 10.42s/it] 85%| | 2750/3250 [8:09:53<1:26:37, 10.40s/it]                                                        85%| | 2750/3250 [8:09:53<1:26:37, 10.40s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.7859185934066772, 'eval_runtime': 2.3341, 'eval_samples_per_second': 5.141, 'eval_steps_per_second': 1.285, 'epoch': 0.85}
                                                        85%| | 2750/3250 [8:09:56<1:26:37, 10.40s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_10/tmp-checkpoint-2750
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2750
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2750
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2750
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2750
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2750/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2750/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2750/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2750/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.495, 'learning_rate': 5.711649083407256e-06, 'epoch': 0.85}
{'loss': 0.5262, 'learning_rate': 5.689223562447843e-06, 'epoch': 0.85}
{'loss': 0.5441, 'learning_rate': 5.666839496499022e-06, 'epoch': 0.85}
{'loss': 0.5028, 'learning_rate': 5.644496906502233e-06, 'epoch': 0.85}
{'loss': 0.5275, 'learning_rate': 5.622195813360126e-06, 'epoch': 0.85}
 85%| | 2751/3250 [8:10:07<1:33:22, 11.23s/it]                                                        85%| | 2751/3250 [8:10:07<1:33:22, 11.23s/it] 85%| | 2752/3250 [8:10:17<1:30:58, 10.96s/it]                                                        85%| | 2752/3250 [8:10:17<1:30:58, 10.96s/it] 85%| | 2753/3250 [8:10:27<1:29:15, 10.78s/it]                                                        85%| | 2753/3250 [8:10:27<1:29:15, 10.78s/it] 85%| | 2754/3250 [8:10:38<1:27:56, 10.64s/it]                                                        85%| | 2754/3250 [8:10:38<1:27:56, 10.64s/it] 85%| | 2755/3250 [8:10:48<1:27:59, 10.67s/it]                                                        85%| | 2755/3250 [8:10:48<1:27:59, 10.67s/it{'loss': 0.5161, 'learning_rate': 5.599936237936515e-06, 'epoch': 0.85}
{'loss': 0.5145, 'learning_rate': 5.577718201056392e-06, 'epoch': 0.85}
{'loss': 0.4796, 'learning_rate': 5.5555417235058526e-06, 'epoch': 0.85}
{'loss': 0.5475, 'learning_rate': 5.533406826032133e-06, 'epoch': 0.85}
{'loss': 0.5128, 'learning_rate': 5.5113135293435815e-06, 'epoch': 0.85}
] 85%| | 2756/3250 [8:10:59<1:27:01, 10.57s/it]                                                        85%| | 2756/3250 [8:10:59<1:27:01, 10.57s/it] 85%| | 2757/3250 [8:11:09<1:26:16, 10.50s/it]                                                        85%| | 2757/3250 [8:11:09<1:26:16, 10.50s/it] 85%| | 2758/3250 [8:11:19<1:25:41, 10.45s/it]                                                        85%| | 2758/3250 [8:11:19<1:25:41, 10.45s/it] 85%| | 2759/3250 [8:11:30<1:25:14, 10.42s/it]                                                        85%| | 2759/3250 [8:11:30<1:25:14, 10.42s/it] 85%| | 2760/3250 [8:11:40<1:24:53, 10.40s/it]                                                        85%| | 2760/3250 [8:11:40<1:24:53, 10.40s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.7866096496582031, 'eval_runtime': 2.1006, 'eval_samples_per_second': 5.713, 'eval_steps_per_second': 1.428, 'epoch': 0.85}
                                                        85%| | 2760/3250 [8:11:42<1:24:53, 10.40s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_10/tmp-checkpoint-2760
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2760
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2760
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2760
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2760
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2760/pytorch_model.bin
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2760/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2760/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2760/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.4831, 'learning_rate': 5.489261854109606e-06, 'epoch': 0.85}
{'loss': 0.4888, 'learning_rate': 5.467251820960701e-06, 'epoch': 0.85}
{'loss': 0.4951, 'learning_rate': 5.4452834504883535e-06, 'epoch': 0.85}
{'loss': 0.5295, 'learning_rate': 5.423356763245119e-06, 'epoch': 0.85}
{'loss': 0.5033, 'learning_rate': 5.401471779744538e-06, 'epoch': 0.85}
 85%| | 2761/3250 [8:11:53<1:31:06, 11.18s/it]                                                        85%| | 2761/3250 [8:11:53<1:31:06, 11.18s/it] 85%| | 2762/3250 [8:12:03<1:28:54, 10.93s/it]                                                        85%| | 2762/3250 [8:12:03<1:28:54, 10.93s/it] 85%| | 2763/3250 [8:12:14<1:27:17, 10.75s/it]                                                        85%| | 2763/3250 [8:12:14<1:27:17, 10.75s/it] 85%| | 2764/3250 [8:12:24<1:26:08, 10.63s/it]                                                        85%| | 2764/3250 [8:12:24<1:26:08, 10.63s/it] 85%| | 2765/3250 [8:12:34<1:25:17, 10.55s/it]                                                        85%| | 2765/3250 [8:12:34<1:25:17, 10.55s/it{'loss': 0.5266, 'learning_rate': 5.3796285204611495e-06, 'epoch': 0.85}
{'loss': 1.0173, 'learning_rate': 5.357827005830435e-06, 'epoch': 0.85}
{'loss': 0.5034, 'learning_rate': 5.336067256248844e-06, 'epoch': 0.85}
{'loss': 0.5224, 'learning_rate': 5.314349292073739e-06, 'epoch': 0.85}
{'loss': 0.5289, 'learning_rate': 5.292673133623371e-06, 'epoch': 0.85}
] 85%| | 2766/3250 [8:12:45<1:24:39, 10.49s/it]                                                        85%| | 2766/3250 [8:12:45<1:24:39, 10.49s/it] 85%| | 2767/3250 [8:12:55<1:24:04, 10.44s/it]                                                        85%| | 2767/3250 [8:12:55<1:24:04, 10.44s/it] 85%| | 2768/3250 [8:13:05<1:23:38, 10.41s/it]                                                        85%| | 2768/3250 [8:13:05<1:23:38, 10.41s/it] 85%| | 2769/3250 [8:13:16<1:23:17, 10.39s/it]                                                        85%| | 2769/3250 [8:13:16<1:23:17, 10.39s/it] 85%| | 2770/3250 [8:13:26<1:23:02, 10.38s/it]                                                        85%| | 2770/3250 [8:13:26<1:23:02, 10.38s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.7863126993179321, 'eval_runtime': 2.0948, 'eval_samples_per_second': 5.728, 'eval_steps_per_second': 1.432, 'epoch': 0.85}
                                                        85%| | 2770/3250 [8:13:28<1:23:02, 10.38s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_10/tmp-checkpoint-2770
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2770
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2770
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2770
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2770
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2770/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2770/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2770/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2770/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.5179, 'learning_rate': 5.271038801176919e-06, 'epoch': 0.85}
{'loss': 0.4988, 'learning_rate': 5.249446314974416e-06, 'epoch': 0.85}
{'loss': 0.5003, 'learning_rate': 5.227895695216739e-06, 'epoch': 0.85}
{'loss': 0.5716, 'learning_rate': 5.206386962065602e-06, 'epoch': 0.85}
{'loss': 0.5167, 'learning_rate': 5.184920135643539e-06, 'epoch': 0.85}
 85%| | 2771/3250 [8:13:39<1:29:37, 11.23s/it]                                                        85%| | 2771/3250 [8:13:39<1:29:37, 11.23s/it] 85%| | 2772/3250 [8:13:50<1:27:20, 10.96s/it]                                                        85%| | 2772/3250 [8:13:50<1:27:20, 10.96s/it] 85%| | 2773/3250 [8:14:00<1:25:43, 10.78s/it]                                                        85%| | 2773/3250 [8:14:00<1:25:43, 10.78s/it] 85%| | 2774/3250 [8:14:10<1:24:30, 10.65s/it]                                                        85%| | 2774/3250 [8:14:10<1:24:30, 10.65s/it] 85%| | 2775/3250 [8:14:21<1:23:34, 10.56s/it]                                                        85%| | 2775/3250 [8:14:21<1:23:34, 10.56s/it{'loss': 0.5067, 'learning_rate': 5.163495236033855e-06, 'epoch': 0.85}
{'loss': 0.4784, 'learning_rate': 5.142112283280653e-06, 'epoch': 0.85}
{'loss': 0.5257, 'learning_rate': 5.120771297388788e-06, 'epoch': 0.85}
{'loss': 0.5388, 'learning_rate': 5.099472298323843e-06, 'epoch': 0.86}
{'loss': 0.5081, 'learning_rate': 5.078215306012135e-06, 'epoch': 0.86}
] 85%| | 2776/3250 [8:14:31<1:22:52, 10.49s/it]                                                        85%| | 2776/3250 [8:14:31<1:22:52, 10.49s/it] 85%| | 2777/3250 [8:14:41<1:22:18, 10.44s/it]                                                        85%| | 2777/3250 [8:14:41<1:22:18, 10.44s/it] 85%| | 2778/3250 [8:14:52<1:21:56, 10.42s/it]                                                        85%| | 2778/3250 [8:14:52<1:21:56, 10.42s/it] 86%| | 2779/3250 [8:15:02<1:21:43, 10.41s/it]                                                        86%| | 2779/3250 [8:15:02<1:21:43, 10.41s/it] 86%| | 2780/3250 [8:15:13<1:21:24, 10.39s/it]                                                        86%| | 2780/3250 [8:15:13<1:21:24, 10.39s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.7874857187271118, 'eval_runtime': 2.337, 'eval_samples_per_second': 5.135, 'eval_steps_per_second': 1.284, 'epoch': 0.86}
                                                        86%| | 2780/3250 [8:15:15<1:21:24, 10.39s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_10/tmp-checkpoint-2780
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2780
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2780
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2780
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2780
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2780/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2780/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2780/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2780/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.5058, 'learning_rate': 5.057000340340678e-06, 'epoch': 0.86}
{'loss': 0.5128, 'learning_rate': 5.035827421157146e-06, 'epoch': 0.86}
{'loss': 0.5191, 'learning_rate': 5.014696568269905e-06, 'epoch': 0.86}
{'loss': 0.5348, 'learning_rate': 4.993607801447958e-06, 'epoch': 0.86}
{'loss': 0.5274, 'learning_rate': 4.9725611404209285e-06, 'epoch': 0.86}
 86%| | 2781/3250 [8:15:26<1:27:53, 11.24s/it]                                                        86%| | 2781/3250 [8:15:26<1:27:53, 11.24s/it] 86%| | 2782/3250 [8:15:36<1:25:40, 10.98s/it]                                                        86%| | 2782/3250 [8:15:36<1:25:40, 10.98s/it] 86%| | 2783/3250 [8:15:47<1:24:01, 10.80s/it]                                                        86%| | 2783/3250 [8:15:47<1:24:01, 10.80s/it] 86%| | 2784/3250 [8:15:57<1:22:54, 10.67s/it]                                                        86%| | 2784/3250 [8:15:57<1:22:54, 10.67s/it] 86%| | 2785/3250 [8:16:07<1:22:01, 10.58s/it]                                                        86%| | 2785/3250 [8:16:07<1:22:01, 10.58s/it{'loss': 0.5035, 'learning_rate': 4.951556604879048e-06, 'epoch': 0.86}
{'loss': 0.5357, 'learning_rate': 4.930594214473144e-06, 'epoch': 0.86}
{'loss': 0.5022, 'learning_rate': 4.909673988814601e-06, 'epoch': 0.86}
{'loss': 0.5229, 'learning_rate': 4.888795947475372e-06, 'epoch': 0.86}
{'loss': 0.5233, 'learning_rate': 4.86796010998794e-06, 'epoch': 0.86}
] 86%| | 2786/3250 [8:16:18<1:21:19, 10.52s/it]                                                        86%| | 2786/3250 [8:16:18<1:21:19, 10.52s/it] 86%| | 2787/3250 [8:16:29<1:22:12, 10.65s/it]                                                        86%| | 2787/3250 [8:16:29<1:22:12, 10.65s/it] 86%| | 2788/3250 [8:16:39<1:21:18, 10.56s/it]                                                        86%| | 2788/3250 [8:16:39<1:21:18, 10.56s/it] 86%| | 2789/3250 [8:16:49<1:20:40, 10.50s/it]                                                        86%| | 2789/3250 [8:16:49<1:20:40, 10.50s/it] 86%| | 2790/3250 [8:17:00<1:20:08, 10.45s/it]                                                        86%| | 2790/3250 [8:17:00<1:20:08, 10.45s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.7858089804649353, 'eval_runtime': 2.1045, 'eval_samples_per_second': 5.702, 'eval_steps_per_second': 1.426, 'epoch': 0.86}
                                                        86%| | 2790/3250 [8:17:02<1:20:08, 10.45s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_10/tmp-checkpoint-2790
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2790
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2790
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2790
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2790
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2790/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2790/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2790/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2790/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.508, 'learning_rate': 4.847166495845301e-06, 'epoch': 0.86}
{'loss': 0.4978, 'learning_rate': 4.826415124500955e-06, 'epoch': 0.86}
{'loss': 0.4894, 'learning_rate': 4.805706015368883e-06, 'epoch': 0.86}
{'loss': 0.5247, 'learning_rate': 4.785039187823503e-06, 'epoch': 0.86}
{'loss': 0.5157, 'learning_rate': 4.764414661199707e-06, 'epoch': 0.86}
 86%| | 2791/3250 [8:17:13<1:25:53, 11.23s/it]                                                        86%| | 2791/3250 [8:17:13<1:25:53, 11.23s/it] 86%| | 2792/3250 [8:17:23<1:23:47, 10.98s/it]                                                        86%| | 2792/3250 [8:17:23<1:23:47, 10.98s/it] 86%| | 2793/3250 [8:17:34<1:22:20, 10.81s/it]                                                        86%| | 2793/3250 [8:17:34<1:22:20, 10.81s/it] 86%| | 2794/3250 [8:17:44<1:21:10, 10.68s/it]                                                        86%| | 2794/3250 [8:17:44<1:21:10, 10.68s/it] 86%| | 2795/3250 [8:17:54<1:20:24, 10.60s/it]                                                        86%| | 2795/3250 [8:17:54<1:20:24, 10.60s/it{'loss': 0.5311, 'learning_rate': 4.743832454792796e-06, 'epoch': 0.86}
{'loss': 1.0304, 'learning_rate': 4.723292587858485e-06, 'epoch': 0.86}
{'loss': 0.4914, 'learning_rate': 4.702795079612876e-06, 'epoch': 0.86}
{'loss': 0.5017, 'learning_rate': 4.682339949232456e-06, 'epoch': 0.86}
{'loss': 0.5272, 'learning_rate': 4.661927215854028e-06, 'epoch': 0.86}
] 86%| | 2796/3250 [8:18:05<1:19:52, 10.56s/it]                                                        86%| | 2796/3250 [8:18:05<1:19:52, 10.56s/it] 86%| | 2797/3250 [8:18:15<1:19:16, 10.50s/it]                                                        86%| | 2797/3250 [8:18:15<1:19:16, 10.50s/it] 86%| | 2798/3250 [8:18:25<1:18:48, 10.46s/it]                                                        86%| | 2798/3250 [8:18:25<1:18:48, 10.46s/it] 86%| | 2799/3250 [8:18:36<1:18:27, 10.44s/it]                                                        86%| | 2799/3250 [8:18:36<1:18:27, 10.44s/it] 86%| | 2800/3250 [8:18:46<1:18:06, 10.41s/it]                                                        86%| | 2800/3250 [8:18:46<1:18:06, 10.41s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.7869482040405273, 'eval_runtime': 2.104, 'eval_samples_per_second': 5.703, 'eval_steps_per_second': 1.426, 'epoch': 0.86}
                                                        86%| | 2800/3250 [8:18:48<1:18:06, 10.41s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_10/tmp-checkpoint-2800
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2800
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2800
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2800
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2800
the pytorch model path is the pytorch model path is/projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2800/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2800/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2800/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2800/pytorch_model.bin
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.5323, 'learning_rate': 4.641556898574762e-06, 'epoch': 0.86}
{'loss': 0.5047, 'learning_rate': 4.621229016452156e-06, 'epoch': 0.86}
{'loss': 0.5017, 'learning_rate': 4.600943588503959e-06, 'epoch': 0.86}
{'loss': 0.5509, 'learning_rate': 4.580700633708246e-06, 'epoch': 0.86}
{'loss': 0.5387, 'learning_rate': 4.5605001710033454e-06, 'epoch': 0.86}
 86%| | 2801/3250 [8:19:00<1:24:27, 11.29s/it]                                                        86%| | 2801/3250 [8:19:00<1:24:27, 11.29s/it] 86%| | 2802/3250 [8:19:10<1:22:14, 11.01s/it]                                                        86%| | 2802/3250 [8:19:10<1:22:14, 11.01s/it] 86%| | 2803/3250 [8:19:20<1:20:37, 10.82s/it]                                                        86%| | 2803/3250 [8:19:20<1:20:37, 10.82s/it] 86%| | 2804/3250 [8:19:31<1:19:57, 10.76s/it]                                                        86%| | 2804/3250 [8:19:31<1:19:57, 10.76s/it] 86%| | 2805/3250 [8:19:41<1:18:54, 10.64s/it]                                                        86%| | 2805/3250 [8:19:41<1:18:54, 10.64s/it{'loss': 0.4974, 'learning_rate': 4.540342219287836e-06, 'epoch': 0.86}
{'loss': 0.473, 'learning_rate': 4.520226797420501e-06, 'epoch': 0.86}
{'loss': 0.5382, 'learning_rate': 4.500153924220357e-06, 'epoch': 0.86}
{'loss': 0.5102, 'learning_rate': 4.48012361846662e-06, 'epoch': 0.86}
{'loss': 0.5174, 'learning_rate': 4.46013589889866e-06, 'epoch': 0.86}
] 86%| | 2806/3250 [8:19:52<1:18:07, 10.56s/it]                                                        86%| | 2806/3250 [8:19:52<1:18:07, 10.56s/it] 86%| | 2807/3250 [8:20:02<1:17:34, 10.51s/it]                                                        86%| | 2807/3250 [8:20:02<1:17:34, 10.51s/it] 86%| | 2808/3250 [8:20:12<1:17:06, 10.47s/it]                                                        86%| | 2808/3250 [8:20:12<1:17:06, 10.47s/it] 86%| | 2809/3250 [8:20:23<1:16:44, 10.44s/it]                                                        86%| | 2809/3250 [8:20:23<1:16:44, 10.44s/it] 86%| | 2810/3250 [8:20:33<1:16:24, 10.42s/it]                                                        86%| | 2810/3250 [8:20:33<1:16:24, 10.42s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.7857710123062134, 'eval_runtime': 2.3395, 'eval_samples_per_second': 5.129, 'eval_steps_per_second': 1.282, 'epoch': 0.86}
                                                        86%| | 2810/3250 [8:20:35<1:16:24, 10.42s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_10/tmp-checkpoint-2810
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2810I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2810

I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2810
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2810
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2810/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2810/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2810/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2810/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.5055, 'learning_rate': 4.4401907842160306e-06, 'epoch': 0.86}
{'loss': 0.5095, 'learning_rate': 4.420288293078395e-06, 'epoch': 0.87}
{'loss': 0.5051, 'learning_rate': 4.4004284441055645e-06, 'epoch': 0.87}
{'loss': 0.542, 'learning_rate': 4.380611255877448e-06, 'epoch': 0.87}
{'loss': 0.5214, 'learning_rate': 4.360836746934055e-06, 'epoch': 0.87}
 86%| | 2811/3250 [8:20:46<1:22:22, 11.26s/it]                                                        86%| | 2811/3250 [8:20:46<1:22:22, 11.26s/it] 87%| | 2812/3250 [8:20:57<1:20:18, 11.00s/it]                                                        87%| | 2812/3250 [8:20:57<1:20:18, 11.00s/it] 87%| | 2813/3250 [8:21:07<1:18:54, 10.83s/it]                                                        87%| | 2813/3250 [8:21:07<1:18:54, 10.83s/it] 87%| | 2814/3250 [8:21:18<1:17:43, 10.70s/it]                                                        87%| | 2814/3250 [8:21:18<1:17:43, 10.70s/it] 87%| | 2815/3250 [8:21:28<1:16:47, 10.59s/it]                                                        87%| | 2815/3250 [8:21:28<1:16:47, 10.59s/it{'loss': 0.5104, 'learning_rate': 4.341104935775442e-06, 'epoch': 0.87}
{'loss': 0.5134, 'learning_rate': 4.321415840861748e-06, 'epoch': 0.87}
{'loss': 0.5309, 'learning_rate': 4.301769480613116e-06, 'epoch': 0.87}
{'loss': 0.4825, 'learning_rate': 4.282165873409743e-06, 'epoch': 0.87}
{'loss': 0.5498, 'learning_rate': 4.262605037591799e-06, 'epoch': 0.87}
] 87%| | 2816/3250 [8:21:38<1:16:08, 10.53s/it]                                                        87%| | 2816/3250 [8:21:38<1:16:08, 10.53s/it] 87%| | 2817/3250 [8:21:49<1:15:39, 10.48s/it]                                                        87%| | 2817/3250 [8:21:49<1:15:39, 10.48s/it] 87%| | 2818/3250 [8:21:59<1:15:14, 10.45s/it]                                                        87%| | 2818/3250 [8:21:59<1:15:14, 10.45s/it] 87%| | 2819/3250 [8:22:09<1:14:56, 10.43s/it]                                                        87%| | 2819/3250 [8:22:09<1:14:56, 10.43s/it] 87%| | 2820/3250 [8:22:20<1:15:37, 10.55s/it]                                                        87%| | 2820/3250 [8:22:20<1:15:37, 10.55s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.7859054803848267, 'eval_runtime': 2.1109, 'eval_samples_per_second': 5.685, 'eval_steps_per_second': 1.421, 'epoch': 0.87}
                                                        87%| | 2820/3250 [8:22:22<1:15:37, 10.55s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_10/tmp-checkpoint-2820
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2820
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2820
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2820
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2820
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2820/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2820/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2820/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2820/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.5082, 'learning_rate': 4.243086991459455e-06, 'epoch': 0.87}
{'loss': 0.5043, 'learning_rate': 4.223611753272849e-06, 'epoch': 0.87}
{'loss': 0.493, 'learning_rate': 4.2041793412520734e-06, 'epoch': 0.87}
{'loss': 0.4997, 'learning_rate': 4.1847897735771464e-06, 'epoch': 0.87}
{'loss': 0.5115, 'learning_rate': 4.165443068387998e-06, 'epoch': 0.87}
 87%| | 2821/3250 [8:22:33<1:20:37, 11.28s/it]                                                        87%| | 2821/3250 [8:22:33<1:20:37, 11.28s/it] 87%| | 2822/3250 [8:22:44<1:18:28, 11.00s/it]                                                        87%| | 2822/3250 [8:22:44<1:18:28, 11.00s/it] 87%| | 2823/3250 [8:22:54<1:16:56, 10.81s/it]                                                        87%| | 2823/3250 [8:22:54<1:16:56, 10.81s/it] 87%| | 2824/3250 [8:23:04<1:15:47, 10.68s/it]                                                        87%| | 2824/3250 [8:23:04<1:15:47, 10.68s/it] 87%| | 2825/3250 [8:23:15<1:15:00, 10.59s/it]                                                        87%| | 2825/3250 [8:23:15<1:15:00, 10.59s/it{'loss': 0.5388, 'learning_rate': 4.146139243784475e-06, 'epoch': 0.87}
{'loss': 0.5179, 'learning_rate': 4.126878317826294e-06, 'epoch': 0.87}
{'loss': 0.9917, 'learning_rate': 4.1076603085330405e-06, 'epoch': 0.87}
{'loss': 0.5058, 'learning_rate': 4.088485233884165e-06, 'epoch': 0.87}
{'loss': 0.5383, 'learning_rate': 4.069353111818913e-06, 'epoch': 0.87}
] 87%| | 2826/3250 [8:23:25<1:14:22, 10.53s/it]                                                        87%| | 2826/3250 [8:23:25<1:14:22, 10.53s/it] 87%| | 2827/3250 [8:23:35<1:13:52, 10.48s/it]                                                        87%| | 2827/3250 [8:23:35<1:13:52, 10.48s/it] 87%| | 2828/3250 [8:23:46<1:13:23, 10.44s/it]                                                        87%| | 2828/3250 [8:23:46<1:13:23, 10.44s/it] 87%| | 2829/3250 [8:23:56<1:13:02, 10.41s/it]                                                        87%| | 2829/3250 [8:23:56<1:13:02, 10.41s/it] 87%| | 2830/3250 [8:24:07<1:12:45, 10.39s/it]                                                        87%| | 2830/3250 [8:24:07<1:12:45, 10.39s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.7870228290557861, 'eval_runtime': 2.1031, 'eval_samples_per_second': 5.706, 'eval_steps_per_second': 1.426, 'epoch': 0.87}
                                                        87%| | 2830/3250 [8:24:09<1:12:45, 10.39s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_10/tmp-checkpoint-2830
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2830
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2830
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2830
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2830
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2830/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2830/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2830/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2830/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.5103, 'learning_rate': 4.050263960236384e-06, 'epoch': 0.87}
{'loss': 0.5249, 'learning_rate': 4.031217796995457e-06, 'epoch': 0.87}
{'loss': 0.5038, 'learning_rate': 4.012214639914796e-06, 'epoch': 0.87}
{'loss': 0.5571, 'learning_rate': 3.9932545067728366e-06, 'epoch': 0.87}
{'loss': 0.5388, 'learning_rate': 3.97433741530776e-06, 'epoch': 0.87}
 87%| | 2831/3250 [8:24:20<1:18:08, 11.19s/it]                                                        87%| | 2831/3250 [8:24:20<1:18:08, 11.19s/it] 87%| | 2832/3250 [8:24:30<1:16:15, 10.95s/it]                                                        87%| | 2832/3250 [8:24:30<1:16:15, 10.95s/it] 87%| | 2833/3250 [8:24:40<1:14:52, 10.77s/it]                                                        87%| | 2833/3250 [8:24:40<1:14:52, 10.77s/it] 87%| | 2834/3250 [8:24:51<1:13:50, 10.65s/it]                                                        87%| | 2834/3250 [8:24:51<1:13:50, 10.65s/it] 87%| | 2835/3250 [8:25:01<1:13:05, 10.57s/it]                                                        87%| | 2835/3250 [8:25:01<1:13:05, 10.57s/it{'loss': 0.5087, 'learning_rate': 3.955463383217478e-06, 'epoch': 0.87}
{'loss': 0.504, 'learning_rate': 3.936632428159609e-06, 'epoch': 0.87}
{'loss': 0.5099, 'learning_rate': 3.917844567751483e-06, 'epoch': 0.87}
{'loss': 0.5042, 'learning_rate': 3.899099819570112e-06, 'epoch': 0.87}
{'loss': 0.5212, 'learning_rate': 3.8803982011521685e-06, 'epoch': 0.87}
] 87%| | 2836/3250 [8:25:11<1:12:30, 10.51s/it]                                                        87%| | 2836/3250 [8:25:11<1:12:30, 10.51s/it] 87%| | 2837/3250 [8:25:23<1:13:34, 10.69s/it]                                                        87%| | 2837/3250 [8:25:23<1:13:34, 10.69s/it] 87%| | 2838/3250 [8:25:33<1:12:45, 10.60s/it]                                                        87%| | 2838/3250 [8:25:33<1:12:45, 10.60s/it] 87%| | 2839/3250 [8:25:43<1:12:11, 10.54s/it]                                                        87%| | 2839/3250 [8:25:43<1:12:11, 10.54s/it] 87%| | 2840/3250 [8:25:54<1:11:41, 10.49s/it]                                                        87%| | 2840/3250 [8:25:54<1:11:41, 10.49s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.7864604592323303, 'eval_runtime': 2.1246, 'eval_samples_per_second': 5.648, 'eval_steps_per_second': 1.412, 'epoch': 0.87}
                                                        87%| | 2840/3250 [8:25:56<1:11:41, 10.49s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_10/tmp-checkpoint-2840
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2840
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2840
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2840
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2840
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2840/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2840/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2840/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2840/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.5092, 'learning_rate': 3.861739729993991e-06, 'epoch': 0.87}
{'loss': 0.5019, 'learning_rate': 3.843124423551536e-06, 'epoch': 0.87}
{'loss': 0.5101, 'learning_rate': 3.824552299240364e-06, 'epoch': 0.87}
{'loss': 0.535, 'learning_rate': 3.8060233744356633e-06, 'epoch': 0.88}
{'loss': 0.5067, 'learning_rate': 3.7875376664722016e-06, 'epoch': 0.88}
 87%| | 2841/3250 [8:26:07<1:17:01, 11.30s/it]                                                        87%| | 2841/3250 [8:26:07<1:17:01, 11.30s/it] 87%| | 2842/3250 [8:26:17<1:14:58, 11.03s/it]                                                        87%| | 2842/3250 [8:26:17<1:14:58, 11.03s/it] 87%| | 2843/3250 [8:26:28<1:13:28, 10.83s/it]                                                        87%| | 2843/3250 [8:26:28<1:13:28, 10.83s/it] 88%| | 2844/3250 [8:26:38<1:12:25, 10.70s/it]                                                        88%| | 2844/3250 [8:26:38<1:12:25, 10.70s/it] 88%| | 2845/3250 [8:26:48<1:11:34, 10.60s/it]                                                        88%| | 2845/3250 [8:26:48<1:11:34, 10.60s/it{'loss': 0.513, 'learning_rate': 3.7690951926443007e-06, 'epoch': 0.88}
{'loss': 0.5194, 'learning_rate': 3.750695970205853e-06, 'epoch': 0.88}
{'loss': 0.5122, 'learning_rate': 3.732340016370267e-06, 'epoch': 0.88}
{'loss': 0.4884, 'learning_rate': 3.7140273483104838e-06, 'epoch': 0.88}
{'loss': 0.554, 'learning_rate': 3.6957579831589538e-06, 'epoch': 0.88}
] 88%| | 2846/3250 [8:26:59<1:11:25, 10.61s/it]                                                        88%| | 2846/3250 [8:26:59<1:11:25, 10.61s/it] 88%| | 2847/3250 [8:27:09<1:10:46, 10.54s/it]                                                        88%| | 2847/3250 [8:27:09<1:10:46, 10.54s/it] 88%| | 2848/3250 [8:27:20<1:10:14, 10.48s/it]                                                        88%| | 2848/3250 [8:27:20<1:10:14, 10.48s/it] 88%| | 2849/3250 [8:27:30<1:09:48, 10.45s/it]                                                        88%| | 2849/3250 [8:27:30<1:09:48, 10.45s/it] 88%| | 2850/3250 [8:27:40<1:09:28, 10.42s/it]                                                        88%| | 2850/3250 [8:27:40<1:09:28, 10.42s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.786618173122406, 'eval_runtime': 2.1084, 'eval_samples_per_second': 5.692, 'eval_steps_per_second': 1.423, 'epoch': 0.88}
                                                        88%| | 2850/3250 [8:27:43<1:09:28, 10.42s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_10/tmp-checkpoint-2850
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2850
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2850
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2850
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2850
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2850/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2850/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2850/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2850/pytorch_model.bin
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.5196, 'learning_rate': 3.6775319380076e-06, 'epoch': 0.88}
{'loss': 0.4956, 'learning_rate': 3.659349229907827e-06, 'epoch': 0.88}
{'loss': 0.498, 'learning_rate': 3.641209875870505e-06, 'epoch': 0.88}
{'loss': 0.4933, 'learning_rate': 3.62311389286592e-06, 'epoch': 0.88}
{'loss': 0.5269, 'learning_rate': 3.6050612978237862e-06, 'epoch': 0.88}
 88%| | 2851/3250 [8:27:54<1:14:39, 11.23s/it]                                                        88%| | 2851/3250 [8:27:54<1:14:39, 11.23s/it] 88%| | 2852/3250 [8:28:04<1:12:45, 10.97s/it]                                                        88%| | 2852/3250 [8:28:04<1:12:45, 10.97s/it] 88%| | 2853/3250 [8:28:15<1:11:56, 10.87s/it]                                                        88%| | 2853/3250 [8:28:15<1:11:56, 10.87s/it] 88%| | 2854/3250 [8:28:25<1:10:44, 10.72s/it]                                                        88%| | 2854/3250 [8:28:25<1:10:44, 10.72s/it] 88%| | 2855/3250 [8:28:35<1:09:52, 10.61s/it]                                                        88%| | 2855/3250 [8:28:35<1:09:52, 10.61s/it{'loss': 0.5004, 'learning_rate': 3.5870521076332486e-06, 'epoch': 0.88}
{'loss': 0.5232, 'learning_rate': 3.5690863391428296e-06, 'epoch': 0.88}
{'loss': 1.0131, 'learning_rate': 3.551164009160429e-06, 'epoch': 0.88}
{'loss': 0.5019, 'learning_rate': 3.533285134453307e-06, 'epoch': 0.88}
{'loss': 0.5098, 'learning_rate': 3.5154497317480774e-06, 'epoch': 0.88}
] 88%| | 2856/3250 [8:28:46<1:09:14, 10.54s/it]                                                        88%| | 2856/3250 [8:28:46<1:09:14, 10.54s/it] 88%| | 2857/3250 [8:28:56<1:08:42, 10.49s/it]                                                        88%| | 2857/3250 [8:28:56<1:08:42, 10.49s/it] 88%| | 2858/3250 [8:29:06<1:08:15, 10.45s/it]                                                        88%| | 2858/3250 [8:29:06<1:08:15, 10.45s/it] 88%| | 2859/3250 [8:29:17<1:07:56, 10.43s/it]                                                        88%| | 2859/3250 [8:29:17<1:07:56, 10.43s/it] 88%| | 2860/3250 [8:29:27<1:07:39, 10.41s/it]                                                        88%| | 2860/3250 [8:29:27<1:07:39, 10.41s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.786119818687439, 'eval_runtime': 2.11, 'eval_samples_per_second': 5.687, 'eval_steps_per_second': 1.422, 'epoch': 0.88}
                                                        88%| | 2860/3250 [8:29:29<1:07:39, 10.41s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_10/tmp-checkpoint-2860
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in   /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2860
/projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2860
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2860
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2860
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2860/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2860/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2860/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2860/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.5382, 'learning_rate': 3.497657817730665e-06, 'epoch': 0.88}
{'loss': 0.5238, 'learning_rate': 3.4799094090463226e-06, 'epoch': 0.88}
{'loss': 0.4913, 'learning_rate': 3.462204522299606e-06, 'epoch': 0.88}
{'loss': 0.4952, 'learning_rate': 3.4445431740543434e-06, 'epoch': 0.88}
{'loss': 0.5668, 'learning_rate': 3.4269253808336455e-06, 'epoch': 0.88}
 88%| | 2861/3250 [8:29:40<1:12:29, 11.18s/it]                                                        88%| | 2861/3250 [8:29:40<1:12:29, 11.18s/it] 88%| | 2862/3250 [8:29:51<1:10:43, 10.94s/it]                                                        88%| | 2862/3250 [8:29:51<1:10:43, 10.94s/it] 88%| | 2863/3250 [8:30:01<1:09:23, 10.76s/it]                                                        88%| | 2863/3250 [8:30:01<1:09:23, 10.76s/it] 88%| | 2864/3250 [8:30:11<1:08:25, 10.64s/it]                                                        88%| | 2864/3250 [8:30:11<1:08:25, 10.64s/it] 88%| | 2865/3250 [8:30:22<1:07:42, 10.55s/it]                                                        88%| | 2865/3250 [8:30:22<1:07:42, 10.55s/it{'loss': 0.5118, 'learning_rate': 3.4093511591198445e-06, 'epoch': 0.88}
{'loss': 0.5113, 'learning_rate': 3.391820525354539e-06, 'epoch': 0.88}
{'loss': 0.4762, 'learning_rate': 3.374333495938542e-06, 'epoch': 0.88}
{'loss': 0.522, 'learning_rate': 3.3568900872318564e-06, 'epoch': 0.88}
{'loss': 0.5252, 'learning_rate': 3.3394903155537115e-06, 'epoch': 0.88}
] 88%| | 2866/3250 [8:30:32<1:07:09, 10.49s/it]                                                        88%| | 2866/3250 [8:30:32<1:07:09, 10.49s/it] 88%| | 2867/3250 [8:30:42<1:06:44, 10.46s/it]                                                        88%| | 2867/3250 [8:30:42<1:06:44, 10.46s/it] 88%| | 2868/3250 [8:30:53<1:06:21, 10.42s/it]                                                        88%| | 2868/3250 [8:30:53<1:06:21, 10.42s/it] 88%| | 2869/3250 [8:31:03<1:06:47, 10.52s/it]                                                        88%| | 2869/3250 [8:31:03<1:06:47, 10.52s/it] 88%| | 2870/3250 [8:31:14<1:06:18, 10.47s/it]                                                        88%| | 2870/3250 [8:31:14<1:06:18, 10.47s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.7861202359199524, 'eval_runtime': 2.1241, 'eval_samples_per_second': 5.65, 'eval_steps_per_second': 1.412, 'epoch': 0.88}
                                                        88%| | 2870/3250 [8:31:16<1:06:18, 10.47s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_10/tmp-checkpoint-2870
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2870
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2870
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2870
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2870
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2870/pytorch_model.bin
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2870/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2870/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2870/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.5031, 'learning_rate': 3.322134197182464e-06, 'epoch': 0.88}
{'loss': 0.4932, 'learning_rate': 3.3048217483556744e-06, 'epoch': 0.88}
{'loss': 0.5251, 'learning_rate': 3.2875529852700147e-06, 'epoch': 0.88}
{'loss': 0.522, 'learning_rate': 3.270327924081301e-06, 'epoch': 0.88}
{'loss': 0.5406, 'learning_rate': 3.253146580904476e-06, 'epoch': 0.88}
 88%| | 2871/3250 [8:31:27<1:11:02, 11.25s/it]                                                        88%| | 2871/3250 [8:31:27<1:11:02, 11.25s/it] 88%| | 2872/3250 [8:31:37<1:09:09, 10.98s/it]                                                        88%| | 2872/3250 [8:31:37<1:09:09, 10.98s/it] 88%| | 2873/3250 [8:31:48<1:07:49, 10.80s/it]                                                        88%| | 2873/3250 [8:31:48<1:07:49, 10.80s/it] 88%| | 2874/3250 [8:31:58<1:06:50, 10.67s/it]                                                        88%| | 2874/3250 [8:31:58<1:06:50, 10.67s/it] 88%| | 2875/3250 [8:32:08<1:06:02, 10.57s/it]                                                        88%| | 2875/3250 [8:32:08<1:06:02, 10.57s/it{'loss': 0.5306, 'learning_rate': 3.2360089718135587e-06, 'epoch': 0.88}
{'loss': 0.5099, 'learning_rate': 3.2189151128416695e-06, 'epoch': 0.89}
{'loss': 0.5323, 'learning_rate': 3.201865019981004e-06, 'epoch': 0.89}
{'loss': 0.5024, 'learning_rate': 3.184858709182775e-06, 'epoch': 0.89}
{'loss': 0.5267, 'learning_rate': 3.167896196357284e-06, 'epoch': 0.89}
] 88%| | 2876/3250 [8:32:19<1:05:27, 10.50s/it]                                                        88%| | 2876/3250 [8:32:19<1:05:27, 10.50s/it] 89%| | 2877/3250 [8:32:29<1:05:01, 10.46s/it]                                                        89%| | 2877/3250 [8:32:29<1:05:01, 10.46s/it] 89%| | 2878/3250 [8:32:39<1:04:41, 10.43s/it]                                                        89%| | 2878/3250 [8:32:39<1:04:41, 10.43s/it] 89%| | 2879/3250 [8:32:50<1:04:23, 10.41s/it]                                                        89%| | 2879/3250 [8:32:50<1:04:23, 10.41s/it] 89%| | 2880/3250 [8:33:00<1:04:08, 10.40s/it]                                                        89%| | 2880/3250 [8:33:00<1:04:08, 10.40s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.7855393290519714, 'eval_runtime': 2.3435, 'eval_samples_per_second': 5.12, 'eval_steps_per_second': 1.28, 'epoch': 0.89}
                                                        89%| | 2880/3250 [8:33:02<1:04:08, 10.40s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_10/tmp-checkpoint-2880
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2880
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2880
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2880
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2880
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2880/pytorch_model.bin
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2880/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2880/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2880/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.5225, 'learning_rate': 3.1509774973738193e-06, 'epoch': 0.89}
{'loss': 0.4961, 'learning_rate': 3.134102628060698e-06, 'epoch': 0.89}
{'loss': 0.5031, 'learning_rate': 3.117271604205241e-06, 'epoch': 0.89}
{'loss': 0.4835, 'learning_rate': 3.1004844415537194e-06, 'epoch': 0.89}
{'loss': 0.5284, 'learning_rate': 3.0837411558113984e-06, 'epoch': 0.89}
 89%| | 2881/3250 [8:33:13<1:09:18, 11.27s/it]                                                        89%| | 2881/3250 [8:33:13<1:09:18, 11.27s/it] 89%| | 2882/3250 [8:33:24<1:07:25, 10.99s/it]                                                        89%| | 2882/3250 [8:33:24<1:07:25, 10.99s/it] 89%| | 2883/3250 [8:33:34<1:06:05, 10.80s/it]                                                        89%| | 2883/3250 [8:33:34<1:06:05, 10.80s/it] 89%| | 2884/3250 [8:33:44<1:05:05, 10.67s/it]                                                        89%| | 2884/3250 [8:33:44<1:05:05, 10.67s/it] 89%| | 2885/3250 [8:33:55<1:04:20, 10.58s/it]                                                        89%| | 2885/3250 [8:33:55<1:04:20, 10.58s/it{'loss': 0.5131, 'learning_rate': 3.067041762642475e-06, 'epoch': 0.89}
{'loss': 0.5272, 'learning_rate': 3.050386277670103e-06, 'epoch': 0.89}
{'loss': 1.0268, 'learning_rate': 3.033774716476329e-06, 'epoch': 0.89}
{'loss': 0.4871, 'learning_rate': 3.017207094602126e-06, 'epoch': 0.89}
{'loss': 0.4831, 'learning_rate': 3.000683427547374e-06, 'epoch': 0.89}
] 89%| | 2886/3250 [8:34:05<1:04:15, 10.59s/it]                                                        89%| | 2886/3250 [8:34:05<1:04:15, 10.59s/it] 89%| | 2887/3250 [8:34:16<1:03:40, 10.52s/it]                                                        89%| | 2887/3250 [8:34:16<1:03:40, 10.52s/it] 89%| | 2888/3250 [8:34:26<1:03:12, 10.48s/it]                                                        89%| | 2888/3250 [8:34:26<1:03:12, 10.48s/it] 89%| | 2889/3250 [8:34:37<1:02:50, 10.44s/it]                                                        89%| | 2889/3250 [8:34:37<1:02:50, 10.44s/it] 89%| | 2890/3250 [8:34:47<1:02:31, 10.42s/it]                                                        89%| | 2890/3250 [8:34:47<1:02:31, 10.42s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.7853160500526428, 'eval_runtime': 2.1172, 'eval_samples_per_second': 5.668, 'eval_steps_per_second': 1.417, 'epoch': 0.89}
                                                        89%| | 2890/3250 [8:34:49<1:02:31, 10.42s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_10/tmp-checkpoint-2890
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2890
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2890
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2890
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2890
the pytorch model path isthe pytorch model path is  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2890/pytorch_model.bin/projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2890/pytorch_model.bin

the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2890/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2890/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.5269, 'learning_rate': 2.9842037307707906e-06, 'epoch': 0.89}
{'loss': 0.5316, 'learning_rate': 2.9677680196899925e-06, 'epoch': 0.89}
{'loss': 0.4903, 'learning_rate': 2.9513763096814305e-06, 'epoch': 0.89}
{'loss': 0.4989, 'learning_rate': 2.935028616080393e-06, 'epoch': 0.89}
{'loss': 0.5616, 'learning_rate': 2.918724954180985e-06, 'epoch': 0.89}
 89%| | 2891/3250 [8:35:00<1:06:59, 11.20s/it]                                                        89%| | 2891/3250 [8:35:00<1:06:59, 11.20s/it] 89%| | 2892/3250 [8:35:10<1:05:18, 10.95s/it]                                                        89%| | 2892/3250 [8:35:10<1:05:18, 10.95s/it] 89%| | 2893/3250 [8:35:21<1:04:05, 10.77s/it]                                                        89%| | 2893/3250 [8:35:21<1:04:05, 10.77s/it] 89%| | 2894/3250 [8:35:31<1:03:10, 10.65s/it]                                                        89%| | 2894/3250 [8:35:31<1:03:10, 10.65s/it] 89%| | 2895/3250 [8:35:41<1:02:28, 10.56s/it]                                                        89%| | 2895/3250 [8:35:41<1:02:28, 10.56s/it{'loss': 0.5383, 'learning_rate': 2.9024653392361324e-06, 'epoch': 0.89}
{'loss': 0.4974, 'learning_rate': 2.886249786457523e-06, 'epoch': 0.89}
{'loss': 0.4756, 'learning_rate': 2.8700783110156503e-06, 'epoch': 0.89}
{'loss': 0.5254, 'learning_rate': 2.8539509280397614e-06, 'epoch': 0.89}
{'loss': 0.5058, 'learning_rate': 2.8378676526178482e-06, 'epoch': 0.89}
] 89%| | 2896/3250 [8:35:52<1:01:54, 10.49s/it]                                                        89%| | 2896/3250 [8:35:52<1:01:54, 10.49s/it] 89%| | 2897/3250 [8:36:02<1:01:30, 10.46s/it]                                                        89%| | 2897/3250 [8:36:02<1:01:30, 10.46s/it] 89%| | 2898/3250 [8:36:12<1:01:10, 10.43s/it]                                                        89%| | 2898/3250 [8:36:12<1:01:10, 10.43s/it] 89%| | 2899/3250 [8:36:23<1:00:52, 10.41s/it]                                                        89%| | 2899/3250 [8:36:23<1:00:52, 10.41s/it] 89%| | 2900/3250 [8:36:33<1:00:37, 10.39s/it]                                                        89%| | 2900/3250 [8:36:33<1:00:37, 10.39s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.7851458787918091, 'eval_runtime': 2.1132, 'eval_samples_per_second': 5.679, 'eval_steps_per_second': 1.42, 'epoch': 0.89}
                                                        89%| | 2900/3250 [8:36:35<1:00:37, 10.39s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_10/tmp-checkpoint-2900
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2900
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2900
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2900
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2900
the pytorch model path isthe pytorch model path is  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2900/pytorch_model.bin/projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2900/pytorch_model.bin

the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2900/pytorch_model.binthe pytorch model path is
 /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2900/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.5103, 'learning_rate': 2.8218284997966527e-06, 'epoch': 0.89}
{'loss': 0.4967, 'learning_rate': 2.8058334845816213e-06, 'epoch': 0.89}
{'loss': 0.511, 'learning_rate': 2.789882621936912e-06, 'epoch': 0.89}
{'loss': 0.5112, 'learning_rate': 2.7739759267853827e-06, 'epoch': 0.89}
{'loss': 0.5401, 'learning_rate': 2.758113414008551e-06, 'epoch': 0.89}
 89%| | 2901/3250 [8:36:46<1:04:56, 11.17s/it]                                                        89%| | 2901/3250 [8:36:46<1:04:56, 11.17s/it] 89%| | 2902/3250 [8:36:57<1:04:02, 11.04s/it]                                                        89%| | 2902/3250 [8:36:57<1:04:02, 11.04s/it] 89%| | 2903/3250 [8:37:07<1:02:42, 10.84s/it]                                                        89%| | 2903/3250 [8:37:07<1:02:42, 10.84s/it] 89%| | 2904/3250 [8:37:18<1:01:43, 10.70s/it]                                                        89%| | 2904/3250 [8:37:18<1:01:43, 10.70s/it] 89%| | 2905/3250 [8:37:28<1:00:58, 10.61s/it]                                                        89%| | 2905/3250 [8:37:28<1:00:58, 10.61s/it{'loss': 0.5215, 'learning_rate': 2.7422950984466233e-06, 'epoch': 0.89}
{'loss': 0.5094, 'learning_rate': 2.7265209948984514e-06, 'epoch': 0.89}
{'loss': 0.514, 'learning_rate': 2.7107911181215197e-06, 'epoch': 0.89}
{'loss': 0.5255, 'learning_rate': 2.695105482831928e-06, 'epoch': 0.9}
{'loss': 0.4823, 'learning_rate': 2.6794641037043988e-06, 'epoch': 0.9}
] 89%| | 2906/3250 [8:37:38<1:00:23, 10.53s/it]                                                        89%| | 2906/3250 [8:37:38<1:00:23, 10.53s/it] 89%| | 2907/3250 [8:37:49<59:57, 10.49s/it]                                                        89%| | 2907/3250 [8:37:49<59:57, 10.49s/it] 89%| | 2908/3250 [8:37:59<59:36, 10.46s/it]                                                      89%| | 2908/3250 [8:37:59<59:36, 10.46s/it] 90%| | 2909/3250 [8:38:09<59:19, 10.44s/it]                                                      90%| | 2909/3250 [8:38:09<59:19, 10.44s/it] 90%| | 2910/3250 [8:38:20<59:03, 10.42s/it]                                                      90%| | 2910/3250 [8:38:20<59:03, 10.42s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.7856692671775818, 'eval_runtime': 2.3565, 'eval_samples_per_second': 5.092, 'eval_steps_per_second': 1.273, 'epoch': 0.9}
                                                      90%| | 2910/3250 [8:38:22<59:03, 10.42s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_10/tmp-checkpoint-2910
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2910
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2910
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2910
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2910
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2910/pytorch_model.bin
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2910/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2910/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2910/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.5439, 'learning_rate': 2.6638669953722496e-06, 'epoch': 0.9}
{'loss': 0.4965, 'learning_rate': 2.648314172427374e-06, 'epoch': 0.9}
{'loss': 0.505, 'learning_rate': 2.6328056494202445e-06, 'epoch': 0.9}
{'loss': 0.4944, 'learning_rate': 2.6173414408598827e-06, 'epoch': 0.9}
{'loss': 0.495, 'learning_rate': 2.6019215612138383e-06, 'epoch': 0.9}
 90%| | 2911/3250 [8:38:33<1:03:42, 11.28s/it]                                                        90%| | 2911/3250 [8:38:33<1:03:42, 11.28s/it] 90%| | 2912/3250 [8:38:44<1:01:59, 11.01s/it]                                                        90%| | 2912/3250 [8:38:44<1:01:59, 11.01s/it] 90%| | 2913/3250 [8:38:54<1:00:44, 10.82s/it]                                                        90%| | 2913/3250 [8:38:54<1:00:44, 10.82s/it] 90%| | 2914/3250 [8:39:04<59:52, 10.69s/it]                                                        90%| | 2914/3250 [8:39:04<59:52, 10.69s/it] 90%| | 2915/3250 [8:39:15<59:10, 10.60s/it]                                                      90%| | 2915/3250 [8:39:15<59:10, 10.60s/it] 90%|{'loss': 0.5165, 'learning_rate': 2.5865460249082097e-06, 'epoch': 0.9}
{'loss': 0.5361, 'learning_rate': 2.571214846327602e-06, 'epoch': 0.9}
{'loss': 0.5091, 'learning_rate': 2.5559280398151253e-06, 'epoch': 0.9}
{'loss': 1.0044, 'learning_rate': 2.5406856196723672e-06, 'epoch': 0.9}
{'loss': 0.5101, 'learning_rate': 2.525487600159404e-06, 'epoch': 0.9}
 | 2916/3250 [8:39:25<58:37, 10.53s/it]                                                      90%| | 2916/3250 [8:39:25<58:37, 10.53s/it] 90%| | 2917/3250 [8:39:35<58:13, 10.49s/it]                                                      90%| | 2917/3250 [8:39:35<58:13, 10.49s/it] 90%| | 2918/3250 [8:39:46<58:18, 10.54s/it]                                                      90%| | 2918/3250 [8:39:46<58:18, 10.54s/it] 90%| | 2919/3250 [8:39:56<57:50, 10.48s/it]                                                      90%| | 2919/3250 [8:39:56<57:50, 10.48s/it] 90%| | 2920/3250 [8:40:07<57:31, 10.46s/it]                                                      90%| | 2920/3250 [8:40:07<57:31, 10.46s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.7869763374328613, 'eval_runtime': 2.1076, 'eval_samples_per_second': 5.694, 'eval_steps_per_second': 1.423, 'epoch': 0.9}
                                                      90%| | 2920/3250 [8:40:09<57:31, 10.46s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_10/tmp-checkpoint-2920
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2920
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2920
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2920
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2920
the pytorch model path isthe pytorch model path is the pytorch model path is/projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2920/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2920/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2920/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2920/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.5234, 'learning_rate': 2.5103339954947626e-06, 'epoch': 0.9}
{'loss': 0.5153, 'learning_rate': 2.4952248198554073e-06, 'epoch': 0.9}
{'loss': 0.5212, 'learning_rate': 2.480160087376754e-06, 'epoch': 0.9}
{'loss': 0.4837, 'learning_rate': 2.465139812152639e-06, 'epoch': 0.9}
{'loss': 0.5588, 'learning_rate': 2.450164008235306e-06, 'epoch': 0.9}
 90%| | 2921/3250 [8:40:20<1:01:30, 11.22s/it]                                                        90%| | 2921/3250 [8:40:20<1:01:30, 11.22s/it] 90%| | 2922/3250 [8:40:30<59:55, 10.96s/it]                                                        90%| | 2922/3250 [8:40:30<59:55, 10.96s/it] 90%| | 2923/3250 [8:40:41<58:46, 10.79s/it]                                                      90%| | 2923/3250 [8:40:41<58:46, 10.79s/it] 90%| | 2924/3250 [8:40:51<57:54, 10.66s/it]                                                      90%| | 2924/3250 [8:40:51<57:54, 10.66s/it] 90%| | 2925/3250 [8:41:01<57:15, 10.57s/it]                                                      90%| | 2925/3250 [8:41:01<57:15, 10.57s/it] 90%|{'loss': 0.5398, 'learning_rate': 2.435232689635386e-06, 'epoch': 0.9}
{'loss': 0.5026, 'learning_rate': 2.4203458703218997e-06, 'epoch': 0.9}
{'loss': 0.5009, 'learning_rate': 2.4055035642222224e-06, 'epoch': 0.9}
{'loss': 0.5113, 'learning_rate': 2.390705785222097e-06, 'epoch': 0.9}
{'loss': 0.5026, 'learning_rate': 2.3759525471656162e-06, 'epoch': 0.9}
 | 2926/3250 [8:41:12<56:44, 10.51s/it]                                                      90%| | 2926/3250 [8:41:12<56:44, 10.51s/it] 90%| | 2927/3250 [8:41:22<56:20, 10.47s/it]                                                      90%| | 2927/3250 [8:41:22<56:20, 10.47s/it] 90%| | 2928/3250 [8:41:32<56:01, 10.44s/it]                                                      90%| | 2928/3250 [8:41:32<56:01, 10.44s/it] 90%| | 2929/3250 [8:41:43<55:44, 10.42s/it]                                                      90%| | 2929/3250 [8:41:43<55:44, 10.42s/it] 90%| | 2930/3250 [8:41:53<55:31, 10.41s/it]                                                      90%| | 2930/3250 [8:41:53<55:31, 10.41s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.7849428653717041, 'eval_runtime': 2.1196, 'eval_samples_per_second': 5.661, 'eval_steps_per_second': 1.415, 'epoch': 0.9}
                                                      90%| | 2930/3250 [8:41:55<55:31, 10.41s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_10/tmp-checkpoint-2930
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2930
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2930
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2930
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2930
the pytorch model path isthe pytorch model path is the pytorch model path is/projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2930/pytorch_model.bin 
/projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2930/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2930/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2930/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.517, 'learning_rate': 2.361243863855184e-06, 'epoch': 0.9}
{'loss': 0.5035, 'learning_rate': 2.3465797490515418e-06, 'epoch': 0.9}
{'loss': 0.4962, 'learning_rate': 2.3319602164737053e-06, 'epoch': 0.9}
{'loss': 0.5131, 'learning_rate': 2.3173852797990114e-06, 'epoch': 0.9}
{'loss': 0.5264, 'learning_rate': 2.3028549526630583e-06, 'epoch': 0.9}
 90%| | 2931/3250 [8:42:06<59:25, 11.18s/it]                                                      90%| | 2931/3250 [8:42:06<59:25, 11.18s/it] 90%| | 2932/3250 [8:42:17<57:58, 10.94s/it]                                                      90%| | 2932/3250 [8:42:17<57:58, 10.94s/it] 90%| | 2933/3250 [8:42:27<56:53, 10.77s/it]                                                      90%| | 2933/3250 [8:42:27<56:53, 10.77s/it] 90%| | 2934/3250 [8:42:37<56:06, 10.65s/it]                                                      90%| | 2934/3250 [8:42:37<56:06, 10.65s/it] 90%| | 2935/3250 [8:42:48<56:03, 10.68s/it]                                                      90%| | 2935/3250 [8:42:48<56:03, 10.68s/it] 90%|{'loss': 0.5078, 'learning_rate': 2.288369248659722e-06, 'epoch': 0.9}
{'loss': 0.5087, 'learning_rate': 2.2739281813411116e-06, 'epoch': 0.9}
{'loss': 0.5157, 'learning_rate': 2.2595317642176038e-06, 'epoch': 0.9}
{'loss': 0.5282, 'learning_rate': 2.2451800107577805e-06, 'epoch': 0.9}
{'loss': 0.4812, 'learning_rate': 2.2308729343884395e-06, 'epoch': 0.9}
 | 2936/3250 [8:42:58<55:25, 10.59s/it]                                                      90%| | 2936/3250 [8:42:58<55:25, 10.59s/it] 90%| | 2937/3250 [8:43:09<54:55, 10.53s/it]                                                      90%| | 2937/3250 [8:43:09<54:55, 10.53s/it] 90%| | 2938/3250 [8:43:19<54:31, 10.49s/it]                                                      90%| | 2938/3250 [8:43:19<54:31, 10.49s/it] 90%| | 2939/3250 [8:43:30<54:10, 10.45s/it]                                                      90%| | 2939/3250 [8:43:30<54:10, 10.45s/it] 90%| | 2940/3250 [8:43:40<53:54, 10.43s/it]                                                      90%| | 2940/3250 [8:43:40<53:54, 10.43s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.7856655716896057, 'eval_runtime': 2.1216, 'eval_samples_per_second': 5.656, 'eval_steps_per_second': 1.414, 'epoch': 0.9}
                                                      90%| | 2940/3250 [8:43:42<53:54, 10.43s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_10/tmp-checkpoint-2940
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2940I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2940

I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2940
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2940
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2940/pytorch_model.bin
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2940/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2940/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2940/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.5509, 'learning_rate': 2.2166105484945853e-06, 'epoch': 0.9}
{'loss': 0.5088, 'learning_rate': 2.202392866419423e-06, 'epoch': 0.91}
{'loss': 0.4891, 'learning_rate': 2.188219901464317e-06, 'epoch': 0.91}
{'loss': 0.4943, 'learning_rate': 2.1740916668888113e-06, 'epoch': 0.91}
{'loss': 0.4896, 'learning_rate': 2.160008175910605e-06, 'epoch': 0.91}
 90%| | 2941/3250 [8:43:53<57:42, 11.21s/it]                                                      90%| | 2941/3250 [8:43:53<57:42, 11.21s/it] 91%| | 2942/3250 [8:44:03<56:14, 10.96s/it]                                                      91%| | 2942/3250 [8:44:03<56:14, 10.96s/it] 91%| | 2943/3250 [8:44:14<55:09, 10.78s/it]                                                      91%| | 2943/3250 [8:44:14<55:09, 10.78s/it] 91%| | 2944/3250 [8:44:24<54:21, 10.66s/it]                                                      91%| | 2944/3250 [8:44:24<54:21, 10.66s/it] 91%| | 2945/3250 [8:44:34<53:44, 10.57s/it]                                                      91%| | 2945/3250 [8:44:34<53:44, 10.57s/it] 91%|{'loss': 0.5269, 'learning_rate': 2.1459694417055034e-06, 'epoch': 0.91}
{'loss': 0.4999, 'learning_rate': 2.131975477407483e-06, 'epoch': 0.91}
{'loss': 0.5263, 'learning_rate': 2.1180262961086108e-06, 'epoch': 0.91}
{'loss': 1.012, 'learning_rate': 2.1041219108590692e-06, 'epoch': 0.91}
{'loss': 0.4966, 'learning_rate': 2.0902623346671258e-06, 'epoch': 0.91}
 | 2946/3250 [8:44:45<53:17, 10.52s/it]                                                      91%| | 2946/3250 [8:44:45<53:17, 10.52s/it] 91%| | 2947/3250 [8:44:55<52:54, 10.48s/it]                                                      91%| | 2947/3250 [8:44:55<52:54, 10.48s/it] 91%| | 2948/3250 [8:45:06<52:35, 10.45s/it]                                                      91%| | 2948/3250 [8:45:06<52:35, 10.45s/it] 91%| | 2949/3250 [8:45:16<52:15, 10.42s/it]                                                      91%| | 2949/3250 [8:45:16<52:15, 10.42s/it] 91%| | 2950/3250 [8:45:26<52:01, 10.41s/it]                                                      91%| | 2950/3250 [8:45:26<52:01, 10.41s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.7858569622039795, 'eval_runtime': 2.3431, 'eval_samples_per_second': 5.122, 'eval_steps_per_second': 1.28, 'epoch': 0.91}
                                                      91%| | 2950/3250 [8:45:29<52:01, 10.41s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_10/tmp-checkpoint-2950
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2950
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2950
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2950
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2950
the pytorch model path is the pytorch model path is/projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2950/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2950/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2950/pytorch_model.binthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2950/pytorch_model.bin

/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.5123, 'learning_rate': 2.076447580499119e-06, 'epoch': 0.91}
{'loss': 0.5387, 'learning_rate': 2.0626776612794607e-06, 'epoch': 0.91}
{'loss': 0.5275, 'learning_rate': 2.0489525898906294e-06, 'epoch': 0.91}
{'loss': 0.495, 'learning_rate': 2.0352723791731366e-06, 'epoch': 0.91}
{'loss': 0.501, 'learning_rate': 2.021637041925506e-06, 'epoch': 0.91}
 91%| | 2951/3250 [8:45:40<56:27, 11.33s/it]                                                      91%| | 2951/3250 [8:45:40<56:27, 11.33s/it] 91%| | 2952/3250 [8:45:50<54:49, 11.04s/it]                                                      91%| | 2952/3250 [8:45:50<54:49, 11.04s/it] 91%| | 2953/3250 [8:46:01<53:38, 10.84s/it]                                                      91%| | 2953/3250 [8:46:01<53:38, 10.84s/it] 91%| | 2954/3250 [8:46:11<52:47, 10.70s/it]                                                      91%| | 2954/3250 [8:46:11<52:47, 10.70s/it] 91%| | 2955/3250 [8:46:21<52:08, 10.60s/it]                                                      91%| | 2955/3250 [8:46:21<52:08, 10.60s/it] 91%|{'loss': 0.5785, 'learning_rate': 2.0080465909043113e-06, 'epoch': 0.91}
{'loss': 0.5093, 'learning_rate': 1.99450103882412e-06, 'epoch': 0.91}
{'loss': 0.5215, 'learning_rate': 1.98100039835748e-06, 'epoch': 0.91}
{'loss': 0.4759, 'learning_rate': 1.967544682134942e-06, 'epoch': 0.91}
{'loss': 0.5258, 'learning_rate': 1.9541339027450256e-06, 'epoch': 0.91}
 | 2956/3250 [8:46:32<51:37, 10.54s/it]                                                      91%| | 2956/3250 [8:46:32<51:37, 10.54s/it] 91%| | 2957/3250 [8:46:42<51:11, 10.48s/it]                                                      91%| | 2957/3250 [8:46:42<51:11, 10.48s/it] 91%| | 2958/3250 [8:46:52<50:49, 10.44s/it]                                                      91%| | 2958/3250 [8:46:52<50:49, 10.44s/it] 91%| | 2959/3250 [8:47:03<50:32, 10.42s/it]                                                      91%| | 2959/3250 [8:47:03<50:32, 10.42s/it] 91%| | 2960/3250 [8:47:13<50:18, 10.41s/it]                                                      91%| | 2960/3250 [8:47:13<50:18, 10.41s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.7857677340507507, 'eval_runtime': 2.1069, 'eval_samples_per_second': 5.696, 'eval_steps_per_second': 1.424, 'epoch': 0.91}
                                                      91%| | 2960/3250 [8:47:15<50:18, 10.41s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_10/tmp-checkpoint-2960
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2960
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2960
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2960
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2960
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2960/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2960/pytorch_model.bin
the pytorch model path is the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2960/pytorch_model.bin
/projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2960/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.5291, 'learning_rate': 1.940768072734195e-06, 'epoch': 0.91}
{'loss': 0.5036, 'learning_rate': 1.92744720460688e-06, 'epoch': 0.91}
{'loss': 0.4973, 'learning_rate': 1.914171310825441e-06, 'epoch': 0.91}
{'loss': 0.5114, 'learning_rate': 1.9009404038101474e-06, 'epoch': 0.91}
{'loss': 0.5203, 'learning_rate': 1.887754495939198e-06, 'epoch': 0.91}
 91%| | 2961/3250 [8:47:26<53:49, 11.17s/it]                                                      91%| | 2961/3250 [8:47:26<53:49, 11.17s/it] 91%| | 2962/3250 [8:47:36<52:28, 10.93s/it]                                                      91%| | 2962/3250 [8:47:36<52:28, 10.93s/it] 91%| | 2963/3250 [8:47:47<51:26, 10.75s/it]                                                      91%| | 2963/3250 [8:47:47<51:26, 10.75s/it] 91%| | 2964/3250 [8:47:57<50:42, 10.64s/it]                                                      91%| | 2964/3250 [8:47:57<50:42, 10.64s/it] 91%| | 2965/3250 [8:48:07<50:07, 10.55s/it]                                                      91%| | 2965/3250 [8:48:07<50:07, 10.55s/it] 91%|{'loss': 0.5297, 'learning_rate': 1.8746135995486858e-06, 'epoch': 0.91}
{'loss': 0.5271, 'learning_rate': 1.8615177269326045e-06, 'epoch': 0.91}
{'loss': 0.5049, 'learning_rate': 1.848466890342815e-06, 'epoch': 0.91}
{'loss': 0.5265, 'learning_rate': 1.8354611019890332e-06, 'epoch': 0.91}
{'loss': 0.4993, 'learning_rate': 1.8225003740388547e-06, 'epoch': 0.91}
| 2966/3250 [8:48:18<49:38, 10.49s/it]                                                      91%|| 2966/3250 [8:48:18<49:38, 10.49s/it] 91%|| 2967/3250 [8:48:28<49:18, 10.45s/it]                                                      91%|| 2967/3250 [8:48:28<49:18, 10.45s/it] 91%|| 2968/3250 [8:48:39<49:33, 10.54s/it]                                                      91%|| 2968/3250 [8:48:39<49:33, 10.54s/it] 91%|| 2969/3250 [8:48:49<49:09, 10.50s/it]                                                      91%|| 2969/3250 [8:48:49<49:09, 10.50s/it] 91%|| 2970/3250 [8:49:00<48:49, 10.46s/it]                                                      91%|| 2970/3250 [8:49:00<48:49, 10.46s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.7850273847579956, 'eval_runtime': 2.1196, 'eval_samples_per_second': 5.662, 'eval_steps_per_second': 1.415, 'epoch': 0.91}
                                                      91%|| 2970/3250 [8:49:02<48:49, 10.46s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_10/tmp-checkpoint-2970
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2970
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2970
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2970
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2970
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2970/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2970/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2970/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2970/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.5207, 'learning_rate': 1.8095847186177073e-06, 'epoch': 0.91}
{'loss': 0.5099, 'learning_rate': 1.7967141478088422e-06, 'epoch': 0.91}
{'loss': 0.4908, 'learning_rate': 1.78388867365335e-06, 'epoch': 0.91}
{'loss': 0.4905, 'learning_rate': 1.7711083081501156e-06, 'epoch': 0.92}
{'loss': 0.4869, 'learning_rate': 1.7583730632558304e-06, 'epoch': 0.92}
 91%|| 2971/3250 [8:49:13<52:07, 11.21s/it]                                                      91%|| 2971/3250 [8:49:13<52:07, 11.21s/it] 91%|| 2972/3250 [8:49:23<50:43, 10.95s/it]                                                      91%|| 2972/3250 [8:49:23<50:43, 10.95s/it] 91%|| 2973/3250 [8:49:33<49:42, 10.77s/it]                                                      91%|| 2973/3250 [8:49:33<49:42, 10.77s/it] 92%|| 2974/3250 [8:49:44<48:56, 10.64s/it]                                                      92%|| 2974/3250 [8:49:44<48:56, 10.64s/it] 92%|| 2975/3250 [8:49:54<48:22, 10.55s/it]                                                      92%|| 2975/3250 [8:49:54<48:22, 10.55s/it] 92%|{'loss': 0.5285, 'learning_rate': 1.7456829508849748e-06, 'epoch': 0.92}
{'loss': 0.5051, 'learning_rate': 1.733037982909791e-06, 'epoch': 0.92}
{'loss': 0.5294, 'learning_rate': 1.7204381711603045e-06, 'epoch': 0.92}
{'loss': 1.0247, 'learning_rate': 1.7078835274242922e-06, 'epoch': 0.92}
{'loss': 0.4754, 'learning_rate': 1.6953740634472581e-06, 'epoch': 0.92}
| 2976/3250 [8:50:04<47:57, 10.50s/it]                                                      92%|| 2976/3250 [8:50:04<47:57, 10.50s/it] 92%|| 2977/3250 [8:50:15<47:34, 10.45s/it]                                                      92%|| 2977/3250 [8:50:15<47:34, 10.45s/it] 92%|| 2978/3250 [8:50:25<47:14, 10.42s/it]                                                      92%|| 2978/3250 [8:50:25<47:14, 10.42s/it] 92%|| 2979/3250 [8:50:35<46:57, 10.40s/it]                                                      92%|| 2979/3250 [8:50:35<46:57, 10.40s/it] 92%|| 2980/3250 [8:50:46<46:44, 10.39s/it]                                                      92%|| 2980/3250 [8:50:46<46:44, 10.39s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.7861565351486206, 'eval_runtime': 2.4321, 'eval_samples_per_second': 4.934, 'eval_steps_per_second': 1.234, 'epoch': 0.92}
                                                      92%|| 2980/3250 [8:50:48<46:44, 10.39s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_10/tmp-checkpoint-2980
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  I AM HERE AND I AM SAVING THE MODEL/projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2980
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2980

I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2980
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2980
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2980/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2980/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2980/pytorch_model.binthe pytorch model path is
 /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2980/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.4903, 'learning_rate': 1.6829097909324632e-06, 'epoch': 0.92}
{'loss': 0.5285, 'learning_rate': 1.6704907215408573e-06, 'epoch': 0.92}
{'loss': 0.5333, 'learning_rate': 1.658116866891135e-06, 'epoch': 0.92}
{'loss': 0.4945, 'learning_rate': 1.6457882385596646e-06, 'epoch': 0.92}
{'loss': 0.508, 'learning_rate': 1.6335048480805083e-06, 'epoch': 0.92}
 92%|| 2981/3250 [8:51:00<51:05, 11.40s/it]                                                      92%|| 2981/3250 [8:51:00<51:05, 11.40s/it] 92%|| 2982/3250 [8:51:10<49:35, 11.10s/it]                                                      92%|| 2982/3250 [8:51:10<49:35, 11.10s/it] 92%|| 2983/3250 [8:51:20<48:25, 10.88s/it]                                                      92%|| 2983/3250 [8:51:20<48:25, 10.88s/it] 92%|| 2984/3250 [8:51:31<47:53, 10.80s/it]                                                      92%|| 2984/3250 [8:51:31<47:53, 10.80s/it] 92%|| 2985/3250 [8:51:41<47:09, 10.68s/it]                                                      92%|| 2985/3250 [8:51:41<47:09, 10.68s/it] 92%|{'loss': 0.5499, 'learning_rate': 1.6212667069454291e-06, 'epoch': 0.92}
{'loss': 0.5396, 'learning_rate': 1.6090738266038186e-06, 'epoch': 0.92}
{'loss': 0.4987, 'learning_rate': 1.596926218462752e-06, 'epoch': 0.92}
{'loss': 0.467, 'learning_rate': 1.584823893886933e-06, 'epoch': 0.92}
{'loss': 0.5234, 'learning_rate': 1.572766864198716e-06, 'epoch': 0.92}
| 2986/3250 [8:51:52<46:46, 10.63s/it]                                                      92%|| 2986/3250 [8:51:52<46:46, 10.63s/it] 92%|| 2987/3250 [8:52:02<46:15, 10.55s/it]                                                      92%|| 2987/3250 [8:52:02<46:15, 10.55s/it] 92%|| 2988/3250 [8:52:13<45:48, 10.49s/it]                                                      92%|| 2988/3250 [8:52:13<45:48, 10.49s/it] 92%|| 2989/3250 [8:52:23<45:25, 10.44s/it]                                                      92%|| 2989/3250 [8:52:23<45:25, 10.44s/it] 92%|| 2990/3250 [8:52:33<45:07, 10.41s/it]                                                      92%|| 2990/3250 [8:52:33<45:07, 10.41s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.786411464214325, 'eval_runtime': 2.1953, 'eval_samples_per_second': 5.466, 'eval_steps_per_second': 1.367, 'epoch': 0.92}
                                                      92%|| 2990/3250 [8:52:35<45:07, 10.41s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_10/tmp-checkpoint-2990
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2990
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2990
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2990
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2990
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2990/pytorch_model.bin
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2990/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2990/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-2990/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.5165, 'learning_rate': 1.5607551406780717e-06, 'epoch': 0.92}
{'loss': 0.5167, 'learning_rate': 1.548788734562584e-06, 'epoch': 0.92}
{'loss': 0.5028, 'learning_rate': 1.5368676570474471e-06, 'epoch': 0.92}
{'loss': 0.5128, 'learning_rate': 1.524991919285429e-06, 'epoch': 0.92}
{'loss': 0.5124, 'learning_rate': 1.5131615323869085e-06, 'epoch': 0.92}
 92%|| 2991/3250 [8:52:46<48:18, 11.19s/it]                                                      92%|| 2991/3250 [8:52:46<48:18, 11.19s/it] 92%|| 2992/3250 [8:52:57<47:06, 10.95s/it]                                                      92%|| 2992/3250 [8:52:57<47:06, 10.95s/it] 92%|| 2993/3250 [8:53:07<46:08, 10.77s/it]                                                      92%|| 2993/3250 [8:53:07<46:08, 10.77s/it] 92%|| 2994/3250 [8:53:17<45:26, 10.65s/it]                                                      92%|| 2994/3250 [8:53:17<45:26, 10.65s/it] 92%|| 2995/3250 [8:53:28<44:52, 10.56s/it]                                                      92%|| 2995/3250 [8:53:28<44:52, 10.56s/it] 92%|{'loss': 0.5406, 'learning_rate': 1.501376507419805e-06, 'epoch': 0.92}
{'loss': 0.5159, 'learning_rate': 1.4896368554096318e-06, 'epoch': 0.92}
{'loss': 0.5098, 'learning_rate': 1.4779425873394259e-06, 'epoch': 0.92}
{'loss': 0.5122, 'learning_rate': 1.4662937141497911e-06, 'epoch': 0.92}
{'loss': 0.531, 'learning_rate': 1.4546902467388268e-06, 'epoch': 0.92}
| 2996/3250 [8:53:38<44:25, 10.49s/it]                                                      92%|| 2996/3250 [8:53:38<44:25, 10.49s/it] 92%|| 2997/3250 [8:53:48<44:01, 10.44s/it]                                                      92%|| 2997/3250 [8:53:48<44:01, 10.44s/it] 92%|| 2998/3250 [8:53:59<43:45, 10.42s/it]                                                      92%|| 2998/3250 [8:53:59<43:45, 10.42s/it] 92%|| 2999/3250 [8:54:09<43:30, 10.40s/it]                                                      92%|| 2999/3250 [8:54:09<43:30, 10.40s/it] 92%|| 3000/3250 [8:54:20<43:46, 10.51s/it]                                                      92%|| 3000/3250 [8:54:20<43:46, 10.51s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.786646842956543, 'eval_runtime': 2.1213, 'eval_samples_per_second': 5.657, 'eval_steps_per_second': 1.414, 'epoch': 0.92}
                                                      92%|| 3000/3250 [8:54:22<43:46, 10.51s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_10/tmp-checkpoint-3000
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-3000
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-3000
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-3000
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-3000
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-3000/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-3000/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-3000/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-3000/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.4829, 'learning_rate': 1.443132195962188e-06, 'epoch': 0.92}
{'loss': 0.5598, 'learning_rate': 1.431619572633014e-06, 'epoch': 0.92}
{'loss': 0.4975, 'learning_rate': 1.4201523875219724e-06, 'epoch': 0.92}
{'loss': 0.5014, 'learning_rate': 1.4087306513571985e-06, 'epoch': 0.92}
{'loss': 0.4865, 'learning_rate': 1.3973543748243e-06, 'epoch': 0.92}
 92%|| 3001/3250 [8:54:33<46:46, 11.27s/it]                                                      92%|| 3001/3250 [8:54:33<46:46, 11.27s/it] 92%|| 3002/3250 [8:54:43<45:26, 10.99s/it]                                                      92%|| 3002/3250 [8:54:43<45:26, 10.99s/it] 92%|| 3003/3250 [8:54:54<44:27, 10.80s/it]                                                      92%|| 3003/3250 [8:54:54<44:27, 10.80s/it] 92%|| 3004/3250 [8:55:04<43:43, 10.66s/it]                                                      92%|| 3004/3250 [8:55:04<43:43, 10.66s/it] 92%|| 3005/3250 [8:55:14<43:09, 10.57s/it]                                                      92%|| 3005/3250 [8:55:14<43:09, 10.57s/it] 92%|{'loss': 0.5028, 'learning_rate': 1.3860235685663913e-06, 'epoch': 0.92}
{'loss': 0.5193, 'learning_rate': 1.3747382431840095e-06, 'epoch': 0.93}
{'loss': 0.5247, 'learning_rate': 1.3634984092351588e-06, 'epoch': 0.93}
{'loss': 0.5143, 'learning_rate': 1.3523040772352835e-06, 'epoch': 0.93}
{'loss': 1.0113, 'learning_rate': 1.341155257657256e-06, 'epoch': 0.93}
| 3006/3250 [8:55:25<42:43, 10.51s/it]                                                      92%|| 3006/3250 [8:55:25<42:43, 10.51s/it] 93%|| 3007/3250 [8:55:35<42:23, 10.47s/it]                                                      93%|| 3007/3250 [8:55:35<42:23, 10.47s/it] 93%|| 3008/3250 [8:55:45<42:05, 10.44s/it]                                                      93%|| 3008/3250 [8:55:45<42:05, 10.44s/it] 93%|| 3009/3250 [8:55:56<41:50, 10.42s/it]                                                      93%|| 3009/3250 [8:55:56<41:50, 10.42s/it] 93%|| 3010/3250 [8:56:06<41:34, 10.39s/it]                                                      93%|| 3010/3250 [8:56:06<41:34, 10.39s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.7863049507141113, 'eval_runtime': 2.7034, 'eval_samples_per_second': 4.439, 'eval_steps_per_second': 1.11, 'epoch': 0.93}
                                                      93%|| 3010/3250 [8:56:09<41:34, 10.39s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_10/tmp-checkpoint-3010
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-3010
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-3010
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-3010
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-3010
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-3010/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-3010/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-3010/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-3010/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.4969, 'learning_rate': 1.3300519609313832e-06, 'epoch': 0.93}
{'loss': 0.523, 'learning_rate': 1.31899419744535e-06, 'epoch': 0.93}
{'loss': 0.5117, 'learning_rate': 1.3079819775442703e-06, 'epoch': 0.93}
{'loss': 0.5153, 'learning_rate': 1.297015311530647e-06, 'epoch': 0.93}
{'loss': 0.4791, 'learning_rate': 1.2860942096643569e-06, 'epoch': 0.93}
 93%|| 3011/3250 [8:56:20<45:14, 11.36s/it]                                                      93%|| 3011/3250 [8:56:20<45:14, 11.36s/it] 93%|| 3012/3250 [8:56:30<43:51, 11.06s/it]                                                      93%|| 3012/3250 [8:56:30<43:51, 11.06s/it] 93%|| 3013/3250 [8:56:40<42:50, 10.85s/it]                                                      93%|| 3013/3250 [8:56:40<42:50, 10.85s/it] 93%|| 3014/3250 [8:56:51<42:05, 10.70s/it]                                                      93%|| 3014/3250 [8:56:51<42:05, 10.70s/it] 93%|| 3015/3250 [8:57:01<41:30, 10.60s/it]                                                      93%|| 3015/3250 [8:57:01<41:30, 10.60s/it] 93%|{'loss': 0.5561, 'learning_rate': 1.2752186821626488e-06, 'epoch': 0.93}
{'loss': 0.5328, 'learning_rate': 1.2643887392001563e-06, 'epoch': 0.93}
{'loss': 0.4979, 'learning_rate': 1.2536043909088191e-06, 'epoch': 0.93}
{'loss': 0.496, 'learning_rate': 1.2428656473779721e-06, 'epoch': 0.93}
{'loss': 0.5213, 'learning_rate': 1.232172518654251e-06, 'epoch': 0.93}
| 3016/3250 [8:57:12<41:02, 10.52s/it]                                                      93%|| 3016/3250 [8:57:12<41:02, 10.52s/it] 93%|| 3017/3250 [8:57:22<40:56, 10.54s/it]                                                      93%|| 3017/3250 [8:57:22<40:56, 10.54s/it] 93%|| 3018/3250 [8:57:32<40:34, 10.49s/it]                                                      93%|| 3018/3250 [8:57:32<40:34, 10.49s/it] 93%|| 3019/3250 [8:57:43<40:13, 10.45s/it]                                                      93%|| 3019/3250 [8:57:43<40:13, 10.45s/it] 93%|| 3020/3250 [8:57:53<39:57, 10.43s/it]                                                      93%|| 3020/3250 [8:57:53<39:57, 10.43s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.7855471968650818, 'eval_runtime': 2.1228, 'eval_samples_per_second': 5.653, 'eval_steps_per_second': 1.413, 'epoch': 0.93}
                                                      93%|| 3020/3250 [8:57:55<39:57, 10.43s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_10/tmp-checkpoint-3020
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-3020
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-3020
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-3020
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-3020
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-3020/pytorch_model.bin
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-3020/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-3020/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-3020/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.5023, 'learning_rate': 1.2215250147416313e-06, 'epoch': 0.93}
{'loss': 0.5141, 'learning_rate': 1.2109231456014058e-06, 'epoch': 0.93}
{'loss': 0.5141, 'learning_rate': 1.2003669211521628e-06, 'epoch': 0.93}
{'loss': 0.5009, 'learning_rate': 1.189856351269797e-06, 'epoch': 0.93}
{'loss': 0.5156, 'learning_rate': 1.1793914457874756e-06, 'epoch': 0.93}
 93%|| 3021/3250 [8:58:06<42:44, 11.20s/it]                                                      93%|| 3021/3250 [8:58:06<42:44, 11.20s/it] 93%|| 3022/3250 [8:58:17<41:34, 10.94s/it]                                                      93%|| 3022/3250 [8:58:17<41:34, 10.94s/it] 93%|| 3023/3250 [8:58:27<40:43, 10.77s/it]                                                      93%|| 3023/3250 [8:58:27<40:43, 10.77s/it] 93%|| 3024/3250 [8:58:37<40:04, 10.64s/it]                                                      93%|| 3024/3250 [8:58:37<40:04, 10.64s/it] 93%|| 3025/3250 [8:58:48<39:33, 10.55s/it]                                                      93%|| 3025/3250 [8:58:48<39:33, 10.55s/it] 93%|{'loss': 0.5272, 'learning_rate': 1.1689722144956671e-06, 'epoch': 0.93}
{'loss': 0.5111, 'learning_rate': 1.1585986671420967e-06, 'epoch': 0.93}
{'loss': 0.5094, 'learning_rate': 1.148270813431751e-06, 'epoch': 0.93}
{'loss': 0.5172, 'learning_rate': 1.1379886630268677e-06, 'epoch': 0.93}
{'loss': 0.5294, 'learning_rate': 1.1277522255469296e-06, 'epoch': 0.93}
| 3026/3250 [8:58:58<39:08, 10.49s/it]                                                      93%|| 3026/3250 [8:58:58<39:08, 10.49s/it] 93%|| 3027/3250 [8:59:08<38:49, 10.45s/it]                                                      93%|| 3027/3250 [8:59:08<38:49, 10.45s/it] 93%|| 3028/3250 [8:59:19<38:33, 10.42s/it]                                                      93%|| 3028/3250 [8:59:19<38:33, 10.42s/it] 93%|| 3029/3250 [8:59:29<38:19, 10.40s/it]                                                      93%|| 3029/3250 [8:59:29<38:19, 10.40s/it] 93%|| 3030/3250 [8:59:39<38:06, 10.39s/it]                                                      93%|| 3030/3250 [8:59:39<38:06, 10.39s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.7858139872550964, 'eval_runtime': 2.1212, 'eval_samples_per_second': 5.657, 'eval_steps_per_second': 1.414, 'epoch': 0.93}
                                                      93%|| 3030/3250 [8:59:41<38:06, 10.39s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_10/tmp-checkpoint-3030
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-3030
/projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-3030
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-3030
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-3030
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-3030/pytorch_model.bin
the pytorch model path isthe pytorch model path is  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-3030/pytorch_model.bin/projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-3030/pytorch_model.bin

the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-3030/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.482, 'learning_rate': 1.1175615105686433e-06, 'epoch': 0.93}
{'loss': 0.5545, 'learning_rate': 1.107416527625954e-06, 'epoch': 0.93}
{'loss': 0.5044, 'learning_rate': 1.0973172862100145e-06, 'epoch': 0.93}
{'loss': 0.4972, 'learning_rate': 1.0872637957691834e-06, 'epoch': 0.93}
{'loss': 0.4904, 'learning_rate': 1.0772560657090202e-06, 'epoch': 0.93}
 93%|| 3031/3250 [8:59:52<40:47, 11.17s/it]                                                      93%|| 3031/3250 [8:59:52<40:47, 11.17s/it] 93%|| 3032/3250 [9:00:03<39:43, 10.93s/it]                                                      93%|| 3032/3250 [9:00:03<39:43, 10.93s/it] 93%|| 3033/3250 [9:00:13<39:20, 10.88s/it]                                                      93%|| 3033/3250 [9:00:13<39:20, 10.88s/it] 93%|| 3034/3250 [9:00:24<38:35, 10.72s/it]                                                      93%|| 3034/3250 [9:00:24<38:35, 10.72s/it] 93%|| 3035/3250 [9:00:34<38:01, 10.61s/it]                                                      93%|| 3035/3250 [9:00:34<38:01, 10.61s/it] 93%|{'loss': 0.4917, 'learning_rate': 1.0672941053922635e-06, 'epoch': 0.93}
{'loss': 0.5198, 'learning_rate': 1.0573779241388471e-06, 'epoch': 0.93}
{'loss': 0.4976, 'learning_rate': 1.0475075312258664e-06, 'epoch': 0.93}
{'loss': 0.5251, 'learning_rate': 1.037682935887585e-06, 'epoch': 0.94}
{'loss': 1.0122, 'learning_rate': 1.0279041473154116e-06, 'epoch': 0.94}
| 3036/3250 [9:00:45<37:34, 10.54s/it]                                                      93%|| 3036/3250 [9:00:45<37:34, 10.54s/it] 93%|| 3037/3250 [9:00:55<37:13, 10.49s/it]                                                      93%|| 3037/3250 [9:00:55<37:13, 10.49s/it] 93%|| 3038/3250 [9:01:05<36:55, 10.45s/it]                                                      93%|| 3038/3250 [9:01:05<36:55, 10.45s/it] 94%|| 3039/3250 [9:01:16<36:43, 10.44s/it]                                                      94%|| 3039/3250 [9:01:16<36:43, 10.44s/it] 94%|| 3040/3250 [9:01:26<36:26, 10.41s/it]                                                      94%|| 3040/3250 [9:01:26<36:26, 10.41s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.7862548828125, 'eval_runtime': 2.4152, 'eval_samples_per_second': 4.969, 'eval_steps_per_second': 1.242, 'epoch': 0.94}
                                                      94%|| 3040/3250 [9:01:28<36:26, 10.41s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_10/tmp-checkpoint-3040
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-3040
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-3040
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-3040
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-3040
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-3040/pytorch_model.bin
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-3040/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-3040/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-3040/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.5074, 'learning_rate': 1.0181711746579058e-06, 'epoch': 0.94}
{'loss': 0.5097, 'learning_rate': 1.008484027020773e-06, 'epoch': 0.94}
{'loss': 0.5362, 'learning_rate': 9.988427134668298e-07, 'epoch': 0.94}
{'loss': 0.518, 'learning_rate': 9.892472430160171e-07, 'epoch': 0.94}
{'loss': 0.4924, 'learning_rate': 9.796976246454037e-07, 'epoch': 0.94}
 94%|| 3041/3250 [9:01:39<39:20, 11.29s/it]                                                      94%|| 3041/3250 [9:01:39<39:20, 11.29s/it] 94%|| 3042/3250 [9:01:50<38:10, 11.01s/it]                                                      94%|| 3042/3250 [9:01:50<38:10, 11.01s/it] 94%|| 3043/3250 [9:02:00<37:19, 10.82s/it]                                                      94%|| 3043/3250 [9:02:00<37:19, 10.82s/it] 94%|| 3044/3250 [9:02:11<36:50, 10.73s/it]                                                      94%|| 3044/3250 [9:02:11<36:50, 10.73s/it] 94%|| 3045/3250 [9:02:21<36:16, 10.62s/it]                                                      94%|| 3045/3250 [9:02:21<36:16, 10.62s/it] 94%|{'loss': 0.49, 'learning_rate': 9.701938672891376e-07, 'epoch': 0.94}
{'loss': 0.5781, 'learning_rate': 9.607359798384785e-07, 'epoch': 0.94}
{'loss': 0.5176, 'learning_rate': 9.513239711417654e-07, 'epoch': 0.94}
{'loss': 0.5235, 'learning_rate': 9.419578500044157e-07, 'epoch': 0.94}
{'loss': 0.4675, 'learning_rate': 9.326376251889202e-07, 'epoch': 0.94}
| 3046/3250 [9:02:31<35:50, 10.54s/it]                                                      94%|| 3046/3250 [9:02:31<35:50, 10.54s/it] 94%|| 3047/3250 [9:02:43<36:53, 10.90s/it]                                                      94%|| 3047/3250 [9:02:43<36:53, 10.90s/it] 94%|| 3048/3250 [9:02:54<36:12, 10.76s/it]                                                      94%|| 3048/3250 [9:02:54<36:12, 10.76s/it] 94%|| 3049/3250 [9:03:04<35:40, 10.65s/it]                                                      94%|| 3049/3250 [9:03:04<35:40, 10.65s/it] 94%|| 3050/3250 [9:03:15<35:41, 10.71s/it]                                                      94%|| 3050/3250 [9:03:15<35:41, 10.71s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.7862995862960815, 'eval_runtime': 2.1648, 'eval_samples_per_second': 5.543, 'eval_steps_per_second': 1.386, 'epoch': 0.94}
                                                      94%|| 3050/3250 [9:03:17<35:41, 10.71s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_10/tmp-checkpoint-3050
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-3050
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-3050
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-3050
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-3050
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-3050/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-3050/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-3050/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-3050/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.5301, 'learning_rate': 9.233633054148205e-07, 'epoch': 0.94}
{'loss': 0.5355, 'learning_rate': 9.141348993587317e-07, 'epoch': 0.94}
{'loss': 0.4982, 'learning_rate': 9.049524156542977e-07, 'epoch': 0.94}
{'loss': 0.4978, 'learning_rate': 8.958158628922019e-07, 'epoch': 0.94}
{'loss': 0.5116, 'learning_rate': 8.867252496201572e-07, 'epoch': 0.94}
 94%|| 3051/3250 [9:03:28<37:53, 11.43s/it]                                                      94%|| 3051/3250 [9:03:28<37:53, 11.43s/it] 94%|| 3052/3250 [9:03:38<36:39, 11.11s/it]                                                      94%|| 3052/3250 [9:03:38<36:39, 11.11s/it] 94%|| 3053/3250 [9:03:49<35:44, 10.89s/it]                                                      94%|| 3053/3250 [9:03:49<35:44, 10.89s/it] 94%|| 3054/3250 [9:03:59<35:05, 10.74s/it]                                                      94%|| 3054/3250 [9:03:59<35:05, 10.74s/it] 94%|| 3055/3250 [9:04:09<34:33, 10.63s/it]                                                      94%|| 3055/3250 [9:04:09<34:33, 10.63s/it] 94%|{'loss': 0.5105, 'learning_rate': 8.776805843429103e-07, 'epoch': 0.94}
{'loss': 0.5431, 'learning_rate': 8.686818755221982e-07, 'epoch': 0.94}
{'loss': 0.5229, 'learning_rate': 8.597291315767808e-07, 'epoch': 0.94}
{'loss': 0.5124, 'learning_rate': 8.508223608824084e-07, 'epoch': 0.94}
{'loss': 0.5211, 'learning_rate': 8.419615717718377e-07, 'epoch': 0.94}
| 3056/3250 [9:04:20<34:06, 10.55s/it]                                                      94%|| 3056/3250 [9:04:20<34:06, 10.55s/it] 94%|| 3057/3250 [9:04:30<33:45, 10.50s/it]                                                      94%|| 3057/3250 [9:04:30<33:45, 10.50s/it] 94%|| 3058/3250 [9:04:41<33:27, 10.46s/it]                                                      94%|| 3058/3250 [9:04:41<33:27, 10.46s/it] 94%|| 3059/3250 [9:04:51<33:11, 10.43s/it]                                                      94%|| 3059/3250 [9:04:51<33:11, 10.43s/it] 94%|| 3060/3250 [9:05:01<33:03, 10.44s/it]                                                      94%|| 3060/3250 [9:05:01<33:03, 10.44s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.7861407399177551, 'eval_runtime': 2.123, 'eval_samples_per_second': 5.652, 'eval_steps_per_second': 1.413, 'epoch': 0.94}
                                                      94%|| 3060/3250 [9:05:03<33:03, 10.44s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_10/tmp-checkpoint-3060
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-3060
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-3060
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-3060
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-3060
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-3060/pytorch_model.binthe pytorch model path is
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-3060/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-3060/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-3060/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.4983, 'learning_rate': 8.331467725347708e-07, 'epoch': 0.94}
{'loss': 0.518, 'learning_rate': 8.243779714179168e-07, 'epoch': 0.94}
{'loss': 0.5089, 'learning_rate': 8.156551766249409e-07, 'epoch': 0.94}
{'loss': 0.4895, 'learning_rate': 8.069783963164656e-07, 'epoch': 0.94}
{'loss': 0.4925, 'learning_rate': 7.983476386100641e-07, 'epoch': 0.94}
 94%|| 3061/3250 [9:05:14<35:17, 11.20s/it]                                                      94%|| 3061/3250 [9:05:14<35:17, 11.20s/it] 94%|| 3062/3250 [9:05:25<34:23, 10.98s/it]                                                      94%|| 3062/3250 [9:05:25<34:23, 10.98s/it] 94%|| 3063/3250 [9:05:35<33:38, 10.80s/it]                                                      94%|| 3063/3250 [9:05:35<33:38, 10.80s/it] 94%|| 3064/3250 [9:05:46<33:04, 10.67s/it]                                                      94%|| 3064/3250 [9:05:46<33:04, 10.67s/it] 94%|| 3065/3250 [9:05:56<32:38, 10.59s/it]                                                      94%|| 3065/3250 [9:05:56<32:38, 10.59s/it] 94%|{'loss': 0.4802, 'learning_rate': 7.897629115802551e-07, 'epoch': 0.94}
{'loss': 0.5262, 'learning_rate': 7.812242232584865e-07, 'epoch': 0.94}
{'loss': 0.5104, 'learning_rate': 7.727315816331515e-07, 'epoch': 0.94}
{'loss': 0.5386, 'learning_rate': 7.642849946495445e-07, 'epoch': 0.94}
{'loss': 1.0292, 'learning_rate': 7.558844702098833e-07, 'epoch': 0.94}
| 3066/3250 [9:06:07<32:31, 10.61s/it]                                                      94%|| 3066/3250 [9:06:07<32:31, 10.61s/it] 94%|| 3067/3250 [9:06:17<32:07, 10.53s/it]                                                      94%|| 3067/3250 [9:06:17<32:07, 10.53s/it] 94%|| 3068/3250 [9:06:27<31:48, 10.49s/it]                                                      94%|| 3068/3250 [9:06:27<31:48, 10.49s/it] 94%|| 3069/3250 [9:06:38<31:31, 10.45s/it]                                                      94%|| 3069/3250 [9:06:38<31:31, 10.45s/it] 94%|| 3070/3250 [9:06:48<31:15, 10.42s/it]                                                      94%|| 3070/3250 [9:06:48<31:15, 10.42s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.786141574382782, 'eval_runtime': 2.3602, 'eval_samples_per_second': 5.084, 'eval_steps_per_second': 1.271, 'epoch': 0.94}
                                                      94%|| 3070/3250 [9:06:50<31:15, 10.42s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_10/tmp-checkpoint-3070
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-3070
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-3070
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-3070
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-3070
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-3070/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-3070/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-3070/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-3070/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.4881, 'learning_rate': 7.475300161732978e-07, 'epoch': 0.94}
{'loss': 0.4991, 'learning_rate': 7.392216403558027e-07, 'epoch': 0.95}
{'loss': 0.525, 'learning_rate': 7.309593505303136e-07, 'epoch': 0.95}
{'loss': 0.53, 'learning_rate': 7.227431544266194e-07, 'epoch': 0.95}
{'loss': 0.4997, 'learning_rate': 7.145730597314049e-07, 'epoch': 0.95}
 94%|| 3071/3250 [9:07:01<33:37, 11.27s/it]                                                      94%|| 3071/3250 [9:07:01<33:37, 11.27s/it] 95%|| 3072/3250 [9:07:12<32:37, 11.00s/it]                                                      95%|| 3072/3250 [9:07:12<32:37, 11.00s/it] 95%|| 3073/3250 [9:07:22<31:51, 10.80s/it]                                                      95%|| 3073/3250 [9:07:22<31:51, 10.80s/it] 95%|| 3074/3250 [9:07:32<31:17, 10.67s/it]                                                      95%|| 3074/3250 [9:07:32<31:17, 10.67s/it] 95%|| 3075/3250 [9:07:43<30:50, 10.57s/it]                                                      95%|| 3075/3250 [9:07:43<30:50, 10.57s/it] 95%|{'loss': 0.5143, 'learning_rate': 7.064490740882057e-07, 'epoch': 0.95}
{'loss': 0.5607, 'learning_rate': 6.983712050974367e-07, 'epoch': 0.95}
{'loss': 0.5361, 'learning_rate': 6.903394603163582e-07, 'epoch': 0.95}
{'loss': 0.4996, 'learning_rate': 6.823538472590707e-07, 'epoch': 0.95}
{'loss': 0.4763, 'learning_rate': 6.744143733965369e-07, 'epoch': 0.95}
| 3076/3250 [9:07:53<30:28, 10.51s/it]                                                      95%|| 3076/3250 [9:07:53<30:28, 10.51s/it] 95%|| 3077/3250 [9:08:03<30:11, 10.47s/it]                                                      95%|| 3077/3250 [9:08:03<30:11, 10.47s/it] 95%|| 3078/3250 [9:08:14<29:55, 10.44s/it]                                                      95%|| 3078/3250 [9:08:14<29:55, 10.44s/it] 95%|| 3079/3250 [9:08:24<29:41, 10.42s/it]                                                      95%|| 3079/3250 [9:08:24<29:41, 10.42s/it] 95%|| 3080/3250 [9:08:35<29:28, 10.40s/it]                                                      95%|| 3080/3250 [9:08:35<29:28, 10.40s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.7868320941925049, 'eval_runtime': 2.1183, 'eval_samples_per_second': 5.665, 'eval_steps_per_second': 1.416, 'epoch': 0.95}
                                                      95%|| 3080/3250 [9:08:37<29:28, 10.40s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_10/tmp-checkpoint-3080
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-3080
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-3080
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-3080
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-3080
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-3080/pytorch_model.binthe pytorch model path is
 /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-3080/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-3080/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-3080/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.525, 'learning_rate': 6.66521046156543e-07, 'epoch': 0.95}
{'loss': 0.5037, 'learning_rate': 6.58673872923693e-07, 'epoch': 0.95}
{'loss': 0.5221, 'learning_rate': 6.508728610394255e-07, 'epoch': 0.95}
{'loss': 0.5134, 'learning_rate': 6.431180178019969e-07, 'epoch': 0.95}
{'loss': 0.5159, 'learning_rate': 6.354093504664538e-07, 'epoch': 0.95}
 95%|| 3081/3250 [9:08:48<31:30, 11.19s/it]                                                      95%|| 3081/3250 [9:08:48<31:30, 11.19s/it] 95%|| 3082/3250 [9:08:58<30:56, 11.05s/it]                                                      95%|| 3082/3250 [9:08:58<30:56, 11.05s/it] 95%|| 3083/3250 [9:09:09<30:10, 10.84s/it]                                                      95%|| 3083/3250 [9:09:09<30:10, 10.84s/it] 95%|| 3084/3250 [9:09:19<29:35, 10.70s/it]                                                      95%|| 3084/3250 [9:09:19<29:35, 10.70s/it] 95%|| 3085/3250 [9:09:29<29:08, 10.60s/it]                                                      95%|| 3085/3250 [9:09:29<29:08, 10.60s/it] 95%|{'loss': 0.5095, 'learning_rate': 6.277468662446495e-07, 'epoch': 0.95}
{'loss': 0.5457, 'learning_rate': 6.201305723052331e-07, 'epoch': 0.95}
{'loss': 0.5213, 'learning_rate': 6.125604757736436e-07, 'epoch': 0.95}
{'loss': 0.522, 'learning_rate': 6.050365837320992e-07, 'epoch': 0.95}
{'loss': 0.5143, 'learning_rate': 5.97558903219575e-07, 'epoch': 0.95}
| 3086/3250 [9:09:41<29:57, 10.96s/it]                                                      95%|| 3086/3250 [9:09:41<29:57, 10.96s/it] 95%|| 3087/3250 [9:09:52<29:34, 10.89s/it]                                                      95%|| 3087/3250 [9:09:52<29:34, 10.89s/it] 95%|| 3088/3250 [9:10:02<28:57, 10.73s/it]                                                      95%|| 3088/3250 [9:10:02<28:57, 10.73s/it] 95%|| 3089/3250 [9:10:13<28:29, 10.62s/it]                                                      95%|| 3089/3250 [9:10:13<28:29, 10.62s/it] 95%|| 3090/3250 [9:10:23<28:05, 10.54s/it]                                                      95%|| 3090/3250 [9:10:23<28:05, 10.54s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.7858682870864868, 'eval_runtime': 2.5872, 'eval_samples_per_second': 4.638, 'eval_steps_per_second': 1.16, 'epoch': 0.95}
                                                      95%|| 3090/3250 [9:10:26<28:05, 10.54s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_10/tmp-checkpoint-3090
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-3090
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-3090
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-3090
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-3090
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-3090/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-3090/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-3090/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-3090/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.5312, 'learning_rate': 5.901274412318359e-07, 'epoch': 0.95}
{'loss': 0.4894, 'learning_rate': 5.827422047213926e-07, 'epoch': 0.95}
{'loss': 0.5565, 'learning_rate': 5.754032005975129e-07, 'epoch': 0.95}
{'loss': 0.5079, 'learning_rate': 5.681104357262101e-07, 'epoch': 0.95}
{'loss': 0.5068, 'learning_rate': 5.608639169302543e-07, 'epoch': 0.95}
 95%|| 3091/3250 [9:10:36<30:17, 11.43s/it]                                                      95%|| 3091/3250 [9:10:36<30:17, 11.43s/it] 95%|| 3092/3250 [9:10:47<29:13, 11.10s/it]                                                      95%|| 3092/3250 [9:10:47<29:13, 11.10s/it] 95%|| 3093/3250 [9:10:57<28:26, 10.87s/it]                                                      95%|| 3093/3250 [9:10:57<28:26, 10.87s/it] 95%|| 3094/3250 [9:11:07<27:50, 10.71s/it]                                                      95%|| 3094/3250 [9:11:07<27:50, 10.71s/it] 95%|| 3095/3250 [9:11:18<27:22, 10.60s/it]                                                      95%|| 3095/3250 [9:11:18<27:22, 10.60s/it] 95%|{'loss': 0.4947, 'learning_rate': 5.536636509891225e-07, 'epoch': 0.95}
{'loss': 0.4984, 'learning_rate': 5.465096446390428e-07, 'epoch': 0.95}
{'loss': 0.5128, 'learning_rate': 5.394019045729448e-07, 'epoch': 0.95}
{'loss': 0.5287, 'learning_rate': 5.323404374404983e-07, 'epoch': 0.95}
{'loss': 0.5143, 'learning_rate': 5.253252498480576e-07, 'epoch': 0.95}
| 3096/3250 [9:11:28<27:00, 10.52s/it]                                                      95%|| 3096/3250 [9:11:28<27:00, 10.52s/it] 95%|| 3097/3250 [9:11:38<26:41, 10.47s/it]                                                      95%|| 3097/3250 [9:11:38<26:41, 10.47s/it] 95%|| 3098/3250 [9:11:49<26:25, 10.43s/it]                                                      95%|| 3098/3250 [9:11:49<26:25, 10.43s/it] 95%|| 3099/3250 [9:11:59<26:24, 10.49s/it]                                                      95%|| 3099/3250 [9:11:59<26:24, 10.49s/it] 95%|| 3100/3250 [9:12:10<26:08, 10.45s/it]                                                      95%|| 3100/3250 [9:12:10<26:08, 10.45s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.7850161790847778, 'eval_runtime': 2.4738, 'eval_samples_per_second': 4.851, 'eval_steps_per_second': 1.213, 'epoch': 0.95}
                                                      95%|| 3100/3250 [9:12:12<26:08, 10.45s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_10/tmp-checkpoint-3100
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-3100
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-3100
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-3100
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-3100
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-3100/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-3100/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-3100/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-3100/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 1.0006, 'learning_rate': 5.183563483587006e-07, 'epoch': 0.95}
{'loss': 0.5005, 'learning_rate': 5.114337394921953e-07, 'epoch': 0.95}
{'loss': 0.53, 'learning_rate': 5.045574297249833e-07, 'epoch': 0.95}
{'loss': 0.52, 'learning_rate': 4.977274254902187e-07, 'epoch': 0.96}
{'loss': 0.5259, 'learning_rate': 4.909437331777179e-07, 'epoch': 0.96}
 95%|| 3101/3250 [9:12:23<28:05, 11.31s/it]                                                      95%|| 3101/3250 [9:12:23<28:05, 11.31s/it] 95%|| 3102/3250 [9:12:33<27:10, 11.02s/it]                                                      95%|| 3102/3250 [9:12:33<27:10, 11.02s/it] 95%|| 3103/3250 [9:12:44<26:28, 10.80s/it]                                                      95%|| 3103/3250 [9:12:44<26:28, 10.80s/it] 96%|| 3104/3250 [9:12:54<25:56, 10.66s/it]                                                      96%|| 3104/3250 [9:12:54<25:56, 10.66s/it] 96%|| 3105/3250 [9:13:04<25:33, 10.57s/it]                                                      96%|| 3105/3250 [9:13:04<25:33, 10.57s/it] 96%|{'loss': 0.499, 'learning_rate': 4.842063591339763e-07, 'epoch': 0.96}
{'loss': 0.5463, 'learning_rate': 4.77515309662152e-07, 'epoch': 0.96}
{'loss': 0.5395, 'learning_rate': 4.7087059102206564e-07, 'epoch': 0.96}
{'loss': 0.4961, 'learning_rate': 4.6427220943019436e-07, 'epoch': 0.96}
{'loss': 0.4971, 'learning_rate': 4.577201710596612e-07, 'epoch': 0.96}
| 3106/3250 [9:13:15<25:12, 10.50s/it]                                                      96%|| 3106/3250 [9:13:15<25:12, 10.50s/it] 96%|| 3107/3250 [9:13:25<24:54, 10.45s/it]                                                      96%|| 3107/3250 [9:13:25<24:54, 10.45s/it] 96%|| 3108/3250 [9:13:35<24:39, 10.42s/it]                                                      96%|| 3108/3250 [9:13:35<24:39, 10.42s/it] 96%|| 3109/3250 [9:13:46<24:25, 10.40s/it]                                                      96%|| 3109/3250 [9:13:46<24:25, 10.40s/it] 96%|| 3110/3250 [9:13:56<24:14, 10.39s/it]                                                      96%|| 3110/3250 [9:13:56<24:14, 10.39s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.7858229875564575, 'eval_runtime': 2.1022, 'eval_samples_per_second': 5.708, 'eval_steps_per_second': 1.427, 'epoch': 0.96}
                                                      96%|| 3110/3250 [9:13:58<24:14, 10.39s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_10/tmp-checkpoint-3110
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-3110I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-3110

I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-3110
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-3110
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-3110/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-3110/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-3110/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-3110/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.5121, 'learning_rate': 4.512144820402409e-07, 'epoch': 0.96}
{'loss': 0.5132, 'learning_rate': 4.447551484583312e-07, 'epoch': 0.96}
{'loss': 0.5047, 'learning_rate': 4.3834217635697616e-07, 'epoch': 0.96}
{'loss': 0.5128, 'learning_rate': 4.3197557173584315e-07, 'epoch': 0.96}
{'loss': 0.5053, 'learning_rate': 4.2565534055121204e-07, 'epoch': 0.96}
 96%|| 3111/3250 [9:14:09<25:50, 11.15s/it]                                                      96%|| 3111/3250 [9:14:09<25:50, 11.15s/it] 96%|| 3112/3250 [9:14:19<25:05, 10.91s/it]                                                      96%|| 3112/3250 [9:14:19<25:05, 10.91s/it] 96%|| 3113/3250 [9:14:30<24:31, 10.74s/it]                                                      96%|| 3113/3250 [9:14:30<24:31, 10.74s/it] 96%|| 3114/3250 [9:14:40<24:04, 10.62s/it]                                                      96%|| 3114/3250 [9:14:40<24:04, 10.62s/it] 96%|| 3115/3250 [9:14:51<23:58, 10.66s/it]                                                      96%|| 3115/3250 [9:14:51<23:58, 10.66s/it] 96%|{'loss': 0.5139, 'learning_rate': 4.193814887159919e-07, 'epoch': 0.96}
{'loss': 0.5288, 'learning_rate': 4.131540220996877e-07, 'epoch': 0.96}
{'loss': 0.5149, 'learning_rate': 4.069729465284167e-07, 'epoch': 0.96}
{'loss': 0.5085, 'learning_rate': 4.0083826778489206e-07, 'epoch': 0.96}
{'loss': 0.5163, 'learning_rate': 3.947499916084285e-07, 'epoch': 0.96}
| 3116/3250 [9:15:01<23:36, 10.57s/it]                                                      96%|| 3116/3250 [9:15:01<23:36, 10.57s/it] 96%|| 3117/3250 [9:15:12<23:17, 10.50s/it]                                                      96%|| 3117/3250 [9:15:12<23:17, 10.50s/it] 96%|| 3118/3250 [9:15:22<23:00, 10.46s/it]                                                      96%|| 3118/3250 [9:15:22<23:00, 10.46s/it] 96%|| 3119/3250 [9:15:32<22:45, 10.43s/it]                                                      96%|| 3119/3250 [9:15:32<22:45, 10.43s/it] 96%|| 3120/3250 [9:15:43<22:32, 10.41s/it]                                                      96%|| 3120/3250 [9:15:43<22:32, 10.41s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.7857582569122314, 'eval_runtime': 2.1106, 'eval_samples_per_second': 5.686, 'eval_steps_per_second': 1.421, 'epoch': 0.96}
                                                      96%|| 3120/3250 [9:15:45<22:32, 10.41s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_10/tmp-checkpoint-3120
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-3120
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-3120
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-3120
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-3120
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-3120/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-3120/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-3120/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-3120/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.5321, 'learning_rate': 3.887081236949086e-07, 'epoch': 0.96}
{'loss': 0.4773, 'learning_rate': 3.8271266969682194e-07, 'epoch': 0.96}
{'loss': 0.5566, 'learning_rate': 3.767636352232151e-07, 'epoch': 0.96}
{'loss': 0.5059, 'learning_rate': 3.7086102583972494e-07, 'epoch': 0.96}
{'loss': 0.4927, 'learning_rate': 3.6500484706853964e-07, 'epoch': 0.96}
 96%|| 3121/3250 [9:15:56<24:06, 11.21s/it]                                                      96%|| 3121/3250 [9:15:56<24:06, 11.21s/it] 96%|| 3122/3250 [9:16:06<23:23, 10.96s/it]                                                      96%|| 3122/3250 [9:16:06<23:23, 10.96s/it] 96%|| 3123/3250 [9:16:17<22:49, 10.78s/it]                                                      96%|| 3123/3250 [9:16:17<22:49, 10.78s/it] 96%|| 3124/3250 [9:16:27<22:23, 10.66s/it]                                                      96%|| 3124/3250 [9:16:27<22:23, 10.66s/it] 96%|| 3125/3250 [9:16:37<22:00, 10.57s/it]                                                      96%|| 3125/3250 [9:16:37<22:00, 10.57s/it] 96%|{'loss': 0.4877, 'learning_rate': 3.591951043884212e-07, 'epoch': 0.96}
{'loss': 0.4937, 'learning_rate': 3.534318032346773e-07, 'epoch': 0.96}
{'loss': 0.5187, 'learning_rate': 3.4771494899917265e-07, 'epoch': 0.96}
{'loss': 0.4956, 'learning_rate': 3.420445470303235e-07, 'epoch': 0.96}
{'loss': 0.5211, 'learning_rate': 3.364206026330752e-07, 'epoch': 0.96}
| 3126/3250 [9:16:48<21:43, 10.51s/it]                                                      96%|| 3126/3250 [9:16:48<21:43, 10.51s/it] 96%|| 3127/3250 [9:16:58<21:27, 10.47s/it]                                                      96%|| 3127/3250 [9:16:58<21:27, 10.47s/it] 96%|| 3128/3250 [9:17:08<21:14, 10.44s/it]                                                      96%|| 3128/3250 [9:17:08<21:14, 10.44s/it] 96%|| 3129/3250 [9:17:19<21:00, 10.42s/it]                                                      96%|| 3129/3250 [9:17:19<21:00, 10.42s/it] 96%|| 3130/3250 [9:17:29<20:48, 10.41s/it]                                                      96%|| 3130/3250 [9:17:29<20:48, 10.41s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.7860001921653748, 'eval_runtime': 2.1114, 'eval_samples_per_second': 5.684, 'eval_steps_per_second': 1.421, 'epoch': 0.96}
                                                      96%|| 3130/3250 [9:17:31<20:48, 10.41s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_10/tmp-checkpoint-3130
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-3130
 /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-3130
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-3130
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-3130
the pytorch model path isthe pytorch model path isthe pytorch model path is  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-3130/pytorch_model.bin/projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-3130/pytorch_model.bin

 /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-3130/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-3130/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 1.015, 'learning_rate': 3.3084312106892446e-07, 'epoch': 0.96}
{'loss': 0.5043, 'learning_rate': 3.2531210755588624e-07, 'epoch': 0.96}
{'loss': 0.4998, 'learning_rate': 3.1982756726851584e-07, 'epoch': 0.96}
{'loss': 0.5287, 'learning_rate': 3.143895053378698e-07, 'epoch': 0.96}
{'loss': 0.5216, 'learning_rate': 3.0899792685155083e-07, 'epoch': 0.96}
 96%|| 3131/3250 [9:17:42<22:08, 11.16s/it]                                                      96%|| 3131/3250 [9:17:42<22:08, 11.16s/it] 96%|| 3132/3250 [9:17:53<21:46, 11.07s/it]                                                      96%|| 3132/3250 [9:17:53<21:46, 11.07s/it] 96%|| 3133/3250 [9:18:03<21:11, 10.87s/it]                                                      96%|| 3133/3250 [9:18:03<21:11, 10.87s/it] 96%|| 3134/3250 [9:18:14<20:42, 10.71s/it]                                                      96%|| 3134/3250 [9:18:14<20:42, 10.71s/it] 96%|| 3135/3250 [9:18:24<20:19, 10.61s/it]                                                      96%|| 3135/3250 [9:18:24<20:19, 10.61s/it] 96%|{'loss': 0.4935, 'learning_rate': 3.036528368536462e-07, 'epoch': 0.96}
{'loss': 0.4934, 'learning_rate': 2.9835424034476146e-07, 'epoch': 0.97}
{'loss': 0.567, 'learning_rate': 2.9310214228202013e-07, 'epoch': 0.97}
{'loss': 0.5039, 'learning_rate': 2.8789654757901965e-07, 'epoch': 0.97}
{'loss': 0.5173, 'learning_rate': 2.8273746110585863e-07, 'epoch': 0.97}
| 3136/3250 [9:18:34<20:00, 10.53s/it]                                                      96%|| 3136/3250 [9:18:34<20:00, 10.53s/it] 97%|| 3137/3250 [9:18:45<19:44, 10.48s/it]                                                      97%|| 3137/3250 [9:18:45<19:44, 10.48s/it] 97%|| 3138/3250 [9:18:55<19:30, 10.46s/it]                                                      97%|| 3138/3250 [9:18:55<19:30, 10.46s/it] 97%|| 3139/3250 [9:19:05<19:17, 10.42s/it]                                                      97%|| 3139/3250 [9:19:05<19:17, 10.42s/it] 97%|| 3140/3250 [9:19:16<19:04, 10.40s/it]                                                      97%|| 3140/3250 [9:19:16<19:04, 10.40s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.786381185054779, 'eval_runtime': 2.1217, 'eval_samples_per_second': 5.656, 'eval_steps_per_second': 1.414, 'epoch': 0.97}
                                                      97%|| 3140/3250 [9:19:18<19:04, 10.40s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_10/tmp-checkpoint-3140
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-3140
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-3140
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-3140
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-3140
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-3140/pytorch_model.binthe pytorch model path is
 /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-3140/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-3140/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-3140/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.4712, 'learning_rate': 2.776248876891319e-07, 'epoch': 0.97}
{'loss': 0.5131, 'learning_rate': 2.725588321119188e-07, 'epoch': 0.97}
{'loss': 0.5362, 'learning_rate': 2.675392991137671e-07, 'epoch': 0.97}
{'loss': 0.5025, 'learning_rate': 2.625662933907147e-07, 'epoch': 0.97}
{'loss': 0.4948, 'learning_rate': 2.5763981959526786e-07, 'epoch': 0.97}
 97%|| 3141/3250 [9:19:29<20:18, 11.18s/it]                                                      97%|| 3141/3250 [9:19:29<20:18, 11.18s/it] 97%|| 3142/3250 [9:19:39<19:40, 10.93s/it]                                                      97%|| 3142/3250 [9:19:39<19:40, 10.93s/it] 97%|| 3143/3250 [9:19:49<19:10, 10.75s/it]                                                      97%|| 3143/3250 [9:19:49<19:10, 10.75s/it] 97%|| 3144/3250 [9:20:00<18:48, 10.64s/it]                                                      97%|| 3144/3250 [9:20:00<18:48, 10.64s/it] 97%|| 3145/3250 [9:20:10<18:27, 10.55s/it]                                                      97%|| 3145/3250 [9:20:10<18:27, 10.55s/it] 97%|{'loss': 0.5198, 'learning_rate': 2.527598823363786e-07, 'epoch': 0.97}
{'loss': 0.5261, 'learning_rate': 2.4792648617950056e-07, 'epoch': 0.97}
{'loss': 0.5418, 'learning_rate': 2.4313963564650546e-07, 'epoch': 0.97}
{'loss': 0.523, 'learning_rate': 2.3839933521575543e-07, 'epoch': 0.97}
{'loss': 0.5076, 'learning_rate': 2.3370558932203635e-07, 'epoch': 0.97}
| 3146/3250 [9:20:21<18:10, 10.49s/it]                                                      97%|| 3146/3250 [9:20:21<18:10, 10.49s/it] 97%|| 3147/3250 [9:20:31<17:55, 10.45s/it]                                                      97%|| 3147/3250 [9:20:31<17:55, 10.45s/it] 97%|| 3148/3250 [9:20:42<17:53, 10.53s/it]                                                      97%|| 3148/3250 [9:20:42<17:53, 10.53s/it] 97%|| 3149/3250 [9:20:52<17:37, 10.47s/it]                                                      97%|| 3149/3250 [9:20:52<17:37, 10.47s/it] 97%|| 3150/3250 [9:21:02<17:25, 10.46s/it]                                                      97%|| 3150/3250 [9:21:02<17:25, 10.46s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.7857178449630737, 'eval_runtime': 2.1156, 'eval_samples_per_second': 5.672, 'eval_steps_per_second': 1.418, 'epoch': 0.97}
                                                      97%|| 3150/3250 [9:21:04<17:25, 10.46s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_10/tmp-checkpoint-3150
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-3150
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-3150
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-3150
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-3150
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-3150/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-3150/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-3150/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-3150/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.5386, 'learning_rate': 2.290584023565856e-07, 'epoch': 0.97}
{'loss': 0.5046, 'learning_rate': 2.2445777866709205e-07, 'epoch': 0.97}
{'loss': 0.5256, 'learning_rate': 2.199037225576739e-07, 'epoch': 0.97}
{'loss': 0.5191, 'learning_rate': 2.153962382888841e-07, 'epoch': 0.97}
{'loss': 0.4988, 'learning_rate': 2.1093533007770506e-07, 'epoch': 0.97}
 97%|| 3151/3250 [9:21:15<18:29, 11.21s/it]                                                      97%|| 3151/3250 [9:21:15<18:29, 11.21s/it] 97%|| 3152/3250 [9:21:26<17:53, 10.96s/it]                                                      97%|| 3152/3250 [9:21:26<17:53, 10.96s/it] 97%|| 3153/3250 [9:21:36<17:25, 10.77s/it]                                                      97%|| 3153/3250 [9:21:36<17:25, 10.77s/it] 97%|| 3154/3250 [9:21:46<17:01, 10.64s/it]                                                      97%|| 3154/3250 [9:21:46<17:01, 10.64s/it] 97%|| 3155/3250 [9:21:57<16:42, 10.55s/it]                                                      97%|| 3155/3250 [9:21:57<16:42, 10.55s/it] 97%|{'loss': 0.4915, 'learning_rate': 2.0652100209755388e-07, 'epoch': 0.97}
{'loss': 0.4861, 'learning_rate': 2.0215325847825485e-07, 'epoch': 0.97}
{'loss': 0.5292, 'learning_rate': 1.978321033060504e-07, 'epoch': 0.97}
{'loss': 0.5062, 'learning_rate': 1.935575406236123e-07, 'epoch': 0.97}
{'loss': 0.5239, 'learning_rate': 1.8932957443000832e-07, 'epoch': 0.97}
| 3156/3250 [9:22:07<16:26, 10.49s/it]                                                      97%|| 3156/3250 [9:22:07<16:26, 10.49s/it] 97%|| 3157/3250 [9:22:17<16:11, 10.45s/it]                                                      97%|| 3157/3250 [9:22:17<16:11, 10.45s/it] 97%|| 3158/3250 [9:22:28<15:58, 10.42s/it]                                                      97%|| 3158/3250 [9:22:28<15:58, 10.42s/it] 97%|| 3159/3250 [9:22:38<15:46, 10.40s/it]                                                      97%|| 3159/3250 [9:22:38<15:46, 10.40s/it] 97%|| 3160/3250 [9:22:48<15:34, 10.38s/it]                                                      97%|| 3160/3250 [9:22:48<15:34, 10.38s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.7858211398124695, 'eval_runtime': 2.11, 'eval_samples_per_second': 5.687, 'eval_steps_per_second': 1.422, 'epoch': 0.97}
                                                      97%|| 3160/3250 [9:22:51<15:34, 10.38s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_10/tmp-checkpoint-3160
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-3160I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-3160

I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-3160
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-3160
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-3160/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-3160/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-3160/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-3160/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 1.0347, 'learning_rate': 1.851482086807299e-07, 'epoch': 0.97}
{'loss': 0.4774, 'learning_rate': 1.8101344728764234e-07, 'epoch': 0.97}
{'loss': 0.5006, 'learning_rate': 1.7692529411904578e-07, 'epoch': 0.97}
{'loss': 0.5232, 'learning_rate': 1.72883752999603e-07, 'epoch': 0.97}
{'loss': 0.5298, 'learning_rate': 1.688888277103895e-07, 'epoch': 0.97}
 97%|| 3161/3250 [9:23:01<16:34, 11.17s/it]                                                      97%|| 3161/3250 [9:23:01<16:34, 11.17s/it] 97%|| 3162/3250 [9:23:12<16:00, 10.92s/it]                                                      97%|| 3162/3250 [9:23:12<16:00, 10.92s/it] 97%|| 3163/3250 [9:23:22<15:34, 10.74s/it]                                                      97%|| 3163/3250 [9:23:22<15:34, 10.74s/it] 97%|| 3164/3250 [9:23:33<15:19, 10.69s/it]                                                      97%|| 3164/3250 [9:23:33<15:19, 10.69s/it] 97%|| 3165/3250 [9:23:43<14:59, 10.59s/it]                                                      97%|| 3165/3250 [9:23:43<14:59, 10.59s/it] 97%|{'loss': 0.5093, 'learning_rate': 1.6494052198886555e-07, 'epoch': 0.97}
{'loss': 0.5071, 'learning_rate': 1.6103883952886534e-07, 'epoch': 0.97}
{'loss': 0.5552, 'learning_rate': 1.5718378398063005e-07, 'epoch': 0.97}
{'loss': 0.5314, 'learning_rate': 1.5337535895074695e-07, 'epoch': 0.98}
{'loss': 0.5013, 'learning_rate': 1.4961356800219927e-07, 'epoch': 0.98}
| 3166/3250 [9:23:53<14:43, 10.52s/it]                                                      97%|| 3166/3250 [9:23:53<14:43, 10.52s/it] 97%|| 3167/3250 [9:24:04<14:29, 10.48s/it]                                                      97%|| 3167/3250 [9:24:04<14:29, 10.48s/it] 97%|| 3168/3250 [9:24:14<14:16, 10.44s/it]                                                      97%|| 3168/3250 [9:24:14<14:16, 10.44s/it] 98%|| 3169/3250 [9:24:24<14:03, 10.42s/it]                                                      98%|| 3169/3250 [9:24:24<14:03, 10.42s/it] 98%|| 3170/3250 [9:24:35<13:51, 10.40s/it]                                                      98%|| 3170/3250 [9:24:35<13:51, 10.40s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.7858821749687195, 'eval_runtime': 2.5085, 'eval_samples_per_second': 4.784, 'eval_steps_per_second': 1.196, 'epoch': 0.98}
                                                      98%|| 3170/3250 [9:24:37<13:51, 10.40s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_10/tmp-checkpoint-3170
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-3170
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-3170
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-3170
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-3170
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-3170/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-3170/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-3170/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-3170/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.4775, 'learning_rate': 1.4589841465433297e-07, 'epoch': 0.98}
{'loss': 0.521, 'learning_rate': 1.4222990238287325e-07, 'epoch': 0.98}
{'loss': 0.5099, 'learning_rate': 1.3860803461989146e-07, 'epoch': 0.98}
{'loss': 0.513, 'learning_rate': 1.3503281475383823e-07, 'epoch': 0.98}
{'loss': 0.514, 'learning_rate': 1.3150424612951573e-07, 'epoch': 0.98}
 98%|| 3171/3250 [9:24:48<14:52, 11.29s/it]                                                      98%|| 3171/3250 [9:24:48<14:52, 11.29s/it] 98%|| 3172/3250 [9:24:59<14:19, 11.02s/it]                                                      98%|| 3172/3250 [9:24:59<14:19, 11.02s/it] 98%|| 3173/3250 [9:25:09<13:53, 10.82s/it]                                                      98%|| 3173/3250 [9:25:09<13:53, 10.82s/it] 98%|| 3174/3250 [9:25:19<13:31, 10.68s/it]                                                      98%|| 3174/3250 [9:25:19<13:31, 10.68s/it] 98%|| 3175/3250 [9:25:30<13:13, 10.58s/it]                                                      98%|| 3175/3250 [9:25:30<13:13, 10.58s/it] 98%|{'loss': 0.5142, 'learning_rate': 1.280223320480778e-07, 'epoch': 0.98}
{'loss': 0.5078, 'learning_rate': 1.2458707576703533e-07, 'epoch': 0.98}
{'loss': 0.5358, 'learning_rate': 1.2119848050025085e-07, 'epoch': 0.98}
{'loss': 0.5218, 'learning_rate': 1.178565494179218e-07, 'epoch': 0.98}
{'loss': 0.5154, 'learning_rate': 1.1456128564660273e-07, 'epoch': 0.98}
| 3176/3250 [9:25:40<12:58, 10.52s/it]                                                      98%|| 3176/3250 [9:25:40<12:58, 10.52s/it] 98%|| 3177/3250 [9:25:50<12:44, 10.47s/it]                                                      98%|| 3177/3250 [9:25:50<12:44, 10.47s/it] 98%|| 3178/3250 [9:26:01<12:31, 10.44s/it]                                                      98%|| 3178/3250 [9:26:01<12:31, 10.44s/it] 98%|| 3179/3250 [9:26:11<12:19, 10.42s/it]                                                      98%|| 3179/3250 [9:26:11<12:19, 10.42s/it] 98%|| 3180/3250 [9:26:22<12:08, 10.41s/it]                                                      98%|| 3180/3250 [9:26:22<12:08, 10.41s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.7859385013580322, 'eval_runtime': 2.1137, 'eval_samples_per_second': 5.677, 'eval_steps_per_second': 1.419, 'epoch': 0.98}
                                                      98%|| 3180/3250 [9:26:24<12:08, 10.41s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_10/tmp-checkpoint-3180
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-3180
/projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-3180
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-3180
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-3180
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-3180/pytorch_model.bin
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-3180/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-3180/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-3180/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.5133, 'learning_rate': 1.1131269226918317e-07, 'epoch': 0.98}
{'loss': 0.5175, 'learning_rate': 1.0811077232488753e-07, 'epoch': 0.98}
{'loss': 0.4765, 'learning_rate': 1.0495552880927517e-07, 'epoch': 0.98}
{'loss': 0.5421, 'learning_rate': 1.0184696467424037e-07, 'epoch': 0.98}
{'loss': 0.4917, 'learning_rate': 9.878508282800126e-08, 'epoch': 0.98}
 98%|| 3181/3250 [9:26:35<13:03, 11.35s/it]                                                      98%|| 3181/3250 [9:26:35<13:03, 11.35s/it] 98%|| 3182/3250 [9:26:45<12:32, 11.06s/it]                                                      98%|| 3182/3250 [9:26:45<12:32, 11.06s/it] 98%|| 3183/3250 [9:26:56<12:06, 10.85s/it]                                                      98%|| 3183/3250 [9:26:56<12:06, 10.85s/it] 98%|| 3184/3250 [9:27:06<11:46, 10.70s/it]                                                      98%|| 3184/3250 [9:27:06<11:46, 10.70s/it] 98%|| 3185/3250 [9:27:17<11:28, 10.60s/it]                                                      98%|| 3185/3250 [9:27:17<11:28, 10.60s/it] 98%|{'loss': 0.498, 'learning_rate': 9.576988613511085e-08, 'epoch': 0.98}
{'loss': 0.4925, 'learning_rate': 9.280137741643491e-08, 'epoch': 0.98}
{'loss': 0.5079, 'learning_rate': 8.987955944917414e-08, 'epoch': 0.98}
{'loss': 0.5057, 'learning_rate': 8.700443496683086e-08, 'epoch': 0.98}
{'loss': 0.5273, 'learning_rate': 8.417600665923675e-08, 'epoch': 0.98}
| 3186/3250 [9:27:27<11:13, 10.53s/it]                                                      98%|| 3186/3250 [9:27:27<11:13, 10.53s/it] 98%|| 3187/3250 [9:27:37<11:00, 10.48s/it]                                                      98%|| 3187/3250 [9:27:37<11:00, 10.48s/it] 98%|| 3188/3250 [9:27:48<10:47, 10.45s/it]                                                      98%|| 3188/3250 [9:27:48<10:47, 10.45s/it] 98%|| 3189/3250 [9:27:58<10:36, 10.43s/it]                                                      98%|| 3189/3250 [9:27:58<10:36, 10.43s/it] 98%|| 3190/3250 [9:28:08<10:24, 10.41s/it]                                                      98%|| 3190/3250 [9:28:08<10:24, 10.41s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.785050630569458, 'eval_runtime': 2.1173, 'eval_samples_per_second': 5.667, 'eval_steps_per_second': 1.417, 'epoch': 0.98}
                                                      98%|| 3190/3250 [9:28:11<10:24, 10.41s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_10/tmp-checkpoint-3190
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-3190I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-3190

I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-3190
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-3190
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-3190/pytorch_model.bin
the pytorch model path is the pytorch model path is/projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-3190/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-3190/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-3190/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.5088, 'learning_rate': 8.139427717253622e-08, 'epoch': 0.98}
{'loss': 1.0056, 'learning_rate': 7.865924910916977e-08, 'epoch': 0.98}
{'loss': 0.4975, 'learning_rate': 7.597092502790725e-08, 'epoch': 0.98}
{'loss': 0.5255, 'learning_rate': 7.332930744380906e-08, 'epoch': 0.98}
{'loss': 0.5121, 'learning_rate': 7.073439882824273e-08, 'epoch': 0.98}
 98%|| 3191/3250 [9:28:21<10:59, 11.18s/it]                                                      98%|| 3191/3250 [9:28:21<10:59, 11.18s/it] 98%|| 3192/3250 [9:28:32<10:34, 10.93s/it]                                                      98%|| 3192/3250 [9:28:32<10:34, 10.93s/it] 98%|| 3193/3250 [9:28:42<10:13, 10.76s/it]                                                      98%|| 3193/3250 [9:28:42<10:13, 10.76s/it] 98%|| 3194/3250 [9:28:52<09:55, 10.63s/it]                                                      98%|| 3194/3250 [9:28:52<09:55, 10.63s/it] 98%|| 3195/3250 [9:29:03<09:40, 10.55s/it]                                                      98%|| 3195/3250 [9:29:03<09:40, 10.55s/it] 98%|{'loss': 0.5236, 'learning_rate': 6.818620160887746e-08, 'epoch': 0.98}
{'loss': 0.4928, 'learning_rate': 6.568471816968957e-08, 'epoch': 0.98}
{'loss': 0.5487, 'learning_rate': 6.322995085094041e-08, 'epoch': 0.98}
{'loss': 0.5404, 'learning_rate': 6.08219019491929e-08, 'epoch': 0.98}
{'loss': 0.5071, 'learning_rate': 5.846057371730052e-08, 'epoch': 0.98}
| 3196/3250 [9:29:13<09:26, 10.49s/it]                                                      98%|| 3196/3250 [9:29:13<09:26, 10.49s/it] 98%|| 3197/3250 [9:29:24<09:17, 10.51s/it]                                                      98%|| 3197/3250 [9:29:24<09:17, 10.51s/it] 98%|| 3198/3250 [9:29:34<09:04, 10.47s/it]                                                      98%|| 3198/3250 [9:29:34<09:04, 10.47s/it] 98%|| 3199/3250 [9:29:44<08:52, 10.44s/it]                                                      98%|| 3199/3250 [9:29:44<08:52, 10.44s/it] 98%|| 3200/3250 [9:29:55<08:41, 10.42s/it]                                                      98%|| 3200/3250 [9:29:55<08:41, 10.42s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.7860618829727173, 'eval_runtime': 2.1241, 'eval_samples_per_second': 5.649, 'eval_steps_per_second': 1.412, 'epoch': 0.98}
                                                      98%|| 3200/3250 [9:29:57<08:41, 10.42s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_10/tmp-checkpoint-3200
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-3200
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-3200
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-3200
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-3200
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-3200/pytorch_model.bin
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-3200/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-3200/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-3200/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.4954, 'learning_rate': 5.614596836440722e-08, 'epoch': 0.98}
{'loss': 0.5128, 'learning_rate': 5.3878088055947515e-08, 'epoch': 0.99}
{'loss': 0.509, 'learning_rate': 5.165693491363532e-08, 'epoch': 0.99}
{'loss': 0.5109, 'learning_rate': 4.9482511015475074e-08, 'epoch': 0.99}
{'loss': 0.5038, 'learning_rate': 4.735481839575617e-08, 'epoch': 0.99}
 98%|| 3201/3250 [9:30:08<09:09, 11.22s/it]                                                      98%|| 3201/3250 [9:30:08<09:09, 11.22s/it] 99%|| 3202/3250 [9:30:18<08:47, 10.99s/it]                                                      99%|| 3202/3250 [9:30:18<08:47, 10.99s/it] 99%|| 3203/3250 [9:30:29<08:27, 10.80s/it]                                                      99%|| 3203/3250 [9:30:29<08:27, 10.80s/it] 99%|| 3204/3250 [9:30:39<08:10, 10.67s/it]                                                      99%|| 3204/3250 [9:30:39<08:10, 10.67s/it] 99%|| 3205/3250 [9:30:49<07:55, 10.57s/it]                                                      99%|| 3205/3250 [9:30:49<07:55, 10.57s/it] 99%|{'loss': 0.5057, 'learning_rate': 4.527385904504189e-08, 'epoch': 0.99}
{'loss': 0.5149, 'learning_rate': 4.323963491016936e-08, 'epoch': 0.99}
{'loss': 0.5316, 'learning_rate': 4.1252147894277336e-08, 'epoch': 0.99}
{'loss': 0.5092, 'learning_rate': 3.9311399856745145e-08, 'epoch': 0.99}
{'loss': 0.5264, 'learning_rate': 3.741739261324817e-08, 'epoch': 0.99}
| 3206/3250 [9:31:00<07:42, 10.51s/it]                                                      99%|| 3206/3250 [9:31:00<07:42, 10.51s/it] 99%|| 3207/3250 [9:31:10<07:30, 10.47s/it]                                                      99%|| 3207/3250 [9:31:10<07:30, 10.47s/it] 99%|| 3208/3250 [9:31:20<07:18, 10.44s/it]                                                      99%|| 3208/3250 [9:31:20<07:18, 10.44s/it] 99%|| 3209/3250 [9:31:31<07:06, 10.41s/it]                                                      99%|| 3209/3250 [9:31:31<07:06, 10.41s/it] 99%|| 3210/3250 [9:31:41<06:55, 10.40s/it]                                                      99%|| 3210/3250 [9:31:41<06:55, 10.40s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.7856452465057373, 'eval_runtime': 2.1599, 'eval_samples_per_second': 5.556, 'eval_steps_per_second': 1.389, 'epoch': 0.99}
                                                      99%|| 3210/3250 [9:31:43<06:55, 10.40s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_10/tmp-checkpoint-3210
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-3210
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-3210
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-3210
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-3210
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-3210/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-3210/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-3210/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-3210/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.5198, 'learning_rate': 3.557012793573011e-08, 'epoch': 0.99}
{'loss': 0.5324, 'learning_rate': 3.376960755240299e-08, 'epoch': 0.99}
{'loss': 0.4795, 'learning_rate': 3.2015833147741594e-08, 'epoch': 0.99}
{'loss': 0.5452, 'learning_rate': 3.030880636249456e-08, 'epoch': 0.99}
{'loss': 0.509, 'learning_rate': 2.8648528793673302e-08, 'epoch': 0.99}
 99%|| 3211/3250 [9:31:54<07:19, 11.26s/it]                                                      99%|| 3211/3250 [9:31:54<07:19, 11.26s/it] 99%|| 3212/3250 [9:32:05<06:57, 10.99s/it]                                                      99%|| 3212/3250 [9:32:05<06:57, 10.99s/it] 99%|| 3213/3250 [9:32:15<06:39, 10.80s/it]                                                      99%|| 3213/3250 [9:32:15<06:39, 10.80s/it] 99%|| 3214/3250 [9:32:26<06:32, 10.89s/it]                                                      99%|| 3214/3250 [9:32:26<06:32, 10.89s/it] 99%|| 3215/3250 [9:32:37<06:15, 10.73s/it]                                                      99%|| 3215/3250 [9:32:37<06:15, 10.73s/it] 99%|{'loss': 0.4897, 'learning_rate': 2.7035001994552e-08, 'epoch': 0.99}
{'loss': 0.4888, 'learning_rate': 2.5468227474667595e-08, 'epoch': 0.99}
{'loss': 0.4928, 'learning_rate': 2.3948206699819786e-08, 'epoch': 0.99}
{'loss': 0.5292, 'learning_rate': 2.2474941092065492e-08, 'epoch': 0.99}
{'loss': 0.4978, 'learning_rate': 2.1048432029718845e-08, 'epoch': 0.99}
| 3216/3250 [9:32:47<06:01, 10.62s/it]                                                      99%|| 3216/3250 [9:32:47<06:01, 10.62s/it] 99%|| 3217/3250 [9:32:57<05:47, 10.54s/it]                                                      99%|| 3217/3250 [9:32:57<05:47, 10.54s/it] 99%|| 3218/3250 [9:33:08<05:35, 10.49s/it]                                                      99%|| 3218/3250 [9:33:08<05:35, 10.49s/it] 99%|| 3219/3250 [9:33:18<05:24, 10.45s/it]                                                      99%|| 3219/3250 [9:33:18<05:24, 10.45s/it] 99%|| 3220/3250 [9:33:28<05:12, 10.43s/it]                                                      99%|| 3220/3250 [9:33:28<05:12, 10.43s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.7859696745872498, 'eval_runtime': 2.1211, 'eval_samples_per_second': 5.658, 'eval_steps_per_second': 1.414, 'epoch': 0.99}
                                                      99%|| 3220/3250 [9:33:31<05:12, 10.43s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_10/tmp-checkpoint-3220
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-3220
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-3220
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-3220
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-3220
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-3220/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-3220/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-3220/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-3220/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.5295, 'learning_rate': 1.9668680847356735e-08, 'epoch': 0.99}
{'loss': 1.0112, 'learning_rate': 1.8335688835802167e-08, 'epoch': 0.99}
{'loss': 0.5069, 'learning_rate': 1.7049457242140908e-08, 'epoch': 0.99}
{'loss': 0.5055, 'learning_rate': 1.580998726971039e-08, 'epoch': 0.99}
{'loss': 0.5287, 'learning_rate': 1.4617280078094152e-08, 'epoch': 0.99}
 99%|| 3221/3250 [9:33:42<05:25, 11.21s/it]                                                      99%|| 3221/3250 [9:33:42<05:25, 11.21s/it] 99%|| 3222/3250 [9:33:52<05:06, 10.96s/it]                                                      99%|| 3222/3250 [9:33:52<05:06, 10.96s/it] 99%|| 3223/3250 [9:34:02<04:51, 10.78s/it]                                                      99%|| 3223/3250 [9:34:02<04:51, 10.78s/it] 99%|| 3224/3250 [9:34:13<04:37, 10.66s/it]                                                      99%|| 3224/3250 [9:34:13<04:37, 10.66s/it] 99%|| 3225/3250 [9:34:23<04:24, 10.57s/it]                                                      99%|| 3225/3250 [9:34:23<04:24, 10.57s/it] 99%|{'loss': 0.5176, 'learning_rate': 1.3471336783132949e-08, 'epoch': 0.99}
{'loss': 0.4902, 'learning_rate': 1.2372158456919192e-08, 'epoch': 0.99}
{'loss': 0.4933, 'learning_rate': 1.1319746127785857e-08, 'epoch': 0.99}
{'loss': 0.5722, 'learning_rate': 1.0314100780317581e-08, 'epoch': 0.99}
{'loss': 0.5076, 'learning_rate': 9.355223355350662e-09, 'epoch': 0.99}
| 3226/3250 [9:34:33<04:12, 10.51s/it]                                                      99%|| 3226/3250 [9:34:33<04:12, 10.51s/it] 99%|| 3227/3250 [9:34:44<04:00, 10.47s/it]                                                      99%|| 3227/3250 [9:34:44<04:00, 10.47s/it] 99%|| 3228/3250 [9:34:54<03:49, 10.44s/it]                                                      99%|| 3228/3250 [9:34:54<03:49, 10.44s/it] 99%|| 3229/3250 [9:35:04<03:38, 10.42s/it]                                                      99%|| 3229/3250 [9:35:04<03:38, 10.42s/it] 99%|| 3230/3250 [9:35:15<03:30, 10.52s/it]                                                      99%|| 3230/3250 [9:35:15<03:30, 10.52s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.7855754494667053, 'eval_runtime': 2.1164, 'eval_samples_per_second': 5.67, 'eval_steps_per_second': 1.417, 'epoch': 0.99}
                                                      99%|| 3230/3250 [9:35:17<03:30, 10.52s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_10/tmp-checkpoint-3230
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-3230
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-3230
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-3230
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-3230
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-3230/pytorch_model.bin
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-3230/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-3230/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-3230/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.5185, 'learning_rate': 8.443114749967506e-09, 'epoch': 0.99}
{'loss': 0.4784, 'learning_rate': 7.577775817485533e-09, 'epoch': 0.99}
{'loss': 0.5192, 'learning_rate': 6.75920736747937e-09, 'epoch': 0.99}
{'loss': 0.5187, 'learning_rate': 5.987410165758656e-09, 'epoch': 1.0}
{'loss': 0.4899, 'learning_rate': 5.26238493438469e-09, 'epoch': 1.0}
 99%|| 3231/3250 [9:35:28<03:33, 11.25s/it]                                                      99%|| 3231/3250 [9:35:28<03:33, 11.25s/it] 99%|| 3232/3250 [9:35:39<03:17, 10.98s/it]                                                      99%|| 3232/3250 [9:35:39<03:17, 10.98s/it] 99%|| 3233/3250 [9:35:49<03:03, 10.80s/it]                                                      99%|| 3233/3250 [9:35:49<03:03, 10.80s/it]100%|| 3234/3250 [9:35:59<02:50, 10.67s/it]                                                     100%|| 3234/3250 [9:35:59<02:50, 10.67s/it]100%|| 3235/3250 [9:36:10<02:38, 10.57s/it]                                                     100%|| 3235/3250 [9:36:10<02:38, 10.57s/it]100%|{'loss': 0.4907, 'learning_rate': 4.584132351642678e-09, 'epoch': 1.0}
{'loss': 0.5168, 'learning_rate': 3.9526530520916926e-09, 'epoch': 1.0}
{'loss': 0.5256, 'learning_rate': 3.3679476264980582e-09, 'epoch': 1.0}
{'loss': 0.537, 'learning_rate': 2.830016621885312e-09, 'epoch': 1.0}
{'loss': 0.5243, 'learning_rate': 2.3388605415231024e-09, 'epoch': 1.0}
| 3236/3250 [9:36:20<02:27, 10.51s/it]                                                     100%|| 3236/3250 [9:36:20<02:27, 10.51s/it]100%|| 3237/3250 [9:36:30<02:16, 10.47s/it]                                                     100%|| 3237/3250 [9:36:30<02:16, 10.47s/it]100%|| 3238/3250 [9:36:41<02:05, 10.43s/it]                                                     100%|| 3238/3250 [9:36:41<02:05, 10.43s/it]100%|| 3239/3250 [9:36:51<01:54, 10.42s/it]                                                     100%|| 3239/3250 [9:36:51<01:54, 10.42s/it]100%|| 3240/3250 [9:37:01<01:44, 10.40s/it]                                                     100%|| 3240/3250 [9:37:01<01:44, 10.40s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.7856894731521606, 'eval_runtime': 2.3458, 'eval_samples_per_second': 5.116, 'eval_steps_per_second': 1.279, 'epoch': 1.0}
                                                     100%|| 3240/3250 [9:37:04<01:44, 10.40s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_10/tmp-checkpoint-3240
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-3240
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-3240
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-3240
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-3240
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-3240/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-3240/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-3240/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-3240/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.5102, 'learning_rate': 1.894479844910535e-09, 'epoch': 1.0}
{'loss': 0.5293, 'learning_rate': 1.4968749477872746e-09, 'epoch': 1.0}
{'loss': 0.4946, 'learning_rate': 1.1460462221279944e-09, 'epoch': 1.0}
{'loss': 0.5334, 'learning_rate': 8.419939961645807e-10, 'epoch': 1.0}
{'loss': 0.5114, 'learning_rate': 5.847185543417233e-10, 'epoch': 1.0}
100%|| 3241/3250 [9:37:15<01:41, 11.26s/it]                                                     100%|| 3241/3250 [9:37:15<01:41, 11.26s/it]100%|| 3242/3250 [9:37:25<01:27, 10.99s/it]                                                     100%|| 3242/3250 [9:37:25<01:27, 10.99s/it]100%|| 3243/3250 [9:37:35<01:15, 10.81s/it]                                                     100%|| 3243/3250 [9:37:35<01:15, 10.81s/it]100%|| 3244/3250 [9:37:46<01:04, 10.67s/it]                                                     100%|| 3244/3250 [9:37:46<01:04, 10.67s/it]100%|| 3245/3250 [9:37:56<00:52, 10.58s/it]                                                     100%|| 3245/3250 [9:37:56<00:52, 10.58s/it]100%|{'loss': 0.507, 'learning_rate': 3.742201373557741e-10, 'epoch': 1.0}
{'loss': 0.4954, 'learning_rate': 2.1049894213809317e-10, 'epoch': 1.0}
{'loss': 0.4838, 'learning_rate': 9.355512186615123e-11, 'epoch': 1.0}
{'loss': 0.5247, 'learning_rate': 2.338878593577398e-11, 'epoch': 1.0}
{'loss': 0.4982, 'learning_rate': 0.0, 'epoch': 1.0}
| 3246/3250 [9:38:07<00:42, 10.60s/it]                                                     100%|| 3246/3250 [9:38:07<00:42, 10.60s/it]100%|| 3247/3250 [9:38:17<00:31, 10.54s/it]                                                     100%|| 3247/3250 [9:38:17<00:31, 10.54s/it]100%|| 3248/3250 [9:38:28<00:20, 10.49s/it]                                                     100%|| 3248/3250 [9:38:28<00:20, 10.49s/it]100%|| 3249/3250 [9:38:38<00:10, 10.45s/it]                                                     100%|| 3249/3250 [9:38:38<00:10, 10.45s/it]100%|| 3250/3250 [9:38:48<00:00, 10.43s/it]                                                     100%|| 3250/3250 [9:38:48<00:00, 10.43s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.7859672904014587, 'eval_runtime': 2.1184, 'eval_samples_per_second': 5.665, 'eval_steps_per_second': 1.416, 'epoch': 1.0}
                                                     100%|| 3250/3250 [9:38:50<00:00, 10.43s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_10/tmp-checkpoint-3250
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  I AM HERE AND I AM SAVING THE MODEL
/projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-3250the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-3250

I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-3250
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-3250
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-3250/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-3250/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-3250/pytorch_model.binthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_10/checkpoint-3250/pytorch_model.bin



Training completed. Do not forget to share your model on huggingface.co/models =)


{'train_runtime': 34752.7389, 'train_samples_per_second': 5.985, 'train_steps_per_second': 0.094, 'train_loss': 0.6742877743152472, 'epoch': 1.0}
Saving last checkpoint of the model
Saving last checkpoint of the model
Saving last checkpoint of the model
                                                     100%|| 3250/3250 [9:38:51<00:00, 10.43s/it]100%|| 3250/3250 [9:38:51<00:00, 10.69s/it]
Saving last checkpoint of the model
wandb: - 0.008 MB of 0.008 MB uploadedwandb: \ 0.008 MB of 0.008 MB uploadedwandb: | 0.008 MB of 0.825 MB uploadedwandb: / 0.825 MB of 0.825 MB uploadedwandb: 
wandb: Run history:
wandb:                      eval/loss 
wandb:                   eval/runtime 
wandb:        eval/samples_per_second 
wandb:          eval/steps_per_second 
wandb:                    train/epoch 
wandb:              train/global_step 
wandb:            train/learning_rate 
wandb:                     train/loss 
wandb:               train/total_flos 
wandb:               train/train_loss 
wandb:            train/train_runtime 
wandb: train/train_samples_per_second 
wandb:   train/train_steps_per_second 
wandb: 
wandb: Run summary:
wandb:                      eval/loss 0.78597
wandb:                   eval/runtime 2.1184
wandb:        eval/samples_per_second 5.665
wandb:          eval/steps_per_second 1.416
wandb:                    train/epoch 1.0
wandb:              train/global_step 3250
wandb:            train/learning_rate 0.0
wandb:                     train/loss 0.4982
wandb:               train/total_flos 1.850842437124096e+18
wandb:               train/train_loss 0.67429
wandb:            train/train_runtime 34752.7389
wandb: train/train_samples_per_second 5.985
wandb:   train/train_steps_per_second 0.094
wandb: 
wandb:  View run FinalRuns-/projects/bbvz/bzd2/checkpoints_experiment_10 at: https://wandb.ai/complex_dnn/huggingface/runs/nctaka3q
wandb:  View job at https://wandb.ai/complex_dnn/huggingface/jobs/QXJ0aWZhY3RDb2xsZWN0aW9uOjExOTYzMDY1Mg==/version_details/v9
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: /projects/bbvz/bzd2/wandb/run-20231213_191346-nctaka3q/logs
/var/spool/slurmd/job2748389/slurm_script: line 271: --save_freq=50: command not found
