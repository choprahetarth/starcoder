Running "module reset". Resetting modules to system default. The following $MODULEPATH directories have been removed: None

Currently Loaded Modules:
  1) cue-login-env/1.0   8) libmd/1.0.4       15) openssl/3.1.3
  2) gcc/11.2.0          9) libbsd/0.11.7     16) util-linux-uuid/2.38.1
  3) ucx/1.11.2         10) expat/2.5.0       17) xz/5.2.4
  4) openmpi/4.1.2      11) gettext/0.19.8.1  18) python/3.11.6
  5) cuda/11.6.1        12) libffi/3.4.4      19) cudnn/8.9.0.131
  6) modtree/gpu        13) libxcrypt/4.4.35  20) anaconda3_gpu/23.9.0
  7) default            14) zlib-ng/2.1.3

 

job is starting on gpub022.delta.ncsa.illinois.edu
WARNING: A conda environment already exists at '/u/bzd2/.conda/envs/hetarth_py10'
Remove existing environment (y/[n])? 

CondaSystemExit: Exiting.

usage: conda [-h] [-v] [--no-plugins] [-V] COMMAND ...
conda: error: argument COMMAND: invalid choice: 'activate' (choose from 'clean', 'compare', 'config', 'create', 'info', 'init', 'install', 'list', 'notices', 'package', 'remove', 'uninstall', 'rename', 'run', 'search', 'update', 'upgrade', 'build', 'content-trust', 'convert', 'debug', 'develop', 'doctor', 'index', 'inspect', 'metapackage', 'render', 'repoquery', 'skeleton', 'server', 'token', 'repo', 'env', 'verify')
Starting script...
Output: Defaulting to user installation because normal site-packages is not writeable
Requirement already satisfied: torch in /sw/external/python/anaconda3/lib/python3.9/site-packages (2.2.0.dev20231003+cu121)
Requirement already satisfied: torchvision in /sw/external/python/anaconda3/lib/python3.9/site-packages (0.17.0.dev20231003+cu121)
Requirement already satisfied: torchaudio in /sw/external/python/anaconda3/lib/python3.9/site-packages (2.2.0.dev20231002)
Requirement already satisfied: filelock in /sw/external/python/anaconda3/lib/python3.9/site-packages (from torch) (3.9.0)
Requirement already satisfied: typing-extensions in /sw/external/python/anaconda3/lib/python3.9/site-packages (from torch) (4.7.1)
Requirement already satisfied: sympy in /sw/external/python/anaconda3/lib/python3.9/site-packages (from torch) (1.11.1)
Requirement already satisfied: networkx in /sw/external/python/anaconda3/lib/python3.9/site-packages (from torch) (3.1)
Requirement already satisfied: jinja2 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from torch) (3.1.2)
Requirement already satisfied: fsspec in /u/bzd2/.local/lib/python3.9/site-packages (from torch) (2023.6.0)
Requirement already satisfied: pytorch-triton==2.1.0+6e4932cda8 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from torch) (2.1.0+6e4932cda8)
Requirement already satisfied: numpy in /sw/external/python/anaconda3/lib/python3.9/site-packages (from torchvision) (1.24.3)
Requirement already satisfied: requests in /sw/external/python/anaconda3/lib/python3.9/site-packages (from torchvision) (2.31.0)
Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from torchvision) (9.4.0)
Requirement already satisfied: MarkupSafe>=2.0 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from jinja2->torch) (2.1.1)
Requirement already satisfied: charset-normalizer<4,>=2 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from requests->torchvision) (2.0.4)
Requirement already satisfied: idna<4,>=2.5 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from requests->torchvision) (3.4)
Requirement already satisfied: urllib3<3,>=1.21.1 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from requests->torchvision) (1.26.16)
Requirement already satisfied: certifi>=2017.4.17 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from requests->torchvision) (2023.7.22)
Requirement already satisfied: mpmath>=0.19 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from sympy->torch) (1.3.0)

Output: Defaulting to user installation because normal site-packages is not writeable
Requirement already satisfied: transformers in /u/bzd2/.local/lib/python3.9/site-packages (4.37.0.dev0)
Requirement already satisfied: filelock in /sw/external/python/anaconda3/lib/python3.9/site-packages (from transformers) (3.9.0)
Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /u/bzd2/.local/lib/python3.9/site-packages (from transformers) (0.19.4)
Requirement already satisfied: numpy>=1.17 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from transformers) (1.24.3)
Requirement already satisfied: packaging>=20.0 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from transformers) (23.1)
Requirement already satisfied: pyyaml>=5.1 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from transformers) (5.4.1)
Requirement already satisfied: regex!=2019.12.17 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from transformers) (2022.7.9)
Requirement already satisfied: requests in /sw/external/python/anaconda3/lib/python3.9/site-packages (from transformers) (2.31.0)
Requirement already satisfied: tokenizers<0.19,>=0.14 in /u/bzd2/.local/lib/python3.9/site-packages (from transformers) (0.15.0)
Requirement already satisfied: safetensors>=0.3.1 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from transformers) (0.3.2)
Requirement already satisfied: tqdm>=4.27 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from transformers) (4.65.0)
Requirement already satisfied: fsspec>=2023.5.0 in /u/bzd2/.local/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (2023.6.0)
Requirement already satisfied: typing-extensions>=3.7.4.3 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (4.7.1)
Requirement already satisfied: charset-normalizer<4,>=2 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from requests->transformers) (2.0.4)
Requirement already satisfied: idna<4,>=2.5 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from requests->transformers) (3.4)
Requirement already satisfied: urllib3<3,>=1.21.1 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from requests->transformers) (1.26.16)
Requirement already satisfied: certifi>=2017.4.17 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from requests->transformers) (2023.7.22)

Output: Defaulting to user installation because normal site-packages is not writeable
Collecting git+https://github.com/huggingface/peft.git
  Cloning https://github.com/huggingface/peft.git to /tmp/pip-req-build-fbl2c49m
  Resolved https://github.com/huggingface/peft.git to commit 0f1e9091cc975eb5458cc163bf1843a34fb42b76
  Installing build dependencies: started
  Installing build dependencies: finished with status 'done'
  Getting requirements to build wheel: started
  Getting requirements to build wheel: finished with status 'done'
  Preparing metadata (pyproject.toml): started
  Preparing metadata (pyproject.toml): finished with status 'done'
Requirement already satisfied: numpy>=1.17 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from peft==0.7.2.dev0) (1.24.3)
Requirement already satisfied: packaging>=20.0 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from peft==0.7.2.dev0) (23.1)
Requirement already satisfied: psutil in /sw/external/python/anaconda3/lib/python3.9/site-packages (from peft==0.7.2.dev0) (5.9.0)
Requirement already satisfied: pyyaml in /sw/external/python/anaconda3/lib/python3.9/site-packages (from peft==0.7.2.dev0) (5.4.1)
Requirement already satisfied: torch>=1.13.0 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from peft==0.7.2.dev0) (2.2.0.dev20231003+cu121)
Requirement already satisfied: transformers in /u/bzd2/.local/lib/python3.9/site-packages (from peft==0.7.2.dev0) (4.37.0.dev0)
Requirement already satisfied: tqdm in /sw/external/python/anaconda3/lib/python3.9/site-packages (from peft==0.7.2.dev0) (4.65.0)
Requirement already satisfied: accelerate>=0.21.0 in /u/bzd2/.local/lib/python3.9/site-packages (from peft==0.7.2.dev0) (0.25.0)
Requirement already satisfied: safetensors in /sw/external/python/anaconda3/lib/python3.9/site-packages (from peft==0.7.2.dev0) (0.3.2)
Requirement already satisfied: huggingface-hub>=0.17.0 in /u/bzd2/.local/lib/python3.9/site-packages (from peft==0.7.2.dev0) (0.19.4)
Requirement already satisfied: filelock in /sw/external/python/anaconda3/lib/python3.9/site-packages (from huggingface-hub>=0.17.0->peft==0.7.2.dev0) (3.9.0)
Requirement already satisfied: fsspec>=2023.5.0 in /u/bzd2/.local/lib/python3.9/site-packages (from huggingface-hub>=0.17.0->peft==0.7.2.dev0) (2023.6.0)
Requirement already satisfied: requests in /sw/external/python/anaconda3/lib/python3.9/site-packages (from huggingface-hub>=0.17.0->peft==0.7.2.dev0) (2.31.0)
Requirement already satisfied: typing-extensions>=3.7.4.3 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from huggingface-hub>=0.17.0->peft==0.7.2.dev0) (4.7.1)
Requirement already satisfied: sympy in /sw/external/python/anaconda3/lib/python3.9/site-packages (from torch>=1.13.0->peft==0.7.2.dev0) (1.11.1)
Requirement already satisfied: networkx in /sw/external/python/anaconda3/lib/python3.9/site-packages (from torch>=1.13.0->peft==0.7.2.dev0) (3.1)
Requirement already satisfied: jinja2 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from torch>=1.13.0->peft==0.7.2.dev0) (3.1.2)
Requirement already satisfied: pytorch-triton==2.1.0+6e4932cda8 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from torch>=1.13.0->peft==0.7.2.dev0) (2.1.0+6e4932cda8)
Requirement already satisfied: regex!=2019.12.17 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from transformers->peft==0.7.2.dev0) (2022.7.9)
Requirement already satisfied: tokenizers<0.19,>=0.14 in /u/bzd2/.local/lib/python3.9/site-packages (from transformers->peft==0.7.2.dev0) (0.15.0)
Requirement already satisfied: MarkupSafe>=2.0 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from jinja2->torch>=1.13.0->peft==0.7.2.dev0) (2.1.1)
Requirement already satisfied: charset-normalizer<4,>=2 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from requests->huggingface-hub>=0.17.0->peft==0.7.2.dev0) (2.0.4)
Requirement already satisfied: idna<4,>=2.5 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from requests->huggingface-hub>=0.17.0->peft==0.7.2.dev0) (3.4)
Requirement already satisfied: urllib3<3,>=1.21.1 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from requests->huggingface-hub>=0.17.0->peft==0.7.2.dev0) (1.26.16)
Requirement already satisfied: certifi>=2017.4.17 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from requests->huggingface-hub>=0.17.0->peft==0.7.2.dev0) (2023.7.22)
Requirement already satisfied: mpmath>=0.19 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from sympy->torch>=1.13.0->peft==0.7.2.dev0) (1.3.0)

Output: Defaulting to user installation because normal site-packages is not writeable
Collecting git+https://github.com/huggingface/transformers
  Cloning https://github.com/huggingface/transformers to /tmp/pip-req-build-wugbka72
  Resolved https://github.com/huggingface/transformers to commit 17506d1256c1780efc9e2a5898a828c10ad4ea69
  Installing build dependencies: started
  Installing build dependencies: finished with status 'done'
  Getting requirements to build wheel: started
  Getting requirements to build wheel: finished with status 'done'
  Preparing metadata (pyproject.toml): started
  Preparing metadata (pyproject.toml): finished with status 'done'
Requirement already satisfied: filelock in /sw/external/python/anaconda3/lib/python3.9/site-packages (from transformers==4.37.0.dev0) (3.9.0)
Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /u/bzd2/.local/lib/python3.9/site-packages (from transformers==4.37.0.dev0) (0.19.4)
Requirement already satisfied: numpy>=1.17 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from transformers==4.37.0.dev0) (1.24.3)
Requirement already satisfied: packaging>=20.0 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from transformers==4.37.0.dev0) (23.1)
Requirement already satisfied: pyyaml>=5.1 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from transformers==4.37.0.dev0) (5.4.1)
Requirement already satisfied: regex!=2019.12.17 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from transformers==4.37.0.dev0) (2022.7.9)
Requirement already satisfied: requests in /sw/external/python/anaconda3/lib/python3.9/site-packages (from transformers==4.37.0.dev0) (2.31.0)
Requirement already satisfied: tokenizers<0.19,>=0.14 in /u/bzd2/.local/lib/python3.9/site-packages (from transformers==4.37.0.dev0) (0.15.0)
Requirement already satisfied: safetensors>=0.3.1 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from transformers==4.37.0.dev0) (0.3.2)
Requirement already satisfied: tqdm>=4.27 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from transformers==4.37.0.dev0) (4.65.0)
Requirement already satisfied: fsspec>=2023.5.0 in /u/bzd2/.local/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers==4.37.0.dev0) (2023.6.0)
Requirement already satisfied: typing-extensions>=3.7.4.3 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers==4.37.0.dev0) (4.7.1)
Requirement already satisfied: charset-normalizer<4,>=2 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from requests->transformers==4.37.0.dev0) (2.0.4)
Requirement already satisfied: idna<4,>=2.5 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from requests->transformers==4.37.0.dev0) (3.4)
Requirement already satisfied: urllib3<3,>=1.21.1 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from requests->transformers==4.37.0.dev0) (1.26.16)
Requirement already satisfied: certifi>=2017.4.17 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from requests->transformers==4.37.0.dev0) (2023.7.22)

Output: Defaulting to user installation because normal site-packages is not writeable
Requirement already satisfied: datasets in /sw/external/python/anaconda3/lib/python3.9/site-packages (2.14.5)
Requirement already satisfied: numpy>=1.17 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from datasets) (1.24.3)
Requirement already satisfied: pyarrow>=8.0.0 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from datasets) (11.0.0)
Requirement already satisfied: dill<0.3.8,>=0.3.0 in /u/bzd2/.local/lib/python3.9/site-packages (from datasets) (0.3.7)
Requirement already satisfied: pandas in /sw/external/python/anaconda3/lib/python3.9/site-packages (from datasets) (2.0.3)
Requirement already satisfied: requests>=2.19.0 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from datasets) (2.31.0)
Requirement already satisfied: tqdm>=4.62.1 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from datasets) (4.65.0)
Requirement already satisfied: xxhash in /sw/external/python/anaconda3/lib/python3.9/site-packages (from datasets) (2.0.2)
Requirement already satisfied: multiprocess in /sw/external/python/anaconda3/lib/python3.9/site-packages (from datasets) (0.70.15)
Requirement already satisfied: fsspec[http]<2023.9.0,>=2023.1.0 in /u/bzd2/.local/lib/python3.9/site-packages (from datasets) (2023.6.0)
Requirement already satisfied: aiohttp in /sw/external/python/anaconda3/lib/python3.9/site-packages (from datasets) (3.8.5)
Requirement already satisfied: huggingface-hub<1.0.0,>=0.14.0 in /u/bzd2/.local/lib/python3.9/site-packages (from datasets) (0.19.4)
Requirement already satisfied: packaging in /sw/external/python/anaconda3/lib/python3.9/site-packages (from datasets) (23.1)
Requirement already satisfied: pyyaml>=5.1 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from datasets) (5.4.1)
Requirement already satisfied: attrs>=17.3.0 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from aiohttp->datasets) (23.1.0)
Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from aiohttp->datasets) (2.0.4)
Requirement already satisfied: multidict<7.0,>=4.5 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from aiohttp->datasets) (6.0.2)
Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from aiohttp->datasets) (4.0.2)
Requirement already satisfied: yarl<2.0,>=1.0 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from aiohttp->datasets) (1.8.1)
Requirement already satisfied: frozenlist>=1.1.1 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from aiohttp->datasets) (1.3.3)
Requirement already satisfied: aiosignal>=1.1.2 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from aiohttp->datasets) (1.2.0)
Requirement already satisfied: filelock in /sw/external/python/anaconda3/lib/python3.9/site-packages (from huggingface-hub<1.0.0,>=0.14.0->datasets) (3.9.0)
Requirement already satisfied: typing-extensions>=3.7.4.3 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from huggingface-hub<1.0.0,>=0.14.0->datasets) (4.7.1)
Requirement already satisfied: idna<4,>=2.5 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from requests>=2.19.0->datasets) (3.4)
Requirement already satisfied: urllib3<3,>=1.21.1 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from requests>=2.19.0->datasets) (1.26.16)
Requirement already satisfied: certifi>=2017.4.17 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from requests>=2.19.0->datasets) (2023.7.22)
Requirement already satisfied: python-dateutil>=2.8.2 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from pandas->datasets) (2.8.2)
Requirement already satisfied: pytz>=2020.1 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from pandas->datasets) (2023.3.post1)
Requirement already satisfied: tzdata>=2022.1 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from pandas->datasets) (2023.3)
Requirement already satisfied: six>=1.5 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)

Output: Defaulting to user installation because normal site-packages is not writeable
Requirement already satisfied: accelerate in /u/bzd2/.local/lib/python3.9/site-packages (0.25.0)
Requirement already satisfied: numpy>=1.17 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from accelerate) (1.24.3)
Requirement already satisfied: packaging>=20.0 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from accelerate) (23.1)
Requirement already satisfied: psutil in /sw/external/python/anaconda3/lib/python3.9/site-packages (from accelerate) (5.9.0)
Requirement already satisfied: pyyaml in /sw/external/python/anaconda3/lib/python3.9/site-packages (from accelerate) (5.4.1)
Requirement already satisfied: torch>=1.10.0 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from accelerate) (2.2.0.dev20231003+cu121)
Requirement already satisfied: huggingface-hub in /u/bzd2/.local/lib/python3.9/site-packages (from accelerate) (0.19.4)
Requirement already satisfied: safetensors>=0.3.1 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from accelerate) (0.3.2)
Requirement already satisfied: filelock in /sw/external/python/anaconda3/lib/python3.9/site-packages (from torch>=1.10.0->accelerate) (3.9.0)
Requirement already satisfied: typing-extensions in /sw/external/python/anaconda3/lib/python3.9/site-packages (from torch>=1.10.0->accelerate) (4.7.1)
Requirement already satisfied: sympy in /sw/external/python/anaconda3/lib/python3.9/site-packages (from torch>=1.10.0->accelerate) (1.11.1)
Requirement already satisfied: networkx in /sw/external/python/anaconda3/lib/python3.9/site-packages (from torch>=1.10.0->accelerate) (3.1)
Requirement already satisfied: jinja2 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from torch>=1.10.0->accelerate) (3.1.2)
Requirement already satisfied: fsspec in /u/bzd2/.local/lib/python3.9/site-packages (from torch>=1.10.0->accelerate) (2023.6.0)
Requirement already satisfied: pytorch-triton==2.1.0+6e4932cda8 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from torch>=1.10.0->accelerate) (2.1.0+6e4932cda8)
Requirement already satisfied: requests in /sw/external/python/anaconda3/lib/python3.9/site-packages (from huggingface-hub->accelerate) (2.31.0)
Requirement already satisfied: tqdm>=4.42.1 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from huggingface-hub->accelerate) (4.65.0)
Requirement already satisfied: MarkupSafe>=2.0 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.1)
Requirement already satisfied: charset-normalizer<4,>=2 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from requests->huggingface-hub->accelerate) (2.0.4)
Requirement already satisfied: idna<4,>=2.5 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from requests->huggingface-hub->accelerate) (3.4)
Requirement already satisfied: urllib3<3,>=1.21.1 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from requests->huggingface-hub->accelerate) (1.26.16)
Requirement already satisfied: certifi>=2017.4.17 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from requests->huggingface-hub->accelerate) (2023.7.22)
Requirement already satisfied: mpmath>=0.19 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)

Output: Defaulting to user installation because normal site-packages is not writeable
Requirement already satisfied: huggingface_hub in /u/bzd2/.local/lib/python3.9/site-packages (0.19.4)
Requirement already satisfied: filelock in /sw/external/python/anaconda3/lib/python3.9/site-packages (from huggingface_hub) (3.9.0)
Requirement already satisfied: fsspec>=2023.5.0 in /u/bzd2/.local/lib/python3.9/site-packages (from huggingface_hub) (2023.6.0)
Requirement already satisfied: requests in /sw/external/python/anaconda3/lib/python3.9/site-packages (from huggingface_hub) (2.31.0)
Requirement already satisfied: tqdm>=4.42.1 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from huggingface_hub) (4.65.0)
Requirement already satisfied: pyyaml>=5.1 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from huggingface_hub) (5.4.1)
Requirement already satisfied: typing-extensions>=3.7.4.3 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from huggingface_hub) (4.7.1)
Requirement already satisfied: packaging>=20.9 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from huggingface_hub) (23.1)
Requirement already satisfied: charset-normalizer<4,>=2 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from requests->huggingface_hub) (2.0.4)
Requirement already satisfied: idna<4,>=2.5 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from requests->huggingface_hub) (3.4)
Requirement already satisfied: urllib3<3,>=1.21.1 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from requests->huggingface_hub) (1.26.16)
Requirement already satisfied: certifi>=2017.4.17 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from requests->huggingface_hub) (2023.7.22)

Output: Defaulting to user installation because normal site-packages is not writeable
Requirement already satisfied: bitsandbytes in /u/bzd2/.local/lib/python3.9/site-packages (0.41.3.post2)

Output: Defaulting to user installation because normal site-packages is not writeable
Requirement already satisfied: wandb in /u/bzd2/.local/lib/python3.9/site-packages (0.16.1)
Requirement already satisfied: Click!=8.0.0,>=7.1 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from wandb) (8.0.4)
Requirement already satisfied: GitPython!=3.1.29,>=1.0.0 in /u/bzd2/.local/lib/python3.9/site-packages (from wandb) (3.1.40)
Requirement already satisfied: requests<3,>=2.0.0 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from wandb) (2.31.0)
Requirement already satisfied: psutil>=5.0.0 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from wandb) (5.9.0)
Requirement already satisfied: sentry-sdk>=1.0.0 in /u/bzd2/.local/lib/python3.9/site-packages (from wandb) (1.38.0)
Requirement already satisfied: docker-pycreds>=0.4.0 in /u/bzd2/.local/lib/python3.9/site-packages (from wandb) (0.4.0)
Requirement already satisfied: PyYAML in /sw/external/python/anaconda3/lib/python3.9/site-packages (from wandb) (5.4.1)
Requirement already satisfied: setproctitle in /sw/external/python/anaconda3/lib/python3.9/site-packages (from wandb) (1.3.2)
Requirement already satisfied: setuptools in /sw/external/python/anaconda3/lib/python3.9/site-packages (from wandb) (68.0.0)
Requirement already satisfied: appdirs>=1.4.3 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from wandb) (1.4.4)
Requirement already satisfied: typing-extensions in /sw/external/python/anaconda3/lib/python3.9/site-packages (from wandb) (4.7.1)
Requirement already satisfied: protobuf!=4.21.0,<5,>=3.15.0 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from wandb) (3.20.3)
Requirement already satisfied: six>=1.4.0 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)
Requirement already satisfied: gitdb<5,>=4.0.1 in /u/bzd2/.local/lib/python3.9/site-packages (from GitPython!=3.1.29,>=1.0.0->wandb) (4.0.11)
Requirement already satisfied: charset-normalizer<4,>=2 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from requests<3,>=2.0.0->wandb) (2.0.4)
Requirement already satisfied: idna<4,>=2.5 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from requests<3,>=2.0.0->wandb) (3.4)
Requirement already satisfied: urllib3<3,>=1.21.1 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from requests<3,>=2.0.0->wandb) (1.26.16)
Requirement already satisfied: certifi>=2017.4.17 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from requests<3,>=2.0.0->wandb) (2023.7.22)
Requirement already satisfied: smmap<6,>=3.0.1 in /u/bzd2/.local/lib/python3.9/site-packages (from gitdb<5,>=4.0.1->GitPython!=3.1.29,>=1.0.0->wandb) (5.0.1)

Output: Defaulting to user installation because normal site-packages is not writeable
Requirement already satisfied: scikit-learn in /sw/external/python/anaconda3/lib/python3.9/site-packages (1.3.0)
Requirement already satisfied: numpy>=1.17.3 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from scikit-learn) (1.24.3)
Requirement already satisfied: scipy>=1.5.0 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from scikit-learn) (1.8.1)
Requirement already satisfied: joblib>=1.1.1 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from scikit-learn) (1.2.0)
Requirement already satisfied: threadpoolctl>=2.0.0 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from scikit-learn) (2.2.0)

Output: Defaulting to user installation because normal site-packages is not writeable
Requirement already satisfied: code_bert_score in /u/bzd2/.local/lib/python3.9/site-packages (0.4.1)
Requirement already satisfied: torch>=1.0.0 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from code_bert_score) (2.2.0.dev20231003+cu121)
Requirement already satisfied: numpy in /sw/external/python/anaconda3/lib/python3.9/site-packages (from code_bert_score) (1.24.3)
Requirement already satisfied: pandas>=1.0.1 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from code_bert_score) (2.0.3)
Requirement already satisfied: requests in /sw/external/python/anaconda3/lib/python3.9/site-packages (from code_bert_score) (2.31.0)
Requirement already satisfied: tqdm>=4.31.1 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from code_bert_score) (4.65.0)
Requirement already satisfied: matplotlib in /sw/external/python/anaconda3/lib/python3.9/site-packages (from code_bert_score) (3.7.2)
Requirement already satisfied: transformers>=3.0.0 in /u/bzd2/.local/lib/python3.9/site-packages (from code_bert_score) (4.37.0.dev0)
Requirement already satisfied: python-dateutil>=2.8.2 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from pandas>=1.0.1->code_bert_score) (2.8.2)
Requirement already satisfied: pytz>=2020.1 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from pandas>=1.0.1->code_bert_score) (2023.3.post1)
Requirement already satisfied: tzdata>=2022.1 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from pandas>=1.0.1->code_bert_score) (2023.3)
Requirement already satisfied: filelock in /sw/external/python/anaconda3/lib/python3.9/site-packages (from torch>=1.0.0->code_bert_score) (3.9.0)
Requirement already satisfied: typing-extensions in /sw/external/python/anaconda3/lib/python3.9/site-packages (from torch>=1.0.0->code_bert_score) (4.7.1)
Requirement already satisfied: sympy in /sw/external/python/anaconda3/lib/python3.9/site-packages (from torch>=1.0.0->code_bert_score) (1.11.1)
Requirement already satisfied: networkx in /sw/external/python/anaconda3/lib/python3.9/site-packages (from torch>=1.0.0->code_bert_score) (3.1)
Requirement already satisfied: jinja2 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from torch>=1.0.0->code_bert_score) (3.1.2)
Requirement already satisfied: fsspec in /u/bzd2/.local/lib/python3.9/site-packages (from torch>=1.0.0->code_bert_score) (2023.6.0)
Requirement already satisfied: pytorch-triton==2.1.0+6e4932cda8 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from torch>=1.0.0->code_bert_score) (2.1.0+6e4932cda8)
Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /u/bzd2/.local/lib/python3.9/site-packages (from transformers>=3.0.0->code_bert_score) (0.19.4)
Requirement already satisfied: packaging>=20.0 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from transformers>=3.0.0->code_bert_score) (23.1)
Requirement already satisfied: pyyaml>=5.1 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from transformers>=3.0.0->code_bert_score) (5.4.1)
Requirement already satisfied: regex!=2019.12.17 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from transformers>=3.0.0->code_bert_score) (2022.7.9)
Requirement already satisfied: tokenizers<0.19,>=0.14 in /u/bzd2/.local/lib/python3.9/site-packages (from transformers>=3.0.0->code_bert_score) (0.15.0)
Requirement already satisfied: safetensors>=0.3.1 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from transformers>=3.0.0->code_bert_score) (0.3.2)
Requirement already satisfied: contourpy>=1.0.1 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from matplotlib->code_bert_score) (1.0.5)
Requirement already satisfied: cycler>=0.10 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from matplotlib->code_bert_score) (0.11.0)
Requirement already satisfied: fonttools>=4.22.0 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from matplotlib->code_bert_score) (4.25.0)
Requirement already satisfied: kiwisolver>=1.0.1 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from matplotlib->code_bert_score) (1.4.4)
Requirement already satisfied: pillow>=6.2.0 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from matplotlib->code_bert_score) (9.4.0)
Requirement already satisfied: pyparsing<3.1,>=2.3.1 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from matplotlib->code_bert_score) (3.0.9)
Requirement already satisfied: importlib-resources>=3.2.0 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from matplotlib->code_bert_score) (5.2.0)
Requirement already satisfied: charset-normalizer<4,>=2 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from requests->code_bert_score) (2.0.4)
Requirement already satisfied: idna<4,>=2.5 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from requests->code_bert_score) (3.4)
Requirement already satisfied: urllib3<3,>=1.21.1 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from requests->code_bert_score) (1.26.16)
Requirement already satisfied: certifi>=2017.4.17 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from requests->code_bert_score) (2023.7.22)
Requirement already satisfied: zipp>=3.1.0 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from importlib-resources>=3.2.0->matplotlib->code_bert_score) (3.11.0)
Requirement already satisfied: six>=1.5 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from python-dateutil>=2.8.2->pandas>=1.0.1->code_bert_score) (1.16.0)
Requirement already satisfied: MarkupSafe>=2.0 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from jinja2->torch>=1.0.0->code_bert_score) (2.1.1)
Requirement already satisfied: mpmath>=0.19 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from sympy->torch>=1.0.0->code_bert_score) (1.3.0)

WARNING:__main__:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
2023-12-13 17:26:07.439650: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
2023-12-13 17:26:07.439660: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
2023-12-13 17:26:07.439665: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
2023-12-13 17:26:07.439674: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
2023-12-13 17:26:07.439718: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2023-12-13 17:26:07.439727: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2023-12-13 17:26:07.439727: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2023-12-13 17:26:07.439737: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2023-12-13 17:26:07.449453: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2023-12-13 17:26:07.449455: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2023-12-13 17:26:07.449455: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2023-12-13 17:26:07.449459: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2023-12-13 17:26:08.415678: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-12-13 17:26:08.415689: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-12-13 17:26:08.415701: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-12-13 17:26:08.415698: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
/u/bzd2/.local/lib/python3.9/site-packages/transformers/models/auto/tokenization_auto.py:691: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/transformers/models/auto/tokenization_auto.py:691: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/transformers/models/auto/tokenization_auto.py:691: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/transformers/models/auto/tokenization_auto.py:691: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.
  warnings.warn(
loading file vocab.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-1b/snapshots/182f0165fdf8da9c9935901eec65c94337f01c11/vocab.json
loading file merges.txt from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-1b/snapshots/182f0165fdf8da9c9935901eec65c94337f01c11/merges.txt
loading file tokenizer.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-1b/snapshots/182f0165fdf8da9c9935901eec65c94337f01c11/tokenizer.json
loading file vocab.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-1b/snapshots/182f0165fdf8da9c9935901eec65c94337f01c11/vocab.json
loading file added_tokens.json from cache at None
loading file special_tokens_map.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-1b/snapshots/182f0165fdf8da9c9935901eec65c94337f01c11/special_tokens_map.json
loading file merges.txt from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-1b/snapshots/182f0165fdf8da9c9935901eec65c94337f01c11/merges.txt
loading file tokenizer_config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-1b/snapshots/182f0165fdf8da9c9935901eec65c94337f01c11/tokenizer_config.json
loading file tokenizer.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-1b/snapshots/182f0165fdf8da9c9935901eec65c94337f01c11/tokenizer.json
loading file added_tokens.json from cache at None
loading file special_tokens_map.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-1b/snapshots/182f0165fdf8da9c9935901eec65c94337f01c11/special_tokens_map.json
loading file tokenizer_config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-1b/snapshots/182f0165fdf8da9c9935901eec65c94337f01c11/tokenizer_config.json
loading file vocab.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-1b/snapshots/182f0165fdf8da9c9935901eec65c94337f01c11/vocab.json
loading file merges.txt from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-1b/snapshots/182f0165fdf8da9c9935901eec65c94337f01c11/merges.txt
loading file tokenizer.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-1b/snapshots/182f0165fdf8da9c9935901eec65c94337f01c11/tokenizer.json
loading file added_tokens.json from cache at None
loading file special_tokens_map.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-1b/snapshots/182f0165fdf8da9c9935901eec65c94337f01c11/special_tokens_map.json
loading file tokenizer_config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-1b/snapshots/182f0165fdf8da9c9935901eec65c94337f01c11/tokenizer_config.json
loading file vocab.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-1b/snapshots/182f0165fdf8da9c9935901eec65c94337f01c11/vocab.json
loading file merges.txt from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-1b/snapshots/182f0165fdf8da9c9935901eec65c94337f01c11/merges.txt
loading file tokenizer.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-1b/snapshots/182f0165fdf8da9c9935901eec65c94337f01c11/tokenizer.json
loading file added_tokens.json from cache at None
loading file special_tokens_map.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-1b/snapshots/182f0165fdf8da9c9935901eec65c94337f01c11/special_tokens_map.json
loading file tokenizer_config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-1b/snapshots/182f0165fdf8da9c9935901eec65c94337f01c11/tokenizer_config.json
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/datasets/load.py:2089: FutureWarning: 'use_auth_token' was deprecated in favor of 'token' in version 2.14.0 and will be removed in 3.0.0.
You can remove this warning by passing 'token=True' instead.
  warnings.warn(
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/datasets/load.py:2089: FutureWarning: 'use_auth_token' was deprecated in favor of 'token' in version 2.14.0 and will be removed in 3.0.0.
You can remove this warning by passing 'token=True' instead.
  warnings.warn(
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/datasets/load.py:2089: FutureWarning: 'use_auth_token' was deprecated in favor of 'token' in version 2.14.0 and will be removed in 3.0.0.
You can remove this warning by passing 'token=True' instead.
  warnings.warn(
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/datasets/load.py:2089: FutureWarning: 'use_auth_token' was deprecated in favor of 'token' in version 2.14.0 and will be removed in 3.0.0.
You can remove this warning by passing 'token=True' instead.
  warnings.warn(
Loading the dataset in streaming mode
Loading the dataset in streaming mode
Loading the dataset in streaming mode
Loading the dataset in streaming mode
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<FOR DEBUGGING PURPOSES ONLY!>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<FOR DEBUGGING PURPOSES ONLY!>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>{'path': 'data/repos/adriagalin/motd/tasks/main.yml', 'org_name': 'adriagalin', 'license': 'GPL-3.0-only', 'download_link': 'https://old-galaxy.ansible.com/adriagalin/motd', 'input': 'name: Delete /etc/motd file', 'output': 'file:\n  path: /etc/motd\n  state: absent\nwhen: ag_motd_add_footer | bool\ntags:\n- motd\n- common\n', 'download_count': '81335', 'repo_name': 'motd'}

<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<FOR DEBUGGING PURPOSES ONLY!>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
{'license': 'GPL-3.0-only', 'repo_name': 'motd', 'download_link': 'https://old-galaxy.ansible.com/adriagalin/motd', 'org_name': 'adriagalin', 'input': 'name: Delete /etc/motd file', 'output': 'file:\n  path: /etc/motd\n  state: absent\nwhen: ag_motd_add_footer | bool\ntags:\n- motd\n- common\n', 'download_count': '81335', 'path': 'data/repos/adriagalin/motd/tasks/main.yml'}{'path': 'data/repos/siamaksade/openshift_gogs/tasks/main.yml', 'org_name': 'siamaksade', 'license': '', 'download_link': 'https://old-galaxy.ansible.com/siamaksade/openshift_gogs', 'input': 'name: check if gogs exists', 'output': 'shell: oc get service gogs -n gogs\nregister: install_gogs\nignore_errors: true\nchanged_when: false\n', 'download_count': '16571', 'repo_name': 'openshift_gogs'}

<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<FOR DEBUGGING PURPOSES ONLY!>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<FOR DEBUGGING PURPOSES ONLY!>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
{'path': 'data/repos/devopsarr/sonarr/integration/targets/sonarr_release_profile/tasks/main.yml', 'org_name': 'devopsarr', 'license': '', 'download_link': 'https://old-galaxy.ansible.com/devopsarr/sonarr', 'input': 'name: assert release profile idempotency', 'output': 'assert:\n  that:\n  - result.changed == false\n  - result.ignored == ["repack", "dvdrip"]\n', 'download_count': '688', 'repo_name': 'sonarr'}
{'license': '', 'repo_name': 'openshift_gogs', 'download_link': 'https://old-galaxy.ansible.com/siamaksade/openshift_gogs', 'org_name': 'siamaksade', 'input': 'name: check if gogs exists', 'output': 'shell: oc get service gogs -n gogs\nregister: install_gogs\nignore_errors: true\nchanged_when: false\n', 'download_count': '16571', 'path': 'data/repos/siamaksade/openshift_gogs/tasks/main.yml'}<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<FOR DEBUGGING PURPOSES ONLY!>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>

<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<FOR DEBUGGING PURPOSES ONLY!>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
{'path': 'data/repos/buluma/anaconda/tasks/main.yml', 'org_name': 'buluma', 'license': 'Apache-2.0-Short,Apache-2.0', 'download_link': 'https://old-galaxy.ansible.com/buluma/anaconda', 'input': 'name: Start and enable anaconda', 'output': 'ansible.builtin.service:\n  name: anaconda\n  state: started\n  enabled: true\n', 'download_count': '9401', 'repo_name': 'anaconda'}
{'license': '', 'repo_name': 'sonarr', 'download_link': 'https://old-galaxy.ansible.com/devopsarr/sonarr', 'org_name': 'devopsarr', 'input': 'name: assert release profile idempotency', 'output': 'assert:\n  that:\n  - result.changed == false\n  - result.ignored == ["repack", "dvdrip"]\n', 'download_count': '688', 'path': 'data/repos/devopsarr/sonarr/integration/targets/sonarr_release_profile/tasks/main.yml'}<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<FOR DEBUGGING PURPOSES ONLY!>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>

<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<FOR DEBUGGING PURPOSES ONLY!>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
{'path': 'data/repos/chrismeyersfsu/ec2_server/tasks/main.yml', 'org_name': 'chrismeyersfsu', 'license': '', 'download_link': 'https://old-galaxy.ansible.com/chrismeyersfsu/ec2_server', 'input': 'name: Add hosts group temporary inventory group without pem path', 'output': "add_host:\n  name: ''\n  groups: ec2_server_hosts\n  ansible_ssh_user: ''\nwith_items: ec2.instances\nwhen: ec2_server_pem_path == ''\n", 'download_count': '1904', 'repo_name': 'ec2_server'}
{'license': 'Apache-2.0-Short,Apache-2.0', 'repo_name': 'anaconda', 'download_link': 'https://old-galaxy.ansible.com/buluma/anaconda', 'org_name': 'buluma', 'input': 'name: Start and enable anaconda', 'output': 'ansible.builtin.service:\n  name: anaconda\n  state: started\n  enabled: true\n', 'download_count': '9401', 'path': 'data/repos/buluma/anaconda/tasks/main.yml'}
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<FOR DEBUGGING PURPOSES ONLY!>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
{'license': '', 'repo_name': 'ec2_server', 'download_link': 'https://old-galaxy.ansible.com/chrismeyersfsu/ec2_server', 'org_name': 'chrismeyersfsu', 'input': 'name: Add hosts group temporary inventory group without pem path', 'output': "add_host:\n  name: ''\n  groups: ec2_server_hosts\n  ansible_ssh_user: ''\nwith_items: ec2.instances\nwhen: ec2_server_pem_path == ''\n", 'download_count': '1904', 'path': 'data/repos/chrismeyersfsu/ec2_server/tasks/main.yml'}
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<FOR DEBUGGING PURPOSES ONLY!>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<FOR DEBUGGING PURPOSES ONLY!>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
{'license': 'GPL-3.0-only', 'download_count': '81335', 'download_link': 'https://old-galaxy.ansible.com/adriagalin/motd', 'input': 'name: Delete /etc/motd file', 'output': 'file:\n  path: /etc/motd\n  state: absent\nwhen: ag_motd_add_footer | bool\ntags:\n- motd\n- common\n', 'path': 'data/repos/adriagalin/motd/tasks/main.yml', 'org_name': 'adriagalin', 'repo_name': 'motd'}
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<FOR DEBUGGING PURPOSES ONLY!>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>{'license': 'GPL-3.0-only', 'download_link': 'https://old-galaxy.ansible.com/adriagalin/motd', 'input': 'name: Delete /etc/motd file', 'download_count': '81335', 'path': 'data/repos/adriagalin/motd/tasks/main.yml', 'org_name': 'adriagalin', 'repo_name': 'motd', 'output': 'file:\n  path: /etc/motd\n  state: absent\nwhen: ag_motd_add_footer | bool\ntags:\n- motd\n- common\n'}

{'license': '', 'download_count': '16571', 'download_link': 'https://old-galaxy.ansible.com/siamaksade/openshift_gogs', 'input': 'name: check if gogs exists', 'output': 'shell: oc get service gogs -n gogs\nregister: install_gogs\nignore_errors: true\nchanged_when: false\n', 'path': 'data/repos/siamaksade/openshift_gogs/tasks/main.yml', 'org_name': 'siamaksade', 'repo_name': 'openshift_gogs'}
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<FOR DEBUGGING PURPOSES ONLY!>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<FOR DEBUGGING PURPOSES ONLY!>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
{'license': '', 'download_link': 'https://old-galaxy.ansible.com/siamaksade/openshift_gogs', 'input': 'name: check if gogs exists', 'download_count': '16571', 'path': 'data/repos/siamaksade/openshift_gogs/tasks/main.yml', 'org_name': 'siamaksade', 'repo_name': 'openshift_gogs', 'output': 'shell: oc get service gogs -n gogs\nregister: install_gogs\nignore_errors: true\nchanged_when: false\n'}{'license': '', 'download_count': '688', 'download_link': 'https://old-galaxy.ansible.com/devopsarr/sonarr', 'input': 'name: assert release profile idempotency', 'output': 'assert:\n  that:\n  - result.changed == false\n  - result.ignored == ["repack", "dvdrip"]\n', 'path': 'data/repos/devopsarr/sonarr/integration/targets/sonarr_release_profile/tasks/main.yml', 'org_name': 'devopsarr', 'repo_name': 'sonarr'}

<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<FOR DEBUGGING PURPOSES ONLY!>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>><<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<FOR DEBUGGING PURPOSES ONLY!>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>

{'license': 'Apache-2.0-Short,Apache-2.0', 'download_count': '9401', 'download_link': 'https://old-galaxy.ansible.com/buluma/anaconda', 'input': 'name: Start and enable anaconda', 'output': 'ansible.builtin.service:\n  name: anaconda\n  state: started\n  enabled: true\n', 'path': 'data/repos/buluma/anaconda/tasks/main.yml', 'org_name': 'buluma', 'repo_name': 'anaconda'}{'license': '', 'download_link': 'https://old-galaxy.ansible.com/devopsarr/sonarr', 'input': 'name: assert release profile idempotency', 'download_count': '688', 'path': 'data/repos/devopsarr/sonarr/integration/targets/sonarr_release_profile/tasks/main.yml', 'org_name': 'devopsarr', 'repo_name': 'sonarr', 'output': 'assert:\n  that:\n  - result.changed == false\n  - result.ignored == ["repack", "dvdrip"]\n'}

<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<FOR DEBUGGING PURPOSES ONLY!>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>><<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<FOR DEBUGGING PURPOSES ONLY!>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>

{'license': '', 'download_count': '1904', 'download_link': 'https://old-galaxy.ansible.com/chrismeyersfsu/ec2_server', 'input': 'name: Add hosts group temporary inventory group without pem path', 'output': "add_host:\n  name: ''\n  groups: ec2_server_hosts\n  ansible_ssh_user: ''\nwith_items: ec2.instances\nwhen: ec2_server_pem_path == ''\n", 'path': 'data/repos/chrismeyersfsu/ec2_server/tasks/main.yml', 'org_name': 'chrismeyersfsu', 'repo_name': 'ec2_server'}{'license': 'Apache-2.0-Short,Apache-2.0', 'download_link': 'https://old-galaxy.ansible.com/buluma/anaconda', 'input': 'name: Start and enable anaconda', 'download_count': '9401', 'path': 'data/repos/buluma/anaconda/tasks/main.yml', 'org_name': 'buluma', 'repo_name': 'anaconda', 'output': 'ansible.builtin.service:\n  name: anaconda\n  state: started\n  enabled: true\n'}

<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<FOR DEBUGGING PURPOSES ONLY!>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<FOR DEBUGGING PURPOSES ONLY!>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
{'license': '', 'download_link': 'https://old-galaxy.ansible.com/chrismeyersfsu/ec2_server', 'input': 'name: Add hosts group temporary inventory group without pem path', 'download_count': '1904', 'path': 'data/repos/chrismeyersfsu/ec2_server/tasks/main.yml', 'org_name': 'chrismeyersfsu', 'repo_name': 'ec2_server', 'output': "add_host:\n  name: ''\n  groups: ec2_server_hosts\n  ansible_ssh_user: ''\nwith_items: ec2.instances\nwhen: ec2_server_pem_path == ''\n"}
{'path': 'data/repos/lifeofguenter/nginx/tasks/nginx.yml', 'org_name': 'lifeofguenter', 'license': 'MIT', 'download_link': 'https://old-galaxy.ansible.com/lifeofguenter/nginx', 'input': 'name: Install', 'output': 'ansible.builtin.command: make install\nargs:\n  chdir: /tmp/nginx-1.24.0\nchanged_when: true\nbecome: true\nnotify: Restart_nginx\n', 'download_count': '561', 'repo_name': 'nginx'}
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<FOR DEBUGGING PURPOSES ONLY!>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<FOR DEBUGGING PURPOSES ONLY!>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
{'path': 'data/repos/robertdebock/locale/tasks/assert.yml', 'org_name': 'robertdebock', 'license': 'Apache-2.0-Short,Apache-2.0', 'download_link': 'https://old-galaxy.ansible.com/robertdebock/locale', 'input': 'name: assert | Test locale_lc_time', 'output': 'ansible.builtin.assert:\n  that:\n  - locale_lc_time is defined\n  - locale_lc_time is string\n  - locale_lc_time is not none\n  quiet: true\n', 'download_count': '34969', 'repo_name': 'locale'}
{'license': 'MIT', 'repo_name': 'nginx', 'download_link': 'https://old-galaxy.ansible.com/lifeofguenter/nginx', 'org_name': 'lifeofguenter', 'input': 'name: Install', 'output': 'ansible.builtin.command: make install\nargs:\n  chdir: /tmp/nginx-1.24.0\nchanged_when: true\nbecome: true\nnotify: Restart_nginx\n', 'download_count': '561', 'path': 'data/repos/lifeofguenter/nginx/tasks/nginx.yml'}<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<FOR DEBUGGING PURPOSES ONLY!>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>

<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<FOR DEBUGGING PURPOSES ONLY!>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
{'path': 'data/repos/brentwg/visual-studio-code/tasks/Debian.yml', 'org_name': 'brentwg', 'license': '', 'download_link': 'https://old-galaxy.ansible.com/brentwg/visual-studio-code', 'input': 'name: Create the APT repository (Debian)', 'output': 'apt_repository:\n  repo: deb [arch=amd64] https://packages.microsoft.com/repos/vscode stable main\n  filename: vscode\n', 'download_count': '513', 'repo_name': 'visual-studio-code'}
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<FOR DEBUGGING PURPOSES ONLY!>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>{'license': 'Apache-2.0-Short,Apache-2.0', 'repo_name': 'locale', 'download_link': 'https://old-galaxy.ansible.com/robertdebock/locale', 'org_name': 'robertdebock', 'input': 'name: assert | Test locale_lc_time', 'output': 'ansible.builtin.assert:\n  that:\n  - locale_lc_time is defined\n  - locale_lc_time is string\n  - locale_lc_time is not none\n  quiet: true\n', 'download_count': '34969', 'path': 'data/repos/robertdebock/locale/tasks/assert.yml'}

<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<FOR DEBUGGING PURPOSES ONLY!>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>{'path': 'data/repos/mahdi22/mariadb_install/tasks/Debian-install.yml', 'org_name': 'mahdi22', 'license': '', 'download_link': 'https://old-galaxy.ansible.com/mahdi22/mariadb_install', 'input': 'name: Add MariaDB Repository Key for', 'output': 'apt_key:\n  url: https://mariadb.org/mariadb_release_signing_key.asc\n  state: present\nwhen: (use_proxy is not defined) or (not use_proxy)\n', 'download_count': '801', 'repo_name': 'mariadb_install'}

<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<FOR DEBUGGING PURPOSES ONLY!>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>{'license': '', 'repo_name': 'visual-studio-code', 'download_link': 'https://old-galaxy.ansible.com/brentwg/visual-studio-code', 'org_name': 'brentwg', 'input': 'name: Create the APT repository (Debian)', 'output': 'apt_repository:\n  repo: deb [arch=amd64] https://packages.microsoft.com/repos/vscode stable main\n  filename: vscode\n', 'download_count': '513', 'path': 'data/repos/brentwg/visual-studio-code/tasks/Debian.yml'}

<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<FOR DEBUGGING PURPOSES ONLY!>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
{'path': 'data/repos/buluma/packer_rhel/tasks/main.yml', 'org_name': 'buluma', 'license': 'Apache-2.0-Short,Apache-2.0', 'download_link': 'https://old-galaxy.ansible.com/buluma/packer_rhel', 'input': 'name: ensure necessary packages are installed.', 'output': 'ansible.builtin.yum:\n  name:\n  - wget\n  - perl\n  - cpp\n  - gcc\n  - make\n  - bzip2\n  - kernel-headers\n  - kernel-devel\n  - libselinux-python\n  - elfutils-libelf-devel\n  - cifs-utils\n  state: present\n', 'download_count': '747', 'repo_name': 'packer_rhel'}
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<FOR DEBUGGING PURPOSES ONLY!>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>{'license': '', 'repo_name': 'mariadb_install', 'download_link': 'https://old-galaxy.ansible.com/mahdi22/mariadb_install', 'org_name': 'mahdi22', 'input': 'name: Add MariaDB Repository Key for', 'output': 'apt_key:\n  url: https://mariadb.org/mariadb_release_signing_key.asc\n  state: present\nwhen: (use_proxy is not defined) or (not use_proxy)\n', 'download_count': '801', 'path': 'data/repos/mahdi22/mariadb_install/tasks/Debian-install.yml'}

<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<FOR DEBUGGING PURPOSES ONLY!>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
{'path': 'data/repos/darkwizard242/packer/tasks/install_debian.yml', 'org_name': 'darkwizard242', 'license': 'MIT', 'download_link': 'https://old-galaxy.ansible.com/darkwizard242/packer', 'input': 'name: Debian/Ubuntu Family | Install unzip if it is currently not in installed state', 'output': 'ansible.builtin.apt:\n  name: unzip\n  state: present\n  force_apt_get: true\n  update_cache: true\n', 'download_count': '4445', 'repo_name': 'packer'}
{'license': 'Apache-2.0-Short,Apache-2.0', 'repo_name': 'packer_rhel', 'download_link': 'https://old-galaxy.ansible.com/buluma/packer_rhel', 'org_name': 'buluma', 'input': 'name: ensure necessary packages are installed.', 'output': 'ansible.builtin.yum:\n  name:\n  - wget\n  - perl\n  - cpp\n  - gcc\n  - make\n  - bzip2\n  - kernel-headers\n  - kernel-devel\n  - libselinux-python\n  - elfutils-libelf-devel\n  - cifs-utils\n  state: present\n', 'download_count': '747', 'path': 'data/repos/buluma/packer_rhel/tasks/main.yml'}
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<FOR DEBUGGING PURPOSES ONLY!>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
{'license': 'MIT', 'repo_name': 'packer', 'download_link': 'https://old-galaxy.ansible.com/darkwizard242/packer', 'org_name': 'darkwizard242', 'input': 'name: Debian/Ubuntu Family | Install unzip if it is currently not in installed state', 'output': 'ansible.builtin.apt:\n  name: unzip\n  state: present\n  force_apt_get: true\n  update_cache: true\n', 'download_count': '4445', 'path': 'data/repos/darkwizard242/packer/tasks/install_debian.yml'}
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<FOR DEBUGGING PURPOSES ONLY!>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
{'license': 'MIT', 'download_count': '561', 'download_link': 'https://old-galaxy.ansible.com/lifeofguenter/nginx', 'input': 'name: Install', 'output': 'ansible.builtin.command: make install\nargs:\n  chdir: /tmp/nginx-1.24.0\nchanged_when: true\nbecome: true\nnotify: Restart_nginx\n', 'path': 'data/repos/lifeofguenter/nginx/tasks/nginx.yml', 'org_name': 'lifeofguenter', 'repo_name': 'nginx'}
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<FOR DEBUGGING PURPOSES ONLY!>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<FOR DEBUGGING PURPOSES ONLY!>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>{'license': 'Apache-2.0-Short,Apache-2.0', 'download_count': '34969', 'download_link': 'https://old-galaxy.ansible.com/robertdebock/locale', 'input': 'name: assert | Test locale_lc_time', 'output': 'ansible.builtin.assert:\n  that:\n  - locale_lc_time is defined\n  - locale_lc_time is string\n  - locale_lc_time is not none\n  quiet: true\n', 'path': 'data/repos/robertdebock/locale/tasks/assert.yml', 'org_name': 'robertdebock', 'repo_name': 'locale'}

<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<FOR DEBUGGING PURPOSES ONLY!>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
{'license': 'MIT', 'download_link': 'https://old-galaxy.ansible.com/lifeofguenter/nginx', 'input': 'name: Install', 'download_count': '561', 'path': 'data/repos/lifeofguenter/nginx/tasks/nginx.yml', 'org_name': 'lifeofguenter', 'repo_name': 'nginx', 'output': 'ansible.builtin.command: make install\nargs:\n  chdir: /tmp/nginx-1.24.0\nchanged_when: true\nbecome: true\nnotify: Restart_nginx\n'}
{'license': '', 'download_count': '513', 'download_link': 'https://old-galaxy.ansible.com/brentwg/visual-studio-code', 'input': 'name: Create the APT repository (Debian)', 'output': 'apt_repository:\n  repo: deb [arch=amd64] https://packages.microsoft.com/repos/vscode stable main\n  filename: vscode\n', 'path': 'data/repos/brentwg/visual-studio-code/tasks/Debian.yml', 'org_name': 'brentwg', 'repo_name': 'visual-studio-code'}<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<FOR DEBUGGING PURPOSES ONLY!>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>

<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<FOR DEBUGGING PURPOSES ONLY!>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
{'license': 'Apache-2.0-Short,Apache-2.0', 'download_link': 'https://old-galaxy.ansible.com/robertdebock/locale', 'input': 'name: assert | Test locale_lc_time', 'download_count': '34969', 'path': 'data/repos/robertdebock/locale/tasks/assert.yml', 'org_name': 'robertdebock', 'repo_name': 'locale', 'output': 'ansible.builtin.assert:\n  that:\n  - locale_lc_time is defined\n  - locale_lc_time is string\n  - locale_lc_time is not none\n  quiet: true\n'}
{'license': '', 'download_count': '801', 'download_link': 'https://old-galaxy.ansible.com/mahdi22/mariadb_install', 'input': 'name: Add MariaDB Repository Key for', 'output': 'apt_key:\n  url: https://mariadb.org/mariadb_release_signing_key.asc\n  state: present\nwhen: (use_proxy is not defined) or (not use_proxy)\n', 'path': 'data/repos/mahdi22/mariadb_install/tasks/Debian-install.yml', 'org_name': 'mahdi22', 'repo_name': 'mariadb_install'}
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<FOR DEBUGGING PURPOSES ONLY!>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<FOR DEBUGGING PURPOSES ONLY!>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
{'license': '', 'download_link': 'https://old-galaxy.ansible.com/brentwg/visual-studio-code', 'input': 'name: Create the APT repository (Debian)', 'download_count': '513', 'path': 'data/repos/brentwg/visual-studio-code/tasks/Debian.yml', 'org_name': 'brentwg', 'repo_name': 'visual-studio-code', 'output': 'apt_repository:\n  repo: deb [arch=amd64] https://packages.microsoft.com/repos/vscode stable main\n  filename: vscode\n'}
{'license': 'Apache-2.0-Short,Apache-2.0', 'download_count': '747', 'download_link': 'https://old-galaxy.ansible.com/buluma/packer_rhel', 'input': 'name: ensure necessary packages are installed.', 'output': 'ansible.builtin.yum:\n  name:\n  - wget\n  - perl\n  - cpp\n  - gcc\n  - make\n  - bzip2\n  - kernel-headers\n  - kernel-devel\n  - libselinux-python\n  - elfutils-libelf-devel\n  - cifs-utils\n  state: present\n', 'path': 'data/repos/buluma/packer_rhel/tasks/main.yml', 'org_name': 'buluma', 'repo_name': 'packer_rhel'}<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<FOR DEBUGGING PURPOSES ONLY!>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>

<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<FOR DEBUGGING PURPOSES ONLY!>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
{'license': '', 'download_link': 'https://old-galaxy.ansible.com/mahdi22/mariadb_install', 'input': 'name: Add MariaDB Repository Key for', 'download_count': '801', 'path': 'data/repos/mahdi22/mariadb_install/tasks/Debian-install.yml', 'org_name': 'mahdi22', 'repo_name': 'mariadb_install', 'output': 'apt_key:\n  url: https://mariadb.org/mariadb_release_signing_key.asc\n  state: present\nwhen: (use_proxy is not defined) or (not use_proxy)\n'}
{'license': 'MIT', 'download_count': '4445', 'download_link': 'https://old-galaxy.ansible.com/darkwizard242/packer', 'input': 'name: Debian/Ubuntu Family | Install unzip if it is currently not in installed state', 'output': 'ansible.builtin.apt:\n  name: unzip\n  state: present\n  force_apt_get: true\n  update_cache: true\n', 'path': 'data/repos/darkwizard242/packer/tasks/install_debian.yml', 'org_name': 'darkwizard242', 'repo_name': 'packer'}
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<FOR DEBUGGING PURPOSES ONLY!>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
{'license': 'Apache-2.0-Short,Apache-2.0', 'download_link': 'https://old-galaxy.ansible.com/buluma/packer_rhel', 'input': 'name: ensure necessary packages are installed.', 'download_count': '747', 'path': 'data/repos/buluma/packer_rhel/tasks/main.yml', 'org_name': 'buluma', 'repo_name': 'packer_rhel', 'output': 'ansible.builtin.yum:\n  name:\n  - wget\n  - perl\n  - cpp\n  - gcc\n  - make\n  - bzip2\n  - kernel-headers\n  - kernel-devel\n  - libselinux-python\n  - elfutils-libelf-devel\n  - cifs-utils\n  state: present\n'}
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<FOR DEBUGGING PURPOSES ONLY!>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
{'license': 'MIT', 'download_link': 'https://old-galaxy.ansible.com/darkwizard242/packer', 'input': 'name: Debian/Ubuntu Family | Install unzip if it is currently not in installed state', 'download_count': '4445', 'path': 'data/repos/darkwizard242/packer/tasks/install_debian.yml', 'org_name': 'darkwizard242', 'repo_name': 'packer', 'output': 'ansible.builtin.apt:\n  name: unzip\n  state: present\n  force_apt_get: true\n  update_cache: true\n'}
  0%|          | 0/400 [00:00<?, ?it/s]  0%|          | 0/400 [00:00<?, ?it/s]  0%|          | 0/400 [00:00<?, ?it/s]  0%|          | 0/400 [00:00<?, ?it/s]  0%|          | 1/400 [00:00<03:06,  2.14it/s]  0%|          | 1/400 [00:00<03:06,  2.14it/s]  0%|          | 1/400 [00:00<03:05,  2.15it/s]  0%|          | 1/400 [00:00<03:05,  2.15it/s]100%|| 400/400 [00:00<00:00, 750.87it/s]
100%|| 400/400 [00:00<00:00, 750.93it/s]
100%|| 400/400 [00:00<00:00, 749.71it/s]
100%|| 400/400 [00:00<00:00, 747.54it/s]
The character to token ratio of the dataset is: 3.18
Loading the model
The character to token ratio of the dataset is: 3.18
Loading the model
The character to token ratio of the dataset is: 3.18
Loading the model
The character to token ratio of the dataset is: 3.18
Loading the model
Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
/u/bzd2/.local/lib/python3.9/site-packages/transformers/models/auto/auto_factory.py:472: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.
  warnings.warn(
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-1b/snapshots/182f0165fdf8da9c9935901eec65c94337f01c11/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "bigcode/starcoderbase-1b",
  "activation_function": "gelu_pytorch_tanh",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 2048,
  "n_head": 16,
  "n_inner": 8192,
  "n_layer": 24,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.37.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

/u/bzd2/.local/lib/python3.9/site-packages/transformers/models/auto/auto_factory.py:472: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/transformers/models/auto/auto_factory.py:472: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/transformers/models/auto/auto_factory.py:472: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.
  warnings.warn(
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-1b/snapshots/182f0165fdf8da9c9935901eec65c94337f01c11/config.json
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-1b/snapshots/182f0165fdf8da9c9935901eec65c94337f01c11/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "bigcode/starcoderbase-1b",
  "activation_function": "gelu_pytorch_tanh",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 2048,
  "n_head": 16,
  "n_inner": 8192,
  "n_layer": 24,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.37.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

Model config GPTBigCodeConfig {
  "_name_or_path": "bigcode/starcoderbase-1b",
  "activation_function": "gelu_pytorch_tanh",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 2048,
  "n_head": 16,
  "n_inner": 8192,
  "n_layer": 24,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.37.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-1b/snapshots/182f0165fdf8da9c9935901eec65c94337f01c11/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "bigcode/starcoderbase-1b",
  "activation_function": "gelu_pytorch_tanh",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 2048,
  "n_head": 16,
  "n_inner": 8192,
  "n_layer": 24,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.37.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

Overriding torch_dtype=None with `torch_dtype=torch.float16` due to requirements of `bitsandbytes` to enable model loading in 8-bit or 4-bit. Pass your own torch_dtype to specify the dtype of the remaining non-linear layers or pass torch_dtype=torch.float16 to remove this warning.
Overriding torch_dtype=None with `torch_dtype=torch.float16` due to requirements of `bitsandbytes` to enable model loading in 8-bit or 4-bit. Pass your own torch_dtype to specify the dtype of the remaining non-linear layers or pass torch_dtype=torch.float16 to remove this warning.
Overriding torch_dtype=None with `torch_dtype=torch.float16` due to requirements of `bitsandbytes` to enable model loading in 8-bit or 4-bit. Pass your own torch_dtype to specify the dtype of the remaining non-linear layers or pass torch_dtype=torch.float16 to remove this warning.
Overriding torch_dtype=None with `torch_dtype=torch.float16` due to requirements of `bitsandbytes` to enable model loading in 8-bit or 4-bit. Pass your own torch_dtype to specify the dtype of the remaining non-linear layers or pass torch_dtype=torch.float16 to remove this warning.
loading weights file model.safetensors from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-1b/snapshots/182f0165fdf8da9c9935901eec65c94337f01c11/model.safetensors
loading weights file model.safetensors from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-1b/snapshots/182f0165fdf8da9c9935901eec65c94337f01c11/model.safetensors
loading weights file model.safetensors from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-1b/snapshots/182f0165fdf8da9c9935901eec65c94337f01c11/model.safetensors
loading weights file model.safetensors from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-1b/snapshots/182f0165fdf8da9c9935901eec65c94337f01c11/model.safetensors
Instantiating GPTBigCodeForCausalLM model under default dtype torch.float16.
Instantiating GPTBigCodeForCausalLM model under default dtype torch.float16.
Instantiating GPTBigCodeForCausalLM model under default dtype torch.float16.
Instantiating GPTBigCodeForCausalLM model under default dtype torch.float16.
Generate config GenerationConfig {
  "bos_token_id": 0,
  "eos_token_id": 0
}

Generate config GenerationConfig {
  "bos_token_id": 0,
  "eos_token_id": 0
}

Generate config GenerationConfig {
  "bos_token_id": 0,
  "eos_token_id": 0
}

Generate config GenerationConfig {
  "bos_token_id": 0,
  "eos_token_id": 0
}

huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Detected 8-bit loading: activating 8-bit loading for this model
Detected 8-bit loading: activating 8-bit loading for this model
Detected 8-bit loading: activating 8-bit loading for this model
Detected 8-bit loading: activating 8-bit loading for this model
All model checkpoint weights were used when initializing GPTBigCodeForCausalLM.

All the weights of GPTBigCodeForCausalLM were initialized from the model checkpoint at bigcode/starcoderbase-1b.
If your task is similar to the task the model of the checkpoint was trained on, you can already use GPTBigCodeForCausalLM for predictions without further training.
loading configuration file generation_config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-1b/snapshots/182f0165fdf8da9c9935901eec65c94337f01c11/generation_config.json
Generate config GenerationConfig {
  "bos_token_id": 0,
  "eos_token_id": 0
}

/u/bzd2/.local/lib/python3.9/site-packages/peft/utils/other.py:141: FutureWarning: prepare_model_for_int8_training is deprecated and will be removed in a future version. Use prepare_model_for_kbit_training instead.
  warnings.warn(
trainable params: 7176192 || all params: 1144383488 || trainable%: 0.6270793029827428
Starting main loop
PyTorch: setting up devices
All model checkpoint weights were used when initializing GPTBigCodeForCausalLM.

All the weights of GPTBigCodeForCausalLM were initialized from the model checkpoint at bigcode/starcoderbase-1b.
If your task is similar to the task the model of the checkpoint was trained on, you can already use GPTBigCodeForCausalLM for predictions without further training.
All model checkpoint weights were used when initializing GPTBigCodeForCausalLM.

All the weights of GPTBigCodeForCausalLM were initialized from the model checkpoint at bigcode/starcoderbase-1b.
If your task is similar to the task the model of the checkpoint was trained on, you can already use GPTBigCodeForCausalLM for predictions without further training.
All model checkpoint weights were used when initializing GPTBigCodeForCausalLM.

All the weights of GPTBigCodeForCausalLM were initialized from the model checkpoint at bigcode/starcoderbase-1b.
If your task is similar to the task the model of the checkpoint was trained on, you can already use GPTBigCodeForCausalLM for predictions without further training.
loading configuration file generation_config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-1b/snapshots/182f0165fdf8da9c9935901eec65c94337f01c11/generation_config.json
Generate config GenerationConfig {
  "bos_token_id": 0,
  "eos_token_id": 0
}

loading configuration file generation_config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-1b/snapshots/182f0165fdf8da9c9935901eec65c94337f01c11/generation_config.json
Generate config GenerationConfig {
  "bos_token_id": 0,
  "eos_token_id": 0
}

loading configuration file generation_config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-1b/snapshots/182f0165fdf8da9c9935901eec65c94337f01c11/generation_config.json
Generate config GenerationConfig {
  "bos_token_id": 0,
  "eos_token_id": 0
}

/u/bzd2/.local/lib/python3.9/site-packages/peft/utils/other.py:141: FutureWarning: prepare_model_for_int8_training is deprecated and will be removed in a future version. Use prepare_model_for_kbit_training instead.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/peft/utils/other.py:141: FutureWarning: prepare_model_for_int8_training is deprecated and will be removed in a future version. Use prepare_model_for_kbit_training instead.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/peft/utils/other.py:141: FutureWarning: prepare_model_for_int8_training is deprecated and will be removed in a future version. Use prepare_model_for_kbit_training instead.
  warnings.warn(
trainable params: 7176192 || all params: 1144383488 || trainable%: 0.6270793029827428
Starting main loop
trainable params: 7176192 || all params: 1144383488 || trainable%: 0.6270793029827428
Starting main loop
trainable params: 7176192 || all params: 1144383488 || trainable%: 0.6270793029827428
Starting main loop
PyTorch: setting up devices
PyTorch: setting up devices
PyTorch: setting up devices
Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable 	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
TOKENIZERS_PARALLELISM=(true | false)
max_steps is given, it will override any value given in num_train_epochs
Using auto half precision backend
Training...
Training...
Training...
Training...
Currently training with a batch size of: 1
***** Running training *****
  Num examples = 208,000
  Num Epochs = 9,223,372,036,854,775,807
  Instantaneous batch size per device = 1
  Total train batch size (w. parallel, distributed & accumulation) = 64
  Gradient Accumulation steps = 16
  Total optimization steps = 3,250
  Number of trainable parameters = 7,176,192
Automatic Weights & Biases logging enabled, to disable set os.environ["WANDB_DISABLED"] = "true"
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
wandb: Currently logged in as: hetarthvader (complex_dnn). Use `wandb login --relogin` to force relogin
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
wandb: Tracking run with wandb version 0.16.1
wandb: Run data is saved locally in /projects/bbvz/bzd2/wandb/run-20231213_172722-ithkj8wu
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run FinalRuns-/projects/bbvz/bzd2/checkpoints_experiment_4
wandb:  View project at https://wandb.ai/complex_dnn/huggingface
wandb:  View run at https://wandb.ai/complex_dnn/huggingface/runs/ithkj8wu
  0%|          | 0/3250 [00:00<?, ?it/s]/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 2.1983, 'learning_rate': 5e-05, 'epoch': 0.0}
{'loss': 2.2568, 'learning_rate': 0.0001, 'epoch': 0.0}
{'loss': 2.1005, 'learning_rate': 9.999997661121407e-05, 'epoch': 0.0}
{'loss': 2.128, 'learning_rate': 9.999990644487813e-05, 'epoch': 0.0}
{'loss': 2.0279, 'learning_rate': 9.999978950105786e-05, 'epoch': 0.0}
{'loss': 2.0588, 'learning_rate': 9.999962577986265e-05, 'epoch': 0.0}
  0%|          | 1/3250 [00:14<12:55:47, 14.33s/it]                                                     0%|          | 1/3250 [00:14<12:55:47, 14.33s/it]  0%|          | 2/3250 [00:19<8:16:35,  9.17s/it]                                                     0%|          | 2/3250 [00:19<8:16:35,  9.17s/it]  0%|          | 3/3250 [00:25<6:43:47,  7.46s/it]                                                    0%|          | 3/3250 [00:25<6:43:47,  7.46s/it]  0%|          | 4/3250 [00:30<6:00:09,  6.66s/it]                                                    0%|          | 4/3250 [00:30<6:00:09,  6.66s/it]  0%|          | 5/3250 [00:36<5:36:07,  6.21s/it]                                                    0%|          | 5/3250 [00:36<5:36:07,  6.21s/it]  0%|          | 6/3250 [00:41<5:21:40,  5.95s/it]                                                    0%|          | 6/3250 [00:41<5:21:40,  5.95s/it]  0%|          | 7/3250 [00:47<5:12:13,  5.78s/it]                                            {'loss': 2.2485, 'learning_rate': 9.999941528144567e-05, 'epoch': 0.0}
{'loss': 1.9013, 'learning_rate': 9.999915800600383e-05, 'epoch': 0.0}
{'loss': 1.8612, 'learning_rate': 9.999885395377788e-05, 'epoch': 0.0}
{'loss': 1.8742, 'learning_rate': 9.999850312505221e-05, 'epoch': 0.0}
        0%|          | 7/3250 [00:47<5:12:13,  5.78s/it]  0%|          | 8/3250 [00:52<5:06:02,  5.66s/it]                                                    0%|          | 8/3250 [00:52<5:06:02,  5.66s/it]  0%|          | 9/3250 [00:57<5:02:01,  5.59s/it]                                                    0%|          | 9/3250 [00:57<5:02:01,  5.59s/it]  0%|          | 10/3250 [01:03<4:59:03,  5.54s/it]                                                     0%|          | 10/3250 [01:03<4:59:03,  5.54s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 1.7188172340393066, 'eval_runtime': 1.4569, 'eval_samples_per_second': 8.237, 'eval_steps_per_second': 2.059, 'epoch': 0.0}
                                                     0%|          | 10/3250 [01:04<4:59:03,  5.54s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_4/tmp-checkpoint-10
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-10
/projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-10
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-10
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-10
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-10/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-10/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-10/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-10/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 1.805, 'learning_rate': 9.99981055201551e-05, 'epoch': 0.0}
{'loss': 1.7048, 'learning_rate': 9.999766113945847e-05, 'epoch': 0.0}
{'loss': 1.7995, 'learning_rate': 9.999716998337812e-05, 'epoch': 0.0}
{'loss': 1.7679, 'learning_rate': 9.99966320523735e-05, 'epoch': 0.0}
{'loss': 1.6822, 'learning_rate': 9.999604734694792e-05, 'epoch': 0.0}
{'loss': 1.6661, 'learning_rate': 9.999541586764836e-05, 'epoch': 0.0}
  0%|          | 11/3250 [01:10<5:32:27,  6.16s/it]                                                     0%|          | 11/3250 [01:10<5:32:27,  6.16s/it]  0%|          | 12/3250 [01:16<5:20:13,  5.93s/it]                                                     0%|          | 12/3250 [01:16<5:20:13,  5.93s/it]  0%|          | 13/3250 [01:21<5:11:47,  5.78s/it]                                                     0%|          | 13/3250 [01:21<5:11:47,  5.78s/it]  0%|          | 14/3250 [01:27<5:05:49,  5.67s/it]                                                     0%|          | 14/3250 [01:27<5:05:49,  5.67s/it]  0%|          | 15/3250 [01:32<5:01:33,  5.59s/it]                                                     0%|          | 15/3250 [01:32<5:01:33,  5.59s/it]  0%|          | 16/3250 [01:37<4:58:35,  5.54s/it]                                                     0%|          | 16/3250 [01:37<4:58:35,  5.54s/it]  1%|          | 17/3250 [01:43<5:03:12,  5.63s/it]                             {'loss': 1.559, 'learning_rate': 9.999473761506563e-05, 'epoch': 0.01}
{'loss': 1.5763, 'learning_rate': 9.999401258983425e-05, 'epoch': 0.01}
{'loss': 1.5667, 'learning_rate': 9.999324079263253e-05, 'epoch': 0.01}
{'loss': 1.5451, 'learning_rate': 9.999242222418252e-05, 'epoch': 0.01}
                        1%|          | 17/3250 [01:43<5:03:12,  5.63s/it]  1%|          | 18/3250 [01:49<4:59:53,  5.57s/it]                                                     1%|          | 18/3250 [01:49<4:59:53,  5.57s/it]  1%|          | 19/3250 [01:54<4:57:10,  5.52s/it]                                                     1%|          | 19/3250 [01:54<4:57:10,  5.52s/it]  1%|          | 20/3250 [02:00<4:55:33,  5.49s/it]                                                     1%|          | 20/3250 [02:00<4:55:33,  5.49s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 1.4590821266174316, 'eval_runtime': 1.6627, 'eval_samples_per_second': 7.217, 'eval_steps_per_second': 1.804, 'epoch': 0.01}
                                                     1%|          | 20/3250 [02:01<4:55:33,  5.49s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_4/tmp-checkpoint-20
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-20
 /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-20
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-20
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-20
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-20/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-20/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-20/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-20/pytorch_model.bin
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 1.5204, 'learning_rate': 9.999155688525004e-05, 'epoch': 0.01}
{'loss': 1.5542, 'learning_rate': 9.999064477664466e-05, 'epoch': 0.01}
{'loss': 1.4982, 'learning_rate': 9.998968589921969e-05, 'epoch': 0.01}
{'loss': 1.4827, 'learning_rate': 9.998868025387223e-05, 'epoch': 0.01}
{'loss': 1.4826, 'learning_rate': 9.998762784154308e-05, 'epoch': 0.01}
{'loss': 1.4556, 'learning_rate': 9.998652866321687e-05, 'epoch': 0.01}
  1%|          | 21/3250 [02:07<5:26:05,  6.06s/it]                                                     1%|          | 21/3250 [02:07<5:26:05,  6.06s/it]  1%|          | 22/3250 [02:12<5:15:44,  5.87s/it]                                                     1%|          | 22/3250 [02:12<5:15:44,  5.87s/it]  1%|          | 23/3250 [02:18<5:08:30,  5.74s/it]                                                     1%|          | 23/3250 [02:18<5:08:30,  5.74s/it]  1%|          | 24/3250 [02:23<5:03:14,  5.64s/it]                                                     1%|          | 24/3250 [02:23<5:03:14,  5.64s/it]  1%|          | 25/3250 [02:29<4:59:34,  5.57s/it]                                                     1%|          | 25/3250 [02:29<4:59:34,  5.57s/it]  1%|          | 26/3250 [02:34<4:57:10,  5.53s/it]                                                     1%|          | 26/3250 [02:34<4:57:10,  5.53s/it]  1%|          | 27/3250 [02:39<4:55:18,  5.50s/it]                             {'loss': 1.4491, 'learning_rate': 9.99853827199219e-05, 'epoch': 0.01}
{'loss': 1.4661, 'learning_rate': 9.998419001273029e-05, 'epoch': 0.01}
{'loss': 1.5, 'learning_rate': 9.998295054275786e-05, 'epoch': 0.01}
{'loss': 1.4908, 'learning_rate': 9.99816643111642e-05, 'epoch': 0.01}
                        1%|          | 27/3250 [02:39<4:55:18,  5.50s/it]  1%|          | 28/3250 [02:45<4:53:52,  5.47s/it]                                                     1%|          | 28/3250 [02:45<4:53:52,  5.47s/it]  1%|          | 29/3250 [02:50<4:53:01,  5.46s/it]                                                     1%|          | 29/3250 [02:50<4:53:01,  5.46s/it]  1%|          | 30/3250 [02:56<4:52:19,  5.45s/it]                                                     1%|          | 30/3250 [02:56<4:52:19,  5.45s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 1.3824808597564697, 'eval_runtime': 1.4016, 'eval_samples_per_second': 8.562, 'eval_steps_per_second': 2.14, 'epoch': 0.01}
                                                     1%|          | 30/3250 [02:57<4:52:19,  5.45s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_4/tmp-checkpoint-30
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-30
 /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-30
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-30
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-30
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-30/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-30/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-30/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-30/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 1.4397, 'learning_rate': 9.998033131915266e-05, 'epoch': 0.01}
{'loss': 1.4426, 'learning_rate': 9.997895156797028e-05, 'epoch': 0.01}
{'loss': 1.4162, 'learning_rate': 9.997752505890794e-05, 'epoch': 0.01}
{'loss': 1.3893, 'learning_rate': 9.997605179330019e-05, 'epoch': 0.01}
{'loss': 1.4109, 'learning_rate': 9.997453177252534e-05, 'epoch': 0.01}
{'loss': 1.4426, 'learning_rate': 9.997296499800545e-05, 'epoch': 0.01}
  1%|          | 31/3250 [03:03<5:20:31,  5.97s/it]                                                     1%|          | 31/3250 [03:03<5:20:31,  5.97s/it]  1%|          | 32/3250 [03:08<5:11:27,  5.81s/it]                                                     1%|          | 32/3250 [03:08<5:11:27,  5.81s/it]  1%|          | 33/3250 [03:14<5:09:30,  5.77s/it]                                                     1%|          | 33/3250 [03:14<5:09:30,  5.77s/it]  1%|          | 34/3250 [03:19<5:04:03,  5.67s/it]                                                     1%|          | 34/3250 [03:19<5:04:03,  5.67s/it]  1%|          | 35/3250 [03:25<4:59:52,  5.60s/it]                                                     1%|          | 35/3250 [03:25<4:59:52,  5.60s/it]  1%|          | 36/3250 [03:30<4:56:59,  5.54s/it]                                                     1%|          | 36/3250 [03:30<4:56:59,  5.54s/it]  1%|          | 37/3250 [03:36<4:55:03,  5.51s/it]                             {'loss': 1.8454, 'learning_rate': 9.997135147120633e-05, 'epoch': 0.01}
{'loss': 1.2989, 'learning_rate': 9.99696911936375e-05, 'epoch': 0.01}
{'loss': 1.3784, 'learning_rate': 9.996798416685228e-05, 'epoch': 0.01}
{'loss': 1.4478, 'learning_rate': 9.99662303924476e-05, 'epoch': 0.01}
                        1%|          | 37/3250 [03:36<4:55:03,  5.51s/it]  1%|          | 38/3250 [03:41<4:53:27,  5.48s/it]                                                     1%|          | 38/3250 [03:41<4:53:27,  5.48s/it]  1%|          | 39/3250 [03:47<4:52:29,  5.47s/it]                                                     1%|          | 39/3250 [03:47<4:52:29,  5.47s/it]  1%|          | 40/3250 [03:52<4:51:26,  5.45s/it]                                                     1%|          | 40/3250 [03:52<4:51:26,  5.45s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 1.3349684476852417, 'eval_runtime': 1.4042, 'eval_samples_per_second': 8.546, 'eval_steps_per_second': 2.136, 'epoch': 0.01}
                                                     1%|          | 40/3250 [03:53<4:51:26,  5.45s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_4/tmp-checkpoint-40
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-40
/projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-40
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-40
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-40
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-40/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-40/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-40/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-40/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 1.3905, 'learning_rate': 9.996442987206428e-05, 'epoch': 0.01}
{'loss': 1.3718, 'learning_rate': 9.996258260738676e-05, 'epoch': 0.01}
{'loss': 1.3914, 'learning_rate': 9.996068860014325e-05, 'epoch': 0.01}
{'loss': 1.4179, 'learning_rate': 9.995874785210573e-05, 'epoch': 0.01}
{'loss': 1.4208, 'learning_rate': 9.995676036508982e-05, 'epoch': 0.01}
{'loss': 1.3644, 'learning_rate': 9.995472614095495e-05, 'epoch': 0.01}
  1%|         | 41/3250 [03:59<5:18:28,  5.95s/it]                                                     1%|         | 41/3250 [03:59<5:18:28,  5.95s/it]  1%|         | 42/3250 [04:05<5:09:34,  5.79s/it]                                                     1%|         | 42/3250 [04:05<5:09:34,  5.79s/it]  1%|         | 43/3250 [04:10<5:03:29,  5.68s/it]                                                     1%|         | 43/3250 [04:10<5:03:29,  5.68s/it]  1%|         | 44/3250 [04:15<4:59:07,  5.60s/it]                                                     1%|         | 44/3250 [04:15<4:59:07,  5.60s/it]  1%|         | 45/3250 [04:21<4:57:02,  5.56s/it]                                                     1%|         | 45/3250 [04:21<4:57:02,  5.56s/it]  1%|         | 46/3250 [04:26<4:54:37,  5.52s/it]                                                     1%|         | 46/3250 [04:26<4:54:37,  5.52s/it]  1%|         | 47/3250 [04:32<4:53:07,  5.49s/it]   {'loss': 1.3027, 'learning_rate': 9.995264518160425e-05, 'epoch': 0.01}
{'loss': 1.3608, 'learning_rate': 9.995051748898453e-05, 'epoch': 0.01}
{'loss': 1.3329, 'learning_rate': 9.994834306508638e-05, 'epoch': 0.02}
{'loss': 1.3637, 'learning_rate': 9.994612191194406e-05, 'epoch': 0.02}
                                                  1%|         | 47/3250 [04:32<4:53:07,  5.49s/it]  1%|         | 48/3250 [04:37<4:51:46,  5.47s/it]                                                     1%|         | 48/3250 [04:37<4:51:46,  5.47s/it]  2%|         | 49/3250 [04:43<4:50:35,  5.45s/it]                                                     2%|         | 49/3250 [04:43<4:50:35,  5.45s/it]  2%|         | 50/3250 [04:48<4:56:21,  5.56s/it]                                                     2%|         | 50/3250 [04:48<4:56:21,  5.56s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 1.297622799873352, 'eval_runtime': 1.643, 'eval_samples_per_second': 7.304, 'eval_steps_per_second': 1.826, 'epoch': 0.02}
                                                     2%|         | 50/3250 [04:50<4:56:21,  5.56s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_4/tmp-checkpoint-50
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in I AM HERE AND I AM SAVING THE MODEL 
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-50
/projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-50
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-50
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-50
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-50/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-50/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-50/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-50/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 1.3301, 'learning_rate': 9.99438540316356e-05, 'epoch': 0.02}
{'loss': 1.3468, 'learning_rate': 9.99415394262827e-05, 'epoch': 0.02}
{'loss': 1.3405, 'learning_rate': 9.993917809805081e-05, 'epoch': 0.02}
{'loss': 1.3382, 'learning_rate': 9.993677004914907e-05, 'epoch': 0.02}
{'loss': 1.3299, 'learning_rate': 9.993431528183032e-05, 'epoch': 0.02}
{'loss': 1.3138, 'learning_rate': 9.993181379839113e-05, 'epoch': 0.02}
  2%|         | 51/3250 [04:56<5:26:01,  6.11s/it]                                                     2%|         | 51/3250 [04:56<5:26:01,  6.11s/it]  2%|         | 52/3250 [05:01<5:14:52,  5.91s/it]                                                     2%|         | 52/3250 [05:01<5:14:52,  5.91s/it]  2%|         | 53/3250 [05:07<5:07:03,  5.76s/it]                                                     2%|         | 53/3250 [05:07<5:07:03,  5.76s/it]  2%|         | 54/3250 [05:12<5:01:28,  5.66s/it]                                                     2%|         | 54/3250 [05:12<5:01:28,  5.66s/it]  2%|         | 55/3250 [05:17<4:57:27,  5.59s/it]                                                     2%|         | 55/3250 [05:17<4:57:27,  5.59s/it]  2%|         | 56/3250 [05:23<4:54:57,  5.54s/it]                                                     2%|         | 56/3250 [05:23<4:54:57,  5.54s/it]  2%|         | 57/3250 [05:28<4:52:55,  5.50s/it]   {'loss': 1.3053, 'learning_rate': 9.992926560117176e-05, 'epoch': 0.02}
{'loss': 1.3499, 'learning_rate': 9.992667069255619e-05, 'epoch': 0.02}
{'loss': 1.3124, 'learning_rate': 9.992402907497209e-05, 'epoch': 0.02}
{'loss': 1.3905, 'learning_rate': 9.992134075089084e-05, 'epoch': 0.02}
                                                  2%|         | 57/3250 [05:28<4:52:55,  5.50s/it]  2%|         | 58/3250 [05:34<4:51:35,  5.48s/it]                                                     2%|         | 58/3250 [05:34<4:51:35,  5.48s/it]  2%|         | 59/3250 [05:39<4:50:39,  5.47s/it]                                                     2%|         | 59/3250 [05:39<4:50:39,  5.47s/it]  2%|         | 60/3250 [05:45<4:49:57,  5.45s/it]                                                     2%|         | 60/3250 [05:45<4:49:57,  5.45s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 1.266208291053772, 'eval_runtime': 1.3985, 'eval_samples_per_second': 8.581, 'eval_steps_per_second': 2.145, 'epoch': 0.02}
                                                     2%|         | 60/3250 [05:46<4:49:57,  5.45s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_4/tmp-checkpoint-60
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-60
/projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-60
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-60
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-60
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-60/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-60/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-60/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-60/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 1.3088, 'learning_rate': 9.991860572282747e-05, 'epoch': 0.02}
{'loss': 1.3361, 'learning_rate': 9.991582399334076e-05, 'epoch': 0.02}
{'loss': 1.3181, 'learning_rate': 9.991299556503318e-05, 'epoch': 0.02}
{'loss': 1.286, 'learning_rate': 9.991012044055084e-05, 'epoch': 0.02}
{'loss': 1.2926, 'learning_rate': 9.990719862258358e-05, 'epoch': 0.02}
{'loss': 1.3047, 'learning_rate': 9.990423011386489e-05, 'epoch': 0.02}
  2%|         | 61/3250 [05:52<5:16:51,  5.96s/it]                                                     2%|         | 61/3250 [05:52<5:16:51,  5.96s/it]  2%|         | 62/3250 [05:57<5:08:12,  5.80s/it]                                                     2%|         | 62/3250 [05:57<5:08:12,  5.80s/it]  2%|         | 63/3250 [06:03<5:01:52,  5.68s/it]                                                     2%|         | 63/3250 [06:03<5:01:52,  5.68s/it]  2%|         | 64/3250 [06:08<4:57:33,  5.60s/it]                                                     2%|         | 64/3250 [06:08<4:57:33,  5.60s/it]  2%|         | 65/3250 [06:13<4:54:20,  5.54s/it]                                                     2%|         | 65/3250 [06:13<4:54:20,  5.54s/it]  2%|         | 66/3250 [06:19<4:56:44,  5.59s/it]                                                     2%|         | 66/3250 [06:19<4:56:44,  5.59s/it]  2%|         | 67/3250 [06:24<4:53:47,  5.54s/it]   {'loss': 1.2914, 'learning_rate': 9.990121491717201e-05, 'epoch': 0.02}
{'loss': 1.6644, 'learning_rate': 9.989815303532577e-05, 'epoch': 0.02}
{'loss': 1.28, 'learning_rate': 9.989504447119073e-05, 'epoch': 0.02}
{'loss': 1.3292, 'learning_rate': 9.989188922767512e-05, 'epoch': 0.02}
                                                  2%|         | 67/3250 [06:24<4:53:47,  5.54s/it]  2%|         | 68/3250 [06:30<4:51:49,  5.50s/it]                                                     2%|         | 68/3250 [06:30<4:51:49,  5.50s/it]  2%|         | 69/3250 [06:35<4:50:24,  5.48s/it]                                                     2%|         | 69/3250 [06:35<4:50:24,  5.48s/it]  2%|         | 70/3250 [06:41<4:49:00,  5.45s/it]                                                     2%|         | 70/3250 [06:41<4:49:00,  5.45s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 1.2399944067001343, 'eval_runtime': 1.3977, 'eval_samples_per_second': 8.585, 'eval_steps_per_second': 2.146, 'epoch': 0.02}
                                                     2%|         | 70/3250 [06:42<4:49:00,  5.45s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_4/tmp-checkpoint-70
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-70
 /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-70
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-70
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-70
the pytorch model path isthe pytorch model path is the pytorch model path is/projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-70/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-70/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-70/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-70/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 1.2984, 'learning_rate': 9.988868730773082e-05, 'epoch': 0.02}
{'loss': 1.2787, 'learning_rate': 9.98854387143534e-05, 'epoch': 0.02}
{'loss': 1.2678, 'learning_rate': 9.988214345058209e-05, 'epoch': 0.02}
{'loss': 1.3335, 'learning_rate': 9.987880151949976e-05, 'epoch': 0.02}
{'loss': 1.3222, 'learning_rate': 9.987541292423298e-05, 'epoch': 0.02}
{'loss': 1.2739, 'learning_rate': 9.987197766795193e-05, 'epoch': 0.02}
  2%|         | 71/3250 [06:48<5:14:52,  5.94s/it]                                                     2%|         | 71/3250 [06:48<5:14:52,  5.94s/it]  2%|         | 72/3250 [06:53<5:06:24,  5.78s/it]                                                     2%|         | 72/3250 [06:53<5:06:24,  5.78s/it]  2%|         | 73/3250 [06:59<5:00:23,  5.67s/it]                                                     2%|         | 73/3250 [06:59<5:00:23,  5.67s/it]  2%|         | 74/3250 [07:04<4:56:14,  5.60s/it]                                                     2%|         | 74/3250 [07:04<4:56:14,  5.60s/it]  2%|         | 75/3250 [07:09<4:53:13,  5.54s/it]                                                     2%|         | 75/3250 [07:09<4:53:13,  5.54s/it]  2%|         | 76/3250 [07:15<4:51:12,  5.50s/it]                                                     2%|         | 76/3250 [07:15<4:51:12,  5.50s/it]  2%|         | 77/3250 [07:20<4:49:37,  5.48s/it]   {'loss': 1.2486, 'learning_rate': 9.986849575387049e-05, 'epoch': 0.02}
{'loss': 1.2577, 'learning_rate': 9.986496718524616e-05, 'epoch': 0.02}
{'loss': 1.2583, 'learning_rate': 9.986139196538011e-05, 'epoch': 0.02}
{'loss': 1.2585, 'learning_rate': 9.985777009761713e-05, 'epoch': 0.02}
                                                  2%|         | 77/3250 [07:20<4:49:37,  5.48s/it]  2%|         | 78/3250 [07:26<4:48:40,  5.46s/it]                                                     2%|         | 78/3250 [07:26<4:48:40,  5.46s/it]  2%|         | 79/3250 [07:31<4:47:52,  5.45s/it]                                                     2%|         | 79/3250 [07:31<4:47:52,  5.45s/it]  2%|         | 80/3250 [07:37<4:46:55,  5.43s/it]                                                     2%|         | 80/3250 [07:37<4:46:55,  5.43s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 1.2195117473602295, 'eval_runtime': 1.3905, 'eval_samples_per_second': 8.63, 'eval_steps_per_second': 2.158, 'epoch': 0.02}
                                                     2%|         | 80/3250 [07:38<4:46:55,  5.43s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_4/tmp-checkpoint-80
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-80
/projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-80
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-80
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-80
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-80/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-80/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-80/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-80/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 1.2562, 'learning_rate': 9.985410158534567e-05, 'epoch': 0.02}
{'loss': 1.2538, 'learning_rate': 9.98503864319978e-05, 'epoch': 0.03}
{'loss': 1.2483, 'learning_rate': 9.984662464104926e-05, 'epoch': 0.03}
{'loss': 1.2514, 'learning_rate': 9.984281621601938e-05, 'epoch': 0.03}
{'loss': 1.2505, 'learning_rate': 9.983896116047113e-05, 'epoch': 0.03}
{'loss': 1.23, 'learning_rate': 9.983505947801115e-05, 'epoch': 0.03}
  2%|         | 81/3250 [07:44<5:12:26,  5.92s/it]                                                     2%|         | 81/3250 [07:44<5:12:26,  5.92s/it]  3%|         | 82/3250 [07:49<5:10:08,  5.87s/it]                                                     3%|         | 82/3250 [07:49<5:10:08,  5.87s/it]  3%|         | 83/3250 [07:55<5:02:28,  5.73s/it]                                                     3%|         | 83/3250 [07:55<5:02:28,  5.73s/it]  3%|         | 84/3250 [08:00<4:57:21,  5.64s/it]                                                     3%|         | 84/3250 [08:00<4:57:21,  5.64s/it]  3%|         | 85/3250 [08:06<4:53:39,  5.57s/it]                                                     3%|         | 85/3250 [08:06<4:53:39,  5.57s/it]  3%|         | 86/3250 [08:11<4:50:54,  5.52s/it]                                                     3%|         | 86/3250 [08:11<4:50:54,  5.52s/it]  3%|         | 87/3250 [08:16<4:49:12,  5.49s/it]   {'loss': 1.2628, 'learning_rate': 9.983111117228961e-05, 'epoch': 0.03}
{'loss': 1.2399, 'learning_rate': 9.982711624700041e-05, 'epoch': 0.03}
{'loss': 1.2667, 'learning_rate': 9.982307470588098e-05, 'epoch': 0.03}
{'loss': 1.3121, 'learning_rate': 9.981898655271235e-05, 'epoch': 0.03}
                                                  3%|         | 87/3250 [08:16<4:49:12,  5.49s/it]  3%|         | 88/3250 [08:22<4:48:35,  5.48s/it]                                                     3%|         | 88/3250 [08:22<4:48:35,  5.48s/it]  3%|         | 89/3250 [08:27<4:47:20,  5.45s/it]                                                     3%|         | 89/3250 [08:27<4:47:20,  5.45s/it]  3%|         | 90/3250 [08:33<4:46:38,  5.44s/it]                                                     3%|         | 90/3250 [08:33<4:46:38,  5.44s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 1.2003085613250732, 'eval_runtime': 1.6484, 'eval_samples_per_second': 7.28, 'eval_steps_per_second': 1.82, 'epoch': 0.03}
                                                     3%|         | 90/3250 [08:34<4:46:38,  5.44s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_4/tmp-checkpoint-90
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-90I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-90

I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-90
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-90
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-90/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-90/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-90/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-90/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 1.2683, 'learning_rate': 9.981485179131929e-05, 'epoch': 0.03}
{'loss': 1.246, 'learning_rate': 9.981067042557e-05, 'epoch': 0.03}
{'loss': 1.2715, 'learning_rate': 9.980644245937639e-05, 'epoch': 0.03}
{'loss': 1.179, 'learning_rate': 9.980216789669395e-05, 'epoch': 0.03}
{'loss': 1.2563, 'learning_rate': 9.979784674152175e-05, 'epoch': 0.03}
{'loss': 1.2112, 'learning_rate': 9.979347899790246e-05, 'epoch': 0.03}
  3%|         | 91/3250 [08:40<5:17:38,  6.03s/it]                                                     3%|         | 91/3250 [08:40<5:17:38,  6.03s/it]  3%|         | 92/3250 [08:45<5:07:40,  5.85s/it]                                                     3%|         | 92/3250 [08:45<5:07:40,  5.85s/it]  3%|         | 93/3250 [08:51<5:00:26,  5.71s/it]                                                     3%|         | 93/3250 [08:51<5:00:26,  5.71s/it]  3%|         | 94/3250 [08:56<4:55:31,  5.62s/it]                                                     3%|         | 94/3250 [08:56<4:55:31,  5.62s/it]  3%|         | 95/3250 [09:02<4:52:19,  5.56s/it]                                                     3%|         | 95/3250 [09:02<4:52:19,  5.56s/it]  3%|         | 96/3250 [09:07<4:49:44,  5.51s/it]                                                     3%|         | 96/3250 [09:07<4:49:44,  5.51s/it]  3%|         | 97/3250 [09:12<4:48:05,  5.48s/it]   {'loss': 1.2708, 'learning_rate': 9.978906466992229e-05, 'epoch': 0.03}
{'loss': 1.6216, 'learning_rate': 9.978460376171112e-05, 'epoch': 0.03}
{'loss': 1.2066, 'learning_rate': 9.978009627744234e-05, 'epoch': 0.03}
{'loss': 1.2419, 'learning_rate': 9.977554222133292e-05, 'epoch': 0.03}
                                                  3%|         | 97/3250 [09:12<4:48:05,  5.48s/it]  3%|         | 98/3250 [09:18<4:46:44,  5.46s/it]                                                     3%|         | 98/3250 [09:18<4:46:44,  5.46s/it]  3%|         | 99/3250 [09:24<4:49:42,  5.52s/it]                                                     3%|         | 99/3250 [09:24<4:49:42,  5.52s/it]  3%|         | 100/3250 [09:29<4:47:57,  5.48s/it]                                                      3%|         | 100/3250 [09:29<4:47:57,  5.48s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 1.1840261220932007, 'eval_runtime': 1.3892, 'eval_samples_per_second': 8.638, 'eval_steps_per_second': 2.16, 'epoch': 0.03}
                                                      3%|         | 100/3250 [09:30<4:47:57,  5.48s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_4/tmp-checkpoint-100
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in I AM HERE AND I AM SAVING THE MODEL
 the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-100
/projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-100
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-100
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-100
the pytorch model path isthe pytorch model path isthe pytorch model path is  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-100/pytorch_model.bin/projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-100/pytorch_model.bin

 /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-100/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-100/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 1.2567, 'learning_rate': 9.977094159764342e-05, 'epoch': 0.03}
{'loss': 1.2201, 'learning_rate': 9.976629441067797e-05, 'epoch': 0.03}
{'loss': 1.1951, 'learning_rate': 9.976160066478425e-05, 'epoch': 0.03}
{'loss': 1.2308, 'learning_rate': 9.97568603643535e-05, 'epoch': 0.03}
{'loss': 1.2789, 'learning_rate': 9.975207351382051e-05, 'epoch': 0.03}
{'loss': 1.2343, 'learning_rate': 9.974724011766363e-05, 'epoch': 0.03}
  3%|         | 101/3250 [09:36<5:12:43,  5.96s/it]                                                      3%|         | 101/3250 [09:36<5:12:43,  5.96s/it]  3%|         | 102/3250 [09:41<5:03:47,  5.79s/it]                                                      3%|         | 102/3250 [09:41<5:03:47,  5.79s/it]  3%|         | 103/3250 [09:47<4:57:34,  5.67s/it]                                                      3%|         | 103/3250 [09:47<4:57:34,  5.67s/it]  3%|         | 104/3250 [09:52<4:53:00,  5.59s/it]                                                      3%|         | 104/3250 [09:52<4:53:00,  5.59s/it]  3%|         | 105/3250 [09:58<4:49:49,  5.53s/it]                                                      3%|         | 105/3250 [09:58<4:49:49,  5.53s/it]  3%|         | 106/3250 [10:03<4:47:38,  5.49s/it]                                                      3%|         | 106/3250 [10:03<4:47:38,  5.49s/it]  3%|         | 107/3250 [10:08<4:46{'loss': 1.2258, 'learning_rate': 9.974236018040474e-05, 'epoch': 0.03}
{'loss': 1.1687, 'learning_rate': 9.973743370660928e-05, 'epoch': 0.03}
{'loss': 1.2271, 'learning_rate': 9.973246070088624e-05, 'epoch': 0.03}
{'loss': 1.2124, 'learning_rate': 9.972744116788809e-05, 'epoch': 0.03}
:15,  5.46s/it]                                                      3%|         | 107/3250 [10:08<4:46:15,  5.46s/it]  3%|         | 108/3250 [10:14<4:44:59,  5.44s/it]                                                      3%|         | 108/3250 [10:14<4:44:59,  5.44s/it]  3%|         | 109/3250 [10:19<4:44:19,  5.43s/it]                                                      3%|         | 109/3250 [10:19<4:44:19,  5.43s/it]  3%|         | 110/3250 [10:25<4:43:50,  5.42s/it]                                                      3%|         | 110/3250 [10:25<4:43:50,  5.42s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 1.1706037521362305, 'eval_runtime': 4.7218, 'eval_samples_per_second': 2.541, 'eval_steps_per_second': 0.635, 'epoch': 0.03}
                                                      3%|         | 110/3250 [10:29<4:43:50,  5.42s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_4/tmp-checkpoint-110
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-110
/projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-110
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-110
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-110
the pytorch model path is the pytorch model path is/projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-110/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-110/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-110/pytorch_model.binthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-110/pytorch_model.bin

/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 1.2081, 'learning_rate': 9.972237511231087e-05, 'epoch': 0.03}
{'loss': 1.176, 'learning_rate': 9.971726253889416e-05, 'epoch': 0.03}
{'loss': 1.2033, 'learning_rate': 9.9712103452421e-05, 'epoch': 0.03}
{'loss': 1.2022, 'learning_rate': 9.970689785771798e-05, 'epoch': 0.04}
{'loss': 1.2167, 'learning_rate': 9.970164575965523e-05, 'epoch': 0.04}
{'loss': 1.1839, 'learning_rate': 9.969634716314635e-05, 'epoch': 0.04}
  3%|         | 111/3250 [10:35<6:04:12,  6.96s/it]                                                      3%|         | 111/3250 [10:35<6:04:12,  6.96s/it]  3%|         | 112/3250 [10:41<5:39:35,  6.49s/it]                                                      3%|         | 112/3250 [10:41<5:39:35,  6.49s/it]  3%|         | 113/3250 [10:46<5:22:38,  6.17s/it]                                                      3%|         | 113/3250 [10:46<5:22:38,  6.17s/it]  4%|         | 114/3250 [10:51<5:10:34,  5.94s/it]                                                      4%|         | 114/3250 [10:51<5:10:34,  5.94s/it]  4%|         | 115/3250 [10:57<5:07:56,  5.89s/it]                                                      4%|         | 115/3250 [10:57<5:07:56,  5.89s/it]  4%|         | 116/3250 [11:03<5:00:28,  5.75s/it]                                                      4%|         | 116/3250 [11:03<5:00:28,  5.75s/it]  4%|         | 117/3250 [11:08<4:55{'loss': 1.183, 'learning_rate': 9.969100207314845e-05, 'epoch': 0.04}
{'loss': 1.2109, 'learning_rate': 9.968561049466214e-05, 'epoch': 0.04}
{'loss': 1.2131, 'learning_rate': 9.968017243273149e-05, 'epoch': 0.04}
{'loss': 1.236, 'learning_rate': 9.967468789244412e-05, 'epoch': 0.04}
:09,  5.65s/it]                                                      4%|         | 117/3250 [11:08<4:55:09,  5.65s/it]  4%|         | 118/3250 [11:13<4:51:26,  5.58s/it]                                                      4%|         | 118/3250 [11:13<4:51:26,  5.58s/it]  4%|         | 119/3250 [11:19<4:49:27,  5.55s/it]                                                      4%|         | 119/3250 [11:19<4:49:27,  5.55s/it]  4%|         | 120/3250 [11:24<4:47:12,  5.51s/it]                                                      4%|         | 120/3250 [11:24<4:47:12,  5.51s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 1.1578267812728882, 'eval_runtime': 1.4019, 'eval_samples_per_second': 8.56, 'eval_steps_per_second': 2.14, 'epoch': 0.04}
                                                      4%|         | 120/3250 [11:26<4:47:12,  5.51s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_4/tmp-checkpoint-120
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-120
/projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-120
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-120
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-120
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-120/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-120/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-120/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-120/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 1.2465, 'learning_rate': 9.966915687893108e-05, 'epoch': 0.04}
{'loss': 1.1901, 'learning_rate': 9.966357939736693e-05, 'epoch': 0.04}
{'loss': 1.1909, 'learning_rate': 9.965795545296967e-05, 'epoch': 0.04}
{'loss': 1.1767, 'learning_rate': 9.965228505100084e-05, 'epoch': 0.04}
{'loss': 1.1898, 'learning_rate': 9.964656819676533e-05, 'epoch': 0.04}
{'loss': 1.1856, 'learning_rate': 9.964080489561159e-05, 'epoch': 0.04}
  4%|         | 121/3250 [11:32<5:13:37,  6.01s/it]                                                      4%|         | 121/3250 [11:32<5:13:37,  6.01s/it]  4%|         | 122/3250 [11:37<5:04:13,  5.84s/it]                                                      4%|         | 122/3250 [11:37<5:04:13,  5.84s/it]  4%|         | 123/3250 [11:42<4:57:34,  5.71s/it]                                                      4%|         | 123/3250 [11:42<4:57:34,  5.71s/it]  4%|         | 124/3250 [11:48<4:53:00,  5.62s/it]                                                      4%|         | 124/3250 [11:48<4:53:00,  5.62s/it]  4%|         | 125/3250 [11:53<4:49:55,  5.57s/it]                                                      4%|         | 125/3250 [11:53<4:49:55,  5.57s/it]  4%|         | 126/3250 [11:59<4:47:32,  5.52s/it]                                                      4%|         | 126/3250 [11:59<4:47:32,  5.52s/it]  4%|         | 127/3250 [12:04<4:46{'loss': 1.2067, 'learning_rate': 9.963499515293147e-05, 'epoch': 0.04}
{'loss': 1.649, 'learning_rate': 9.962913897416028e-05, 'epoch': 0.04}
{'loss': 1.1036, 'learning_rate': 9.96232363647768e-05, 'epoch': 0.04}
{'loss': 1.1604, 'learning_rate': 9.961728733030318e-05, 'epoch': 0.04}
:00,  5.49s/it]                                                      4%|         | 127/3250 [12:04<4:46:00,  5.49s/it]  4%|         | 128/3250 [12:09<4:44:38,  5.47s/it]                                                      4%|         | 128/3250 [12:09<4:44:38,  5.47s/it]  4%|         | 129/3250 [12:15<4:43:42,  5.45s/it]                                                      4%|         | 129/3250 [12:15<4:43:42,  5.45s/it]  4%|         | 130/3250 [12:20<4:43:08,  5.44s/it]                                                      4%|         | 130/3250 [12:20<4:43:08,  5.44s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 1.1477365493774414, 'eval_runtime': 1.6368, 'eval_samples_per_second': 7.331, 'eval_steps_per_second': 1.833, 'epoch': 0.04}
                                                      4%|         | 130/3250 [12:22<4:43:08,  5.44s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_4/tmp-checkpoint-130
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-130
/projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-130
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-130
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-130
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-130/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-130/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-130/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-130/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 1.2249, 'learning_rate': 9.961129187630509e-05, 'epoch': 0.04}
{'loss': 1.1821, 'learning_rate': 9.960525000839159e-05, 'epoch': 0.04}
{'loss': 1.1679, 'learning_rate': 9.959916173221511e-05, 'epoch': 0.04}
{'loss': 1.181, 'learning_rate': 9.959302705347158e-05, 'epoch': 0.04}
{'loss': 1.2264, 'learning_rate': 9.958684597790031e-05, 'epoch': 0.04}
{'loss': 1.2128, 'learning_rate': 9.958061851128402e-05, 'epoch': 0.04}
  4%|         | 131/3250 [12:28<5:12:24,  6.01s/it]                                                      4%|         | 131/3250 [12:28<5:12:24,  6.01s/it]  4%|         | 132/3250 [12:33<5:06:54,  5.91s/it]                                                      4%|         | 132/3250 [12:33<5:06:54,  5.91s/it]  4%|         | 133/3250 [12:39<4:59:15,  5.76s/it]                                                      4%|         | 133/3250 [12:39<4:59:15,  5.76s/it]  4%|         | 134/3250 [12:44<4:54:08,  5.66s/it]                                                      4%|         | 134/3250 [12:44<4:54:08,  5.66s/it]  4%|         | 135/3250 [12:50<4:50:21,  5.59s/it]                                                      4%|         | 135/3250 [12:50<4:50:21,  5.59s/it]  4%|         | 136/3250 [12:55<4:47:37,  5.54s/it]                                                      4%|         | 136/3250 [12:55<4:47:37,  5.54s/it]  4%|         | 137/3250 [13:00<4:45{'loss': 1.1782, 'learning_rate': 9.957434465944879e-05, 'epoch': 0.04}
{'loss': 1.1186, 'learning_rate': 9.956802442826416e-05, 'epoch': 0.04}
{'loss': 1.1981, 'learning_rate': 9.956165782364303e-05, 'epoch': 0.04}
{'loss': 1.1509, 'learning_rate': 9.955524485154168e-05, 'epoch': 0.04}
:41,  5.51s/it]                                                      4%|         | 137/3250 [13:00<4:45:41,  5.51s/it]  4%|         | 138/3250 [13:06<4:44:35,  5.49s/it]                                                      4%|         | 138/3250 [13:06<4:44:35,  5.49s/it]  4%|         | 139/3250 [13:11<4:43:35,  5.47s/it]                                                      4%|         | 139/3250 [13:11<4:43:35,  5.47s/it]  4%|         | 140/3250 [13:17<4:42:47,  5.46s/it]                                                      4%|         | 140/3250 [13:17<4:42:47,  5.46s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 1.139865756034851, 'eval_runtime': 1.4046, 'eval_samples_per_second': 8.544, 'eval_steps_per_second': 2.136, 'epoch': 0.04}
                                                      4%|         | 140/3250 [13:18<4:42:47,  5.46s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_4/tmp-checkpoint-140
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in I AM HERE AND I AM SAVING THE MODEL 
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-140
/projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-140
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-140
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-140
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-140/pytorch_model.binthe pytorch model path is
 /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-140/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-140/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-140/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 1.1819, 'learning_rate': 9.954878551795976e-05, 'epoch': 0.04}
{'loss': 1.1461, 'learning_rate': 9.954227982894034e-05, 'epoch': 0.04}
{'loss': 1.1574, 'learning_rate': 9.953572779056981e-05, 'epoch': 0.04}
{'loss': 1.1686, 'learning_rate': 9.952912940897793e-05, 'epoch': 0.04}
{'loss': 1.1678, 'learning_rate': 9.952248469033785e-05, 'epoch': 0.04}
{'loss': 1.1648, 'learning_rate': 9.951579364086602e-05, 'epoch': 0.04}
  4%|         | 141/3250 [13:24<5:08:22,  5.95s/it]                                                      4%|         | 141/3250 [13:24<5:08:22,  5.95s/it]  4%|         | 142/3250 [13:29<4:59:48,  5.79s/it]                                                      4%|         | 142/3250 [13:29<4:59:48,  5.79s/it]  4%|         | 143/3250 [13:35<4:53:59,  5.68s/it]                                                      4%|         | 143/3250 [13:35<4:53:59,  5.68s/it]  4%|         | 144/3250 [13:40<4:49:54,  5.60s/it]                                                      4%|         | 144/3250 [13:40<4:49:54,  5.60s/it]  4%|         | 145/3250 [13:45<4:46:53,  5.54s/it]                                                      4%|         | 145/3250 [13:45<4:46:53,  5.54s/it]  4%|         | 146/3250 [13:51<4:44:51,  5.51s/it]                                                      4%|         | 146/3250 [13:51<4:44:51,  5.51s/it]  5%|         | 147/3250 [13:56<4:43{'loss': 1.1707, 'learning_rate': 9.950905626682228e-05, 'epoch': 0.05}
{'loss': 1.1475, 'learning_rate': 9.95022725745098e-05, 'epoch': 0.05}
{'loss': 1.1961, 'learning_rate': 9.949544257027502e-05, 'epoch': 0.05}
{'loss': 1.1602, 'learning_rate': 9.948856626050781e-05, 'epoch': 0.05}
:16,  5.48s/it]                                                      5%|         | 147/3250 [13:56<4:43:16,  5.48s/it]  5%|         | 148/3250 [14:02<4:48:21,  5.58s/it]                                                      5%|         | 148/3250 [14:02<4:48:21,  5.58s/it]  5%|         | 149/3250 [14:08<4:46:02,  5.53s/it]                                                      5%|         | 149/3250 [14:08<4:46:02,  5.53s/it]  5%|         | 150/3250 [14:13<4:44:10,  5.50s/it]                                                      5%|         | 150/3250 [14:13<4:44:10,  5.50s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 1.1303139925003052, 'eval_runtime': 1.399, 'eval_samples_per_second': 8.578, 'eval_steps_per_second': 2.144, 'epoch': 0.05}
                                                      5%|         | 150/3250 [14:14<4:44:10,  5.50s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_4/tmp-checkpoint-150
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-150I AM HERE AND I AM SAVING THE MODEL

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-150
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-150
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-150
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-150/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-150/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-150/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-150/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 1.2429, 'learning_rate': 9.94816436516413e-05, 'epoch': 0.05}
{'loss': 1.143, 'learning_rate': 9.947467475015196e-05, 'epoch': 0.05}
{'loss': 1.1819, 'learning_rate': 9.94676595625595e-05, 'epoch': 0.05}
{'loss': 1.1645, 'learning_rate': 9.946059809542707e-05, 'epoch': 0.05}
{'loss': 1.1342, 'learning_rate': 9.945349035536097e-05, 'epoch': 0.05}
{'loss': 1.1629, 'learning_rate': 9.944633634901088e-05, 'epoch': 0.05}
  5%|         | 151/3250 [14:20<5:09:27,  5.99s/it]                                                      5%|         | 151/3250 [14:20<5:09:27,  5.99s/it]  5%|         | 152/3250 [14:26<5:00:25,  5.82s/it]                                                      5%|         | 152/3250 [14:26<5:00:25,  5.82s/it]  5%|         | 153/3250 [14:31<4:54:08,  5.70s/it]                                                      5%|         | 153/3250 [14:31<4:54:08,  5.70s/it]  5%|         | 154/3250 [14:36<4:49:47,  5.62s/it]                                                      5%|         | 154/3250 [14:36<4:49:47,  5.62s/it]  5%|         | 155/3250 [14:42<4:46:46,  5.56s/it]                                                      5%|         | 155/3250 [14:42<4:46:46,  5.56s/it]  5%|         | 156/3250 [14:47<4:44:27,  5.52s/it]                                                      5%|         | 156/3250 [14:47<4:44:27,  5.52s/it]  5%|         | 157/3250 [14:53<4:42{'loss': 1.1576, 'learning_rate': 9.943913608306975e-05, 'epoch': 0.05}
{'loss': 1.1605, 'learning_rate': 9.94318895642738e-05, 'epoch': 0.05}
{'loss': 1.5357, 'learning_rate': 9.94245967994025e-05, 'epoch': 0.05}
{'loss': 1.1431, 'learning_rate': 9.941725779527861e-05, 'epoch': 0.05}
:54,  5.49s/it]                                                      5%|         | 157/3250 [14:53<4:42:54,  5.49s/it]  5%|         | 158/3250 [14:58<4:41:48,  5.47s/it]                                                      5%|         | 158/3250 [14:58<4:41:48,  5.47s/it]  5%|         | 159/3250 [15:03<4:40:51,  5.45s/it]                                                      5%|         | 159/3250 [15:03<4:40:51,  5.45s/it]  5%|         | 160/3250 [15:09<4:40:14,  5.44s/it]                                                      5%|         | 160/3250 [15:09<4:40:14,  5.44s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 1.124099850654602, 'eval_runtime': 1.3888, 'eval_samples_per_second': 8.641, 'eval_steps_per_second': 2.16, 'epoch': 0.05}
                                                      5%|         | 160/3250 [15:10<4:40:14,  5.44s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_4/tmp-checkpoint-160
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-160/projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-160

I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-160
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-160
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-160/pytorch_model.bin
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-160/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-160/pytorch_model.binthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-160/pytorch_model.bin

/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 1.1957, 'learning_rate': 9.940987255876817e-05, 'epoch': 0.05}
{'loss': 1.1744, 'learning_rate': 9.940244109678043e-05, 'epoch': 0.05}
{'loss': 1.1477, 'learning_rate': 9.939496341626791e-05, 'epoch': 0.05}
{'loss': 1.1266, 'learning_rate': 9.938743952422636e-05, 'epoch': 0.05}
{'loss': 1.2004, 'learning_rate': 9.937986942769477e-05, 'epoch': 0.05}
{'loss': 1.2021, 'learning_rate': 9.937225313375535e-05, 'epoch': 0.05}
  5%|         | 161/3250 [15:16<5:06:51,  5.96s/it]                                                      5%|         | 161/3250 [15:16<5:06:51,  5.96s/it]  5%|         | 162/3250 [15:22<4:58:48,  5.81s/it]                                                      5%|         | 162/3250 [15:22<4:58:48,  5.81s/it]  5%|         | 163/3250 [15:27<4:52:32,  5.69s/it]                                                      5%|         | 163/3250 [15:27<4:52:32,  5.69s/it]  5%|         | 164/3250 [15:33<4:55:20,  5.74s/it]                                                      5%|         | 164/3250 [15:33<4:55:20,  5.74s/it]  5%|         | 165/3250 [15:38<4:50:10,  5.64s/it]                                                      5%|         | 165/3250 [15:38<4:50:10,  5.64s/it]  5%|         | 166/3250 [15:44<4:46:34,  5.58s/it]                                                      5%|         | 166/3250 [15:44<4:46:34,  5.58s/it]  5%|         | 167/3250 [15:49<4:44{'loss': 1.1412, 'learning_rate': 9.936459064953355e-05, 'epoch': 0.05}
{'loss': 1.1263, 'learning_rate': 9.935688198219801e-05, 'epoch': 0.05}
{'loss': 1.1394, 'learning_rate': 9.934912713896057e-05, 'epoch': 0.05}
{'loss': 1.1296, 'learning_rate': 9.934132612707632e-05, 'epoch': 0.05}
:02,  5.53s/it]                                                      5%|         | 167/3250 [15:49<4:44:02,  5.53s/it]  5%|         | 168/3250 [15:54<4:42:17,  5.50s/it]                                                      5%|         | 168/3250 [15:54<4:42:17,  5.50s/it]  5%|         | 169/3250 [16:00<4:41:08,  5.47s/it]                                                      5%|         | 169/3250 [16:00<4:41:08,  5.47s/it]  5%|         | 170/3250 [16:05<4:40:16,  5.46s/it]                                                      5%|         | 170/3250 [16:05<4:40:16,  5.46s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 1.11720609664917, 'eval_runtime': 1.3941, 'eval_samples_per_second': 8.608, 'eval_steps_per_second': 2.152, 'epoch': 0.05}
                                                      5%|         | 170/3250 [16:07<4:40:16,  5.46s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_4/tmp-checkpoint-170
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in I AM HERE AND I AM SAVING THE MODEL 
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-170
/projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-170
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-170
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-170
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-170/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-170/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-170/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-170/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 1.1359, 'learning_rate': 9.933347895384346e-05, 'epoch': 0.05}
{'loss': 1.1221, 'learning_rate': 9.932558562660347e-05, 'epoch': 0.05}
{'loss': 1.121, 'learning_rate': 9.931764615274093e-05, 'epoch': 0.05}
{'loss': 1.1285, 'learning_rate': 9.930966053968364e-05, 'epoch': 0.05}
{'loss': 1.1538, 'learning_rate': 9.930162879490257e-05, 'epoch': 0.05}
{'loss': 1.1286, 'learning_rate': 9.92935509259118e-05, 'epoch': 0.05}
  5%|         | 171/3250 [16:12<5:06:34,  5.97s/it]                                                      5%|         | 171/3250 [16:12<5:06:34,  5.97s/it]  5%|         | 172/3250 [16:18<4:57:58,  5.81s/it]                                                      5%|         | 172/3250 [16:18<4:57:58,  5.81s/it]  5%|         | 173/3250 [16:23<4:52:36,  5.71s/it]                                                      5%|         | 173/3250 [16:23<4:52:36,  5.71s/it]  5%|         | 174/3250 [16:29<4:47:55,  5.62s/it]                                                      5%|         | 174/3250 [16:29<4:47:55,  5.62s/it]  5%|         | 175/3250 [16:34<4:44:45,  5.56s/it]                                                      5%|         | 175/3250 [16:34<4:44:45,  5.56s/it]  5%|         | 176/3250 [16:40<4:42:30,  5.51s/it]                                                      5%|         | 176/3250 [16:40<4:42:30,  5.51s/it]  5%|         | 177/3250 [16:45<4:40{'loss': 1.1079, 'learning_rate': 9.928542694026862e-05, 'epoch': 0.05}
{'loss': 1.168, 'learning_rate': 9.927725684557338e-05, 'epoch': 0.05}
{'loss': 1.137, 'learning_rate': 9.926904064946969e-05, 'epoch': 0.06}
{'loss': 1.1304, 'learning_rate': 9.92607783596442e-05, 'epoch': 0.06}
:55,  5.49s/it]                                                      5%|         | 177/3250 [16:45<4:40:55,  5.49s/it]  5%|         | 178/3250 [16:50<4:39:49,  5.47s/it]                                                      5%|         | 178/3250 [16:50<4:39:49,  5.47s/it]  6%|         | 179/3250 [16:56<4:38:52,  5.45s/it]                                                      6%|         | 179/3250 [16:56<4:38:52,  5.45s/it]  6%|         | 180/3250 [17:01<4:38:05,  5.43s/it]                                                      6%|         | 180/3250 [17:01<4:38:05,  5.43s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 1.1064118146896362, 'eval_runtime': 1.3833, 'eval_samples_per_second': 8.675, 'eval_steps_per_second': 2.169, 'epoch': 0.06}
                                                      6%|         | 180/3250 [17:03<4:38:05,  5.43s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_4/tmp-checkpoint-180
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-180
/projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-180
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-180
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-180
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-180/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-180/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-180/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-180/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 1.2036, 'learning_rate': 9.925246998382671e-05, 'epoch': 0.06}
{'loss': 1.1483, 'learning_rate': 9.924411552979012e-05, 'epoch': 0.06}
{'loss': 1.1267, 'learning_rate': 9.923571500535047e-05, 'epoch': 0.06}
{'loss': 1.1582, 'learning_rate': 9.922726841836684e-05, 'epoch': 0.06}
{'loss': 1.0703, 'learning_rate': 9.921877577674152e-05, 'epoch': 0.06}
{'loss': 1.1561, 'learning_rate': 9.921023708841975e-05, 'epoch': 0.06}
  6%|         | 181/3250 [17:09<5:09:50,  6.06s/it]                                                      6%|         | 181/3250 [17:09<5:09:50,  6.06s/it]  6%|         | 182/3250 [17:14<4:59:49,  5.86s/it]                                                      6%|         | 182/3250 [17:14<4:59:49,  5.86s/it]  6%|         | 183/3250 [17:20<4:53:45,  5.75s/it]                                                      6%|         | 183/3250 [17:20<4:53:45,  5.75s/it]  6%|         | 184/3250 [17:25<4:49:29,  5.67s/it]                                                      6%|         | 184/3250 [17:25<4:49:29,  5.67s/it]  6%|         | 185/3250 [17:31<4:45:31,  5.59s/it]                                                      6%|         | 185/3250 [17:31<4:45:31,  5.59s/it]  6%|         | 186/3250 [17:36<4:42:47,  5.54s/it]                                                      6%|         | 186/3250 [17:36<4:42:47,  5.54s/it]  6%|         | 187/3250 [17:41<4:40{'loss': 1.1036, 'learning_rate': 9.920165236138994e-05, 'epoch': 0.06}
{'loss': 1.1682, 'learning_rate': 9.919302160368353e-05, 'epoch': 0.06}
{'loss': 1.5293, 'learning_rate': 9.918434482337506e-05, 'epoch': 0.06}
{'loss': 1.1019, 'learning_rate': 9.917562202858208e-05, 'epoch': 0.06}
:57,  5.50s/it]                                                      6%|         | 187/3250 [17:41<4:40:57,  5.50s/it]  6%|         | 188/3250 [17:47<4:39:34,  5.48s/it]                                                      6%|         | 188/3250 [17:47<4:39:34,  5.48s/it]  6%|         | 189/3250 [17:52<4:38:16,  5.45s/it]                                                      6%|         | 189/3250 [17:52<4:38:16,  5.45s/it]  6%|         | 190/3250 [17:58<4:37:31,  5.44s/it]                                                      6%|         | 190/3250 [17:58<4:37:31,  5.44s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 1.1011539697647095, 'eval_runtime': 1.3946, 'eval_samples_per_second': 8.604, 'eval_steps_per_second': 2.151, 'epoch': 0.06}
                                                      6%|         | 190/3250 [17:59<4:37:31,  5.44s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_4/tmp-checkpoint-190
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-190
/projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-190
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-190
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-190
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-190/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-190/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-190/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-190/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 1.1402, 'learning_rate': 9.916685322746524e-05, 'epoch': 0.06}
{'loss': 1.1639, 'learning_rate': 9.915803842822817e-05, 'epoch': 0.06}
{'loss': 1.1205, 'learning_rate': 9.914917763911759e-05, 'epoch': 0.06}
{'loss': 1.1001, 'learning_rate': 9.914027086842322e-05, 'epoch': 0.06}
{'loss': 1.1321, 'learning_rate': 9.913131812447781e-05, 'epoch': 0.06}
{'loss': 1.1814, 'learning_rate': 9.912231941565711e-05, 'epoch': 0.06}
  6%|         | 191/3250 [18:05<5:03:36,  5.96s/it]                                                      6%|         | 191/3250 [18:05<5:03:36,  5.96s/it]  6%|         | 192/3250 [18:10<4:54:59,  5.79s/it]                                                      6%|         | 192/3250 [18:10<4:54:59,  5.79s/it]  6%|         | 193/3250 [18:16<4:49:14,  5.68s/it]                                                      6%|         | 193/3250 [18:16<4:49:14,  5.68s/it]  6%|         | 194/3250 [18:21<4:45:51,  5.61s/it]                                                      6%|         | 194/3250 [18:21<4:45:51,  5.61s/it]  6%|         | 195/3250 [18:26<4:42:39,  5.55s/it]                                                      6%|         | 195/3250 [18:26<4:42:39,  5.55s/it]  6%|         | 196/3250 [18:32<4:40:24,  5.51s/it]                                                      6%|         | 196/3250 [18:32<4:40:24,  5.51s/it]  6%|         | 197/3250 [18:38<4:48{'loss': 1.1465, 'learning_rate': 9.911327475037985e-05, 'epoch': 0.06}
{'loss': 1.1298, 'learning_rate': 9.91041841371078e-05, 'epoch': 0.06}
{'loss': 1.0691, 'learning_rate': 9.909504758434571e-05, 'epoch': 0.06}
{'loss': 1.1236, 'learning_rate': 9.908586510064127e-05, 'epoch': 0.06}
:07,  5.66s/it]                                                      6%|         | 197/3250 [18:38<4:48:07,  5.66s/it]  6%|         | 198/3250 [18:43<4:44:12,  5.59s/it]                                                      6%|         | 198/3250 [18:43<4:44:12,  5.59s/it]  6%|         | 199/3250 [18:49<4:41:42,  5.54s/it]                                                      6%|         | 199/3250 [18:49<4:41:42,  5.54s/it]  6%|         | 200/3250 [18:54<4:39:31,  5.50s/it]                                                      6%|         | 200/3250 [18:54<4:39:31,  5.50s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 1.096759557723999, 'eval_runtime': 1.3841, 'eval_samples_per_second': 8.67, 'eval_steps_per_second': 2.168, 'epoch': 0.06}
                                                      6%|         | 200/3250 [18:56<4:39:31,  5.50s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_4/tmp-checkpoint-200
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-200
/projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-200
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-200
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-200
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-200/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-200/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-200/pytorch_model.binthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-200/pytorch_model.bin

/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 1.1183, 'learning_rate': 9.907663669458518e-05, 'epoch': 0.06}
{'loss': 1.1173, 'learning_rate': 9.906736237481109e-05, 'epoch': 0.06}
{'loss': 1.0992, 'learning_rate': 9.905804214999558e-05, 'epoch': 0.06}
{'loss': 1.1155, 'learning_rate': 9.904867602885824e-05, 'epoch': 0.06}
{'loss': 1.1166, 'learning_rate': 9.903926402016153e-05, 'epoch': 0.06}
{'loss': 1.1236, 'learning_rate': 9.902980613271087e-05, 'epoch': 0.06}
  6%|         | 201/3250 [19:01<5:03:15,  5.97s/it]                                                      6%|         | 201/3250 [19:01<5:03:15,  5.97s/it]  6%|         | 202/3250 [19:07<4:54:38,  5.80s/it]                                                      6%|         | 202/3250 [19:07<4:54:38,  5.80s/it]  6%|         | 203/3250 [19:12<4:48:41,  5.68s/it]                                                      6%|         | 203/3250 [19:12<4:48:41,  5.68s/it]  6%|         | 204/3250 [19:17<4:44:25,  5.60s/it]                                                      6%|         | 204/3250 [19:17<4:44:25,  5.60s/it]  6%|         | 205/3250 [19:23<4:42:14,  5.56s/it]                                                      6%|         | 205/3250 [19:23<4:42:14,  5.56s/it]  6%|         | 206/3250 [19:28<4:39:45,  5.51s/it]                                                      6%|         | 206/3250 [19:28<4:39:45,  5.51s/it]  6%|         | 207/3250 [19:34<4:38{'loss': 1.0989, 'learning_rate': 9.90203023753546e-05, 'epoch': 0.06}
{'loss': 1.1243, 'learning_rate': 9.9010752756984e-05, 'epoch': 0.06}
{'loss': 1.1276, 'learning_rate': 9.900115728653319e-05, 'epoch': 0.06}
{'loss': 1.1203, 'learning_rate': 9.899151597297923e-05, 'epoch': 0.06}
:03,  5.48s/it]                                                      6%|         | 207/3250 [19:34<4:38:03,  5.48s/it]  6%|         | 208/3250 [19:39<4:36:45,  5.46s/it]                                                      6%|         | 208/3250 [19:39<4:36:45,  5.46s/it]  6%|         | 209/3250 [19:45<4:35:56,  5.44s/it]                                                      6%|         | 209/3250 [19:45<4:35:56,  5.44s/it]  6%|         | 210/3250 [19:50<4:35:18,  5.43s/it]                                                      6%|         | 210/3250 [19:50<4:35:18,  5.43s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 1.0876966714859009, 'eval_runtime': 1.3877, 'eval_samples_per_second': 8.647, 'eval_steps_per_second': 2.162, 'epoch': 0.06}
                                                      6%|         | 210/3250 [19:51<4:35:18,  5.43s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_4/tmp-checkpoint-210
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in I AM HERE AND I AM SAVING THE MODEL
 the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-210
/projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-210
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-210
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-210
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-210/pytorch_model.bin
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-210/pytorch_model.bin 
/projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-210/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-210/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 1.159, 'learning_rate': 9.89818288253421e-05, 'epoch': 0.06}
{'loss': 1.1487, 'learning_rate': 9.897209585268458e-05, 'epoch': 0.07}
{'loss': 1.1085, 'learning_rate': 9.896231706411242e-05, 'epoch': 0.07}
{'loss': 1.0922, 'learning_rate': 9.895249246877415e-05, 'epoch': 0.07}
{'loss': 1.0918, 'learning_rate': 9.894262207586116e-05, 'epoch': 0.07}
{'loss': 1.1115, 'learning_rate': 9.893270589460775e-05, 'epoch': 0.07}
  6%|         | 211/3250 [19:57<4:59:50,  5.92s/it]                                                      6%|         | 211/3250 [19:57<4:59:50,  5.92s/it]  7%|         | 212/3250 [20:02<4:52:10,  5.77s/it]                                                      7%|         | 212/3250 [20:02<4:52:10,  5.77s/it]  7%|         | 213/3250 [20:08<4:46:43,  5.66s/it]                                                      7%|         | 213/3250 [20:08<4:46:43,  5.66s/it]  7%|         | 214/3250 [20:14<4:47:04,  5.67s/it]                                                      7%|         | 214/3250 [20:14<4:47:04,  5.67s/it]  7%|         | 215/3250 [20:19<4:43:16,  5.60s/it]                                                      7%|         | 215/3250 [20:19<4:43:16,  5.60s/it]  7%|         | 216/3250 [20:24<4:40:40,  5.55s/it]                                                      7%|         | 216/3250 [20:24<4:40:40,  5.55s/it]  7%|         | 217/3250 [20:30<4:38{'loss': 1.0894, 'learning_rate': 9.8922743934291e-05, 'epoch': 0.07}
{'loss': 1.1248, 'learning_rate': 9.891273620423083e-05, 'epoch': 0.07}
{'loss': 1.5738, 'learning_rate': 9.890268271379e-05, 'epoch': 0.07}
{'loss': 1.0213, 'learning_rate': 9.889258347237404e-05, 'epoch': 0.07}
:36,  5.51s/it]                                                      7%|         | 217/3250 [20:30<4:38:36,  5.51s/it]  7%|         | 218/3250 [20:35<4:37:15,  5.49s/it]                                                      7%|         | 218/3250 [20:35<4:37:15,  5.49s/it]  7%|         | 219/3250 [20:41<4:36:03,  5.46s/it]                                                      7%|         | 219/3250 [20:41<4:36:03,  5.46s/it]  7%|         | 220/3250 [20:46<4:35:30,  5.46s/it]                                                      7%|         | 220/3250 [20:46<4:35:30,  5.46s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 1.0830808877944946, 'eval_runtime': 1.4019, 'eval_samples_per_second': 8.56, 'eval_steps_per_second': 2.14, 'epoch': 0.07}
                                                      7%|         | 220/3250 [20:47<4:35:30,  5.46s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_4/tmp-checkpoint-220
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-220I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-220

I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-220
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-220
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-220/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-220/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-220/pytorch_model.binthe pytorch model path is 
/projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-220/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 1.0833, 'learning_rate': 9.888243848943136e-05, 'epoch': 0.07}
{'loss': 1.1396, 'learning_rate': 9.887224777445308e-05, 'epoch': 0.07}
{'loss': 1.0999, 'learning_rate': 9.886201133697314e-05, 'epoch': 0.07}
{'loss': 1.0954, 'learning_rate': 9.885172918656826e-05, 'epoch': 0.07}
{'loss': 1.0963, 'learning_rate': 9.884140133285791e-05, 'epoch': 0.07}
{'loss': 1.1367, 'learning_rate': 9.883102778550434e-05, 'epoch': 0.07}
  7%|         | 221/3250 [20:53<5:01:34,  5.97s/it]                                                      7%|         | 221/3250 [20:53<5:01:34,  5.97s/it]  7%|         | 222/3250 [20:59<4:53:32,  5.82s/it]                                                      7%|         | 222/3250 [20:59<4:53:32,  5.82s/it]  7%|         | 223/3250 [21:04<4:47:53,  5.71s/it]                                                      7%|         | 223/3250 [21:04<4:47:53,  5.71s/it]  7%|         | 224/3250 [21:10<4:43:50,  5.63s/it]                                                      7%|         | 224/3250 [21:10<4:43:50,  5.63s/it]  7%|         | 225/3250 [21:15<4:41:02,  5.57s/it]                                                      7%|         | 225/3250 [21:15<4:41:02,  5.57s/it]  7%|         | 226/3250 [21:20<4:38:51,  5.53s/it]                                                      7%|         | 226/3250 [21:20<4:38:51,  5.53s/it]  7%|         | 227/3250 [21:26<4:37{'loss': 1.135, 'learning_rate': 9.882060855421253e-05, 'epoch': 0.07}
{'loss': 1.095, 'learning_rate': 9.881014364873021e-05, 'epoch': 0.07}
{'loss': 1.042, 'learning_rate': 9.879963307884784e-05, 'epoch': 0.07}
{'loss': 1.1119, 'learning_rate': 9.87890768543986e-05, 'epoch': 0.07}
:29,  5.51s/it]                                                      7%|         | 227/3250 [21:26<4:37:29,  5.51s/it]  7%|         | 228/3250 [21:31<4:36:22,  5.49s/it]                                                      7%|         | 228/3250 [21:31<4:36:22,  5.49s/it]  7%|         | 229/3250 [21:37<4:35:41,  5.48s/it]                                                      7%|         | 229/3250 [21:37<4:35:41,  5.48s/it]  7%|         | 230/3250 [21:43<4:44:50,  5.66s/it]                                                      7%|         | 230/3250 [21:43<4:44:50,  5.66s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 1.0791881084442139, 'eval_runtime': 1.4024, 'eval_samples_per_second': 8.557, 'eval_steps_per_second': 2.139, 'epoch': 0.07}
                                                      7%|         | 230/3250 [21:44<4:44:50,  5.66s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_4/tmp-checkpoint-230
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  I AM HERE AND I AM SAVING THE MODEL
/projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-230the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-230

I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-230
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-230
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-230/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-230/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-230/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-230/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 1.0866, 'learning_rate': 9.877847498525837e-05, 'epoch': 0.07}
{'loss': 1.0943, 'learning_rate': 9.876782748134576e-05, 'epoch': 0.07}
{'loss': 1.0772, 'learning_rate': 9.875713435262203e-05, 'epoch': 0.07}
{'loss': 1.0851, 'learning_rate': 9.874639560909117e-05, 'epoch': 0.07}
{'loss': 1.0986, 'learning_rate': 9.873561126079985e-05, 'epoch': 0.07}
{'loss': 1.0893, 'learning_rate': 9.872478131783736e-05, 'epoch': 0.07}
  7%|         | 231/3250 [21:50<5:08:23,  6.13s/it]                                                      7%|         | 231/3250 [21:50<5:08:23,  6.13s/it]  7%|         | 232/3250 [21:56<4:57:55,  5.92s/it]                                                      7%|         | 232/3250 [21:56<4:57:55,  5.92s/it]  7%|         | 233/3250 [22:01<4:50:31,  5.78s/it]                                                      7%|         | 233/3250 [22:01<4:50:31,  5.78s/it]  7%|         | 234/3250 [22:06<4:45:25,  5.68s/it]                                                      7%|         | 234/3250 [22:06<4:45:25,  5.68s/it]  7%|         | 235/3250 [22:12<4:41:46,  5.61s/it]                                                      7%|         | 235/3250 [22:12<4:41:46,  5.61s/it]  7%|         | 236/3250 [22:17<4:39:11,  5.56s/it]                                                      7%|         | 236/3250 [22:17<4:39:11,  5.56s/it]  7%|         | 237/3250 [22:23<4:37{'loss': 1.0893, 'learning_rate': 9.871390579033564e-05, 'epoch': 0.07}
{'loss': 1.0991, 'learning_rate': 9.870298468846936e-05, 'epoch': 0.07}
{'loss': 1.0702, 'learning_rate': 9.869201802245573e-05, 'epoch': 0.07}
{'loss': 1.1223, 'learning_rate': 9.868100580255466e-05, 'epoch': 0.07}
:13,  5.52s/it]                                                      7%|         | 237/3250 [22:23<4:37:13,  5.52s/it]  7%|         | 238/3250 [22:28<4:35:52,  5.50s/it]                                                      7%|         | 238/3250 [22:28<4:35:52,  5.50s/it]  7%|         | 239/3250 [22:34<4:35:13,  5.48s/it]                                                      7%|         | 239/3250 [22:34<4:35:13,  5.48s/it]  7%|         | 240/3250 [22:39<4:34:25,  5.47s/it]                                                      7%|         | 240/3250 [22:39<4:34:25,  5.47s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 1.0723639726638794, 'eval_runtime': 1.394, 'eval_samples_per_second': 8.608, 'eval_steps_per_second': 2.152, 'epoch': 0.07}
                                                      7%|         | 240/3250 [22:41<4:34:25,  5.47s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_4/tmp-checkpoint-240
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  I AM HERE AND I AM SAVING THE MODEL
/projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-240the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-240

I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-240
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-240
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-240/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-240/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-240/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-240/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 1.0841, 'learning_rate': 9.866994803906862e-05, 'epoch': 0.07}
{'loss': 1.1728, 'learning_rate': 9.865884474234274e-05, 'epoch': 0.07}
{'loss': 1.0698, 'learning_rate': 9.864769592276472e-05, 'epoch': 0.07}
{'loss': 1.1035, 'learning_rate': 9.863650159076485e-05, 'epoch': 0.08}
{'loss': 1.0741, 'learning_rate': 9.8625261756816e-05, 'epoch': 0.08}
{'loss': 1.0638, 'learning_rate': 9.861397643143362e-05, 'epoch': 0.08}
  7%|         | 241/3250 [22:46<5:00:30,  5.99s/it]                                                      7%|         | 241/3250 [22:46<5:00:30,  5.99s/it]  7%|         | 242/3250 [22:52<4:51:56,  5.82s/it]                                                      7%|         | 242/3250 [22:52<4:51:56,  5.82s/it]  7%|         | 243/3250 [22:57<4:46:01,  5.71s/it]                                                      7%|         | 243/3250 [22:57<4:46:01,  5.71s/it]  8%|         | 244/3250 [23:03<4:41:45,  5.62s/it]                                                      8%|         | 244/3250 [23:03<4:41:45,  5.62s/it]  8%|         | 245/3250 [23:08<4:38:41,  5.56s/it]                                                      8%|         | 245/3250 [23:08<4:38:41,  5.56s/it]  8%|         | 246/3250 [23:14<4:41:08,  5.62s/it]                                                      8%|         | 246/3250 [23:14<4:41:08,  5.62s/it]  8%|         | 247/3250 [23:19<4:38{'loss': 1.0777, 'learning_rate': 9.86026456251757e-05, 'epoch': 0.08}
{'loss': 1.0764, 'learning_rate': 9.859126934864281e-05, 'epoch': 0.08}
{'loss': 1.0979, 'learning_rate': 9.857984761247803e-05, 'epoch': 0.08}
{'loss': 1.4842, 'learning_rate': 9.856838042736699e-05, 'epoch': 0.08}
:20,  5.56s/it]                                                      8%|         | 247/3250 [23:19<4:38:20,  5.56s/it]  8%|         | 248/3250 [23:25<4:36:38,  5.53s/it]                                                      8%|         | 248/3250 [23:25<4:36:38,  5.53s/it]  8%|         | 249/3250 [23:30<4:35:13,  5.50s/it]                                                      8%|         | 249/3250 [23:30<4:35:13,  5.50s/it]  8%|         | 250/3250 [23:36<4:34:11,  5.48s/it]                                                      8%|         | 250/3250 [23:36<4:34:11,  5.48s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 1.0681405067443848, 'eval_runtime': 1.3963, 'eval_samples_per_second': 8.594, 'eval_steps_per_second': 2.149, 'epoch': 0.08}
                                                      8%|         | 250/3250 [23:37<4:34:11,  5.48s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_4/tmp-checkpoint-250
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in I AM HERE AND I AM SAVING THE MODEL
 the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-250
/projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-250
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-250
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-250
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-250/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-250/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-250/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-250/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 1.0671, 'learning_rate': 9.855686780403783e-05, 'epoch': 0.08}
{'loss': 1.1072, 'learning_rate': 9.854530975326117e-05, 'epoch': 0.08}
{'loss': 1.0968, 'learning_rate': 9.853370628585021e-05, 'epoch': 0.08}
{'loss': 1.073, 'learning_rate': 9.852205741266058e-05, 'epoch': 0.08}
{'loss': 1.0608, 'learning_rate': 9.851036314459037e-05, 'epoch': 0.08}
{'loss': 1.1323, 'learning_rate': 9.84986234925802e-05, 'epoch': 0.08}
  8%|         | 251/3250 [23:43<4:59:52,  6.00s/it]                                                      8%|         | 251/3250 [23:43<4:59:52,  6.00s/it]  8%|         | 252/3250 [23:48<4:51:06,  5.83s/it]                                                      8%|         | 252/3250 [23:48<4:51:06,  5.83s/it]  8%|         | 253/3250 [23:54<4:45:07,  5.71s/it]                                                      8%|         | 253/3250 [23:54<4:45:07,  5.71s/it]  8%|         | 254/3250 [23:59<4:40:54,  5.63s/it]                                                      8%|         | 254/3250 [23:59<4:40:54,  5.63s/it]  8%|         | 255/3250 [24:04<4:37:45,  5.56s/it]                                                      8%|         | 255/3250 [24:04<4:37:45,  5.56s/it]  8%|         | 256/3250 [24:10<4:35:21,  5.52s/it]                                                      8%|         | 256/3250 [24:10<4:35:21,  5.52s/it]  8%|         | 257/3250 [24:15<4:33{'loss': 1.1268, 'learning_rate': 9.848683846761311e-05, 'epoch': 0.08}
{'loss': 1.0689, 'learning_rate': 9.847500808071457e-05, 'epoch': 0.08}
{'loss': 1.0572, 'learning_rate': 9.846313234295256e-05, 'epoch': 0.08}
{'loss': 1.074, 'learning_rate': 9.845121126543742e-05, 'epoch': 0.08}
:51,  5.49s/it]                                                      8%|         | 257/3250 [24:15<4:33:51,  5.49s/it]  8%|         | 258/3250 [24:21<4:33:40,  5.49s/it]                                                      8%|         | 258/3250 [24:21<4:33:40,  5.49s/it]  8%|         | 259/3250 [24:26<4:32:55,  5.47s/it]                                                      8%|         | 259/3250 [24:26<4:32:55,  5.47s/it]  8%|         | 260/3250 [24:32<4:32:11,  5.46s/it]                                                      8%|         | 260/3250 [24:32<4:32:11,  5.46s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 1.0639020204544067, 'eval_runtime': 1.3883, 'eval_samples_per_second': 8.644, 'eval_steps_per_second': 2.161, 'epoch': 0.08}
                                                      8%|         | 260/3250 [24:33<4:32:11,  5.46s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_4/tmp-checkpoint-260
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  I AM HERE AND I AM SAVING THE MODEL/projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-260
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-260

I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-260
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-260
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-260/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-260/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-260/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-260/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 1.0629, 'learning_rate': 9.843924485932194e-05, 'epoch': 0.08}
{'loss': 1.0644, 'learning_rate': 9.842723313580128e-05, 'epoch': 0.08}
{'loss': 1.0621, 'learning_rate': 9.841517610611309e-05, 'epoch': 0.08}
{'loss': 1.0486, 'learning_rate': 9.840307378153726e-05, 'epoch': 0.08}
{'loss': 1.0625, 'learning_rate': 9.83909261733962e-05, 'epoch': 0.08}
{'loss': 1.0912, 'learning_rate': 9.837873329305459e-05, 'epoch': 0.08}
  8%|         | 261/3250 [24:39<4:57:13,  5.97s/it]                                                      8%|         | 261/3250 [24:39<4:57:13,  5.97s/it]  8%|         | 262/3250 [24:44<4:48:57,  5.80s/it]                                                      8%|         | 262/3250 [24:44<4:48:57,  5.80s/it]  8%|         | 263/3250 [24:50<4:52:36,  5.88s/it]                                                      8%|         | 263/3250 [24:50<4:52:36,  5.88s/it]  8%|         | 264/3250 [24:56<4:45:53,  5.74s/it]                                                      8%|         | 264/3250 [24:56<4:45:53,  5.74s/it]  8%|         | 265/3250 [25:01<4:40:55,  5.65s/it]                                                      8%|         | 265/3250 [25:01<4:40:55,  5.65s/it]  8%|         | 266/3250 [25:07<4:37:36,  5.58s/it]                                                      8%|         | 266/3250 [25:07<4:37:36,  5.58s/it]  8%|         | 267/3250 [25:12<4:35{'loss': 1.0529, 'learning_rate': 9.836649515191949e-05, 'epoch': 0.08}
{'loss': 1.0471, 'learning_rate': 9.835421176144035e-05, 'epoch': 0.08}
{'loss': 1.0947, 'learning_rate': 9.834188313310886e-05, 'epoch': 0.08}
{'loss': 1.0804, 'learning_rate': 9.832950927845914e-05, 'epoch': 0.08}
:06,  5.53s/it]                                                      8%|         | 267/3250 [25:12<4:35:06,  5.53s/it]  8%|         | 268/3250 [25:17<4:33:18,  5.50s/it]                                                      8%|         | 268/3250 [25:17<4:33:18,  5.50s/it]  8%|         | 269/3250 [25:23<4:33:21,  5.50s/it]                                                      8%|         | 269/3250 [25:23<4:33:21,  5.50s/it]  8%|         | 270/3250 [25:28<4:32:05,  5.48s/it]                                                      8%|         | 270/3250 [25:28<4:32:05,  5.48s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 1.0604974031448364, 'eval_runtime': 1.407, 'eval_samples_per_second': 8.529, 'eval_steps_per_second': 2.132, 'epoch': 0.08}
                                                      8%|         | 270/3250 [25:30<4:32:05,  5.48s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_4/tmp-checkpoint-270
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-270I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-270

I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-270
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-270
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-270/pytorch_model.bin
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-270/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-270/pytorch_model.binthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-270/pytorch_model.bin

/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 1.0601, 'learning_rate': 9.831709020906754e-05, 'epoch': 0.08}
{'loss': 1.1521, 'learning_rate': 9.830462593655274e-05, 'epoch': 0.08}
{'loss': 1.0833, 'learning_rate': 9.829211647257571e-05, 'epoch': 0.08}
{'loss': 1.0632, 'learning_rate': 9.82795618288397e-05, 'epoch': 0.08}
{'loss': 1.0695, 'learning_rate': 9.826696201709021e-05, 'epoch': 0.08}
{'loss': 1.0228, 'learning_rate': 9.825431704911503e-05, 'epoch': 0.08}
  8%|         | 271/3250 [25:35<4:56:44,  5.98s/it]                                                      8%|         | 271/3250 [25:35<4:56:44,  5.98s/it]  8%|         | 272/3250 [25:41<4:48:17,  5.81s/it]                                                      8%|         | 272/3250 [25:41<4:48:17,  5.81s/it]  8%|         | 273/3250 [25:46<4:42:11,  5.69s/it]                                                      8%|         | 273/3250 [25:46<4:42:11,  5.69s/it]  8%|         | 274/3250 [25:52<4:37:54,  5.60s/it]                                                      8%|         | 274/3250 [25:52<4:37:54,  5.60s/it]  8%|         | 275/3250 [25:57<4:35:05,  5.55s/it]                                                      8%|         | 275/3250 [25:57<4:35:05,  5.55s/it]  8%|         | 276/3250 [26:03<4:33:27,  5.52s/it]                                                      8%|         | 276/3250 [26:03<4:33:27,  5.52s/it]  9%|         | 277/3250 [26:08<4:32{'loss': 1.093, 'learning_rate': 9.824162693674417e-05, 'epoch': 0.09}
{'loss': 1.0322, 'learning_rate': 9.82288916918499e-05, 'epoch': 0.09}
{'loss': 1.1017, 'learning_rate': 9.821611132634666e-05, 'epoch': 0.09}
{'loss': 1.4743, 'learning_rate': 9.820328585219117e-05, 'epoch': 0.09}
:10,  5.49s/it]                                                      9%|         | 277/3250 [26:08<4:32:10,  5.49s/it]  9%|         | 278/3250 [26:13<4:31:06,  5.47s/it]                                                      9%|         | 278/3250 [26:13<4:31:06,  5.47s/it]  9%|         | 279/3250 [26:19<4:34:25,  5.54s/it]                                                      9%|         | 279/3250 [26:19<4:34:25,  5.54s/it]  9%|         | 280/3250 [26:25<4:32:30,  5.51s/it]                                                      9%|         | 280/3250 [26:25<4:32:30,  5.51s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 1.0570067167282104, 'eval_runtime': 1.4012, 'eval_samples_per_second': 8.564, 'eval_steps_per_second': 2.141, 'epoch': 0.09}
                                                      9%|         | 280/3250 [26:26<4:32:30,  5.51s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_4/tmp-checkpoint-280
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-280
/projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-280
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-280
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-280
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-280/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-280/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-280/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-280/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 1.0299, 'learning_rate': 9.819041528138231e-05, 'epoch': 0.09}
{'loss': 1.0767, 'learning_rate': 9.817749962596115e-05, 'epoch': 0.09}
{'loss': 1.1087, 'learning_rate': 9.816453889801098e-05, 'epoch': 0.09}
{'loss': 1.0616, 'learning_rate': 9.815153310965718e-05, 'epoch': 0.09}
{'loss': 1.0335, 'learning_rate': 9.81384822730674e-05, 'epoch': 0.09}
{'loss': 1.0677, 'learning_rate': 9.812538640045132e-05, 'epoch': 0.09}
  9%|         | 281/3250 [26:32<4:57:20,  6.01s/it]                                                      9%|         | 281/3250 [26:32<4:57:20,  6.01s/it]  9%|         | 282/3250 [26:37<4:48:32,  5.83s/it]                                                      9%|         | 282/3250 [26:37<4:48:32,  5.83s/it]  9%|         | 283/3250 [26:43<4:42:10,  5.71s/it]                                                      9%|         | 283/3250 [26:43<4:42:10,  5.71s/it]  9%|         | 284/3250 [26:48<4:37:47,  5.62s/it]                                                      9%|         | 284/3250 [26:48<4:37:47,  5.62s/it]  9%|         | 285/3250 [26:53<4:34:35,  5.56s/it]                                                      9%|         | 285/3250 [26:53<4:34:35,  5.56s/it]  9%|         | 286/3250 [26:59<4:32:26,  5.51s/it]                                                      9%|         | 286/3250 [26:59<4:32:26,  5.51s/it]  9%|         | 287/3250 [27:04<4:30{'loss': 1.1309, 'learning_rate': 9.811224550406082e-05, 'epoch': 0.09}
{'loss': 1.0745, 'learning_rate': 9.809905959618985e-05, 'epoch': 0.09}
{'loss': 1.0666, 'learning_rate': 9.808582868917458e-05, 'epoch': 0.09}
{'loss': 1.0167, 'learning_rate': 9.807255279539313e-05, 'epoch': 0.09}
:58,  5.49s/it]                                                      9%|         | 287/3250 [27:04<4:30:58,  5.49s/it]  9%|         | 288/3250 [27:10<4:29:56,  5.47s/it]                                                      9%|         | 288/3250 [27:10<4:29:56,  5.47s/it]  9%|         | 289/3250 [27:15<4:29:03,  5.45s/it]                                                      9%|         | 289/3250 [27:15<4:29:03,  5.45s/it]  9%|         | 290/3250 [27:20<4:28:25,  5.44s/it]                                                      9%|         | 290/3250 [27:20<4:28:25,  5.44s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 1.0523093938827515, 'eval_runtime': 1.3935, 'eval_samples_per_second': 8.611, 'eval_steps_per_second': 2.153, 'epoch': 0.09}
                                                      9%|         | 290/3250 [27:22<4:28:25,  5.44s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_4/tmp-checkpoint-290
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  I AM HERE AND I AM SAVING THE MODEL/projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-290
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-290

I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-290
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-290
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-290/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-290/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-290/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-290/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 1.0647, 'learning_rate': 9.805923192726581e-05, 'epoch': 0.09}
{'loss': 1.0676, 'learning_rate': 9.804586609725499e-05, 'epoch': 0.09}
{'loss': 1.0555, 'learning_rate': 9.803245531786506e-05, 'epoch': 0.09}
{'loss': 1.0298, 'learning_rate': 9.801899960164253e-05, 'epoch': 0.09}
{'loss': 1.0522, 'learning_rate': 9.800549896117589e-05, 'epoch': 0.09}
{'loss': 1.0581, 'learning_rate': 9.79919534090957e-05, 'epoch': 0.09}
  9%|         | 291/3250 [27:28<4:53:21,  5.95s/it]                                                      9%|         | 291/3250 [27:28<4:53:21,  5.95s/it]  9%|         | 292/3250 [27:33<4:45:30,  5.79s/it]                                                      9%|         | 292/3250 [27:33<4:45:30,  5.79s/it]  9%|         | 293/3250 [27:38<4:39:52,  5.68s/it]                                                      9%|         | 293/3250 [27:38<4:39:52,  5.68s/it]  9%|         | 294/3250 [27:44<4:35:53,  5.60s/it]                                                      9%|         | 294/3250 [27:44<4:35:53,  5.60s/it]  9%|         | 295/3250 [27:49<4:33:07,  5.55s/it]                                                      9%|         | 295/3250 [27:49<4:33:07,  5.55s/it]  9%|         | 296/3250 [27:55<4:40:35,  5.70s/it]                                                      9%|         | 296/3250 [27:55<4:40:35,  5.70s/it]  9%|         | 297/3250 [28:01<4:36{'loss': 1.0666, 'learning_rate': 9.79783629580745e-05, 'epoch': 0.09}
{'loss': 1.0465, 'learning_rate': 9.796472762082687e-05, 'epoch': 0.09}
{'loss': 1.0437, 'learning_rate': 9.795104741010938e-05, 'epoch': 0.09}
{'loss': 1.0787, 'learning_rate': 9.793732233872056e-05, 'epoch': 0.09}
:31,  5.62s/it]                                                      9%|         | 297/3250 [28:01<4:36:31,  5.62s/it]  9%|         | 298/3250 [28:06<4:33:32,  5.56s/it]                                                      9%|         | 298/3250 [28:06<4:33:32,  5.56s/it]  9%|         | 299/3250 [28:12<4:31:36,  5.52s/it]                                                      9%|         | 299/3250 [28:12<4:31:36,  5.52s/it]  9%|         | 300/3250 [28:17<4:30:08,  5.49s/it]                                                      9%|         | 300/3250 [28:17<4:30:08,  5.49s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 1.0469825267791748, 'eval_runtime': 1.3865, 'eval_samples_per_second': 8.655, 'eval_steps_per_second': 2.164, 'epoch': 0.09}
                                                      9%|         | 300/3250 [28:18<4:30:08,  5.49s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_4/tmp-checkpoint-300
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-300I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-300

I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-300
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-300
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-300/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-300/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-300/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-300/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 1.0502, 'learning_rate': 9.792355241950088e-05, 'epoch': 0.09}
{'loss': 1.0875, 'learning_rate': 9.790973766533288e-05, 'epoch': 0.09}
{'loss': 1.0914, 'learning_rate': 9.789587808914093e-05, 'epoch': 0.09}
{'loss': 1.0468, 'learning_rate': 9.78819737038914e-05, 'epoch': 0.09}
{'loss': 1.0261, 'learning_rate': 9.786802452259251e-05, 'epoch': 0.09}
{'loss': 1.0468, 'learning_rate': 9.785403055829449e-05, 'epoch': 0.09}
  9%|         | 301/3250 [28:24<4:54:08,  5.98s/it]                                                      9%|         | 301/3250 [28:24<4:54:08,  5.98s/it]  9%|         | 302/3250 [28:30<4:45:44,  5.82s/it]                                                      9%|         | 302/3250 [28:30<4:45:44,  5.82s/it]  9%|         | 303/3250 [28:35<4:40:07,  5.70s/it]                                                      9%|         | 303/3250 [28:35<4:40:07,  5.70s/it]  9%|         | 304/3250 [28:40<4:35:54,  5.62s/it]                                                      9%|         | 304/3250 [28:40<4:35:54,  5.62s/it]  9%|         | 305/3250 [28:46<4:32:49,  5.56s/it]                                                      9%|         | 305/3250 [28:46<4:32:49,  5.56s/it]  9%|         | 306/3250 [28:51<4:30:48,  5.52s/it]                                                      9%|         | 306/3250 [28:51<4:30:48,  5.52s/it]  9%|         | 307/3250 [28:57<4:29{'loss': 1.0479, 'learning_rate': 9.783999182408941e-05, 'epoch': 0.09}
{'loss': 1.0373, 'learning_rate': 9.78259083331112e-05, 'epoch': 0.09}
{'loss': 1.0897, 'learning_rate': 9.781178009853568e-05, 'epoch': 0.1}
{'loss': 1.5346, 'learning_rate': 9.779760713358059e-05, 'epoch': 0.1}
:13,  5.49s/it]                                                      9%|         | 307/3250 [28:57<4:29:13,  5.49s/it]  9%|         | 308/3250 [29:02<4:28:09,  5.47s/it]                                                      9%|         | 308/3250 [29:02<4:28:09,  5.47s/it] 10%|         | 309/3250 [29:08<4:27:39,  5.46s/it]                                                     10%|         | 309/3250 [29:08<4:27:39,  5.46s/it] 10%|         | 310/3250 [29:13<4:27:05,  5.45s/it]                                                     10%|         | 310/3250 [29:13<4:27:05,  5.45s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 1.0444278717041016, 'eval_runtime': 1.3895, 'eval_samples_per_second': 8.636, 'eval_steps_per_second': 2.159, 'epoch': 0.1}
                                                     10%|         | 310/3250 [29:14<4:27:05,  5.45s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_4/tmp-checkpoint-310
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-310I AM HERE AND I AM SAVING THE MODEL

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-310
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-310
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-310
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-310/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-310/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-310/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-310/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.9689, 'learning_rate': 9.778338945150542e-05, 'epoch': 0.1}
{'loss': 1.0341, 'learning_rate': 9.776912706561156e-05, 'epoch': 0.1}
{'loss': 1.0839, 'learning_rate': 9.775481998924222e-05, 'epoch': 0.1}
{'loss': 1.0498, 'learning_rate': 9.77404682357824e-05, 'epoch': 0.1}
{'loss': 1.0394, 'learning_rate': 9.77260718186589e-05, 'epoch': 0.1}
{'loss': 1.0414, 'learning_rate': 9.771163075134029e-05, 'epoch': 0.1}
 10%|         | 311/3250 [29:20<4:52:13,  5.97s/it]                                                     10%|         | 311/3250 [29:20<4:52:13,  5.97s/it] 10%|         | 312/3250 [29:26<4:47:55,  5.88s/it]                                                     10%|         | 312/3250 [29:26<4:47:55,  5.88s/it] 10%|         | 313/3250 [29:31<4:40:50,  5.74s/it]                                                     10%|         | 313/3250 [29:31<4:40:50,  5.74s/it] 10%|         | 314/3250 [29:37<4:36:15,  5.65s/it]                                                     10%|         | 314/3250 [29:37<4:36:15,  5.65s/it] 10%|         | 315/3250 [29:42<4:32:59,  5.58s/it]                                                     10%|         | 315/3250 [29:42<4:32:59,  5.58s/it] 10%|         | 316/3250 [29:48<4:30:39,  5.53s/it]                                                     10%|         | 316/3250 [29:48<4:30:39,  5.53s/it] 10%|         | 317/3250 [29:53<4:28{'loss': 1.0814, 'learning_rate': 9.769714504733694e-05, 'epoch': 0.1}
{'loss': 1.0905, 'learning_rate': 9.768261472020099e-05, 'epoch': 0.1}
{'loss': 1.0287, 'learning_rate': 9.76680397835263e-05, 'epoch': 0.1}
{'loss': 0.98, 'learning_rate': 9.765342025094847e-05, 'epoch': 0.1}
:46,  5.50s/it]                                                     10%|         | 317/3250 [29:53<4:28:46,  5.50s/it] 10%|         | 318/3250 [29:58<4:27:35,  5.48s/it]                                                     10%|         | 318/3250 [29:58<4:27:35,  5.48s/it] 10%|         | 319/3250 [30:04<4:26:54,  5.46s/it]                                                     10%|         | 319/3250 [30:04<4:26:54,  5.46s/it] 10%|         | 320/3250 [30:09<4:26:11,  5.45s/it]                                                     10%|         | 320/3250 [30:09<4:26:11,  5.45s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 1.0407397747039795, 'eval_runtime': 1.392, 'eval_samples_per_second': 8.621, 'eval_steps_per_second': 2.155, 'epoch': 0.1}
                                                     10%|         | 320/3250 [30:11<4:26:11,  5.45s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_4/tmp-checkpoint-320
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in   /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-320
/projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-320
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-320
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-320
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-320/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-320/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-320/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-320/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 1.0698, 'learning_rate': 9.763875613614482e-05, 'epoch': 0.1}
{'loss': 1.0358, 'learning_rate': 9.762404745283439e-05, 'epoch': 0.1}
{'loss': 1.046, 'learning_rate': 9.760929421477791e-05, 'epoch': 0.1}
{'loss': 1.023, 'learning_rate': 9.759449643577778e-05, 'epoch': 0.1}
{'loss': 1.0256, 'learning_rate': 9.757965412967811e-05, 'epoch': 0.1}
{'loss': 1.0381, 'learning_rate': 9.756476731036461e-05, 'epoch': 0.1}
 10%|         | 321/3250 [30:16<4:49:56,  5.94s/it]                                                     10%|         | 321/3250 [30:16<4:49:56,  5.94s/it] 10%|         | 322/3250 [30:22<4:42:16,  5.78s/it]                                                     10%|         | 322/3250 [30:22<4:42:16,  5.78s/it] 10%|         | 323/3250 [30:27<4:36:47,  5.67s/it]                                                     10%|         | 323/3250 [30:27<4:36:47,  5.67s/it] 10%|         | 324/3250 [30:33<4:33:10,  5.60s/it]                                                     10%|         | 324/3250 [30:33<4:33:10,  5.60s/it] 10%|         | 325/3250 [30:38<4:30:20,  5.55s/it]                                                     10%|         | 325/3250 [30:38<4:30:20,  5.55s/it] 10%|         | 326/3250 [30:43<4:28:30,  5.51s/it]                                                     10%|         | 326/3250 [30:43<4:28:30,  5.51s/it] 10%|         | 327/3250 [30:49<4:26{'loss': 1.0371, 'learning_rate': 9.75498359917647e-05, 'epoch': 0.1}
{'loss': 1.0452, 'learning_rate': 9.753486018784736e-05, 'epoch': 0.1}
{'loss': 1.0504, 'learning_rate': 9.751983991262326e-05, 'epoch': 0.1}
{'loss': 1.0268, 'learning_rate': 9.750477518014461e-05, 'epoch': 0.1}
:56,  5.48s/it]                                                     10%|         | 327/3250 [30:49<4:26:56,  5.48s/it] 10%|         | 328/3250 [30:55<4:35:30,  5.66s/it]                                                     10%|         | 328/3250 [30:55<4:35:30,  5.66s/it] 10%|         | 329/3250 [31:00<4:31:59,  5.59s/it]                                                     10%|         | 329/3250 [31:00<4:31:59,  5.59s/it] 10%|         | 330/3250 [31:06<4:29:36,  5.54s/it]                                                     10%|         | 330/3250 [31:06<4:29:36,  5.54s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 1.0365164279937744, 'eval_runtime': 1.3874, 'eval_samples_per_second': 8.649, 'eval_steps_per_second': 2.162, 'epoch': 0.1}
                                                     10%|         | 330/3250 [31:07<4:29:36,  5.54s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_4/tmp-checkpoint-330
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-330I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-330

I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-330
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-330
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-330/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-330/pytorch_model.bin
the pytorch model path is the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-330/pytorch_model.bin
/projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-330/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 1.0632, 'learning_rate': 9.748966600450525e-05, 'epoch': 0.1}
{'loss': 1.0306, 'learning_rate': 9.74745123998406e-05, 'epoch': 0.1}
{'loss': 1.1121, 'learning_rate': 9.745931438032763e-05, 'epoch': 0.1}
{'loss': 1.0127, 'learning_rate': 9.744407196018488e-05, 'epoch': 0.1}
{'loss': 1.0393, 'learning_rate': 9.74287851536724e-05, 'epoch': 0.1}
{'loss': 1.0406, 'learning_rate': 9.74134539750918e-05, 'epoch': 0.1}
 10%|         | 331/3250 [31:13<4:52:21,  6.01s/it]                                                     10%|         | 331/3250 [31:13<4:52:21,  6.01s/it] 10%|         | 332/3250 [31:18<4:43:43,  5.83s/it]                                                     10%|         | 332/3250 [31:18<4:43:43,  5.83s/it] 10%|         | 333/3250 [31:24<4:37:26,  5.71s/it]                                                     10%|         | 333/3250 [31:24<4:37:26,  5.71s/it] 10%|         | 334/3250 [31:29<4:33:10,  5.62s/it]                                                     10%|         | 334/3250 [31:29<4:33:10,  5.62s/it] 10%|         | 335/3250 [31:35<4:30:21,  5.56s/it]                                                     10%|         | 335/3250 [31:35<4:30:21,  5.56s/it] 10%|         | 336/3250 [31:40<4:28:14,  5.52s/it]                                                     10%|         | 336/3250 [31:40<4:28:14,  5.52s/it] 10%|         | 337/3250 [31:45<4:27{'loss': 1.0304, 'learning_rate': 9.739807843878617e-05, 'epoch': 0.1}
{'loss': 1.0267, 'learning_rate': 9.738265855914013e-05, 'epoch': 0.1}
{'loss': 1.0384, 'learning_rate': 9.736719435057976e-05, 'epoch': 0.1}
{'loss': 1.0595, 'learning_rate': 9.735168582757264e-05, 'epoch': 0.1}
:04,  5.50s/it]                                                     10%|         | 337/3250 [31:45<4:27:04,  5.50s/it] 10%|         | 338/3250 [31:51<4:26:09,  5.48s/it]                                                     10%|         | 338/3250 [31:51<4:26:09,  5.48s/it] 10%|         | 339/3250 [31:56<4:25:30,  5.47s/it]                                                     10%|         | 339/3250 [31:56<4:25:30,  5.47s/it] 10%|         | 340/3250 [32:02<4:24:46,  5.46s/it]                                                     10%|         | 340/3250 [32:02<4:24:46,  5.46s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 1.032064437866211, 'eval_runtime': 1.3876, 'eval_samples_per_second': 8.648, 'eval_steps_per_second': 2.162, 'epoch': 0.1}
                                                     10%|         | 340/3250 [32:03<4:24:46,  5.46s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_4/tmp-checkpoint-340
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-340
/projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-340
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-340
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-340
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-340/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
the pytorch model path isthe pytorch model path is  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-340/pytorch_model.bin/projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-340/pytorch_model.bin

the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-340/pytorch_model.bin
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 1.4432, 'learning_rate': 9.733613300462776e-05, 'epoch': 0.1}
{'loss': 1.0247, 'learning_rate': 9.732053589629561e-05, 'epoch': 0.11}
{'loss': 1.0661, 'learning_rate': 9.730489451716809e-05, 'epoch': 0.11}
{'loss': 1.0419, 'learning_rate': 9.728920888187849e-05, 'epoch': 0.11}
{'loss': 1.0352, 'learning_rate': 9.727347900510155e-05, 'epoch': 0.11}
{'loss': 1.0145, 'learning_rate': 9.725770490155338e-05, 'epoch': 0.11}
 10%|         | 341/3250 [32:09<4:48:09,  5.94s/it]                                                     10%|         | 341/3250 [32:09<4:48:09,  5.94s/it] 11%|         | 342/3250 [32:14<4:40:39,  5.79s/it]                                                     11%|         | 342/3250 [32:14<4:40:39,  5.79s/it] 11%|         | 343/3250 [32:20<4:35:30,  5.69s/it]                                                     11%|         | 343/3250 [32:20<4:35:30,  5.69s/it] 11%|         | 344/3250 [32:25<4:31:30,  5.61s/it]                                                     11%|         | 344/3250 [32:25<4:31:30,  5.61s/it] 11%|         | 345/3250 [32:31<4:32:15,  5.62s/it]                                                     11%|         | 345/3250 [32:31<4:32:15,  5.62s/it] 11%|         | 346/3250 [32:36<4:29:20,  5.56s/it]                                                     11%|         | 346/3250 [32:36<4:29:20,  5.56s/it] 11%|         | 347/3250 [32:42<4:27{'loss': 1.0913, 'learning_rate': 9.724188658599146e-05, 'epoch': 0.11}
{'loss': 1.0701, 'learning_rate': 9.722602407321463e-05, 'epoch': 0.11}
{'loss': 1.0231, 'learning_rate': 9.721011737806309e-05, 'epoch': 0.11}
{'loss': 1.0098, 'learning_rate': 9.719416651541839e-05, 'epoch': 0.11}
:05,  5.52s/it]                                                     11%|         | 347/3250 [32:42<4:27:05,  5.52s/it] 11%|         | 348/3250 [32:47<4:25:41,  5.49s/it]                                                     11%|         | 348/3250 [32:47<4:25:41,  5.49s/it] 11%|         | 349/3250 [32:53<4:24:35,  5.47s/it]                                                     11%|         | 349/3250 [32:53<4:24:35,  5.47s/it] 11%|         | 350/3250 [32:58<4:23:28,  5.45s/it]                                                     11%|         | 350/3250 [32:58<4:23:28,  5.45s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 1.0285683870315552, 'eval_runtime': 1.3965, 'eval_samples_per_second': 8.593, 'eval_steps_per_second': 2.148, 'epoch': 0.11}
                                                     11%|         | 350/3250 [32:59<4:23:28,  5.45s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_4/tmp-checkpoint-350
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  I AM HERE AND I AM SAVING THE MODEL/projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-350
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-350

I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-350
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-350
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-350/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-350/pytorch_model.bin
the pytorch model path is the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-350/pytorch_model.bin
/projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-350/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 1.0182, 'learning_rate': 9.717817150020336e-05, 'epoch': 0.11}
{'loss': 1.0155, 'learning_rate': 9.716213234738215e-05, 'epoch': 0.11}
{'loss': 1.0206, 'learning_rate': 9.714604907196025e-05, 'epoch': 0.11}
{'loss': 1.0216, 'learning_rate': 9.712992168898436e-05, 'epoch': 0.11}
{'loss': 1.0204, 'learning_rate': 9.711375021354248e-05, 'epoch': 0.11}
{'loss': 1.0125, 'learning_rate': 9.709753466076387e-05, 'epoch': 0.11}
 11%|         | 351/3250 [33:05<4:47:49,  5.96s/it]                                                     11%|         | 351/3250 [33:05<4:47:49,  5.96s/it] 11%|         | 352/3250 [33:10<4:39:35,  5.79s/it]                                                     11%|         | 352/3250 [33:10<4:39:35,  5.79s/it] 11%|         | 353/3250 [33:16<4:33:50,  5.67s/it]                                                     11%|         | 353/3250 [33:16<4:33:50,  5.67s/it] 11%|         | 354/3250 [33:21<4:30:41,  5.61s/it]                                                     11%|         | 354/3250 [33:21<4:30:41,  5.61s/it] 11%|         | 355/3250 [33:27<4:27:43,  5.55s/it]                                                     11%|         | 355/3250 [33:27<4:27:43,  5.55s/it] 11%|         | 356/3250 [33:32<4:25:38,  5.51s/it]                                                     11%|         | 356/3250 [33:32<4:25:38,  5.51s/it] 11%|         | 357/3250 [33:38<4:23{'loss': 1.056, 'learning_rate': 9.708127504581902e-05, 'epoch': 0.11}
{'loss': 0.9848, 'learning_rate': 9.706497138391961e-05, 'epoch': 0.11}
{'loss': 1.0115, 'learning_rate': 9.704862369031857e-05, 'epoch': 0.11}
{'loss': 1.0481, 'learning_rate': 9.703223198031002e-05, 'epoch': 0.11}
:58,  5.47s/it]                                                     11%|         | 357/3250 [33:38<4:23:58,  5.47s/it] 11%|         | 358/3250 [33:43<4:22:54,  5.45s/it]                                                     11%|         | 358/3250 [33:43<4:22:54,  5.45s/it] 11%|         | 359/3250 [33:48<4:22:11,  5.44s/it]                                                     11%|         | 359/3250 [33:48<4:22:11,  5.44s/it] 11%|         | 360/3250 [33:54<4:21:45,  5.43s/it]                                                     11%|         | 360/3250 [33:54<4:21:45,  5.43s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 1.0267997980117798, 'eval_runtime': 1.3891, 'eval_samples_per_second': 8.639, 'eval_steps_per_second': 2.16, 'epoch': 0.11}
                                                     11%|         | 360/3250 [33:55<4:21:45,  5.43s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_4/tmp-checkpoint-360
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-360
/projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-360
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-360
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-360
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-360/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-360/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-360/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-360/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 1.0245, 'learning_rate': 9.701579626922922e-05, 'epoch': 0.11}
{'loss': 1.0056, 'learning_rate': 9.699931657245263e-05, 'epoch': 0.11}
{'loss': 1.0966, 'learning_rate': 9.698279290539788e-05, 'epoch': 0.11}
{'loss': 1.0348, 'learning_rate': 9.696622528352368e-05, 'epoch': 0.11}
{'loss': 1.0119, 'learning_rate': 9.694961372232991e-05, 'epoch': 0.11}
{'loss': 1.0204, 'learning_rate': 9.693295823735753e-05, 'epoch': 0.11}
 11%|         | 361/3250 [34:01<4:54:36,  6.12s/it]                                                     11%|         | 361/3250 [34:01<4:54:36,  6.12s/it] 11%|         | 362/3250 [34:07<4:44:15,  5.91s/it]                                                     11%|         | 362/3250 [34:07<4:44:15,  5.91s/it] 11%|         | 363/3250 [34:12<4:37:06,  5.76s/it]                                                     11%|         | 363/3250 [34:12<4:37:06,  5.76s/it] 11%|         | 364/3250 [34:18<4:32:05,  5.66s/it]                                                     11%|         | 364/3250 [34:18<4:32:05,  5.66s/it] 11%|         | 365/3250 [34:23<4:28:23,  5.58s/it]                                                     11%|         | 365/3250 [34:23<4:28:23,  5.58s/it] 11%|        | 366/3250 [34:29<4:25:55,  5.53s/it]                                                     11%|        | 366/3250 [34:29<4:25:55,  5.53s/it] 11%|        | 367/3250 [34:3{'loss': 0.9802, 'learning_rate': 9.69162588441886e-05, 'epoch': 0.11}
{'loss': 1.0492, 'learning_rate': 9.689951555844628e-05, 'epoch': 0.11}
{'loss': 0.9879, 'learning_rate': 9.688272839579477e-05, 'epoch': 0.11}
{'loss': 1.0554, 'learning_rate': 9.686589737193929e-05, 'epoch': 0.11}
4<4:24:20,  5.50s/it]                                                     11%|        | 367/3250 [34:34<4:24:20,  5.50s/it] 11%|        | 368/3250 [34:39<4:23:08,  5.48s/it]                                                     11%|        | 368/3250 [34:39<4:23:08,  5.48s/it] 11%|        | 369/3250 [34:45<4:22:10,  5.46s/it]                                                     11%|        | 369/3250 [34:45<4:22:10,  5.46s/it] 11%|        | 370/3250 [34:50<4:21:39,  5.45s/it]                                                     11%|        | 370/3250 [34:50<4:21:39,  5.45s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 1.0223573446273804, 'eval_runtime': 1.399, 'eval_samples_per_second': 8.577, 'eval_steps_per_second': 2.144, 'epoch': 0.11}
                                                     11%|        | 370/3250 [34:52<4:21:39,  5.45s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_4/tmp-checkpoint-370
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-370
 /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-370
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-370
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-370
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-370/pytorch_model.bin
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-370/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-370/pytorch_model.binthe pytorch model path is
 /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-370/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 1.4437, 'learning_rate': 9.684902250262618e-05, 'epoch': 0.11}
{'loss': 0.9864, 'learning_rate': 9.683210380364272e-05, 'epoch': 0.11}
{'loss': 1.0259, 'learning_rate': 9.681514129081724e-05, 'epoch': 0.11}
{'loss': 1.0595, 'learning_rate': 9.6798134980019e-05, 'epoch': 0.12}
{'loss': 1.0175, 'learning_rate': 9.678108488715833e-05, 'epoch': 0.12}
{'loss': 0.9779, 'learning_rate': 9.676399102818646e-05, 'epoch': 0.12}
 11%|        | 371/3250 [34:57<4:46:08,  5.96s/it]                                                     11%|        | 371/3250 [34:57<4:46:08,  5.96s/it] 11%|        | 372/3250 [35:03<4:38:16,  5.80s/it]                                                     11%|        | 372/3250 [35:03<4:38:16,  5.80s/it] 11%|        | 373/3250 [35:08<4:32:38,  5.69s/it]                                                     11%|        | 373/3250 [35:08<4:32:38,  5.69s/it] 12%|        | 374/3250 [35:14<4:28:39,  5.60s/it]                                                     12%|        | 374/3250 [35:14<4:28:39,  5.60s/it] 12%|        | 375/3250 [35:19<4:26:31,  5.56s/it]                                                     12%|        | 375/3250 [35:19<4:26:31,  5.56s/it] 12%|        | 376/3250 [35:25<4:24:18,  5.52s/it]                                                     12%|        | 376/3250 [35:25<4:24:18,  5.52s/it] 12%|    {'loss': 1.0163, 'learning_rate': 9.674685341909552e-05, 'epoch': 0.12}
{'loss': 1.088, 'learning_rate': 9.67296720759187e-05, 'epoch': 0.12}
{'loss': 1.019, 'learning_rate': 9.671244701472999e-05, 'epoch': 0.12}
{'loss': 1.0188, 'learning_rate': 9.669517825164434e-05, 'epoch': 0.12}
    | 377/3250 [35:30<4:22:51,  5.49s/it]                                                     12%|        | 377/3250 [35:30<4:22:51,  5.49s/it] 12%|        | 378/3250 [35:36<4:25:02,  5.54s/it]                                                     12%|        | 378/3250 [35:36<4:25:02,  5.54s/it] 12%|        | 379/3250 [35:41<4:23:14,  5.50s/it]                                                     12%|        | 379/3250 [35:41<4:23:14,  5.50s/it] 12%|        | 380/3250 [35:46<4:21:57,  5.48s/it]                                                     12%|        | 380/3250 [35:46<4:21:57,  5.48s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 1.021392822265625, 'eval_runtime': 1.3957, 'eval_samples_per_second': 8.598, 'eval_steps_per_second': 2.15, 'epoch': 0.12}
                                                     12%|        | 380/3250 [35:48<4:21:57,  5.48s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_4/tmp-checkpoint-380
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-380I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-380

I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-380
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-380
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-380/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-380/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-380/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-380/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.9634, 'learning_rate': 9.667786580281755e-05, 'epoch': 0.12}
{'loss': 1.0139, 'learning_rate': 9.66605096844463e-05, 'epoch': 0.12}
{'loss': 1.0171, 'learning_rate': 9.664310991276815e-05, 'epoch': 0.12}
{'loss': 1.0112, 'learning_rate': 9.662566650406146e-05, 'epoch': 0.12}
{'loss': 0.9752, 'learning_rate': 9.660817947464547e-05, 'epoch': 0.12}
{'loss': 1.0078, 'learning_rate': 9.659064884088016e-05, 'epoch': 0.12}
 12%|        | 381/3250 [35:54<4:46:50,  6.00s/it]                                                     12%|        | 381/3250 [35:54<4:46:50,  6.00s/it] 12%|        | 382/3250 [35:59<4:38:25,  5.82s/it]                                                     12%|        | 382/3250 [35:59<4:38:25,  5.82s/it] 12%|        | 383/3250 [36:04<4:32:24,  5.70s/it]                                                     12%|        | 383/3250 [36:04<4:32:24,  5.70s/it] 12%|        | 384/3250 [36:10<4:27:58,  5.61s/it]                                                     12%|        | 384/3250 [36:10<4:27:58,  5.61s/it] 12%|        | 385/3250 [36:15<4:25:00,  5.55s/it]                                                     12%|        | 385/3250 [36:15<4:25:00,  5.55s/it] 12%|        | 386/3250 [36:21<4:23:28,  5.52s/it]                                                     12%|        | 386/3250 [36:21<4:23:28,  5.52s/it] 12%|    {'loss': 1.0135, 'learning_rate': 9.657307461916635e-05, 'epoch': 0.12}
{'loss': 1.0239, 'learning_rate': 9.655545682594566e-05, 'epoch': 0.12}
{'loss': 1.002, 'learning_rate': 9.65377954777004e-05, 'epoch': 0.12}
{'loss': 1.0027, 'learning_rate': 9.652009059095369e-05, 'epoch': 0.12}
    | 387/3250 [36:26<4:21:57,  5.49s/it]                                                     12%|        | 387/3250 [36:26<4:21:57,  5.49s/it] 12%|        | 388/3250 [36:32<4:20:51,  5.47s/it]                                                     12%|        | 388/3250 [36:32<4:20:51,  5.47s/it] 12%|        | 389/3250 [36:37<4:19:49,  5.45s/it]                                                     12%|        | 389/3250 [36:37<4:19:49,  5.45s/it] 12%|        | 390/3250 [36:42<4:19:21,  5.44s/it]                                                     12%|        | 390/3250 [36:42<4:19:21,  5.44s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 1.0162487030029297, 'eval_runtime': 1.3844, 'eval_samples_per_second': 8.668, 'eval_steps_per_second': 2.167, 'epoch': 0.12}
                                                     12%|        | 390/3250 [36:44<4:19:21,  5.44s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_4/tmp-checkpoint-390
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  I AM HERE AND I AM SAVING THE MODEL
/projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-390the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-390

I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-390
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-390
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-390/pytorch_model.binthe pytorch model path is
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-390/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-390/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-390/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 1.034, 'learning_rate': 9.650234218226934e-05, 'epoch': 0.12}
{'loss': 1.0067, 'learning_rate': 9.648455026825194e-05, 'epoch': 0.12}
{'loss': 1.0422, 'learning_rate': 9.64667148655467e-05, 'epoch': 0.12}
{'loss': 1.0473, 'learning_rate': 9.644883599083958e-05, 'epoch': 0.12}
{'loss': 1.0022, 'learning_rate': 9.643091366085717e-05, 'epoch': 0.12}
{'loss': 0.9825, 'learning_rate': 9.641294789236676e-05, 'epoch': 0.12}
 12%|        | 391/3250 [36:49<4:42:34,  5.93s/it]                                                     12%|        | 391/3250 [36:49<4:42:34,  5.93s/it] 12%|        | 392/3250 [36:55<4:35:03,  5.77s/it]                                                     12%|        | 392/3250 [36:55<4:35:03,  5.77s/it] 12%|        | 393/3250 [37:00<4:29:50,  5.67s/it]                                                     12%|        | 393/3250 [37:00<4:29:50,  5.67s/it] 12%|        | 394/3250 [37:06<4:35:21,  5.78s/it]                                                     12%|        | 394/3250 [37:06<4:35:21,  5.78s/it] 12%|        | 395/3250 [37:12<4:30:05,  5.68s/it]                                                     12%|        | 395/3250 [37:12<4:30:05,  5.68s/it] 12%|        | 396/3250 [37:17<4:26:15,  5.60s/it]                                                     12%|        | 396/3250 [37:17<4:26:15,  5.60s/it] 12%|    {'loss': 0.9923, 'learning_rate': 9.639493870217622e-05, 'epoch': 0.12}
{'loss': 1.0201, 'learning_rate': 9.637688610713409e-05, 'epoch': 0.12}
{'loss': 0.9875, 'learning_rate': 9.635879012412951e-05, 'epoch': 0.12}
{'loss': 1.0328, 'learning_rate': 9.634065077009218e-05, 'epoch': 0.12}
    | 397/3250 [37:23<4:25:06,  5.58s/it]                                                     12%|        | 397/3250 [37:23<4:25:06,  5.58s/it] 12%|        | 398/3250 [37:28<4:22:54,  5.53s/it]                                                     12%|        | 398/3250 [37:28<4:22:54,  5.53s/it] 12%|        | 399/3250 [37:34<4:21:26,  5.50s/it]                                                     12%|        | 399/3250 [37:34<4:21:26,  5.50s/it] 12%|        | 400/3250 [37:39<4:20:16,  5.48s/it]                                                     12%|        | 400/3250 [37:39<4:20:16,  5.48s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 1.0137834548950195, 'eval_runtime': 1.3869, 'eval_samples_per_second': 8.652, 'eval_steps_per_second': 2.163, 'epoch': 0.12}
                                                     12%|        | 400/3250 [37:40<4:20:16,  5.48s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_4/tmp-checkpoint-400
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in   /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-400
/projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-400
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-400
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-400
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-400/pytorch_model.binthe pytorch model path is
 /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-400/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-400/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-400/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 1.4945, 'learning_rate': 9.632246806199241e-05, 'epoch': 0.12}
{'loss': 0.9342, 'learning_rate': 9.630424201684105e-05, 'epoch': 0.12}
{'loss': 0.9721, 'learning_rate': 9.628597265168953e-05, 'epoch': 0.12}
{'loss': 1.0374, 'learning_rate': 9.626765998362974e-05, 'epoch': 0.12}
{'loss': 1.0059, 'learning_rate': 9.624930402979416e-05, 'epoch': 0.12}
{'loss': 0.9908, 'learning_rate': 9.62309048073557e-05, 'epoch': 0.12}
 12%|        | 401/3250 [37:46<4:43:31,  5.97s/it]                                                     12%|        | 401/3250 [37:46<4:43:31,  5.97s/it] 12%|        | 402/3250 [37:52<4:35:38,  5.81s/it]                                                     12%|        | 402/3250 [37:52<4:35:38,  5.81s/it] 12%|        | 403/3250 [37:57<4:29:43,  5.68s/it]                                                     12%|        | 403/3250 [37:57<4:29:43,  5.68s/it] 12%|        | 404/3250 [38:02<4:25:44,  5.60s/it]                                                     12%|        | 404/3250 [38:02<4:25:44,  5.60s/it] 12%|        | 405/3250 [38:08<4:23:01,  5.55s/it]                                                     12%|        | 405/3250 [38:08<4:23:01,  5.55s/it] 12%|        | 406/3250 [38:13<4:21:02,  5.51s/it]                                                     12%|        | 406/3250 [38:13<4:21:02,  5.51s/it] 13%|    {'loss': 0.9853, 'learning_rate': 9.62124623335278e-05, 'epoch': 0.13}
{'loss': 1.0459, 'learning_rate': 9.619397662556435e-05, 'epoch': 0.13}
{'loss': 1.033, 'learning_rate': 9.617544770075965e-05, 'epoch': 0.13}
{'loss': 0.9907, 'learning_rate': 9.615687557644848e-05, 'epoch': 0.13}
    | 407/3250 [38:19<4:20:12,  5.49s/it]                                                     13%|        | 407/3250 [38:19<4:20:12,  5.49s/it] 13%|        | 408/3250 [38:24<4:19:28,  5.48s/it]                                                     13%|        | 408/3250 [38:24<4:19:28,  5.48s/it] 13%|        | 409/3250 [38:30<4:18:25,  5.46s/it]                                                     13%|        | 409/3250 [38:30<4:18:25,  5.46s/it] 13%|        | 410/3250 [38:35<4:23:06,  5.56s/it]                                                     13%|        | 410/3250 [38:35<4:23:06,  5.56s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 1.012376308441162, 'eval_runtime': 1.396, 'eval_samples_per_second': 8.596, 'eval_steps_per_second': 2.149, 'epoch': 0.13}
                                                     13%|        | 410/3250 [38:37<4:23:06,  5.56s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_4/tmp-checkpoint-410
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-410I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-410

I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-410
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-410
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-410/pytorch_model.bin
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-410/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-410/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-410/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.9419, 'learning_rate': 9.613826027000601e-05, 'epoch': 0.13}
{'loss': 1.0259, 'learning_rate': 9.611960179884783e-05, 'epoch': 0.13}
{'loss': 0.9955, 'learning_rate': 9.61009001804299e-05, 'epoch': 0.13}
{'loss': 1.0018, 'learning_rate': 9.608215543224853e-05, 'epoch': 0.13}
{'loss': 0.9891, 'learning_rate': 9.60633675718404e-05, 'epoch': 0.13}
{'loss': 0.9925, 'learning_rate': 9.604453661678253e-05, 'epoch': 0.13}
 13%|        | 411/3250 [38:42<4:44:46,  6.02s/it]                                                     13%|        | 411/3250 [38:42<4:44:46,  6.02s/it] 13%|        | 412/3250 [38:48<4:36:23,  5.84s/it]                                                     13%|        | 412/3250 [38:48<4:36:23,  5.84s/it] 13%|        | 413/3250 [38:53<4:30:16,  5.72s/it]                                                     13%|        | 413/3250 [38:53<4:30:16,  5.72s/it] 13%|        | 414/3250 [38:59<4:26:10,  5.63s/it]                                                     13%|        | 414/3250 [38:59<4:26:10,  5.63s/it] 13%|        | 415/3250 [39:04<4:23:11,  5.57s/it]                                                     13%|        | 415/3250 [39:04<4:23:11,  5.57s/it] 13%|        | 416/3250 [39:10<4:21:07,  5.53s/it]                                                     13%|        | 416/3250 [39:10<4:21:07,  5.53s/it] 13%|    {'loss': 0.9865, 'learning_rate': 9.602566258469225e-05, 'epoch': 0.13}
{'loss': 0.9904, 'learning_rate': 9.600674549322717e-05, 'epoch': 0.13}
{'loss': 0.9919, 'learning_rate': 9.598778536008522e-05, 'epoch': 0.13}
{'loss': 1.0092, 'learning_rate': 9.596878220300454e-05, 'epoch': 0.13}
    | 417/3250 [39:15<4:19:32,  5.50s/it]                                                     13%|        | 417/3250 [39:15<4:19:32,  5.50s/it] 13%|        | 418/3250 [39:20<4:18:13,  5.47s/it]                                                     13%|        | 418/3250 [39:20<4:18:13,  5.47s/it] 13%|        | 419/3250 [39:26<4:17:18,  5.45s/it]                                                     13%|        | 419/3250 [39:26<4:17:18,  5.45s/it] 13%|        | 420/3250 [39:31<4:16:49,  5.45s/it]                                                     13%|        | 420/3250 [39:31<4:16:49,  5.45s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 1.0086334943771362, 'eval_runtime': 1.3872, 'eval_samples_per_second': 8.651, 'eval_steps_per_second': 2.163, 'epoch': 0.13}
                                                     13%|        | 420/3250 [39:33<4:16:49,  5.45s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_4/tmp-checkpoint-420
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  I AM HERE AND I AM SAVING THE MODEL
/projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-420the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-420

I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-420
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-420
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-420/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-420/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-420/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-420/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.9847, 'learning_rate': 9.594973603976363e-05, 'epoch': 0.13}
{'loss': 1.0294, 'learning_rate': 9.59306468881811e-05, 'epoch': 0.13}
{'loss': 0.9758, 'learning_rate': 9.591151476611584e-05, 'epoch': 0.13}
{'loss': 1.0709, 'learning_rate': 9.589233969146695e-05, 'epoch': 0.13}
{'loss': 0.9697, 'learning_rate': 9.58731216821737e-05, 'epoch': 0.13}
{'loss': 0.9859, 'learning_rate': 9.585386075621554e-05, 'epoch': 0.13}
 13%|        | 421/3250 [39:38<4:40:04,  5.94s/it]                                                     13%|        | 421/3250 [39:38<4:40:04,  5.94s/it] 13%|        | 422/3250 [39:44<4:32:50,  5.79s/it]                                                     13%|        | 422/3250 [39:44<4:32:50,  5.79s/it] 13%|        | 423/3250 [39:49<4:27:34,  5.68s/it]                                                     13%|        | 423/3250 [39:49<4:27:34,  5.68s/it] 13%|        | 424/3250 [39:55<4:23:56,  5.60s/it]                                                     13%|        | 424/3250 [39:55<4:23:56,  5.60s/it] 13%|        | 425/3250 [40:00<4:21:16,  5.55s/it]                                                     13%|        | 425/3250 [40:00<4:21:16,  5.55s/it] 13%|        | 426/3250 [40:05<4:19:12,  5.51s/it]                                                     13%|        | 426/3250 [40:05<4:19:12,  5.51s/it] 13%|    {'loss': 0.9985, 'learning_rate': 9.583455693161201e-05, 'epoch': 0.13}
{'loss': 0.9864, 'learning_rate': 9.581521022642286e-05, 'epoch': 0.13}
{'loss': 0.9911, 'learning_rate': 9.579582065874793e-05, 'epoch': 0.13}
{'loss': 0.9987, 'learning_rate': 9.577638824672715e-05, 'epoch': 0.13}
    | 427/3250 [40:11<4:25:12,  5.64s/it]                                                     13%|        | 427/3250 [40:11<4:25:12,  5.64s/it] 13%|        | 428/3250 [40:17<4:22:11,  5.57s/it]                                                     13%|        | 428/3250 [40:17<4:22:11,  5.57s/it] 13%|        | 429/3250 [40:22<4:19:48,  5.53s/it]                                                     13%|        | 429/3250 [40:22<4:19:48,  5.53s/it] 13%|        | 430/3250 [40:28<4:18:18,  5.50s/it]                                                     13%|        | 430/3250 [40:28<4:18:18,  5.50s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 1.004228949546814, 'eval_runtime': 1.3827, 'eval_samples_per_second': 8.679, 'eval_steps_per_second': 2.17, 'epoch': 0.13}
                                                     13%|        | 430/3250 [40:29<4:18:18,  5.50s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_4/tmp-checkpoint-430
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-430I AM HERE AND I AM SAVING THE MODEL

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-430
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-430
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-430
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-430/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-430/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-430/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-430/pytorch_model.bin
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 1.018, 'learning_rate': 9.575691300854055e-05, 'epoch': 0.13}
{'loss': 1.405, 'learning_rate': 9.57373949624082e-05, 'epoch': 0.13}
{'loss': 0.9732, 'learning_rate': 9.571783412659027e-05, 'epoch': 0.13}
{'loss': 1.0157, 'learning_rate': 9.56982305193869e-05, 'epoch': 0.13}
{'loss': 0.9989, 'learning_rate': 9.567858415913826e-05, 'epoch': 0.13}
{'loss': 0.9792, 'learning_rate': 9.565889506422456e-05, 'epoch': 0.13}
 13%|        | 431/3250 [40:35<4:42:04,  6.00s/it]                                                     13%|        | 431/3250 [40:35<4:42:04,  6.00s/it] 13%|        | 432/3250 [40:40<4:33:40,  5.83s/it]                                                     13%|        | 432/3250 [40:40<4:33:40,  5.83s/it] 13%|        | 433/3250 [40:46<4:27:47,  5.70s/it]                                                     13%|        | 433/3250 [40:46<4:27:47,  5.70s/it] 13%|        | 434/3250 [40:51<4:23:30,  5.61s/it]                                                     13%|        | 434/3250 [40:51<4:23:30,  5.61s/it] 13%|        | 435/3250 [40:56<4:20:32,  5.55s/it]                                                     13%|        | 435/3250 [40:56<4:20:32,  5.55s/it] 13%|        | 436/3250 [41:02<4:18:29,  5.51s/it]                                                     13%|        | 436/3250 [41:02<4:18:29,  5.51s/it] 13%|    {'loss': 0.9682, 'learning_rate': 9.563916325306594e-05, 'epoch': 0.13}
{'loss': 1.0238, 'learning_rate': 9.561938874412255e-05, 'epoch': 0.13}
{'loss': 1.0251, 'learning_rate': 9.559957155589444e-05, 'epoch': 0.14}
{'loss': 0.9858, 'learning_rate': 9.557971170692161e-05, 'epoch': 0.14}
    | 437/3250 [41:07<4:16:57,  5.48s/it]                                                     13%|        | 437/3250 [41:07<4:16:57,  5.48s/it] 13%|        | 438/3250 [41:13<4:16:04,  5.46s/it]                                                     13%|        | 438/3250 [41:13<4:16:04,  5.46s/it] 14%|        | 439/3250 [41:18<4:15:10,  5.45s/it]                                                     14%|        | 439/3250 [41:18<4:15:10,  5.45s/it] 14%|        | 440/3250 [41:24<4:14:42,  5.44s/it]                                                     14%|        | 440/3250 [41:24<4:14:42,  5.44s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 1.0034499168395996, 'eval_runtime': 1.386, 'eval_samples_per_second': 8.658, 'eval_steps_per_second': 2.164, 'epoch': 0.14}
                                                     14%|        | 440/3250 [41:25<4:14:42,  5.44s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_4/tmp-checkpoint-440
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-440
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-440
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-440
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-440
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-440/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-440/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-440/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-440/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.9693, 'learning_rate': 9.555980921578398e-05, 'epoch': 0.14}
{'loss': 0.9833, 'learning_rate': 9.553986410110134e-05, 'epoch': 0.14}
{'loss': 0.9618, 'learning_rate': 9.551987638153339e-05, 'epoch': 0.14}
{'loss': 0.9797, 'learning_rate': 9.549984607577964e-05, 'epoch': 0.14}
{'loss': 0.9747, 'learning_rate': 9.54797732025795e-05, 'epoch': 0.14}
{'loss': 0.9626, 'learning_rate': 9.545965778071218e-05, 'epoch': 0.14}
 14%|        | 441/3250 [41:31<4:39:00,  5.96s/it]                                                     14%|        | 441/3250 [41:31<4:39:00,  5.96s/it] 14%|        | 442/3250 [41:36<4:31:13,  5.80s/it]                                                     14%|        | 442/3250 [41:36<4:31:13,  5.80s/it] 14%|        | 443/3250 [41:42<4:31:00,  5.79s/it]                                                     14%|        | 443/3250 [41:42<4:31:00,  5.79s/it] 14%|        | 444/3250 [41:47<4:25:40,  5.68s/it]                                                     14%|        | 444/3250 [41:47<4:25:40,  5.68s/it] 14%|        | 445/3250 [41:53<4:21:43,  5.60s/it]                                                     14%|        | 445/3250 [41:53<4:21:43,  5.60s/it] 14%|        | 446/3250 [41:58<4:18:59,  5.54s/it]                                                     14%|        | 446/3250 [41:58<4:18:59,  5.54s/it] 14%|    {'loss': 0.9767, 'learning_rate': 9.543949982899667e-05, 'epoch': 0.14}
{'loss': 0.9993, 'learning_rate': 9.541929936629175e-05, 'epoch': 0.14}
{'loss': 0.9689, 'learning_rate': 9.539905641149605e-05, 'epoch': 0.14}
{'loss': 0.9686, 'learning_rate': 9.537877098354786e-05, 'epoch': 0.14}
    | 447/3250 [42:04<4:17:29,  5.51s/it]                                                     14%|        | 447/3250 [42:04<4:17:29,  5.51s/it] 14%|        | 448/3250 [42:09<4:16:10,  5.49s/it]                                                     14%|        | 448/3250 [42:09<4:16:10,  5.49s/it] 14%|        | 449/3250 [42:14<4:15:05,  5.46s/it]                                                     14%|        | 449/3250 [42:14<4:15:05,  5.46s/it] 14%|        | 450/3250 [42:20<4:14:14,  5.45s/it]                                                     14%|        | 450/3250 [42:20<4:14:14,  5.45s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 1.0014764070510864, 'eval_runtime': 1.3939, 'eval_samples_per_second': 8.609, 'eval_steps_per_second': 2.152, 'epoch': 0.14}
                                                     14%|        | 450/3250 [42:21<4:14:14,  5.45s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_4/tmp-checkpoint-450
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-450
/projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-450
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-450
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-450
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-450/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-450/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-450/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-450/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 1.0078, 'learning_rate': 9.535844310142524e-05, 'epoch': 0.14}
{'loss': 0.9869, 'learning_rate': 9.533807278414597e-05, 'epoch': 0.14}
{'loss': 0.9602, 'learning_rate': 9.531766005076755e-05, 'epoch': 0.14}
{'loss': 1.0577, 'learning_rate': 9.529720492038712e-05, 'epoch': 0.14}
{'loss': 0.9957, 'learning_rate': 9.527670741214152e-05, 'epoch': 0.14}
{'loss': 0.9698, 'learning_rate': 9.525616754520721e-05, 'epoch': 0.14}
 14%|        | 451/3250 [42:27<4:37:06,  5.94s/it]                                                     14%|        | 451/3250 [42:27<4:37:06,  5.94s/it] 14%|        | 452/3250 [42:32<4:29:42,  5.78s/it]                                                     14%|        | 452/3250 [42:32<4:29:42,  5.78s/it] 14%|        | 453/3250 [42:38<4:24:16,  5.67s/it]                                                     14%|        | 453/3250 [42:38<4:24:16,  5.67s/it] 14%|        | 454/3250 [42:43<4:20:40,  5.59s/it]                                                     14%|        | 454/3250 [42:43<4:20:40,  5.59s/it] 14%|        | 455/3250 [42:49<4:17:53,  5.54s/it]                                                     14%|        | 455/3250 [42:49<4:17:53,  5.54s/it] 14%|        | 456/3250 [42:54<4:15:55,  5.50s/it]                                                     14%|        | 456/3250 [42:54<4:15:55,  5.50s/it] 14%|    {'loss': 0.9683, 'learning_rate': 9.52355853388003e-05, 'epoch': 0.14}
{'loss': 0.945, 'learning_rate': 9.521496081217651e-05, 'epoch': 0.14}
{'loss': 1.007, 'learning_rate': 9.519429398463114e-05, 'epoch': 0.14}
{'loss': 0.9522, 'learning_rate': 9.517358487549906e-05, 'epoch': 0.14}
    | 457/3250 [42:59<4:14:36,  5.47s/it]                                                     14%|        | 457/3250 [42:59<4:14:36,  5.47s/it] 14%|        | 458/3250 [43:05<4:13:42,  5.45s/it]                                                     14%|        | 458/3250 [43:05<4:13:42,  5.45s/it] 14%|        | 459/3250 [43:10<4:16:46,  5.52s/it]                                                     14%|        | 459/3250 [43:10<4:16:46,  5.52s/it] 14%|        | 460/3250 [43:16<4:15:22,  5.49s/it]                                                     14%|        | 460/3250 [43:16<4:15:22,  5.49s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.9971595406532288, 'eval_runtime': 1.3935, 'eval_samples_per_second': 8.612, 'eval_steps_per_second': 2.153, 'epoch': 0.14}
                                                     14%|        | 460/3250 [43:17<4:15:22,  5.49s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_4/tmp-checkpoint-460
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in I AM HERE AND I AM SAVING THE MODEL 
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-460
/projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-460
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-460
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-460
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-460/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-460/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-460/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-460/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 1.0267, 'learning_rate': 9.51528335041547e-05, 'epoch': 0.14}
{'loss': 1.4136, 'learning_rate': 9.513203989001207e-05, 'epoch': 0.14}
{'loss': 0.9448, 'learning_rate': 9.511120405252464e-05, 'epoch': 0.14}
{'loss': 0.9918, 'learning_rate': 9.509032601118541e-05, 'epoch': 0.14}
{'loss': 1.0167, 'learning_rate': 9.506940578552688e-05, 'epoch': 0.14}
{'loss': 0.9834, 'learning_rate': 9.504844339512095e-05, 'epoch': 0.14}
 14%|        | 461/3250 [43:23<4:38:17,  5.99s/it]                                                     14%|        | 461/3250 [43:23<4:38:17,  5.99s/it] 14%|        | 462/3250 [43:28<4:30:06,  5.81s/it]                                                     14%|        | 462/3250 [43:28<4:30:06,  5.81s/it] 14%|        | 463/3250 [43:34<4:24:24,  5.69s/it]                                                     14%|        | 463/3250 [43:34<4:24:24,  5.69s/it] 14%|        | 464/3250 [43:39<4:20:20,  5.61s/it]                                                     14%|        | 464/3250 [43:39<4:20:20,  5.61s/it] 14%|        | 465/3250 [43:45<4:17:30,  5.55s/it]                                                     14%|        | 465/3250 [43:45<4:17:30,  5.55s/it] 14%|        | 466/3250 [43:50<4:15:27,  5.51s/it]                                                     14%|        | 466/3250 [43:50<4:15:27,  5.51s/it] 14%|    {'loss': 0.9473, 'learning_rate': 9.502743885957907e-05, 'epoch': 0.14}
{'loss': 0.9668, 'learning_rate': 9.500639219855206e-05, 'epoch': 0.14}
{'loss': 1.0536, 'learning_rate': 9.49853034317301e-05, 'epoch': 0.14}
{'loss': 0.9809, 'learning_rate': 9.496417257884285e-05, 'epoch': 0.14}
    | 467/3250 [43:55<4:13:54,  5.47s/it]                                                     14%|        | 467/3250 [43:55<4:13:54,  5.47s/it] 14%|        | 468/3250 [44:01<4:12:47,  5.45s/it]                                                     14%|        | 468/3250 [44:01<4:12:47,  5.45s/it] 14%|        | 469/3250 [44:06<4:12:09,  5.44s/it]                                                     14%|        | 469/3250 [44:06<4:12:09,  5.44s/it] 14%|        | 470/3250 [44:12<4:11:49,  5.44s/it]                                                     14%|        | 470/3250 [44:12<4:11:49,  5.44s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.9953363537788391, 'eval_runtime': 1.6198, 'eval_samples_per_second': 7.408, 'eval_steps_per_second': 1.852, 'epoch': 0.14}
                                                     14%|        | 470/3250 [44:13<4:11:49,  5.44s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_4/tmp-checkpoint-470
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-470I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-470

I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-470
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-470
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-470/pytorch_model.bin
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-470/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-470/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-470/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.9755, 'learning_rate': 9.494299965965933e-05, 'epoch': 0.14}
{'loss': 0.9205, 'learning_rate': 9.492178469398787e-05, 'epoch': 0.15}
{'loss': 0.9664, 'learning_rate': 9.490052770167617e-05, 'epoch': 0.15}
{'loss': 0.9795, 'learning_rate': 9.487922870261122e-05, 'epoch': 0.15}
{'loss': 0.9701, 'learning_rate': 9.485788771671935e-05, 'epoch': 0.15}
{'loss': 0.9464, 'learning_rate': 9.483650476396615e-05, 'epoch': 0.15}
 14%|        | 471/3250 [44:19<4:38:55,  6.02s/it]                                                     14%|        | 471/3250 [44:19<4:38:55,  6.02s/it] 15%|        | 472/3250 [44:25<4:30:20,  5.84s/it]                                                     15%|        | 472/3250 [44:25<4:30:20,  5.84s/it] 15%|        | 473/3250 [44:30<4:24:21,  5.71s/it]                                                     15%|        | 473/3250 [44:30<4:24:21,  5.71s/it] 15%|        | 474/3250 [44:35<4:19:59,  5.62s/it]                                                     15%|        | 474/3250 [44:35<4:19:59,  5.62s/it] 15%|        | 475/3250 [44:41<4:17:02,  5.56s/it]                                                     15%|        | 475/3250 [44:41<4:17:02,  5.56s/it] 15%|        | 476/3250 [44:47<4:20:21,  5.63s/it]                                                     15%|        | 476/3250 [44:47<4:20:21,  5.63s/it] 15%|    {'loss': 0.9622, 'learning_rate': 9.481507986435647e-05, 'epoch': 0.15}
{'loss': 0.9687, 'learning_rate': 9.47936130379344e-05, 'epoch': 0.15}
{'loss': 0.9837, 'learning_rate': 9.477210430478327e-05, 'epoch': 0.15}
{'loss': 0.9677, 'learning_rate': 9.475055368502559e-05, 'epoch': 0.15}
    | 477/3250 [44:52<4:17:25,  5.57s/it]                                                     15%|        | 477/3250 [44:52<4:17:25,  5.57s/it] 15%|        | 478/3250 [44:57<4:15:16,  5.53s/it]                                                     15%|        | 478/3250 [44:57<4:15:16,  5.53s/it] 15%|        | 479/3250 [45:03<4:13:42,  5.49s/it]                                                     15%|        | 479/3250 [45:03<4:13:42,  5.49s/it] 15%|        | 480/3250 [45:08<4:12:42,  5.47s/it]                                                     15%|        | 480/3250 [45:08<4:12:42,  5.47s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.9932664036750793, 'eval_runtime': 1.3959, 'eval_samples_per_second': 8.597, 'eval_steps_per_second': 2.149, 'epoch': 0.15}
                                                     15%|        | 480/3250 [45:10<4:12:42,  5.47s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_4/tmp-checkpoint-480
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  I AM HERE AND I AM SAVING THE MODEL
/projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-480the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-480

I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-480
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-480
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-480/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-480/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-480/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-480/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.9676, 'learning_rate': 9.472896119882308e-05, 'epoch': 0.15}
{'loss': 0.9941, 'learning_rate': 9.470732686637664e-05, 'epoch': 0.15}
{'loss': 0.9727, 'learning_rate': 9.468565070792628e-05, 'epoch': 0.15}
{'loss': 0.9899, 'learning_rate': 9.466393274375116e-05, 'epoch': 0.15}
{'loss': 1.002, 'learning_rate': 9.464217299416956e-05, 'epoch': 0.15}
{'loss': 0.9609, 'learning_rate': 9.462037147953886e-05, 'epoch': 0.15}
 15%|        | 481/3250 [45:15<4:35:28,  5.97s/it]                                                     15%|        | 481/3250 [45:15<4:35:28,  5.97s/it] 15%|        | 482/3250 [45:21<4:27:49,  5.81s/it]                                                     15%|        | 482/3250 [45:21<4:27:49,  5.81s/it] 15%|        | 483/3250 [45:26<4:22:27,  5.69s/it]                                                     15%|        | 483/3250 [45:26<4:22:27,  5.69s/it] 15%|        | 484/3250 [45:32<4:18:28,  5.61s/it]                                                     15%|        | 484/3250 [45:32<4:18:28,  5.61s/it] 15%|        | 485/3250 [45:37<4:15:55,  5.55s/it]                                                     15%|        | 485/3250 [45:37<4:15:55,  5.55s/it] 15%|        | 486/3250 [45:42<4:14:08,  5.52s/it]                                                     15%|        | 486/3250 [45:43<4:14:08,  5.52s/it] 15%|    {'loss': 0.9504, 'learning_rate': 9.459852822025546e-05, 'epoch': 0.15}
{'loss': 0.9473, 'learning_rate': 9.457664323675489e-05, 'epoch': 0.15}
{'loss': 0.9897, 'learning_rate': 9.455471654951165e-05, 'epoch': 0.15}
{'loss': 0.9436, 'learning_rate': 9.453274817903931e-05, 'epoch': 0.15}
    | 487/3250 [45:48<4:12:40,  5.49s/it]                                                     15%|        | 487/3250 [45:48<4:12:40,  5.49s/it] 15%|        | 488/3250 [45:53<4:11:43,  5.47s/it]                                                     15%|        | 488/3250 [45:53<4:11:43,  5.47s/it] 15%|        | 489/3250 [45:59<4:11:01,  5.46s/it]                                                     15%|        | 489/3250 [45:59<4:11:01,  5.46s/it] 15%|        | 490/3250 [46:04<4:10:26,  5.44s/it]                                                     15%|        | 490/3250 [46:04<4:10:26,  5.44s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.9888570308685303, 'eval_runtime': 1.3859, 'eval_samples_per_second': 8.658, 'eval_steps_per_second': 2.165, 'epoch': 0.15}
                                                     15%|        | 490/3250 [46:06<4:10:26,  5.44s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_4/tmp-checkpoint-490
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-490I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-490

I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-490
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-490
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-490/pytorch_model.bin
the pytorch model path isthe pytorch model path is  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-490/pytorch_model.bin/projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-490/pytorch_model.bin

/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-490/pytorch_model.bin
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 1.0146, 'learning_rate': 9.45107381458904e-05, 'epoch': 0.15}
{'loss': 1.442, 'learning_rate': 9.448868647065642e-05, 'epoch': 0.15}
{'loss': 0.887, 'learning_rate': 9.446659317396787e-05, 'epoch': 0.15}
{'loss': 0.9546, 'learning_rate': 9.444445827649415e-05, 'epoch': 0.15}
{'loss': 0.9921, 'learning_rate': 9.442228179894362e-05, 'epoch': 0.15}
{'loss': 0.9745, 'learning_rate': 9.440006376206349e-05, 'epoch': 0.15}
 15%|        | 491/3250 [46:11<4:33:03,  5.94s/it]                                                     15%|        | 491/3250 [46:11<4:33:03,  5.94s/it] 15%|        | 492/3250 [46:17<4:29:16,  5.86s/it]                                                     15%|        | 492/3250 [46:17<4:29:16,  5.86s/it] 15%|        | 493/3250 [46:22<4:22:55,  5.72s/it]                                                     15%|        | 493/3250 [46:22<4:22:55,  5.72s/it] 15%|        | 494/3250 [46:28<4:18:41,  5.63s/it]                                                     15%|        | 494/3250 [46:28<4:18:41,  5.63s/it] 15%|        | 495/3250 [46:33<4:15:26,  5.56s/it]                                                     15%|        | 495/3250 [46:33<4:15:26,  5.56s/it] 15%|        | 496/3250 [46:39<4:13:30,  5.52s/it]                                                     15%|        | 496/3250 [46:39<4:13:30,  5.52s/it] 15%|    {'loss': 0.9669, 'learning_rate': 9.437780418663988e-05, 'epoch': 0.15}
{'loss': 0.9466, 'learning_rate': 9.435550309349777e-05, 'epoch': 0.15}
{'loss': 1.0124, 'learning_rate': 9.433316050350099e-05, 'epoch': 0.15}
{'loss': 1.0054, 'learning_rate': 9.431077643755217e-05, 'epoch': 0.15}
    | 497/3250 [46:44<4:11:51,  5.49s/it]                                                     15%|        | 497/3250 [46:44<4:11:51,  5.49s/it] 15%|        | 498/3250 [46:49<4:10:44,  5.47s/it]                                                     15%|        | 498/3250 [46:49<4:10:44,  5.47s/it] 15%|        | 499/3250 [46:55<4:10:02,  5.45s/it]                                                     15%|        | 499/3250 [46:55<4:10:02,  5.45s/it] 15%|        | 500/3250 [47:00<4:09:21,  5.44s/it]                                                     15%|        | 500/3250 [47:00<4:09:21,  5.44s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.9891020059585571, 'eval_runtime': 1.3978, 'eval_samples_per_second': 8.585, 'eval_steps_per_second': 2.146, 'epoch': 0.15}
                                                     15%|        | 500/3250 [47:02<4:09:21,  5.44s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_4/tmp-checkpoint-500
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-500
 /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-500
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-500
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-500
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-500/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-500/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-500/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-500/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.9584, 'learning_rate': 9.428835091659276e-05, 'epoch': 0.15}
{'loss': 0.9011, 'learning_rate': 9.426588396160299e-05, 'epoch': 0.15}
{'loss': 0.9766, 'learning_rate': 9.424337559360183e-05, 'epoch': 0.15}
{'loss': 0.9507, 'learning_rate': 9.422082583364706e-05, 'epoch': 0.16}
{'loss': 0.9672, 'learning_rate': 9.419823470283511e-05, 'epoch': 0.16}
{'loss': 0.9402, 'learning_rate': 9.417560222230115e-05, 'epoch': 0.16}
 15%|        | 501/3250 [47:07<4:32:07,  5.94s/it]                                                     15%|        | 501/3250 [47:07<4:32:07,  5.94s/it] 15%|        | 502/3250 [47:13<4:24:52,  5.78s/it]                                                     15%|        | 502/3250 [47:13<4:24:52,  5.78s/it] 15%|        | 503/3250 [47:18<4:19:46,  5.67s/it]                                                     15%|        | 503/3250 [47:18<4:19:46,  5.67s/it] 16%|        | 504/3250 [47:24<4:16:00,  5.59s/it]                                                     16%|        | 504/3250 [47:24<4:16:00,  5.59s/it] 16%|        | 505/3250 [47:29<4:13:22,  5.54s/it]                                                     16%|        | 505/3250 [47:29<4:13:22,  5.54s/it] 16%|        | 506/3250 [47:34<4:11:41,  5.50s/it]                                                     16%|        | 506/3250 [47:34<4:11:41,  5.50s/it] 16%|    {'loss': 0.9513, 'learning_rate': 9.415292841321903e-05, 'epoch': 0.16}
{'loss': 0.9541, 'learning_rate': 9.413021329680128e-05, 'epoch': 0.16}
{'loss': 0.957, 'learning_rate': 9.4107456894299e-05, 'epoch': 0.16}
{'loss': 0.966, 'learning_rate': 9.408465922700206e-05, 'epoch': 0.16}
    | 507/3250 [47:40<4:10:17,  5.47s/it]                                                     16%|        | 507/3250 [47:40<4:10:17,  5.47s/it] 16%|        | 508/3250 [47:45<4:09:13,  5.45s/it]                                                     16%|        | 508/3250 [47:45<4:09:13,  5.45s/it] 16%|        | 509/3250 [47:51<4:17:08,  5.63s/it]                                                     16%|        | 509/3250 [47:51<4:17:08,  5.63s/it] 16%|        | 510/3250 [47:57<4:13:49,  5.56s/it]                                                     16%|        | 510/3250 [47:57<4:13:49,  5.56s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.9854913949966431, 'eval_runtime': 1.386, 'eval_samples_per_second': 8.658, 'eval_steps_per_second': 2.164, 'epoch': 0.16}
                                                     16%|        | 510/3250 [47:58<4:13:49,  5.56s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_4/tmp-checkpoint-510
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-510I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-510

I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-510
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-510
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-510/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-510/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-510/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-510/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.9648, 'learning_rate': 9.40618203162388e-05, 'epoch': 0.16}
{'loss': 0.9471, 'learning_rate': 9.403894018337622e-05, 'epoch': 0.16}
{'loss': 0.9861, 'learning_rate': 9.401601884981983e-05, 'epoch': 0.16}
{'loss': 0.9392, 'learning_rate': 9.399305633701373e-05, 'epoch': 0.16}
{'loss': 1.0327, 'learning_rate': 9.397005266644054e-05, 'epoch': 0.16}
{'loss': 0.9391, 'learning_rate': 9.394700785962139e-05, 'epoch': 0.16}
 16%|        | 511/3250 [48:04<4:35:45,  6.04s/it]                                                     16%|        | 511/3250 [48:04<4:35:45,  6.04s/it] 16%|        | 512/3250 [48:09<4:26:57,  5.85s/it]                                                     16%|        | 512/3250 [48:09<4:26:57,  5.85s/it] 16%|        | 513/3250 [48:15<4:20:56,  5.72s/it]                                                     16%|        | 513/3250 [48:15<4:20:56,  5.72s/it] 16%|        | 514/3250 [48:20<4:16:37,  5.63s/it]                                                     16%|        | 514/3250 [48:20<4:16:37,  5.63s/it] 16%|        | 515/3250 [48:25<4:13:29,  5.56s/it]                                                     16%|        | 515/3250 [48:25<4:13:29,  5.56s/it] 16%|        | 516/3250 [48:31<4:11:21,  5.52s/it]                                                     16%|        | 516/3250 [48:31<4:11:21,  5.52s/it] 16%|    {'loss': 0.9426, 'learning_rate': 9.392392193811584e-05, 'epoch': 0.16}
{'loss': 0.9441, 'learning_rate': 9.390079492352199e-05, 'epoch': 0.16}
{'loss': 0.9453, 'learning_rate': 9.387762683747636e-05, 'epoch': 0.16}
{'loss': 0.9451, 'learning_rate': 9.385441770165385e-05, 'epoch': 0.16}
    | 517/3250 [48:36<4:09:42,  5.48s/it]                                                     16%|        | 517/3250 [48:36<4:09:42,  5.48s/it] 16%|        | 518/3250 [48:42<4:08:42,  5.46s/it]                                                     16%|        | 518/3250 [48:42<4:08:42,  5.46s/it] 16%|        | 519/3250 [48:47<4:08:01,  5.45s/it]                                                     16%|        | 519/3250 [48:47<4:08:01,  5.45s/it] 16%|        | 520/3250 [48:53<4:07:19,  5.44s/it]                                                     16%|        | 520/3250 [48:53<4:07:19,  5.44s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.9821326732635498, 'eval_runtime': 1.3864, 'eval_samples_per_second': 8.656, 'eval_steps_per_second': 2.164, 'epoch': 0.16}
                                                     16%|        | 520/3250 [48:54<4:07:19,  5.44s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_4/tmp-checkpoint-520
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-520I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-520

I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-520
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-520
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-520/pytorch_model.bin
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-520/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-520/pytorch_model.binthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-520/pytorch_model.bin

/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.9558, 'learning_rate': 9.383116753776784e-05, 'epoch': 0.16}
{'loss': 0.9743, 'learning_rate': 9.380787636757002e-05, 'epoch': 0.16}
{'loss': 1.3783, 'learning_rate': 9.378454421285049e-05, 'epoch': 0.16}
{'loss': 0.9362, 'learning_rate': 9.376117109543769e-05, 'epoch': 0.16}
{'loss': 0.978, 'learning_rate': 9.373775703719836e-05, 'epoch': 0.16}
{'loss': 0.9594, 'learning_rate': 9.371430206003758e-05, 'epoch': 0.16}
 16%|        | 521/3250 [49:00<4:30:00,  5.94s/it]                                                     16%|        | 521/3250 [49:00<4:30:00,  5.94s/it] 16%|        | 522/3250 [49:05<4:22:48,  5.78s/it]                                                     16%|        | 522/3250 [49:05<4:22:48,  5.78s/it] 16%|        | 523/3250 [49:10<4:17:41,  5.67s/it]                                                     16%|        | 523/3250 [49:10<4:17:41,  5.67s/it] 16%|        | 524/3250 [49:16<4:14:05,  5.59s/it]                                                     16%|        | 524/3250 [49:16<4:14:05,  5.59s/it] 16%|        | 525/3250 [49:22<4:14:41,  5.61s/it]                                                     16%|        | 525/3250 [49:22<4:14:41,  5.61s/it] 16%|        | 526/3250 [49:27<4:11:55,  5.55s/it]                                                     16%|        | 526/3250 [49:27<4:11:55,  5.55s/it] 16%|    {'loss': 0.9555, 'learning_rate': 9.369080618589864e-05, 'epoch': 0.16}
{'loss': 0.9288, 'learning_rate': 9.366726943676321e-05, 'epoch': 0.16}
{'loss': 0.9977, 'learning_rate': 9.364369183465106e-05, 'epoch': 0.16}
{'loss': 0.9856, 'learning_rate': 9.362007340162029e-05, 'epoch': 0.16}
    | 527/3250 [49:32<4:09:51,  5.51s/it]                                                     16%|        | 527/3250 [49:32<4:09:51,  5.51s/it] 16%|        | 528/3250 [49:38<4:08:16,  5.47s/it]                                                     16%|        | 528/3250 [49:38<4:08:16,  5.47s/it] 16%|        | 529/3250 [49:43<4:07:23,  5.46s/it]                                                     16%|        | 529/3250 [49:43<4:07:23,  5.46s/it] 16%|        | 530/3250 [49:49<4:06:44,  5.44s/it]                                                     16%|        | 530/3250 [49:49<4:06:44,  5.44s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.9807910323143005, 'eval_runtime': 1.3964, 'eval_samples_per_second': 8.594, 'eval_steps_per_second': 2.148, 'epoch': 0.16}
                                                     16%|        | 530/3250 [49:50<4:06:44,  5.44s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_4/tmp-checkpoint-530
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-530
/projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-530
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-530
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-530
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-530/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-530/pytorch_model.bin
the pytorch model path is the pytorch model path is/projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-530/pytorch_model.bin /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-530/pytorch_model.bin

/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.9495, 'learning_rate': 9.359641415976714e-05, 'epoch': 0.16}
{'loss': 0.9226, 'learning_rate': 9.357271413122606e-05, 'epoch': 0.16}
{'loss': 0.947, 'learning_rate': 9.354897333816963e-05, 'epoch': 0.16}
{'loss': 0.9355, 'learning_rate': 9.35251918028086e-05, 'epoch': 0.16}
{'loss': 0.9437, 'learning_rate': 9.350136954739183e-05, 'epoch': 0.16}
{'loss': 0.9544, 'learning_rate': 9.347750659420623e-05, 'epoch': 0.16}
 16%|        | 531/3250 [49:56<4:31:15,  5.99s/it]                                                     16%|        | 531/3250 [49:56<4:31:15,  5.99s/it] 16%|        | 532/3250 [50:01<4:23:29,  5.82s/it]                                                     16%|        | 532/3250 [50:01<4:23:29,  5.82s/it] 16%|        | 533/3250 [50:07<4:17:54,  5.70s/it]                                                     16%|        | 533/3250 [50:07<4:17:54,  5.70s/it] 16%|        | 534/3250 [50:12<4:14:03,  5.61s/it]                                                     16%|        | 534/3250 [50:12<4:14:03,  5.61s/it] 16%|        | 535/3250 [50:17<4:11:06,  5.55s/it]                                                     16%|        | 535/3250 [50:17<4:11:06,  5.55s/it] 16%|        | 536/3250 [50:23<4:09:00,  5.50s/it]                                                     16%|        | 536/3250 [50:23<4:09:00,  5.50s/it] 17%|    {'loss': 0.93, 'learning_rate': 9.345360296557684e-05, 'epoch': 0.17}
{'loss': 0.9295, 'learning_rate': 9.342965868386674e-05, 'epoch': 0.17}
{'loss': 0.9662, 'learning_rate': 9.340567377147702e-05, 'epoch': 0.17}
{'loss': 0.9279, 'learning_rate': 9.338164825084682e-05, 'epoch': 0.17}
    | 537/3250 [50:28<4:07:29,  5.47s/it]                                                     17%|        | 537/3250 [50:28<4:07:29,  5.47s/it] 17%|        | 538/3250 [50:34<4:06:33,  5.45s/it]                                                     17%|        | 538/3250 [50:34<4:06:33,  5.45s/it] 17%|        | 539/3250 [50:39<4:05:53,  5.44s/it]                                                     17%|        | 539/3250 [50:39<4:05:53,  5.44s/it] 17%|        | 540/3250 [50:45<4:05:26,  5.43s/it]                                                     17%|        | 540/3250 [50:45<4:05:26,  5.43s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.9824106097221375, 'eval_runtime': 1.6219, 'eval_samples_per_second': 7.399, 'eval_steps_per_second': 1.85, 'epoch': 0.17}
                                                     17%|        | 540/3250 [50:46<4:05:26,  5.43s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_4/tmp-checkpoint-540
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-540
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-540
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-540
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-540
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-540/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-540/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-540/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-540/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.9452, 'learning_rate': 9.335758214445324e-05, 'epoch': 0.17}
{'loss': 0.9681, 'learning_rate': 9.333347547481135e-05, 'epoch': 0.17}
{'loss': 0.9669, 'learning_rate': 9.330932826447421e-05, 'epoch': 0.17}
{'loss': 0.9083, 'learning_rate': 9.328514053603272e-05, 'epoch': 0.17}
{'loss': 1.0199, 'learning_rate': 9.326091231211582e-05, 'epoch': 0.17}
{'loss': 0.9548, 'learning_rate': 9.323664361539019e-05, 'epoch': 0.17}
 17%|        | 541/3250 [50:52<4:36:16,  6.12s/it]                                                     17%|        | 541/3250 [50:52<4:36:16,  6.12s/it] 17%|        | 542/3250 [50:58<4:26:53,  5.91s/it]                                                     17%|        | 542/3250 [50:58<4:26:53,  5.91s/it] 17%|        | 543/3250 [51:03<4:20:15,  5.77s/it]                                                     17%|        | 543/3250 [51:03<4:20:15,  5.77s/it] 17%|        | 544/3250 [51:09<4:15:18,  5.66s/it]                                                     17%|        | 544/3250 [51:09<4:15:18,  5.66s/it] 17%|        | 545/3250 [51:14<4:11:53,  5.59s/it]                                                     17%|        | 545/3250 [51:14<4:11:53,  5.59s/it] 17%|        | 546/3250 [51:19<4:09:40,  5.54s/it]                                                     17%|        | 546/3250 [51:19<4:09:40,  5.54s/it] 17%|    {'loss': 0.9265, 'learning_rate': 9.32123344685605e-05, 'epoch': 0.17}
{'loss': 0.9287, 'learning_rate': 9.318798489436917e-05, 'epoch': 0.17}
{'loss': 0.8926, 'learning_rate': 9.31635949155965e-05, 'epoch': 0.17}
{'loss': 0.9705, 'learning_rate': 9.313916455506055e-05, 'epoch': 0.17}
    | 547/3250 [51:25<4:07:58,  5.50s/it]                                                     17%|        | 547/3250 [51:25<4:07:58,  5.50s/it] 17%|        | 548/3250 [51:30<4:06:36,  5.48s/it]                                                     17%|        | 548/3250 [51:30<4:06:36,  5.48s/it] 17%|        | 549/3250 [51:36<4:05:38,  5.46s/it]                                                     17%|        | 549/3250 [51:36<4:05:38,  5.46s/it] 17%|        | 550/3250 [51:41<4:05:02,  5.45s/it]                                                     17%|        | 550/3250 [51:41<4:05:02,  5.45s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.9763926863670349, 'eval_runtime': 1.3917, 'eval_samples_per_second': 8.623, 'eval_steps_per_second': 2.156, 'epoch': 0.17}
                                                     17%|        | 550/3250 [51:42<4:05:02,  5.45s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_4/tmp-checkpoint-550
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-550
/projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-550
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-550
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-550
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-550/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-550/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-550/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-550/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.9203, 'learning_rate': 9.311469383561719e-05, 'epoch': 0.17}
{'loss': 0.9808, 'learning_rate': 9.309018278016004e-05, 'epoch': 0.17}
{'loss': 1.385, 'learning_rate': 9.306563141162046e-05, 'epoch': 0.17}
{'loss': 0.897, 'learning_rate': 9.30410397529675e-05, 'epoch': 0.17}
{'loss': 0.9498, 'learning_rate': 9.301640782720792e-05, 'epoch': 0.17}
{'loss': 0.9738, 'learning_rate': 9.299173565738617e-05, 'epoch': 0.17}
 17%|        | 551/3250 [51:48<4:28:09,  5.96s/it]                                                     17%|        | 551/3250 [51:48<4:28:09,  5.96s/it] 17%|        | 552/3250 [51:54<4:20:45,  5.80s/it]                                                     17%|        | 552/3250 [51:54<4:20:45,  5.80s/it] 17%|        | 553/3250 [51:59<4:15:35,  5.69s/it]                                                     17%|        | 553/3250 [51:59<4:15:35,  5.69s/it] 17%|        | 554/3250 [52:04<4:11:55,  5.61s/it]                                                     17%|        | 554/3250 [52:04<4:11:55,  5.61s/it] 17%|        | 555/3250 [52:10<4:09:08,  5.55s/it]                                                     17%|        | 555/3250 [52:10<4:09:08,  5.55s/it] 17%|        | 556/3250 [52:15<4:07:23,  5.51s/it]                                                     17%|        | 556/3250 [52:15<4:07:23,  5.51s/it] 17%|    {'loss': 0.9517, 'learning_rate': 9.296702326658433e-05, 'epoch': 0.17}
{'loss': 0.8969, 'learning_rate': 9.294227067792211e-05, 'epoch': 0.17}
{'loss': 0.9256, 'learning_rate': 9.291747791455682e-05, 'epoch': 0.17}
{'loss': 1.0175, 'learning_rate': 9.289264499968339e-05, 'epoch': 0.17}
    | 557/3250 [52:21<4:06:12,  5.49s/it]                                                     17%|        | 557/3250 [52:21<4:06:12,  5.49s/it] 17%|        | 558/3250 [52:26<4:08:15,  5.53s/it]                                                     17%|        | 558/3250 [52:26<4:08:15,  5.53s/it] 17%|        | 559/3250 [52:32<4:06:42,  5.50s/it]                                                     17%|        | 559/3250 [52:32<4:06:42,  5.50s/it] 17%|        | 560/3250 [52:37<4:05:24,  5.47s/it]                                                     17%|        | 560/3250 [52:37<4:05:24,  5.47s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.9755061268806458, 'eval_runtime': 1.4062, 'eval_samples_per_second': 8.534, 'eval_steps_per_second': 2.133, 'epoch': 0.17}
                                                     17%|        | 560/3250 [52:39<4:05:24,  5.47s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_4/tmp-checkpoint-560
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-560
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-560
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-560
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-560
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-560/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-560/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-560/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-560/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.9384, 'learning_rate': 9.286777195653426e-05, 'epoch': 0.17}
{'loss': 0.9465, 'learning_rate': 9.284285880837946e-05, 'epoch': 0.17}
{'loss': 0.8949, 'learning_rate': 9.281790557852652e-05, 'epoch': 0.17}
{'loss': 0.9263, 'learning_rate': 9.279291229032048e-05, 'epoch': 0.17}
{'loss': 0.9373, 'learning_rate': 9.276787896714382e-05, 'epoch': 0.17}
{'loss': 0.9319, 'learning_rate': 9.27428056324165e-05, 'epoch': 0.17}
 17%|        | 561/3250 [52:44<4:28:23,  5.99s/it]                                                     17%|        | 561/3250 [52:44<4:28:23,  5.99s/it] 17%|        | 562/3250 [52:50<4:20:30,  5.81s/it]                                                     17%|        | 562/3250 [52:50<4:20:30,  5.81s/it] 17%|        | 563/3250 [52:55<4:15:05,  5.70s/it]                                                     17%|        | 563/3250 [52:55<4:15:05,  5.70s/it] 17%|        | 564/3250 [53:01<4:11:23,  5.62s/it]                                                     17%|        | 564/3250 [53:01<4:11:23,  5.62s/it] 17%|        | 565/3250 [53:06<4:08:27,  5.55s/it]                                                     17%|        | 565/3250 [53:06<4:08:27,  5.55s/it] 17%|        | 566/3250 [53:11<4:06:31,  5.51s/it]                                                     17%|        | 566/3250 [53:11<4:06:31,  5.51s/it] 17%|    {'loss': 0.9043, 'learning_rate': 9.271769230959596e-05, 'epoch': 0.17}
{'loss': 0.9249, 'learning_rate': 9.269253902217696e-05, 'epoch': 0.17}
{'loss': 0.9415, 'learning_rate': 9.266734579369172e-05, 'epoch': 0.18}
{'loss': 0.9451, 'learning_rate': 9.264211264770976e-05, 'epoch': 0.18}
    | 567/3250 [53:17<4:05:00,  5.48s/it]                                                     17%|        | 567/3250 [53:17<4:05:00,  5.48s/it] 17%|        | 568/3250 [53:22<4:03:59,  5.46s/it]                                                     17%|        | 568/3250 [53:22<4:03:59,  5.46s/it] 18%|        | 569/3250 [53:28<4:03:28,  5.45s/it]                                                     18%|        | 569/3250 [53:28<4:03:28,  5.45s/it] 18%|        | 570/3250 [53:33<4:02:43,  5.43s/it]                                                     18%|        | 570/3250 [53:33<4:02:43,  5.43s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.973868191242218, 'eval_runtime': 1.3874, 'eval_samples_per_second': 8.65, 'eval_steps_per_second': 2.162, 'epoch': 0.18}
                                                     18%|        | 570/3250 [53:34<4:02:43,  5.43s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_4/tmp-checkpoint-570
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-570I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in 
 /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-570
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-570
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-570
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-570/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-570/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-570/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-570/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.9392, 'learning_rate': 9.261683960783804e-05, 'epoch': 0.18}
{'loss': 0.9359, 'learning_rate': 9.259152669772078e-05, 'epoch': 0.18}
{'loss': 0.9642, 'learning_rate': 9.256617394103946e-05, 'epoch': 0.18}
{'loss': 0.9402, 'learning_rate': 9.254078136151295e-05, 'epoch': 0.18}
{'loss': 0.9556, 'learning_rate': 9.251534898289726e-05, 'epoch': 0.18}
{'loss': 0.9549, 'learning_rate': 9.248987682898575e-05, 'epoch': 0.18}
 18%|        | 571/3250 [53:40<4:25:19,  5.94s/it]                                                     18%|        | 571/3250 [53:40<4:25:19,  5.94s/it] 18%|        | 572/3250 [53:46<4:18:13,  5.79s/it]                                                     18%|        | 572/3250 [53:46<4:18:13,  5.79s/it] 18%|        | 573/3250 [53:51<4:13:12,  5.68s/it]                                                     18%|        | 573/3250 [53:51<4:13:12,  5.68s/it] 18%|        | 574/3250 [53:57<4:18:01,  5.79s/it]                                                     18%|        | 574/3250 [53:57<4:18:01,  5.79s/it] 18%|        | 575/3250 [54:03<4:13:04,  5.68s/it]                                                     18%|        | 575/3250 [54:03<4:13:04,  5.68s/it] 18%|        | 576/3250 [54:08<4:09:24,  5.60s/it]                                                     18%|        | 576/3250 [54:08<4:09:24,  5.60s/it] 18%|    {'loss': 0.9147, 'learning_rate': 9.246436492360888e-05, 'epoch': 0.18}
{'loss': 0.9084, 'learning_rate': 9.243881329063435e-05, 'epoch': 0.18}
{'loss': 0.918, 'learning_rate': 9.241322195396707e-05, 'epoch': 0.18}
{'loss': 0.9655, 'learning_rate': 9.2387590937549e-05, 'epoch': 0.18}
    | 577/3250 [54:13<4:07:03,  5.55s/it]                                                     18%|        | 577/3250 [54:13<4:07:03,  5.55s/it] 18%|        | 578/3250 [54:19<4:05:23,  5.51s/it]                                                     18%|        | 578/3250 [54:19<4:05:23,  5.51s/it] 18%|        | 579/3250 [54:24<4:04:07,  5.48s/it]                                                     18%|        | 579/3250 [54:24<4:04:07,  5.48s/it] 18%|        | 580/3250 [54:30<4:03:05,  5.46s/it]                                                     18%|        | 580/3250 [54:30<4:03:05,  5.46s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.9692808985710144, 'eval_runtime': 1.3875, 'eval_samples_per_second': 8.649, 'eval_steps_per_second': 2.162, 'epoch': 0.18}
                                                     18%|        | 580/3250 [54:31<4:03:05,  5.46s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_4/tmp-checkpoint-580
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-580
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-580
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-580
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-580
the pytorch model path isthe pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-580/pytorch_model.bin /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-580/pytorch_model.bin

 /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-580/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-580/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.8957, 'learning_rate': 9.23619202653593e-05, 'epoch': 0.18}
{'loss': 0.9963, 'learning_rate': 9.233620996141421e-05, 'epoch': 0.18}
{'loss': 1.4133, 'learning_rate': 9.231046004976704e-05, 'epoch': 0.18}
{'loss': 0.8493, 'learning_rate': 9.228467055450813e-05, 'epoch': 0.18}
{'loss': 0.9081, 'learning_rate': 9.225884149976493e-05, 'epoch': 0.18}
{'loss': 0.954, 'learning_rate': 9.22329729097018e-05, 'epoch': 0.18}
 18%|        | 581/3250 [54:37<4:25:11,  5.96s/it]                                                     18%|        | 581/3250 [54:37<4:25:11,  5.96s/it] 18%|        | 582/3250 [54:42<4:17:59,  5.80s/it]                                                     18%|        | 582/3250 [54:42<4:17:59,  5.80s/it] 18%|        | 583/3250 [54:48<4:12:50,  5.69s/it]                                                     18%|        | 583/3250 [54:48<4:12:50,  5.69s/it] 18%|        | 584/3250 [54:53<4:08:57,  5.60s/it]                                                     18%|        | 584/3250 [54:53<4:08:57,  5.60s/it] 18%|        | 585/3250 [54:58<4:06:19,  5.55s/it]                                                     18%|        | 585/3250 [54:58<4:06:19,  5.55s/it] 18%|        | 586/3250 [55:04<4:04:20,  5.50s/it]                                                     18%|        | 586/3250 [55:04<4:04:20,  5.50s/it] 18%|    {'loss': 0.9432, 'learning_rate': 9.220706480852016e-05, 'epoch': 0.18}
{'loss': 0.9124, 'learning_rate': 9.218111722045837e-05, 'epoch': 0.18}
{'loss': 0.9264, 'learning_rate': 9.215513016979172e-05, 'epoch': 0.18}
{'loss': 0.9682, 'learning_rate': 9.212910368083245e-05, 'epoch': 0.18}
    | 587/3250 [55:09<4:03:00,  5.48s/it]                                                     18%|        | 587/3250 [55:09<4:03:00,  5.48s/it] 18%|        | 588/3250 [55:15<4:02:10,  5.46s/it]                                                     18%|        | 588/3250 [55:15<4:02:10,  5.46s/it] 18%|        | 589/3250 [55:20<4:01:17,  5.44s/it]                                                     18%|        | 589/3250 [55:20<4:01:17,  5.44s/it] 18%|        | 590/3250 [55:25<4:01:04,  5.44s/it]                                                     18%|        | 590/3250 [55:25<4:01:04,  5.44s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.9693326950073242, 'eval_runtime': 1.3888, 'eval_samples_per_second': 8.641, 'eval_steps_per_second': 2.16, 'epoch': 0.18}
                                                     18%|        | 590/3250 [55:27<4:01:04,  5.44s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_4/tmp-checkpoint-590
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-590
/projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-590
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-590
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-590
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-590/pytorch_model.binthe pytorch model path is
 /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-590/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-590/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-590/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.955, 'learning_rate': 9.210303777792968e-05, 'epoch': 0.18}
{'loss': 0.9339, 'learning_rate': 9.20769324854694e-05, 'epoch': 0.18}
{'loss': 0.8708, 'learning_rate': 9.205078782787445e-05, 'epoch': 0.18}
{'loss': 0.9418, 'learning_rate': 9.202460382960448e-05, 'epoch': 0.18}
{'loss': 0.9269, 'learning_rate': 9.1998380515156e-05, 'epoch': 0.18}
{'loss': 0.9257, 'learning_rate': 9.197211790906227e-05, 'epoch': 0.18}
 18%|        | 591/3250 [55:33<4:25:34,  5.99s/it]                                                     18%|        | 591/3250 [55:33<4:25:34,  5.99s/it] 18%|        | 592/3250 [55:38<4:17:42,  5.82s/it]                                                     18%|        | 592/3250 [55:38<4:17:42,  5.82s/it] 18%|        | 593/3250 [55:44<4:12:18,  5.70s/it]                                                     18%|        | 593/3250 [55:44<4:12:18,  5.70s/it] 18%|        | 594/3250 [55:49<4:08:37,  5.62s/it]                                                     18%|        | 594/3250 [55:49<4:08:37,  5.62s/it] 18%|        | 595/3250 [55:54<4:05:55,  5.56s/it]                                                     18%|        | 595/3250 [55:54<4:05:55,  5.56s/it] 18%|        | 596/3250 [56:00<4:03:56,  5.51s/it]                                                     18%|        | 596/3250 [56:00<4:03:56,  5.51s/it] 18%|    {'loss': 0.9256, 'learning_rate': 9.194581603589328e-05, 'epoch': 0.18}
{'loss': 0.9188, 'learning_rate': 9.191947492025582e-05, 'epoch': 0.18}
{'loss': 0.9139, 'learning_rate': 9.189309458679331e-05, 'epoch': 0.18}
{'loss': 0.9337, 'learning_rate': 9.186667506018596e-05, 'epoch': 0.18}
    | 597/3250 [56:05<4:02:26,  5.48s/it]                                                     18%|        | 597/3250 [56:05<4:02:26,  5.48s/it] 18%|        | 598/3250 [56:11<4:01:26,  5.46s/it]                                                     18%|        | 598/3250 [56:11<4:01:26,  5.46s/it] 18%|        | 599/3250 [56:16<4:00:44,  5.45s/it]                                                     18%|        | 599/3250 [56:16<4:00:44,  5.45s/it] 18%|        | 600/3250 [56:22<4:00:07,  5.44s/it]                                                     18%|        | 600/3250 [56:22<4:00:07,  5.44s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.9677848219871521, 'eval_runtime': 1.3968, 'eval_samples_per_second': 8.591, 'eval_steps_per_second': 2.148, 'epoch': 0.18}
                                                     18%|        | 600/3250 [56:23<4:00:07,  5.44s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_4/tmp-checkpoint-600
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-600I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-600

I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-600
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-600
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-600/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-600/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-600/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-600/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.936, 'learning_rate': 9.184021636515058e-05, 'epoch': 0.18}
{'loss': 0.9407, 'learning_rate': 9.181371852644063e-05, 'epoch': 0.19}
{'loss': 0.9223, 'learning_rate': 9.178718156884621e-05, 'epoch': 0.19}
{'loss': 0.9507, 'learning_rate': 9.1760605517194e-05, 'epoch': 0.19}
{'loss': 0.9004, 'learning_rate': 9.173399039634729e-05, 'epoch': 0.19}
{'loss': 1.0029, 'learning_rate': 9.170733623120585e-05, 'epoch': 0.19}
 18%|        | 601/3250 [56:29<4:21:56,  5.93s/it]                                                     18%|        | 601/3250 [56:29<4:21:56,  5.93s/it] 19%|        | 602/3250 [56:34<4:14:36,  5.77s/it]                                                     19%|        | 602/3250 [56:34<4:14:36,  5.77s/it] 19%|        | 603/3250 [56:39<4:09:29,  5.66s/it]                                                     19%|        | 603/3250 [56:39<4:09:29,  5.66s/it] 19%|        | 604/3250 [56:45<4:05:51,  5.58s/it]                                                     19%|        | 604/3250 [56:45<4:05:51,  5.58s/it] 19%|        | 605/3250 [56:50<4:03:22,  5.52s/it]                                                     19%|        | 605/3250 [56:50<4:03:22,  5.52s/it] 19%|        | 606/3250 [56:56<4:01:40,  5.48s/it]                                                     19%|        | 606/3250 [56:56<4:01:40,  5.48s/it] 19%|    {'loss': 0.8922, 'learning_rate': 9.168064304670606e-05, 'epoch': 0.19}
{'loss': 0.9128, 'learning_rate': 9.165391086782074e-05, 'epoch': 0.19}
{'loss': 0.922, 'learning_rate': 9.162713971955925e-05, 'epoch': 0.19}
{'loss': 0.894, 'learning_rate': 9.160032962696734e-05, 'epoch': 0.19}
    | 607/3250 [57:02<4:08:32,  5.64s/it]                                                     19%|        | 607/3250 [57:02<4:08:32,  5.64s/it] 19%|        | 608/3250 [57:07<4:05:03,  5.57s/it]                                                     19%|        | 608/3250 [57:07<4:05:03,  5.57s/it] 19%|        | 609/3250 [57:12<4:02:36,  5.51s/it]                                                     19%|        | 609/3250 [57:12<4:02:36,  5.51s/it] 19%|        | 610/3250 [57:18<4:00:45,  5.47s/it]                                                     19%|        | 610/3250 [57:18<4:00:45,  5.47s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.9626853466033936, 'eval_runtime': 1.3858, 'eval_samples_per_second': 8.659, 'eval_steps_per_second': 2.165, 'epoch': 0.19}
                                                     19%|        | 610/3250 [57:19<4:00:45,  5.47s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_4/tmp-checkpoint-610
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-610
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-610
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-610
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-610
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-610/pytorch_model.bin
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-610/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-610/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-610/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.921, 'learning_rate': 9.157348061512727e-05, 'epoch': 0.19}
{'loss': 0.9344, 'learning_rate': 9.154659270915764e-05, 'epoch': 0.19}
{'loss': 0.9406, 'learning_rate': 9.151966593421347e-05, 'epoch': 0.19}
{'loss': 1.343, 'learning_rate': 9.149270031548617e-05, 'epoch': 0.19}
{'loss': 0.8942, 'learning_rate': 9.146569587820344e-05, 'epoch': 0.19}
{'loss': 0.94, 'learning_rate': 9.143865264762931e-05, 'epoch': 0.19}
 19%|        | 611/3250 [57:25<4:22:04,  5.96s/it]                                                     19%|        | 611/3250 [57:25<4:22:04,  5.96s/it] 19%|        | 612/3250 [57:30<4:14:22,  5.79s/it]                                                     19%|        | 612/3250 [57:30<4:14:22,  5.79s/it] 19%|        | 613/3250 [57:36<4:09:03,  5.67s/it]                                                     19%|        | 613/3250 [57:36<4:09:03,  5.67s/it] 19%|        | 614/3250 [57:41<4:05:08,  5.58s/it]                                                     19%|        | 614/3250 [57:41<4:05:08,  5.58s/it] 19%|        | 615/3250 [57:46<4:02:20,  5.52s/it]                                                     19%|        | 615/3250 [57:46<4:02:20,  5.52s/it] 19%|        | 616/3250 [57:52<4:00:28,  5.48s/it]                                                     19%|        | 616/3250 [57:52<4:00:28,  5.48s/it] 19%|    {'loss': 0.923, 'learning_rate': 9.141157064906414e-05, 'epoch': 0.19}
{'loss': 0.9353, 'learning_rate': 9.138444990784453e-05, 'epoch': 0.19}
{'loss': 0.8964, 'learning_rate': 9.135729044934331e-05, 'epoch': 0.19}
{'loss': 0.9665, 'learning_rate': 9.133009229896957e-05, 'epoch': 0.19}
    | 617/3250 [57:57<3:59:17,  5.45s/it]                                                     19%|        | 617/3250 [57:57<3:59:17,  5.45s/it] 19%|        | 618/3250 [58:03<3:58:25,  5.44s/it]                                                     19%|        | 618/3250 [58:03<3:58:25,  5.44s/it] 19%|        | 619/3250 [58:08<3:57:38,  5.42s/it]                                                     19%|        | 619/3250 [58:08<3:57:38,  5.42s/it] 19%|        | 620/3250 [58:13<3:57:01,  5.41s/it]                                                     19%|        | 620/3250 [58:13<3:57:01,  5.41s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.9608363509178162, 'eval_runtime': 1.3856, 'eval_samples_per_second': 8.661, 'eval_steps_per_second': 2.165, 'epoch': 0.19}
                                                     19%|        | 620/3250 [58:15<3:57:01,  5.41s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_4/tmp-checkpoint-620
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-620
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-620
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-620
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-620
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-620/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-620/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-620/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-620/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.947, 'learning_rate': 9.130285548216857e-05, 'epoch': 0.19}
{'loss': 0.9183, 'learning_rate': 9.127558002442174e-05, 'epoch': 0.19}
{'loss': 0.8838, 'learning_rate': 9.124826595124671e-05, 'epoch': 0.19}
{'loss': 0.9111, 'learning_rate': 9.122091328819715e-05, 'epoch': 0.19}
{'loss': 0.9003, 'learning_rate': 9.119352206086293e-05, 'epoch': 0.19}
{'loss': 0.9099, 'learning_rate': 9.116609229486992e-05, 'epoch': 0.19}
 19%|        | 621/3250 [58:20<4:18:52,  5.91s/it]                                                     19%|        | 621/3250 [58:20<4:18:52,  5.91s/it] 19%|        | 622/3250 [58:26<4:11:57,  5.75s/it]                                                     19%|        | 622/3250 [58:26<4:11:57,  5.75s/it] 19%|        | 623/3250 [58:31<4:11:42,  5.75s/it]                                                     19%|        | 623/3250 [58:31<4:11:42,  5.75s/it] 19%|        | 624/3250 [58:37<4:06:45,  5.64s/it]                                                     19%|        | 624/3250 [58:37<4:06:45,  5.64s/it] 19%|        | 625/3250 [58:42<4:03:16,  5.56s/it]                                                     19%|        | 625/3250 [58:42<4:03:16,  5.56s/it] 19%|        | 626/3250 [58:48<4:00:50,  5.51s/it]                                                     19%|        | 626/3250 [58:48<4:00:50,  5.51s/it] 19%|    {'loss': 0.9115, 'learning_rate': 9.113862401588009e-05, 'epoch': 0.19}
{'loss': 0.8939, 'learning_rate': 9.111111724959143e-05, 'epoch': 0.19}
{'loss': 0.9095, 'learning_rate': 9.108357202173794e-05, 'epoch': 0.19}
{'loss': 0.929, 'learning_rate': 9.105598835808957e-05, 'epoch': 0.19}
    | 627/3250 [58:53<3:59:04,  5.47s/it]                                                     19%|        | 627/3250 [58:53<3:59:04,  5.47s/it] 19%|        | 628/3250 [58:58<3:57:56,  5.45s/it]                                                     19%|        | 628/3250 [58:58<3:57:56,  5.45s/it] 19%|        | 629/3250 [59:04<3:57:11,  5.43s/it]                                                     19%|        | 629/3250 [59:04<3:57:11,  5.43s/it] 19%|        | 630/3250 [59:09<3:56:29,  5.42s/it]                                                     19%|        | 630/3250 [59:09<3:56:29,  5.42s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.96183842420578, 'eval_runtime': 1.3902, 'eval_samples_per_second': 8.632, 'eval_steps_per_second': 2.158, 'epoch': 0.19}
                                                     19%|        | 630/3250 [59:11<3:56:29,  5.42s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_4/tmp-checkpoint-630
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-630
/projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-630
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-630
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-630
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-630/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-630/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-630/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-630/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.9043, 'learning_rate': 9.10283662844523e-05, 'epoch': 0.19}
{'loss': 0.9107, 'learning_rate': 9.100070582666795e-05, 'epoch': 0.19}
{'loss': 0.9268, 'learning_rate': 9.097300701061434e-05, 'epoch': 0.19}
{'loss': 0.9399, 'learning_rate': 9.094526986220512e-05, 'epoch': 0.2}
{'loss': 0.8719, 'learning_rate': 9.091749440738984e-05, 'epoch': 0.2}
{'loss': 0.9898, 'learning_rate': 9.088968067215383e-05, 'epoch': 0.2}
 19%|        | 631/3250 [59:16<4:19:03,  5.93s/it]                                                     19%|        | 631/3250 [59:16<4:19:03,  5.93s/it] 19%|        | 632/3250 [59:22<4:11:27,  5.76s/it]                                                     19%|        | 632/3250 [59:22<4:11:27,  5.76s/it] 19%|        | 633/3250 [59:27<4:06:25,  5.65s/it]                                                     19%|        | 633/3250 [59:27<4:06:25,  5.65s/it] 20%|        | 634/3250 [59:32<4:02:49,  5.57s/it]                                                     20%|        | 634/3250 [59:32<4:02:49,  5.57s/it] 20%|        | 635/3250 [59:38<4:00:15,  5.51s/it]                                                     20%|        | 635/3250 [59:38<4:00:15,  5.51s/it] 20%|        | 636/3250 [59:43<3:58:21,  5.47s/it]                                                     20%|        | 636/3250 [59:43<3:58:21,  5.47s/it] 20%|    {'loss': 0.9163, 'learning_rate': 9.08618286825183e-05, 'epoch': 0.2}
{'loss': 0.8992, 'learning_rate': 9.08339384645402e-05, 'epoch': 0.2}
{'loss': 0.9051, 'learning_rate': 9.080601004431229e-05, 'epoch': 0.2}
{'loss': 0.8766, 'learning_rate': 9.077804344796302e-05, 'epoch': 0.2}
    | 637/3250 [59:49<3:56:58,  5.44s/it]                                                     20%|        | 637/3250 [59:49<3:56:58,  5.44s/it] 20%|        | 638/3250 [59:54<3:56:04,  5.42s/it]                                                     20%|        | 638/3250 [59:54<3:56:04,  5.42s/it] 20%|        | 639/3250 [59:59<3:55:16,  5.41s/it]                                                     20%|        | 639/3250 [59:59<3:55:16,  5.41s/it] 20%|        | 640/3250 [1:00:05<4:01:21,  5.55s/it]                                                       20%|        | 640/3250 [1:00:05<4:01:21,  5.55s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.9562934041023254, 'eval_runtime': 1.3825, 'eval_samples_per_second': 8.68, 'eval_steps_per_second': 2.17, 'epoch': 0.2}
                                                       20%|        | 640/3250 [1:00:07<4:01:21,  5.55s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_4/tmp-checkpoint-640
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-640I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-640

I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-640
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-640
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-640/pytorch_model.bin
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-640/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-640/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-640/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.9338, 'learning_rate': 9.075003870165657e-05, 'epoch': 0.2}
{'loss': 0.8906, 'learning_rate': 9.072199583159286e-05, 'epoch': 0.2}
{'loss': 0.9541, 'learning_rate': 9.069391486400741e-05, 'epoch': 0.2}
{'loss': 1.3594, 'learning_rate': 9.066579582517139e-05, 'epoch': 0.2}
{'loss': 0.8853, 'learning_rate': 9.063763874139164e-05, 'epoch': 0.2}
{'loss': 0.9054, 'learning_rate': 9.060944363901056e-05, 'epoch': 0.2}
 20%|        | 641/3250 [1:00:12<4:20:50,  6.00s/it]                                                       20%|        | 641/3250 [1:00:12<4:20:50,  6.00s/it] 20%|        | 642/3250 [1:00:18<4:12:36,  5.81s/it]                                                       20%|        | 642/3250 [1:00:18<4:12:36,  5.81s/it] 20%|        | 643/3250 [1:00:23<4:06:48,  5.68s/it]                                                       20%|        | 643/3250 [1:00:23<4:06:48,  5.68s/it] 20%|        | 644/3250 [1:00:28<4:02:37,  5.59s/it]                                                       20%|        | 644/3250 [1:00:28<4:02:37,  5.59s/it] 20%|        | 645/3250 [1:00:34<3:59:46,  5.52s/it]                                                       20%|        | 645/3250 [1:00:34<3:59:46,  5.52s/it] 20%|        | 646/3250 [1:00:39<3:57:46,  5.48s/it]                                                       20%|        | 646/3250 [1:00:39{'loss': 0.9428, 'learning_rate': 9.058121054440612e-05, 'epoch': 0.2}
{'loss': 0.921, 'learning_rate': 9.055293948399179e-05, 'epoch': 0.2}
{'loss': 0.865, 'learning_rate': 9.052463048421665e-05, 'epoch': 0.2}
{'loss': 0.894, 'learning_rate': 9.04962835715652e-05, 'epoch': 0.2}
<3:57:46,  5.48s/it] 20%|        | 647/3250 [1:00:44<3:56:18,  5.45s/it]                                                       20%|        | 647/3250 [1:00:44<3:56:18,  5.45s/it] 20%|        | 648/3250 [1:00:50<3:55:30,  5.43s/it]                                                       20%|        | 648/3250 [1:00:50<3:55:30,  5.43s/it] 20%|        | 649/3250 [1:00:55<3:54:45,  5.42s/it]                                                       20%|        | 649/3250 [1:00:55<3:54:45,  5.42s/it] 20%|        | 650/3250 [1:01:01<3:54:08,  5.40s/it]                                                       20%|        | 650/3250 [1:01:01<3:54:08,  5.40s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.9561387896537781, 'eval_runtime': 1.3817, 'eval_samples_per_second': 8.685, 'eval_steps_per_second': 2.171, 'epoch': 0.2}
                                                       20%|        | 650/3250 [1:01:02<3:54:08,  5.40s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_4/tmp-checkpoint-650
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-650
/projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-650
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-650
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-650
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-650/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-650/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-650/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-650/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.9794, 'learning_rate': 9.046789877255746e-05, 'epoch': 0.2}
{'loss': 0.9124, 'learning_rate': 9.043947611374886e-05, 'epoch': 0.2}
{'loss': 0.9228, 'learning_rate': 9.041101562173023e-05, 'epoch': 0.2}
{'loss': 0.848, 'learning_rate': 9.038251732312783e-05, 'epoch': 0.2}
{'loss': 0.9001, 'learning_rate': 9.035398124460333e-05, 'epoch': 0.2}
{'loss': 0.9131, 'learning_rate': 9.032540741285367e-05, 'epoch': 0.2}
 20%|        | 651/3250 [1:01:08<4:16:34,  5.92s/it]                                                       20%|        | 651/3250 [1:01:08<4:16:34,  5.92s/it] 20%|        | 652/3250 [1:01:13<4:09:18,  5.76s/it]                                                       20%|        | 652/3250 [1:01:13<4:09:18,  5.76s/it] 20%|        | 653/3250 [1:01:19<4:04:21,  5.65s/it]                                                       20%|        | 653/3250 [1:01:19<4:04:21,  5.65s/it] 20%|        | 654/3250 [1:01:24<4:00:38,  5.56s/it]                                                       20%|        | 654/3250 [1:01:24<4:00:38,  5.56s/it] 20%|        | 655/3250 [1:01:29<3:58:07,  5.51s/it]                                                       20%|        | 655/3250 [1:01:29<3:58:07,  5.51s/it] 20%|        | 656/3250 [1:01:35<4:00:45,  5.57s/it]                                                       20%|        | 656/3250 [1:01:35{'loss': 0.8892, 'learning_rate': 9.029679585461113e-05, 'epoch': 0.2}
{'loss': 0.8867, 'learning_rate': 9.026814659664331e-05, 'epoch': 0.2}
{'loss': 0.8936, 'learning_rate': 9.023945966575304e-05, 'epoch': 0.2}
{'loss': 0.9113, 'learning_rate': 9.021073508877845e-05, 'epoch': 0.2}
<4:00:45,  5.57s/it] 20%|        | 657/3250 [1:01:40<3:57:58,  5.51s/it]                                                       20%|        | 657/3250 [1:01:40<3:57:58,  5.51s/it] 20%|        | 658/3250 [1:01:46<3:56:06,  5.47s/it]                                                       20%|        | 658/3250 [1:01:46<3:56:06,  5.47s/it] 20%|        | 659/3250 [1:01:51<3:54:49,  5.44s/it]                                                       20%|        | 659/3250 [1:01:51<3:54:49,  5.44s/it] 20%|        | 660/3250 [1:01:56<3:53:53,  5.42s/it]                                                       20%|        | 660/3250 [1:01:56<3:53:53,  5.42s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.9558538794517517, 'eval_runtime': 1.3866, 'eval_samples_per_second': 8.654, 'eval_steps_per_second': 2.164, 'epoch': 0.2}
                                                       20%|        | 660/3250 [1:01:58<3:53:53,  5.42s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_4/tmp-checkpoint-660
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-660
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-660
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-660
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-660
the pytorch model path isthe pytorch model path is the pytorch model path is/projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-660/pytorch_model.bin 
/projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-660/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-660/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-660/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.9224, 'learning_rate': 9.018197289259285e-05, 'epoch': 0.2}
{'loss': 0.9044, 'learning_rate': 9.015317310410474e-05, 'epoch': 0.2}
{'loss': 0.9031, 'learning_rate': 9.012433575025783e-05, 'epoch': 0.2}
{'loss': 0.9239, 'learning_rate': 9.009546085803092e-05, 'epoch': 0.2}
{'loss': 0.9051, 'learning_rate': 9.006654845443797e-05, 'epoch': 0.2}
{'loss': 0.9272, 'learning_rate': 9.003759856652803e-05, 'epoch': 0.2}
 20%|        | 661/3250 [1:02:04<4:15:16,  5.92s/it]                                                       20%|        | 661/3250 [1:02:04<4:15:16,  5.92s/it] 20%|        | 662/3250 [1:02:09<4:08:00,  5.75s/it]                                                       20%|        | 662/3250 [1:02:09<4:08:00,  5.75s/it] 20%|        | 663/3250 [1:02:14<4:03:07,  5.64s/it]                                                       20%|        | 663/3250 [1:02:14<4:03:07,  5.64s/it] 20%|        | 664/3250 [1:02:20<3:59:45,  5.56s/it]                                                       20%|        | 664/3250 [1:02:20<3:59:45,  5.56s/it] 20%|        | 665/3250 [1:02:25<3:57:10,  5.50s/it]                                                       20%|        | 665/3250 [1:02:25<3:57:10,  5.50s/it] 20%|        | 666/3250 [1:02:30<3:55:19,  5.46s/it]                                                       20%|        | 666/3250 [1:02:30{'loss': 0.9266, 'learning_rate': 9.000861122138517e-05, 'epoch': 0.21}
{'loss': 0.895, 'learning_rate': 8.997958644612861e-05, 'epoch': 0.21}
{'loss': 0.8781, 'learning_rate': 8.995052426791247e-05, 'epoch': 0.21}
{'loss': 0.8781, 'learning_rate': 8.99214247139259e-05, 'epoch': 0.21}
<3:55:19,  5.46s/it] 21%|        | 667/3250 [1:02:36<3:54:02,  5.44s/it]                                                       21%|        | 667/3250 [1:02:36<3:54:02,  5.44s/it] 21%|        | 668/3250 [1:02:41<3:53:11,  5.42s/it]                                                       21%|        | 668/3250 [1:02:41<3:53:11,  5.42s/it] 21%|        | 669/3250 [1:02:47<3:52:24,  5.40s/it]                                                       21%|        | 669/3250 [1:02:47<3:52:24,  5.40s/it] 21%|        | 670/3250 [1:02:52<3:51:47,  5.39s/it]                                                       21%|        | 670/3250 [1:02:52<3:51:47,  5.39s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.9518007040023804, 'eval_runtime': 1.3832, 'eval_samples_per_second': 8.676, 'eval_steps_per_second': 2.169, 'epoch': 0.21}
                                                       21%|        | 670/3250 [1:02:53<3:51:47,  5.39s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_4/tmp-checkpoint-670
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  I AM HERE AND I AM SAVING THE MODEL
/projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-670the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-670

I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-670
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-670
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-670/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-670/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-670/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-670/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.9309, 'learning_rate': 8.989228781139307e-05, 'epoch': 0.21}
{'loss': 0.8688, 'learning_rate': 8.986311358757304e-05, 'epoch': 0.21}
{'loss': 0.9612, 'learning_rate': 8.98339020697598e-05, 'epoch': 0.21}
{'loss': 1.3794, 'learning_rate': 8.980465328528219e-05, 'epoch': 0.21}
{'loss': 0.8321, 'learning_rate': 8.977536726150399e-05, 'epoch': 0.21}
{'loss': 0.8755, 'learning_rate': 8.974604402582379e-05, 'epoch': 0.21}
 21%|        | 671/3250 [1:02:59<4:13:46,  5.90s/it]                                                       21%|        | 671/3250 [1:02:59<4:13:46,  5.90s/it] 21%|        | 672/3250 [1:03:04<4:06:50,  5.74s/it]                                                       21%|        | 672/3250 [1:03:04<4:06:50,  5.74s/it] 21%|        | 673/3250 [1:03:11<4:13:13,  5.90s/it]                                                       21%|        | 673/3250 [1:03:11<4:13:13,  5.90s/it] 21%|        | 674/3250 [1:03:16<4:06:22,  5.74s/it]                                                       21%|        | 674/3250 [1:03:16<4:06:22,  5.74s/it] 21%|        | 675/3250 [1:03:21<4:01:30,  5.63s/it]                                                       21%|        | 675/3250 [1:03:21<4:01:30,  5.63s/it] 21%|        | 676/3250 [1:03:27<3:58:07,  5.55s/it]                                                       21%|        | 676/3250 [1:03:27{'loss': 0.92, 'learning_rate': 8.971668360567496e-05, 'epoch': 0.21}
{'loss': 0.9191, 'learning_rate': 8.968728602852569e-05, 'epoch': 0.21}
{'loss': 0.8767, 'learning_rate': 8.965785132187894e-05, 'epoch': 0.21}
{'loss': 0.8896, 'learning_rate': 8.962837951327236e-05, 'epoch': 0.21}
<3:58:07,  5.55s/it] 21%|        | 677/3250 [1:03:32<3:55:40,  5.50s/it]                                                       21%|        | 677/3250 [1:03:32<3:55:40,  5.50s/it] 21%|        | 678/3250 [1:03:37<3:54:09,  5.46s/it]                                                       21%|        | 678/3250 [1:03:37<3:54:09,  5.46s/it] 21%|        | 679/3250 [1:03:43<3:53:03,  5.44s/it]                                                       21%|        | 679/3250 [1:03:43<3:53:03,  5.44s/it] 21%|        | 680/3250 [1:03:48<3:52:02,  5.42s/it]                                                       21%|        | 680/3250 [1:03:48<3:52:02,  5.42s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.9503593444824219, 'eval_runtime': 1.3917, 'eval_samples_per_second': 8.622, 'eval_steps_per_second': 2.156, 'epoch': 0.21}
                                                       21%|        | 680/3250 [1:03:50<3:52:02,  5.42s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_4/tmp-checkpoint-680
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-680I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in 
 /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-680
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-680
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-680
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-680/pytorch_model.bin
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-680/pytorch_model.bin 
/projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-680/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-680/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.941, 'learning_rate': 8.959887063027837e-05, 'epoch': 0.21}
{'loss': 0.9273, 'learning_rate': 8.956932470050404e-05, 'epoch': 0.21}
{'loss': 0.8884, 'learning_rate': 8.953974175159111e-05, 'epoch': 0.21}
{'loss': 0.8415, 'learning_rate': 8.951012181121594e-05, 'epoch': 0.21}
{'loss': 0.9125, 'learning_rate': 8.948046490708953e-05, 'epoch': 0.21}
{'loss': 0.8865, 'learning_rate': 8.94507710669574e-05, 'epoch': 0.21}
 21%|        | 681/3250 [1:03:55<4:14:08,  5.94s/it]                                                       21%|        | 681/3250 [1:03:55<4:14:08,  5.94s/it] 21%|        | 682/3250 [1:04:01<4:06:52,  5.77s/it]                                                       21%|        | 682/3250 [1:04:01<4:06:52,  5.77s/it] 21%|        | 683/3250 [1:04:06<4:01:52,  5.65s/it]                                                       21%|        | 683/3250 [1:04:06<4:01:52,  5.65s/it] 21%|        | 684/3250 [1:04:12<3:58:26,  5.58s/it]                                                       21%|        | 684/3250 [1:04:12<3:58:26,  5.58s/it] 21%|        | 685/3250 [1:04:17<3:55:49,  5.52s/it]                                                       21%|        | 685/3250 [1:04:17<3:55:49,  5.52s/it] 21%|        | 686/3250 [1:04:22<3:54:03,  5.48s/it]                                                       21%|        | 686/3250 [1:04:22{'loss': 0.9031, 'learning_rate': 8.942104031859972e-05, 'epoch': 0.21}
{'loss': 0.8666, 'learning_rate': 8.939127268983108e-05, 'epoch': 0.21}
{'loss': 0.8809, 'learning_rate': 8.936146820850067e-05, 'epoch': 0.21}
{'loss': 0.8764, 'learning_rate': 8.933162690249208e-05, 'epoch': 0.21}
<3:54:03,  5.48s/it] 21%|        | 687/3250 [1:04:28<3:52:50,  5.45s/it]                                                       21%|        | 687/3250 [1:04:28<3:52:50,  5.45s/it] 21%|        | 688/3250 [1:04:33<3:51:41,  5.43s/it]                                                       21%|        | 688/3250 [1:04:33<3:51:41,  5.43s/it] 21%|        | 689/3250 [1:04:39<3:57:15,  5.56s/it]                                                       21%|        | 689/3250 [1:04:39<3:57:15,  5.56s/it] 21%|        | 690/3250 [1:04:45<3:57:35,  5.57s/it]                                                       21%|        | 690/3250 [1:04:45<3:57:35,  5.57s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.9506756663322449, 'eval_runtime': 1.3853, 'eval_samples_per_second': 8.663, 'eval_steps_per_second': 2.166, 'epoch': 0.21}
                                                       21%|        | 690/3250 [1:04:46<3:57:35,  5.57s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_4/tmp-checkpoint-690
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-690
/projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-690
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-690
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-690
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-690/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-690/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-690/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-690/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.8966, 'learning_rate': 8.930174879972342e-05, 'epoch': 0.21}
{'loss': 0.9037, 'learning_rate': 8.927183392814718e-05, 'epoch': 0.21}
{'loss': 0.8971, 'learning_rate': 8.924188231575024e-05, 'epoch': 0.21}
{'loss': 0.8949, 'learning_rate': 8.921189399055389e-05, 'epoch': 0.21}
{'loss': 0.915, 'learning_rate': 8.918186898061376e-05, 'epoch': 0.21}
{'loss': 0.8733, 'learning_rate': 8.915180731401978e-05, 'epoch': 0.21}
 21%|       | 691/3250 [1:04:52<4:17:16,  6.03s/it]                                                       21%|       | 691/3250 [1:04:52<4:17:16,  6.03s/it] 21%|       | 692/3250 [1:04:57<4:09:26,  5.85s/it]                                                       21%|       | 692/3250 [1:04:57<4:09:26,  5.85s/it] 21%|       | 693/3250 [1:05:02<4:03:09,  5.71s/it]                                                       21%|       | 693/3250 [1:05:02<4:03:09,  5.71s/it] 21%|       | 694/3250 [1:05:10<4:29:58,  6.34s/it]                                                       21%|       | 694/3250 [1:05:10<4:29:58,  6.34s/it] 21%|       | 695/3250 [1:05:16<4:18:03,  6.06s/it]                                                       21%|       | 695/3250 [1:05:16<4:18:03,  6.06s/it] 21%|       | 696/3250 [1:05:21<4:09:49,  5.87s/it]                                                       21%|  {'loss': 0.9593, 'learning_rate': 8.91217090188962e-05, 'epoch': 0.21}
{'loss': 0.8716, 'learning_rate': 8.90915741234015e-05, 'epoch': 0.21}
{'loss': 0.8882, 'learning_rate': 8.906140265572843e-05, 'epoch': 0.22}
{'loss': 0.8842, 'learning_rate': 8.903119464410397e-05, 'epoch': 0.22}
     | 696/3250 [1:05:21<4:09:49,  5.87s/it] 21%|       | 697/3250 [1:05:26<4:03:22,  5.72s/it]                                                       21%|       | 697/3250 [1:05:26<4:03:22,  5.72s/it] 21%|       | 698/3250 [1:05:32<3:58:40,  5.61s/it]                                                       21%|       | 698/3250 [1:05:32<3:58:40,  5.61s/it] 22%|       | 699/3250 [1:05:37<3:55:34,  5.54s/it]                                                       22%|       | 699/3250 [1:05:37<3:55:34,  5.54s/it] 22%|       | 700/3250 [1:05:43<3:53:27,  5.49s/it]                                                       22%|       | 700/3250 [1:05:43<3:53:27,  5.49s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.9463714361190796, 'eval_runtime': 1.3775, 'eval_samples_per_second': 8.711, 'eval_steps_per_second': 2.178, 'epoch': 0.22}
                                                       22%|       | 700/3250 [1:05:44<3:53:27,  5.49s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_4/tmp-checkpoint-700
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-700
/projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-700
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-700
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-700
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-700/pytorch_model.bin
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-700/pytorch_model.bin
 the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-700/pytorch_model.bin
/projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-700/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.8748, 'learning_rate': 8.900095011678924e-05, 'epoch': 0.22}
{'loss': 0.8871, 'learning_rate': 8.897066910207958e-05, 'epoch': 0.22}
{'loss': 0.9094, 'learning_rate': 8.894035162830443e-05, 'epoch': 0.22}
{'loss': 0.9111, 'learning_rate': 8.890999772382732e-05, 'epoch': 0.22}
{'loss': 1.3238, 'learning_rate': 8.887960741704592e-05, 'epoch': 0.22}
{'loss': 0.8631, 'learning_rate': 8.88491807363919e-05, 'epoch': 0.22}
 22%|       | 701/3250 [1:05:50<4:13:43,  5.97s/it]                                                       22%|       | 701/3250 [1:05:50<4:13:43,  5.97s/it] 22%|       | 702/3250 [1:05:55<4:06:00,  5.79s/it]                                                       22%|       | 702/3250 [1:05:55<4:06:00,  5.79s/it] 22%|       | 703/3250 [1:06:00<4:00:28,  5.67s/it]                                                       22%|       | 703/3250 [1:06:00<4:00:28,  5.67s/it] 22%|       | 704/3250 [1:06:06<3:56:38,  5.58s/it]                                                       22%|       | 704/3250 [1:06:06<3:56:38,  5.58s/it] 22%|       | 705/3250 [1:06:12<4:03:47,  5.75s/it]                                                       22%|       | 705/3250 [1:06:12<4:03:47,  5.75s/it] 22%|       | 706/3250 [1:06:17<3:59:10,  5.64s/it]                                                       22%|  {'loss': 0.9071, 'learning_rate': 8.881871771033102e-05, 'epoch': 0.22}
{'loss': 0.9032, 'learning_rate': 8.878821836736297e-05, 'epoch': 0.22}
{'loss': 0.8982, 'learning_rate': 8.875768273602148e-05, 'epoch': 0.22}
{'loss': 0.8684, 'learning_rate': 8.872711084487418e-05, 'epoch': 0.22}
     | 706/3250 [1:06:17<3:59:10,  5.64s/it] 22%|       | 707/3250 [1:06:23<3:55:43,  5.56s/it]                                                       22%|       | 707/3250 [1:06:23<3:55:43,  5.56s/it] 22%|       | 708/3250 [1:06:28<3:53:23,  5.51s/it]                                                       22%|       | 708/3250 [1:06:28<3:53:23,  5.51s/it] 22%|       | 709/3250 [1:06:33<3:51:39,  5.47s/it]                                                       22%|       | 709/3250 [1:06:33<3:51:39,  5.47s/it] 22%|       | 710/3250 [1:06:39<3:50:26,  5.44s/it]                                                       22%|       | 710/3250 [1:06:39<3:50:26,  5.44s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.9442979693412781, 'eval_runtime': 1.4456, 'eval_samples_per_second': 8.301, 'eval_steps_per_second': 2.075, 'epoch': 0.22}
                                                       22%|       | 710/3250 [1:06:40<3:50:26,  5.44s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_4/tmp-checkpoint-710
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  I AM HERE AND I AM SAVING THE MODEL/projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-710
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-710

I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-710
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-710
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-710/pytorch_model.bin
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-710/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-710/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-710/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.9369, 'learning_rate': 8.869650272252267e-05, 'epoch': 0.22}
{'loss': 0.9271, 'learning_rate': 8.866585839760242e-05, 'epoch': 0.22}
{'loss': 0.8834, 'learning_rate': 8.863517789878275e-05, 'epoch': 0.22}
{'loss': 0.8639, 'learning_rate': 8.860446125476687e-05, 'epoch': 0.22}
{'loss': 0.8905, 'learning_rate': 8.857370849429178e-05, 'epoch': 0.22}
{'loss': 0.8836, 'learning_rate': 8.854291964612825e-05, 'epoch': 0.22}
 22%|       | 711/3250 [1:06:46<4:11:54,  5.95s/it]                                                       22%|       | 711/3250 [1:06:46<4:11:54,  5.95s/it] 22%|       | 712/3250 [1:06:51<4:04:36,  5.78s/it]                                                       22%|       | 712/3250 [1:06:51<4:04:36,  5.78s/it] 22%|       | 713/3250 [1:06:57<3:59:41,  5.67s/it]                                                       22%|       | 713/3250 [1:06:57<3:59:41,  5.67s/it] 22%|       | 714/3250 [1:07:02<3:56:03,  5.59s/it]                                                       22%|       | 714/3250 [1:07:02<3:56:03,  5.59s/it] 22%|       | 715/3250 [1:07:08<3:53:36,  5.53s/it]                                                       22%|       | 715/3250 [1:07:08<3:53:36,  5.53s/it] 22%|       | 716/3250 [1:07:13<3:51:48,  5.49s/it]                                                       22%|  {'loss': 0.8823, 'learning_rate': 8.851209473908083e-05, 'epoch': 0.22}
{'loss': 0.8792, 'learning_rate': 8.848123380198783e-05, 'epoch': 0.22}
{'loss': 0.8609, 'learning_rate': 8.845033686372123e-05, 'epoch': 0.22}
{'loss': 0.8865, 'learning_rate': 8.84194039531867e-05, 'epoch': 0.22}
     | 716/3250 [1:07:13<3:51:48,  5.49s/it] 22%|       | 717/3250 [1:07:18<3:50:27,  5.46s/it]                                                       22%|       | 717/3250 [1:07:18<3:50:27,  5.46s/it] 22%|       | 718/3250 [1:07:24<3:49:33,  5.44s/it]                                                       22%|       | 718/3250 [1:07:24<3:49:33,  5.44s/it] 22%|       | 719/3250 [1:07:29<3:48:50,  5.42s/it]                                                       22%|       | 719/3250 [1:07:29<3:48:50,  5.42s/it] 22%|       | 720/3250 [1:07:34<3:48:19,  5.41s/it]                                                       22%|       | 720/3250 [1:07:34<3:48:19,  5.41s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.9449717998504639, 'eval_runtime': 1.3864, 'eval_samples_per_second': 8.656, 'eval_steps_per_second': 2.164, 'epoch': 0.22}
                                                       22%|       | 720/3250 [1:07:36<3:48:19,  5.41s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_4/tmp-checkpoint-720
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-720
/projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-720
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-720
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-720
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-720/pytorch_model.bin
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-720/pytorch_model.bin 
/projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-720/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-720/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.8881, 'learning_rate': 8.83884350993236e-05, 'epoch': 0.22}
{'loss': 0.8709, 'learning_rate': 8.835743033110482e-05, 'epoch': 0.22}
{'loss': 0.8861, 'learning_rate': 8.832638967753699e-05, 'epoch': 0.22}
{'loss': 0.8981, 'learning_rate': 8.82953131676602e-05, 'epoch': 0.22}
{'loss': 0.9117, 'learning_rate': 8.826420083054812e-05, 'epoch': 0.22}
{'loss': 0.8348, 'learning_rate': 8.823305269530795e-05, 'epoch': 0.22}
 22%|       | 721/3250 [1:07:42<4:09:33,  5.92s/it]                                                       22%|       | 721/3250 [1:07:42<4:09:33,  5.92s/it] 22%|       | 722/3250 [1:07:47<4:08:00,  5.89s/it]                                                       22%|       | 722/3250 [1:07:47<4:08:00,  5.89s/it] 22%|       | 723/3250 [1:07:53<4:01:55,  5.74s/it]                                                       22%|       | 723/3250 [1:07:53<4:01:55,  5.74s/it] 22%|       | 724/3250 [1:07:58<3:57:23,  5.64s/it]                                                       22%|       | 724/3250 [1:07:58<3:57:23,  5.64s/it] 22%|       | 725/3250 [1:08:04<3:53:58,  5.56s/it]                                                       22%|       | 725/3250 [1:08:04<3:53:58,  5.56s/it] 22%|       | 726/3250 [1:08:09<3:51:44,  5.51s/it]                                                       22%|  {'loss': 0.9562, 'learning_rate': 8.820186879108038e-05, 'epoch': 0.22}
{'loss': 0.8846, 'learning_rate': 8.817064914703954e-05, 'epoch': 0.22}
{'loss': 0.8677, 'learning_rate': 8.813939379239303e-05, 'epoch': 0.22}
{'loss': 0.8798, 'learning_rate': 8.810810275638183e-05, 'epoch': 0.22}
     | 726/3250 [1:08:09<3:51:44,  5.51s/it] 22%|       | 727/3250 [1:08:14<3:50:07,  5.47s/it]                                                       22%|       | 727/3250 [1:08:14<3:50:07,  5.47s/it] 22%|       | 728/3250 [1:08:20<3:49:04,  5.45s/it]                                                       22%|       | 728/3250 [1:08:20<3:49:04,  5.45s/it] 22%|       | 729/3250 [1:08:25<3:48:20,  5.43s/it]                                                       22%|       | 729/3250 [1:08:25<3:48:20,  5.43s/it] 22%|       | 730/3250 [1:08:31<3:47:51,  5.43s/it]                                                       22%|       | 730/3250 [1:08:31<3:47:51,  5.43s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.9416534900665283, 'eval_runtime': 1.4054, 'eval_samples_per_second': 8.539, 'eval_steps_per_second': 2.135, 'epoch': 0.22}
                                                       22%|       | 730/3250 [1:08:32<3:47:51,  5.43s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_4/tmp-checkpoint-730
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-730
/projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-730
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-730
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-730
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-730/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-730/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-730/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-730/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.8355, 'learning_rate': 8.80767760682803e-05, 'epoch': 0.22}
{'loss': 0.9118, 'learning_rate': 8.804541375739623e-05, 'epoch': 0.23}
{'loss': 0.8684, 'learning_rate': 8.801401585307058e-05, 'epoch': 0.23}
{'loss': 0.9155, 'learning_rate': 8.79825823846778e-05, 'epoch': 0.23}
{'loss': 1.3357, 'learning_rate': 8.795111338162544e-05, 'epoch': 0.23}
{'loss': 0.8646, 'learning_rate': 8.791960887335441e-05, 'epoch': 0.23}
 22%|       | 731/3250 [1:08:38<4:10:07,  5.96s/it]                                                       22%|       | 731/3250 [1:08:38<4:10:07,  5.96s/it] 23%|       | 732/3250 [1:08:43<4:02:36,  5.78s/it]                                                       23%|       | 732/3250 [1:08:43<4:02:36,  5.78s/it] 23%|       | 733/3250 [1:08:49<3:57:31,  5.66s/it]                                                       23%|       | 733/3250 [1:08:49<3:57:31,  5.66s/it] 23%|       | 734/3250 [1:08:54<3:53:51,  5.58s/it]                                                       23%|       | 734/3250 [1:08:54<3:53:51,  5.58s/it] 23%|       | 735/3250 [1:08:59<3:51:01,  5.51s/it]                                                       23%|       | 735/3250 [1:08:59<3:51:01,  5.51s/it] 23%|       | 736/3250 [1:09:05<3:49:17,  5.47s/it]                                                       23%|  {'loss': 0.8695, 'learning_rate': 8.788806888933881e-05, 'epoch': 0.23}
{'loss': 0.9105, 'learning_rate': 8.785649345908588e-05, 'epoch': 0.23}
{'loss': 0.9038, 'learning_rate': 8.782488261213608e-05, 'epoch': 0.23}
{'loss': 0.8492, 'learning_rate': 8.779323637806299e-05, 'epoch': 0.23}
     | 736/3250 [1:09:05<3:49:17,  5.47s/it] 23%|       | 737/3250 [1:09:10<3:47:52,  5.44s/it]                                                       23%|       | 737/3250 [1:09:10<3:47:52,  5.44s/it] 23%|       | 738/3250 [1:09:16<3:50:02,  5.49s/it]                                                       23%|       | 738/3250 [1:09:16<3:50:02,  5.49s/it] 23%|       | 739/3250 [1:09:21<3:48:32,  5.46s/it]                                                       23%|       | 739/3250 [1:09:21<3:48:32,  5.46s/it] 23%|       | 740/3250 [1:09:26<3:47:20,  5.43s/it]                                                       23%|       | 740/3250 [1:09:26<3:47:20,  5.43s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.9398583769798279, 'eval_runtime': 1.6156, 'eval_samples_per_second': 7.428, 'eval_steps_per_second': 1.857, 'epoch': 0.23}
                                                       23%|       | 740/3250 [1:09:28<3:47:20,  5.43s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_4/tmp-checkpoint-740
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-740I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-740

I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-740
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-740
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-740/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-740/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-740/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-740/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.8586, 'learning_rate': 8.776155478647326e-05, 'epoch': 0.23}
{'loss': 0.9515, 'learning_rate': 8.772983786700668e-05, 'epoch': 0.23}
{'loss': 0.8689, 'learning_rate': 8.769808564933605e-05, 'epoch': 0.23}
{'loss': 0.888, 'learning_rate': 8.766629816316721e-05, 'epoch': 0.23}
{'loss': 0.8241, 'learning_rate': 8.763447543823896e-05, 'epoch': 0.23}
{'loss': 0.8616, 'learning_rate': 8.760261750432313e-05, 'epoch': 0.23}
 23%|       | 741/3250 [1:09:34<4:11:16,  6.01s/it]                                                       23%|       | 741/3250 [1:09:34<4:11:16,  6.01s/it] 23%|       | 742/3250 [1:09:39<4:03:23,  5.82s/it]                                                       23%|       | 742/3250 [1:09:39<4:03:23,  5.82s/it] 23%|       | 743/3250 [1:09:44<3:57:42,  5.69s/it]                                                       23%|       | 743/3250 [1:09:44<3:57:42,  5.69s/it] 23%|       | 744/3250 [1:09:50<3:53:49,  5.60s/it]                                                       23%|       | 744/3250 [1:09:50<3:53:49,  5.60s/it] 23%|       | 745/3250 [1:09:55<3:50:59,  5.53s/it]                                                       23%|       | 745/3250 [1:09:55<3:50:59,  5.53s/it] 23%|       | 746/3250 [1:10:01<3:48:58,  5.49s/it]                                                       23%|  {'loss': 0.8904, 'learning_rate': 8.757072439122444e-05, 'epoch': 0.23}
{'loss': 0.874, 'learning_rate': 8.753879612878054e-05, 'epoch': 0.23}
{'loss': 0.8527, 'learning_rate': 8.750683274686196e-05, 'epoch': 0.23}
{'loss': 0.8636, 'learning_rate': 8.747483427537209e-05, 'epoch': 0.23}
     | 746/3250 [1:10:01<3:48:58,  5.49s/it] 23%|       | 747/3250 [1:10:06<3:47:36,  5.46s/it]                                                       23%|       | 747/3250 [1:10:06<3:47:36,  5.46s/it] 23%|       | 748/3250 [1:10:11<3:46:31,  5.43s/it]                                                       23%|       | 748/3250 [1:10:11<3:46:31,  5.43s/it] 23%|       | 749/3250 [1:10:17<3:45:54,  5.42s/it]                                                       23%|       | 749/3250 [1:10:17<3:45:54,  5.42s/it] 23%|       | 750/3250 [1:10:22<3:45:24,  5.41s/it]                                                       23%|       | 750/3250 [1:10:22<3:45:24,  5.41s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.9403356313705444, 'eval_runtime': 1.3835, 'eval_samples_per_second': 8.674, 'eval_steps_per_second': 2.168, 'epoch': 0.23}
                                                       23%|       | 750/3250 [1:10:24<3:45:24,  5.41s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_4/tmp-checkpoint-750
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-750
/projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-750
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-750
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-750
the pytorch model path is the pytorch model path is/projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-750/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-750/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-750/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-750/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.8845, 'learning_rate': 8.744280074424713e-05, 'epoch': 0.23}
{'loss': 0.8922, 'learning_rate': 8.741073218345614e-05, 'epoch': 0.23}
{'loss': 0.8888, 'learning_rate': 8.737862862300085e-05, 'epoch': 0.23}
{'loss': 0.8802, 'learning_rate': 8.734649009291585e-05, 'epoch': 0.23}
{'loss': 0.9064, 'learning_rate': 8.731431662326835e-05, 'epoch': 0.23}
{'loss': 0.8738, 'learning_rate': 8.728210824415827e-05, 'epoch': 0.23}
 23%|       | 751/3250 [1:10:29<4:06:06,  5.91s/it]                                                       23%|       | 751/3250 [1:10:29<4:06:06,  5.91s/it] 23%|       | 752/3250 [1:10:35<3:59:34,  5.75s/it]                                                       23%|       | 752/3250 [1:10:35<3:59:34,  5.75s/it] 23%|       | 753/3250 [1:10:40<3:54:51,  5.64s/it]                                                       23%|       | 753/3250 [1:10:40<3:54:51,  5.64s/it] 23%|       | 754/3250 [1:10:46<3:56:13,  5.68s/it]                                                       23%|       | 754/3250 [1:10:46<3:56:13,  5.68s/it] 23%|       | 755/3250 [1:10:51<3:52:56,  5.60s/it]                                                       23%|       | 755/3250 [1:10:51<3:52:56,  5.60s/it] 23%|       | 756/3250 [1:10:57<3:50:43,  5.55s/it]                                                       23%|  {'loss': 0.8842, 'learning_rate': 8.724986498571828e-05, 'epoch': 0.23}
{'loss': 0.905, 'learning_rate': 8.721758687811352e-05, 'epoch': 0.23}
{'loss': 0.863, 'learning_rate': 8.718527395154187e-05, 'epoch': 0.23}
{'loss': 0.8493, 'learning_rate': 8.715292623623373e-05, 'epoch': 0.23}
     | 756/3250 [1:10:57<3:50:43,  5.55s/it] 23%|       | 757/3250 [1:11:02<3:48:52,  5.51s/it]                                                       23%|       | 757/3250 [1:11:02<3:48:52,  5.51s/it] 23%|       | 758/3250 [1:11:07<3:47:28,  5.48s/it]                                                       23%|       | 758/3250 [1:11:07<3:47:28,  5.48s/it] 23%|       | 759/3250 [1:11:13<3:46:43,  5.46s/it]                                                       23%|       | 759/3250 [1:11:13<3:46:43,  5.46s/it] 23%|       | 760/3250 [1:11:18<3:46:00,  5.45s/it]                                                       23%|       | 760/3250 [1:11:18<3:46:00,  5.45s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.9382234811782837, 'eval_runtime': 1.3975, 'eval_samples_per_second': 8.587, 'eval_steps_per_second': 2.147, 'epoch': 0.23}
                                                       23%|       | 760/3250 [1:11:20<3:46:00,  5.45s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_4/tmp-checkpoint-760
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-760 /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-760

I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-760
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-760
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-760/pytorch_model.bin
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-760/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-760/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-760/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.851, 'learning_rate': 8.712054376245202e-05, 'epoch': 0.23}
{'loss': 0.9105, 'learning_rate': 8.708812656049226e-05, 'epoch': 0.23}
{'loss': 0.8429, 'learning_rate': 8.705567466068237e-05, 'epoch': 0.23}
{'loss': 0.9407, 'learning_rate': 8.702318809338278e-05, 'epoch': 0.24}
{'loss': 1.3635, 'learning_rate': 8.699066688898635e-05, 'epoch': 0.24}
{'loss': 0.7989, 'learning_rate': 8.695811107791836e-05, 'epoch': 0.24}
 23%|       | 761/3250 [1:11:25<4:07:59,  5.98s/it]                                                       23%|       | 761/3250 [1:11:26<4:07:59,  5.98s/it] 23%|       | 762/3250 [1:11:31<4:00:30,  5.80s/it]                                                       23%|       | 762/3250 [1:11:31<4:00:30,  5.80s/it] 23%|       | 763/3250 [1:11:36<3:55:11,  5.67s/it]                                                       23%|       | 763/3250 [1:11:36<3:55:11,  5.67s/it] 24%|       | 764/3250 [1:11:42<3:51:38,  5.59s/it]                                                       24%|       | 764/3250 [1:11:42<3:51:38,  5.59s/it] 24%|       | 765/3250 [1:11:47<3:49:00,  5.53s/it]                                                       24%|       | 765/3250 [1:11:47<3:49:00,  5.53s/it] 24%|       | 766/3250 [1:11:52<3:47:03,  5.48s/it]                                                       24%|  {'loss': 0.8503, 'learning_rate': 8.692552069063641e-05, 'epoch': 0.24}
{'loss': 0.8926, 'learning_rate': 8.689289575763051e-05, 'epoch': 0.24}
{'loss': 0.8853, 'learning_rate': 8.686023630942292e-05, 'epoch': 0.24}
{'loss': 0.8517, 'learning_rate': 8.68275423765683e-05, 'epoch': 0.24}
     | 766/3250 [1:11:52<3:47:03,  5.48s/it] 24%|       | 767/3250 [1:11:58<3:45:55,  5.46s/it]                                                       24%|       | 767/3250 [1:11:58<3:45:55,  5.46s/it] 24%|       | 768/3250 [1:12:03<3:44:57,  5.44s/it]                                                       24%|       | 768/3250 [1:12:03<3:44:57,  5.44s/it] 24%|       | 769/3250 [1:12:09<3:44:28,  5.43s/it]                                                       24%|       | 769/3250 [1:12:09<3:44:28,  5.43s/it] 24%|       | 770/3250 [1:12:14<3:43:56,  5.42s/it]                                                       24%|       | 770/3250 [1:12:14<3:43:56,  5.42s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.9360669851303101, 'eval_runtime': 1.3945, 'eval_samples_per_second': 8.605, 'eval_steps_per_second': 2.151, 'epoch': 0.24}
                                                       24%|       | 770/3250 [1:12:15<3:43:56,  5.42s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_4/tmp-checkpoint-770
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-770I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-770

I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-770
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-770
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-770/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-770/pytorch_model.binthe pytorch model path is
 /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-770/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-770/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.8705, 'learning_rate': 8.679481398965347e-05, 'epoch': 0.24}
{'loss': 0.9026, 'learning_rate': 8.676205117929752e-05, 'epoch': 0.24}
{'loss': 0.895, 'learning_rate': 8.672925397615173e-05, 'epoch': 0.24}
{'loss': 0.8521, 'learning_rate': 8.669642241089959e-05, 'epoch': 0.24}
{'loss': 0.8138, 'learning_rate': 8.666355651425672e-05, 'epoch': 0.24}
{'loss': 0.871, 'learning_rate': 8.663065631697085e-05, 'epoch': 0.24}
 24%|       | 771/3250 [1:12:21<4:07:47,  6.00s/it]                                                       24%|       | 771/3250 [1:12:21<4:07:47,  6.00s/it] 24%|       | 772/3250 [1:12:27<4:00:07,  5.81s/it]                                                       24%|       | 772/3250 [1:12:27<4:00:07,  5.81s/it] 24%|       | 773/3250 [1:12:32<3:54:51,  5.69s/it]                                                       24%|       | 773/3250 [1:12:32<3:54:51,  5.69s/it] 24%|       | 774/3250 [1:12:38<3:51:07,  5.60s/it]                                                       24%|       | 774/3250 [1:12:38<3:51:07,  5.60s/it] 24%|       | 775/3250 [1:12:43<3:48:38,  5.54s/it]                                                       24%|       | 775/3250 [1:12:43<3:48:38,  5.54s/it] 24%|       | 776/3250 [1:12:48<3:46:47,  5.50s/it]                                                       24%|  {'loss': 0.8606, 'learning_rate': 8.65977218498218e-05, 'epoch': 0.24}
{'loss': 0.8679, 'learning_rate': 8.656475314362148e-05, 'epoch': 0.24}
{'loss': 0.8489, 'learning_rate': 8.65317502292138e-05, 'epoch': 0.24}
{'loss': 0.8571, 'learning_rate': 8.649871313747466e-05, 'epoch': 0.24}
     | 776/3250 [1:12:48<3:46:47,  5.50s/it] 24%|       | 777/3250 [1:12:54<3:45:29,  5.47s/it]                                                       24%|       | 777/3250 [1:12:54<3:45:29,  5.47s/it] 24%|       | 778/3250 [1:12:59<3:44:22,  5.45s/it]                                                       24%|       | 778/3250 [1:12:59<3:44:22,  5.45s/it] 24%|       | 779/3250 [1:13:05<3:43:51,  5.44s/it]                                                       24%|       | 779/3250 [1:13:05<3:43:51,  5.44s/it] 24%|       | 780/3250 [1:13:10<3:43:20,  5.43s/it]                                                       24%|       | 780/3250 [1:13:10<3:43:20,  5.43s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.9353591203689575, 'eval_runtime': 1.6376, 'eval_samples_per_second': 7.328, 'eval_steps_per_second': 1.832, 'epoch': 0.24}
                                                       24%|       | 780/3250 [1:13:12<3:43:20,  5.43s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_4/tmp-checkpoint-780
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-780I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-780

I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-780
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-780
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-780/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-780/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-780/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-780/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.8646, 'learning_rate': 8.646564189931199e-05, 'epoch': 0.24}
{'loss': 0.877, 'learning_rate': 8.64325365456656e-05, 'epoch': 0.24}
{'loss': 0.8772, 'learning_rate': 8.63993971075073e-05, 'epoch': 0.24}
{'loss': 0.884, 'learning_rate': 8.636622361584072e-05, 'epoch': 0.24}
{'loss': 0.856, 'learning_rate': 8.633301610170135e-05, 'epoch': 0.24}
{'loss': 0.89, 'learning_rate': 8.629977459615655e-05, 'epoch': 0.24}
 24%|       | 781/3250 [1:13:17<4:06:29,  5.99s/it]                                                       24%|       | 781/3250 [1:13:17<4:06:29,  5.99s/it] 24%|       | 782/3250 [1:13:23<3:58:57,  5.81s/it]                                                       24%|       | 782/3250 [1:13:23<3:58:57,  5.81s/it] 24%|       | 783/3250 [1:13:28<3:53:28,  5.68s/it]                                                       24%|       | 783/3250 [1:13:28<3:53:28,  5.68s/it] 24%|       | 784/3250 [1:13:33<3:49:50,  5.59s/it]                                                       24%|       | 784/3250 [1:13:33<3:49:50,  5.59s/it] 24%|       | 785/3250 [1:13:39<3:47:07,  5.53s/it]                                                       24%|       | 785/3250 [1:13:39<3:47:07,  5.53s/it] 24%|       | 786/3250 [1:13:44<3:45:23,  5.49s/it]                                                       24%|  {'loss': 0.8335, 'learning_rate': 8.626649913030545e-05, 'epoch': 0.24}
{'loss': 0.9456, 'learning_rate': 8.623318973527893e-05, 'epoch': 0.24}
{'loss': 0.8481, 'learning_rate': 8.619984644223968e-05, 'epoch': 0.24}
{'loss': 0.8631, 'learning_rate': 8.616646928238205e-05, 'epoch': 0.24}
     | 786/3250 [1:13:44<3:45:23,  5.49s/it] 24%|       | 787/3250 [1:13:50<3:48:51,  5.58s/it]                                                       24%|       | 787/3250 [1:13:50<3:48:51,  5.58s/it] 24%|       | 788/3250 [1:13:55<3:46:22,  5.52s/it]                                                       24%|       | 788/3250 [1:13:55<3:46:22,  5.52s/it] 24%|       | 789/3250 [1:14:01<3:44:40,  5.48s/it]                                                       24%|       | 789/3250 [1:14:01<3:44:40,  5.48s/it] 24%|       | 790/3250 [1:14:06<3:43:30,  5.45s/it]                                                       24%|       | 790/3250 [1:14:06<3:43:30,  5.45s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.9334472417831421, 'eval_runtime': 1.3928, 'eval_samples_per_second': 8.616, 'eval_steps_per_second': 2.154, 'epoch': 0.24}
                                                       24%|       | 790/3250 [1:14:08<3:43:30,  5.45s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_4/tmp-checkpoint-790
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in   /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-790
/projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-790
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-790
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-790
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-790/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-790/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-790/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-790/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.8602, 'learning_rate': 8.613305828693212e-05, 'epoch': 0.24}
{'loss': 0.8527, 'learning_rate': 8.609961348714756e-05, 'epoch': 0.24}
{'loss': 0.8628, 'learning_rate': 8.60661349143177e-05, 'epoch': 0.24}
{'loss': 0.882, 'learning_rate': 8.603262259976348e-05, 'epoch': 0.24}
{'loss': 0.8864, 'learning_rate': 8.59990765748374e-05, 'epoch': 0.24}
{'loss': 1.3108, 'learning_rate': 8.596549687092348e-05, 'epoch': 0.24}
 24%|       | 791/3250 [1:14:13<4:03:36,  5.94s/it]                                                       24%|       | 791/3250 [1:14:13<4:03:36,  5.94s/it] 24%|       | 792/3250 [1:14:19<3:56:55,  5.78s/it]                                                       24%|       | 792/3250 [1:14:19<3:56:55,  5.78s/it] 24%|       | 793/3250 [1:14:24<3:52:07,  5.67s/it]                                                       24%|       | 793/3250 [1:14:24<3:52:07,  5.67s/it] 24%|       | 794/3250 [1:14:29<3:48:37,  5.59s/it]                                                       24%|       | 794/3250 [1:14:29<3:48:37,  5.59s/it] 24%|       | 795/3250 [1:14:35<3:46:15,  5.53s/it]                                                       24%|       | 795/3250 [1:14:35<3:46:15,  5.53s/it] 24%|       | 796/3250 [1:14:40<3:44:20,  5.48s/it]                                                       24%|  {'loss': 0.8488, 'learning_rate': 8.593188351943726e-05, 'epoch': 0.25}
{'loss': 0.8761, 'learning_rate': 8.589823655182576e-05, 'epoch': 0.25}
{'loss': 0.8778, 'learning_rate': 8.586455599956746e-05, 'epoch': 0.25}
{'loss': 0.8745, 'learning_rate': 8.583084189417224e-05, 'epoch': 0.25}
     | 796/3250 [1:14:40<3:44:20,  5.48s/it] 25%|       | 797/3250 [1:14:46<3:43:07,  5.46s/it]                                                       25%|       | 797/3250 [1:14:46<3:43:07,  5.46s/it] 25%|       | 798/3250 [1:14:51<3:42:00,  5.43s/it]                                                       25%|       | 798/3250 [1:14:51<3:42:00,  5.43s/it] 25%|       | 799/3250 [1:14:56<3:41:22,  5.42s/it]                                                       25%|       | 799/3250 [1:14:56<3:41:22,  5.42s/it] 25%|       | 800/3250 [1:15:02<3:40:50,  5.41s/it]                                                       25%|       | 800/3250 [1:15:02<3:40:50,  5.41s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.9311189651489258, 'eval_runtime': 1.3807, 'eval_samples_per_second': 8.691, 'eval_steps_per_second': 2.173, 'epoch': 0.25}
                                                       25%|       | 800/3250 [1:15:03<3:40:50,  5.41s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_4/tmp-checkpoint-800
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-800
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-800
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-800
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-800
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-800/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-800/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-800/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-800/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.8281, 'learning_rate': 8.579709426718137e-05, 'epoch': 0.25}
{'loss': 0.91, 'learning_rate': 8.576331315016753e-05, 'epoch': 0.25}
{'loss': 0.8975, 'learning_rate': 8.572949857473462e-05, 'epoch': 0.25}
{'loss': 0.8481, 'learning_rate': 8.569565057251799e-05, 'epoch': 0.25}
{'loss': 0.8409, 'learning_rate': 8.566176917518416e-05, 'epoch': 0.25}
{'loss': 0.8382, 'learning_rate': 8.56278544144309e-05, 'epoch': 0.25}
 25%|       | 801/3250 [1:15:09<4:02:02,  5.93s/it]                                                       25%|       | 801/3250 [1:15:09<4:02:02,  5.93s/it] 25%|       | 802/3250 [1:15:14<3:55:12,  5.76s/it]                                                       25%|       | 802/3250 [1:15:14<3:55:12,  5.76s/it] 25%|       | 803/3250 [1:15:20<3:50:20,  5.65s/it]                                                       25%|       | 803/3250 [1:15:20<3:50:20,  5.65s/it] 25%|       | 804/3250 [1:15:26<3:54:34,  5.75s/it]                                                       25%|       | 804/3250 [1:15:26<3:54:34,  5.75s/it] 25%|       | 805/3250 [1:15:31<3:49:51,  5.64s/it]                                                       25%|       | 805/3250 [1:15:31<3:49:51,  5.64s/it] 25%|       | 806/3250 [1:15:36<3:46:27,  5.56s/it]                                                       25%|  {'loss': 0.8369, 'learning_rate': 8.559390632198723e-05, 'epoch': 0.25}
{'loss': 0.8539, 'learning_rate': 8.555992492961334e-05, 'epoch': 0.25}
{'loss': 0.8555, 'learning_rate': 8.552591026910058e-05, 'epoch': 0.25}
{'loss': 0.8357, 'learning_rate': 8.549186237227138e-05, 'epoch': 0.25}
     | 806/3250 [1:15:36<3:46:27,  5.56s/it] 25%|       | 807/3250 [1:15:42<3:44:01,  5.50s/it]                                                       25%|       | 807/3250 [1:15:42<3:44:01,  5.50s/it] 25%|       | 808/3250 [1:15:47<3:42:28,  5.47s/it]                                                       25%|       | 808/3250 [1:15:47<3:42:28,  5.47s/it] 25%|       | 809/3250 [1:15:53<3:41:08,  5.44s/it]                                                       25%|       | 809/3250 [1:15:53<3:41:08,  5.44s/it] 25%|       | 810/3250 [1:15:58<3:40:17,  5.42s/it]                                                       25%|       | 810/3250 [1:15:58<3:40:17,  5.42s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.9308554530143738, 'eval_runtime': 1.3775, 'eval_samples_per_second': 8.711, 'eval_steps_per_second': 2.178, 'epoch': 0.25}
                                                       25%|       | 810/3250 [1:15:59<3:40:17,  5.42s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_4/tmp-checkpoint-810
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in I AM HERE AND I AM SAVING THE MODEL 
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-810
/projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-810
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-810
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-810
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-810/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-810/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-810/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-810/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.8712, 'learning_rate': 8.545778127097933e-05, 'epoch': 0.25}
{'loss': 0.8641, 'learning_rate': 8.542366699710905e-05, 'epoch': 0.25}
{'loss': 0.8482, 'learning_rate': 8.538951958257617e-05, 'epoch': 0.25}
{'loss': 0.8673, 'learning_rate': 8.535533905932738e-05, 'epoch': 0.25}
{'loss': 0.8703, 'learning_rate': 8.532112545934032e-05, 'epoch': 0.25}
{'loss': 0.8781, 'learning_rate': 8.528687881462357e-05, 'epoch': 0.25}
 25%|       | 811/3250 [1:16:05<4:00:53,  5.93s/it]                                                       25%|       | 811/3250 [1:16:05<4:00:53,  5.93s/it] 25%|       | 812/3250 [1:16:10<3:54:00,  5.76s/it]                                                       25%|       | 812/3250 [1:16:10<3:54:00,  5.76s/it] 25%|       | 813/3250 [1:16:16<3:49:16,  5.64s/it]                                                       25%|       | 813/3250 [1:16:16<3:49:16,  5.64s/it] 25%|       | 814/3250 [1:16:21<3:45:49,  5.56s/it]                                                       25%|       | 814/3250 [1:16:21<3:45:49,  5.56s/it] 25%|       | 815/3250 [1:16:26<3:43:32,  5.51s/it]                                                       25%|       | 815/3250 [1:16:26<3:43:32,  5.51s/it] 25%|       | 816/3250 [1:16:32<3:41:41,  5.46s/it]                                                       25%|  {'loss': 0.8133, 'learning_rate': 8.52525991572166e-05, 'epoch': 0.25}
{'loss': 0.9326, 'learning_rate': 8.521828651918981e-05, 'epoch': 0.25}
{'loss': 0.8713, 'learning_rate': 8.518394093264448e-05, 'epoch': 0.25}
{'loss': 0.8482, 'learning_rate': 8.514956242971262e-05, 'epoch': 0.25}
     | 816/3250 [1:16:32<3:41:41,  5.46s/it] 25%|       | 817/3250 [1:16:37<3:40:28,  5.44s/it]                                                       25%|       | 817/3250 [1:16:37<3:40:28,  5.44s/it] 25%|       | 818/3250 [1:16:43<3:39:32,  5.42s/it]                                                       25%|       | 818/3250 [1:16:43<3:39:32,  5.42s/it] 25%|       | 819/3250 [1:16:48<3:38:45,  5.40s/it]                                                       25%|       | 819/3250 [1:16:48<3:38:45,  5.40s/it] 25%|       | 820/3250 [1:16:54<3:41:35,  5.47s/it]                                                       25%|       | 820/3250 [1:16:54<3:41:35,  5.47s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.9297117590904236, 'eval_runtime': 1.3894, 'eval_samples_per_second': 8.637, 'eval_steps_per_second': 2.159, 'epoch': 0.25}
                                                       25%|       | 820/3250 [1:16:55<3:41:35,  5.47s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_4/tmp-checkpoint-820
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-820I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-820

I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-820
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-820
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-820/pytorch_model.bin
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-820/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-820/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-820/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.8518, 'learning_rate': 8.51151510425571e-05, 'epoch': 0.25}
{'loss': 0.8209, 'learning_rate': 8.508070680337152e-05, 'epoch': 0.25}
{'loss': 0.8834, 'learning_rate': 8.504622974438028e-05, 'epoch': 0.25}
{'loss': 0.8354, 'learning_rate': 8.501171989783845e-05, 'epoch': 0.25}
{'loss': 0.8884, 'learning_rate': 8.497717729603172e-05, 'epoch': 0.25}
{'loss': 1.3095, 'learning_rate': 8.49426019712765e-05, 'epoch': 0.25}
 25%|       | 821/3250 [1:17:01<4:00:10,  5.93s/it]                                                       25%|       | 821/3250 [1:17:01<4:00:10,  5.93s/it] 25%|       | 822/3250 [1:17:06<3:53:20,  5.77s/it]                                                       25%|       | 822/3250 [1:17:06<3:53:20,  5.77s/it] 25%|       | 823/3250 [1:17:11<3:48:21,  5.65s/it]                                                       25%|       | 823/3250 [1:17:11<3:48:21,  5.65s/it] 25%|       | 824/3250 [1:17:17<3:44:57,  5.56s/it]                                                       25%|       | 824/3250 [1:17:17<3:44:57,  5.56s/it] 25%|       | 825/3250 [1:17:22<3:42:35,  5.51s/it]                                                       25%|       | 825/3250 [1:17:22<3:42:35,  5.51s/it] 25%|       | 826/3250 [1:17:27<3:40:45,  5.46s/it]                                                       25%|  {'loss': 0.8413, 'learning_rate': 8.490799395591977e-05, 'epoch': 0.25}
{'loss': 0.8373, 'learning_rate': 8.487335328233912e-05, 'epoch': 0.25}
{'loss': 0.8942, 'learning_rate': 8.483867998294266e-05, 'epoch': 0.26}
{'loss': 0.8691, 'learning_rate': 8.480397409016909e-05, 'epoch': 0.26}
     | 826/3250 [1:17:27<3:40:45,  5.46s/it] 25%|       | 827/3250 [1:17:33<3:39:31,  5.44s/it]                                                       25%|       | 827/3250 [1:17:33<3:39:31,  5.44s/it] 25%|       | 828/3250 [1:17:38<3:38:29,  5.41s/it]                                                       25%|       | 828/3250 [1:17:38<3:38:29,  5.41s/it] 26%|       | 829/3250 [1:17:44<3:37:58,  5.40s/it]                                                       26%|       | 829/3250 [1:17:44<3:37:58,  5.40s/it] 26%|       | 830/3250 [1:17:49<3:37:36,  5.40s/it]                                                       26%|       | 830/3250 [1:17:49<3:37:36,  5.40s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.9272652268409729, 'eval_runtime': 1.376, 'eval_samples_per_second': 8.721, 'eval_steps_per_second': 2.18, 'epoch': 0.26}
                                                       26%|       | 830/3250 [1:17:50<3:37:36,  5.40s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_4/tmp-checkpoint-830
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-830
 /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-830
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-830
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-830
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-830/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-830/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-830/pytorch_model.binthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-830/pytorch_model.bin

/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.8203, 'learning_rate': 8.476923563648752e-05, 'epoch': 0.26}
{'loss': 0.8245, 'learning_rate': 8.473446465439758e-05, 'epoch': 0.26}
{'loss': 0.9207, 'learning_rate': 8.469966117642929e-05, 'epoch': 0.26}
{'loss': 0.8428, 'learning_rate': 8.46648252351431e-05, 'epoch': 0.26}
{'loss': 0.8622, 'learning_rate': 8.462995686312985e-05, 'epoch': 0.26}
{'loss': 0.792, 'learning_rate': 8.459505609301069e-05, 'epoch': 0.26}
 26%|       | 831/3250 [1:17:56<3:58:27,  5.91s/it]                                                       26%|       | 831/3250 [1:17:56<3:58:27,  5.91s/it] 26%|       | 832/3250 [1:18:01<3:51:43,  5.75s/it]                                                       26%|       | 832/3250 [1:18:01<3:51:43,  5.75s/it] 26%|       | 833/3250 [1:18:07<3:47:01,  5.64s/it]                                                       26%|       | 833/3250 [1:18:07<3:47:01,  5.64s/it] 26%|       | 834/3250 [1:18:12<3:43:37,  5.55s/it]                                                       26%|       | 834/3250 [1:18:12<3:43:37,  5.55s/it] 26%|       | 835/3250 [1:18:18<3:41:23,  5.50s/it]                                                       26%|       | 835/3250 [1:18:18<3:41:23,  5.50s/it] 26%|       | 836/3250 [1:18:23<3:39:46,  5.46s/it]                                                       26%|  {'loss': 0.8261, 'learning_rate': 8.456012295743706e-05, 'epoch': 0.26}
{'loss': 0.8708, 'learning_rate': 8.452515748909069e-05, 'epoch': 0.26}
{'loss': 0.8461, 'learning_rate': 8.449015972068363e-05, 'epoch': 0.26}
{'loss': 0.8379, 'learning_rate': 8.445512968495806e-05, 'epoch': 0.26}
     | 836/3250 [1:18:23<3:39:46,  5.46s/it] 26%|       | 837/3250 [1:18:29<3:43:05,  5.55s/it]                                                       26%|       | 837/3250 [1:18:29<3:43:05,  5.55s/it] 26%|       | 838/3250 [1:18:34<3:40:43,  5.49s/it]                                                       26%|       | 838/3250 [1:18:34<3:40:43,  5.49s/it] 26%|       | 839/3250 [1:18:39<3:39:07,  5.45s/it]                                                       26%|       | 839/3250 [1:18:39<3:39:07,  5.45s/it] 26%|       | 840/3250 [1:18:45<3:38:03,  5.43s/it]                                                       26%|       | 840/3250 [1:18:45<3:38:03,  5.43s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.9228727221488953, 'eval_runtime': 1.3911, 'eval_samples_per_second': 8.626, 'eval_steps_per_second': 2.157, 'epoch': 0.26}
                                                       26%|       | 840/3250 [1:18:46<3:38:03,  5.43s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_4/tmp-checkpoint-840
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-840
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-840
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-840
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-840
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-840/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-840/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-840/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-840/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.8357, 'learning_rate': 8.442006741468639e-05, 'epoch': 0.26}
{'loss': 0.8525, 'learning_rate': 8.438497294267117e-05, 'epoch': 0.26}
{'loss': 0.8538, 'learning_rate': 8.434984630174509e-05, 'epoch': 0.26}
{'loss': 0.8624, 'learning_rate': 8.431468752477091e-05, 'epoch': 0.26}
{'loss': 0.8523, 'learning_rate': 8.427949664464152e-05, 'epoch': 0.26}
{'loss': 0.8743, 'learning_rate': 8.424427369427974e-05, 'epoch': 0.26}
 26%|       | 841/3250 [1:18:52<3:58:26,  5.94s/it]                                                       26%|       | 841/3250 [1:18:52<3:58:26,  5.94s/it] 26%|       | 842/3250 [1:18:57<3:51:58,  5.78s/it]                                                       26%|       | 842/3250 [1:18:57<3:51:58,  5.78s/it] 26%|       | 843/3250 [1:19:03<3:47:28,  5.67s/it]                                                       26%|       | 843/3250 [1:19:03<3:47:28,  5.67s/it] 26%|       | 844/3250 [1:19:08<3:44:08,  5.59s/it]                                                       26%|       | 844/3250 [1:19:08<3:44:08,  5.59s/it] 26%|       | 845/3250 [1:19:14<3:42:01,  5.54s/it]                                                       26%|       | 845/3250 [1:19:14<3:42:01,  5.54s/it] 26%|       | 846/3250 [1:19:19<3:40:21,  5.50s/it]                                                       26%|  {'loss': 0.8421, 'learning_rate': 8.42090187066385e-05, 'epoch': 0.26}
{'loss': 0.8639, 'learning_rate': 8.417373171470063e-05, 'epoch': 0.26}
{'loss': 0.8746, 'learning_rate': 8.413841275147892e-05, 'epoch': 0.26}
{'loss': 0.8349, 'learning_rate': 8.410306185001611e-05, 'epoch': 0.26}
     | 846/3250 [1:19:19<3:40:21,  5.50s/it] 26%|       | 847/3250 [1:19:24<3:39:15,  5.47s/it]                                                       26%|       | 847/3250 [1:19:24<3:39:15,  5.47s/it] 26%|       | 848/3250 [1:19:30<3:38:30,  5.46s/it]                                                       26%|       | 848/3250 [1:19:30<3:38:30,  5.46s/it] 26%|       | 849/3250 [1:19:35<3:37:41,  5.44s/it]                                                       26%|       | 849/3250 [1:19:35<3:37:41,  5.44s/it] 26%|       | 850/3250 [1:19:41<3:37:17,  5.43s/it]                                                       26%|       | 850/3250 [1:19:41<3:37:17,  5.43s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.923623263835907, 'eval_runtime': 1.6015, 'eval_samples_per_second': 7.493, 'eval_steps_per_second': 1.873, 'epoch': 0.26}
                                                       26%|       | 850/3250 [1:19:42<3:37:17,  5.43s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_4/tmp-checkpoint-850
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  I AM HERE AND I AM SAVING THE MODEL/projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-850
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-850

I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-850
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-850
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-850/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-850/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-850/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-850/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.8279, 'learning_rate': 8.406767904338475e-05, 'epoch': 0.26}
{'loss': 0.8229, 'learning_rate': 8.403226436468728e-05, 'epoch': 0.26}
{'loss': 0.8765, 'learning_rate': 8.399681784705599e-05, 'epoch': 0.26}
{'loss': 0.8088, 'learning_rate': 8.396133952365288e-05, 'epoch': 0.26}
{'loss': 0.9016, 'learning_rate': 8.392582942766976e-05, 'epoch': 0.26}
{'loss': 1.3434, 'learning_rate': 8.389028759232815e-05, 'epoch': 0.26}
 26%|       | 851/3250 [1:19:48<3:59:40,  5.99s/it]                                                       26%|       | 851/3250 [1:19:48<3:59:40,  5.99s/it] 26%|       | 852/3250 [1:19:53<3:52:30,  5.82s/it]                                                       26%|       | 852/3250 [1:19:53<3:52:30,  5.82s/it] 26%|       | 853/3250 [1:19:59<3:50:39,  5.77s/it]                                                       26%|       | 853/3250 [1:19:59<3:50:39,  5.77s/it] 26%|       | 854/3250 [1:20:04<3:46:14,  5.67s/it]                                                       26%|       | 854/3250 [1:20:04<3:46:14,  5.67s/it] 26%|       | 855/3250 [1:20:10<3:42:57,  5.59s/it]                                                       26%|       | 855/3250 [1:20:10<3:42:57,  5.59s/it] 26%|       | 856/3250 [1:20:15<3:40:43,  5.53s/it]                                                       26%|  {'loss': 0.7849, 'learning_rate': 8.385471405087927e-05, 'epoch': 0.26}
{'loss': 0.8152, 'learning_rate': 8.3819108836604e-05, 'epoch': 0.26}
{'loss': 0.8648, 'learning_rate': 8.378347198281284e-05, 'epoch': 0.26}
{'loss': 0.8601, 'learning_rate': 8.37478035228459e-05, 'epoch': 0.26}
     | 856/3250 [1:20:15<3:40:43,  5.53s/it] 26%|       | 857/3250 [1:20:21<3:39:14,  5.50s/it]                                                       26%|       | 857/3250 [1:20:21<3:39:14,  5.50s/it] 26%|       | 858/3250 [1:20:26<3:37:59,  5.47s/it]                                                       26%|       | 858/3250 [1:20:26<3:37:59,  5.47s/it] 26%|       | 859/3250 [1:20:31<3:37:06,  5.45s/it]                                                       26%|       | 859/3250 [1:20:31<3:37:06,  5.45s/it] 26%|       | 860/3250 [1:20:37<3:36:35,  5.44s/it]                                                       26%|       | 860/3250 [1:20:37<3:36:35,  5.44s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.9240946769714355, 'eval_runtime': 1.3899, 'eval_samples_per_second': 8.634, 'eval_steps_per_second': 2.158, 'epoch': 0.26}
                                                       26%|       | 860/3250 [1:20:38<3:36:35,  5.44s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_4/tmp-checkpoint-860
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  I AM HERE AND I AM SAVING THE MODEL
/projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-860the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-860

I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-860
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-860
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-860/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-860/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-860/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-860/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.814, 'learning_rate': 8.371210349007286e-05, 'epoch': 0.26}
{'loss': 0.8388, 'learning_rate': 8.367637191789296e-05, 'epoch': 0.27}
{'loss': 0.8904, 'learning_rate': 8.364060883973489e-05, 'epoch': 0.27}
{'loss': 0.8732, 'learning_rate': 8.360481428905686e-05, 'epoch': 0.27}
{'loss': 0.8376, 'learning_rate': 8.356898829934652e-05, 'epoch': 0.27}
{'loss': 0.7978, 'learning_rate': 8.353313090412093e-05, 'epoch': 0.27}
 26%|       | 861/3250 [1:20:44<3:56:09,  5.93s/it]                                                       26%|       | 861/3250 [1:20:44<3:56:09,  5.93s/it] 27%|       | 862/3250 [1:20:49<3:49:49,  5.77s/it]                                                       27%|       | 862/3250 [1:20:49<3:49:49,  5.77s/it] 27%|       | 863/3250 [1:20:55<3:45:19,  5.66s/it]                                                       27%|       | 863/3250 [1:20:55<3:45:19,  5.66s/it] 27%|       | 864/3250 [1:21:00<3:41:59,  5.58s/it]                                                       27%|       | 864/3250 [1:21:00<3:41:59,  5.58s/it] 27%|       | 865/3250 [1:21:06<3:39:59,  5.53s/it]                                                       27%|       | 865/3250 [1:21:06<3:39:59,  5.53s/it] 27%|       | 866/3250 [1:21:11<3:38:21,  5.50s/it]                                                       27%|  {'loss': 0.8555, 'learning_rate': 8.349724213692651e-05, 'epoch': 0.27}
{'loss': 0.83, 'learning_rate': 8.346132203133906e-05, 'epoch': 0.27}
{'loss': 0.8383, 'learning_rate': 8.34253706209637e-05, 'epoch': 0.27}
{'loss': 0.8294, 'learning_rate': 8.338938793943478e-05, 'epoch': 0.27}
     | 866/3250 [1:21:11<3:38:21,  5.50s/it] 27%|       | 867/3250 [1:21:16<3:37:21,  5.47s/it]                                                       27%|       | 867/3250 [1:21:16<3:37:21,  5.47s/it] 27%|       | 868/3250 [1:21:22<3:36:36,  5.46s/it]                                                       27%|       | 868/3250 [1:21:22<3:36:36,  5.46s/it] 27%|       | 869/3250 [1:21:28<3:40:14,  5.55s/it]                                                       27%|       | 869/3250 [1:21:28<3:40:14,  5.55s/it] 27%|       | 870/3250 [1:21:33<3:38:19,  5.50s/it]                                                       27%|       | 870/3250 [1:21:33<3:38:19,  5.50s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.9198924899101257, 'eval_runtime': 1.3933, 'eval_samples_per_second': 8.613, 'eval_steps_per_second': 2.153, 'epoch': 0.27}
                                                       27%|       | 870/3250 [1:21:34<3:38:19,  5.50s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_4/tmp-checkpoint-870
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  I AM HERE AND I AM SAVING THE MODEL/projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-870
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-870

I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-870
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-870
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-870/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-870/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-870/pytorch_model.binthe pytorch model path is
 /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-870/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.8313, 'learning_rate': 8.335337402041601e-05, 'epoch': 0.27}
{'loss': 0.8348, 'learning_rate': 8.33173288976002e-05, 'epoch': 0.27}
{'loss': 0.8519, 'learning_rate': 8.328125260470947e-05, 'epoch': 0.27}
{'loss': 0.8564, 'learning_rate': 8.3245145175495e-05, 'epoch': 0.27}
{'loss': 0.8487, 'learning_rate': 8.320900664373719e-05, 'epoch': 0.27}
{'loss': 0.8386, 'learning_rate': 8.317283704324549e-05, 'epoch': 0.27}
 27%|       | 871/3250 [1:21:40<3:56:58,  5.98s/it]                                                       27%|       | 871/3250 [1:21:40<3:56:58,  5.98s/it] 27%|       | 872/3250 [1:21:45<3:50:10,  5.81s/it]                                                       27%|       | 872/3250 [1:21:45<3:50:10,  5.81s/it] 27%|       | 873/3250 [1:21:51<3:45:02,  5.68s/it]                                                       27%|       | 873/3250 [1:21:51<3:45:02,  5.68s/it] 27%|       | 874/3250 [1:21:56<3:41:22,  5.59s/it]                                                       27%|       | 874/3250 [1:21:56<3:41:22,  5.59s/it] 27%|       | 875/3250 [1:22:02<3:38:55,  5.53s/it]                                                       27%|       | 875/3250 [1:22:02<3:38:55,  5.53s/it] 27%|       | 876/3250 [1:22:07<3:37:14,  5.49s/it]                                                       27%|  {'loss': 0.8691, 'learning_rate': 8.313663640785839e-05, 'epoch': 0.27}
{'loss': 0.8087, 'learning_rate': 8.310040477144347e-05, 'epoch': 0.27}
{'loss': 0.9115, 'learning_rate': 8.30641421678973e-05, 'epoch': 0.27}
{'loss': 0.8159, 'learning_rate': 8.30278486311454e-05, 'epoch': 0.27}
     | 876/3250 [1:22:07<3:37:14,  5.49s/it] 27%|       | 877/3250 [1:22:12<3:35:59,  5.46s/it]                                                       27%|       | 877/3250 [1:22:12<3:35:59,  5.46s/it] 27%|       | 878/3250 [1:22:18<3:35:12,  5.44s/it]                                                       27%|       | 878/3250 [1:22:18<3:35:12,  5.44s/it] 27%|       | 879/3250 [1:22:23<3:34:30,  5.43s/it]                                                       27%|       | 879/3250 [1:22:23<3:34:30,  5.43s/it] 27%|       | 880/3250 [1:22:29<3:34:05,  5.42s/it]                                                       27%|       | 880/3250 [1:22:29<3:34:05,  5.42s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.9199395775794983, 'eval_runtime': 1.3791, 'eval_samples_per_second': 8.701, 'eval_steps_per_second': 2.175, 'epoch': 0.27}
                                                       27%|       | 880/3250 [1:22:30<3:34:05,  5.42s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_4/tmp-checkpoint-880
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-880I AM HERE AND I AM SAVING THE MODEL

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-880
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-880
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-880
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-880/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-880/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-880/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-880/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.8255, 'learning_rate': 8.29915241951422e-05, 'epoch': 0.27}
{'loss': 0.8317, 'learning_rate': 8.295516889387115e-05, 'epoch': 0.27}
{'loss': 0.8306, 'learning_rate': 8.291878276134447e-05, 'epoch': 0.27}
{'loss': 0.8348, 'learning_rate': 8.288236583160322e-05, 'epoch': 0.27}
{'loss': 0.8653, 'learning_rate': 8.284591813871738e-05, 'epoch': 0.27}
{'loss': 0.8629, 'learning_rate': 8.280943971678562e-05, 'epoch': 0.27}
 27%|       | 881/3250 [1:22:36<3:53:21,  5.91s/it]                                                       27%|       | 881/3250 [1:22:36<3:53:21,  5.91s/it] 27%|       | 882/3250 [1:22:41<3:46:54,  5.75s/it]                                                       27%|       | 882/3250 [1:22:41<3:46:54,  5.75s/it] 27%|       | 883/3250 [1:22:46<3:42:34,  5.64s/it]                                                       27%|       | 883/3250 [1:22:46<3:42:34,  5.64s/it] 27%|       | 884/3250 [1:22:52<3:39:21,  5.56s/it]                                                       27%|       | 884/3250 [1:22:52<3:39:21,  5.56s/it] 27%|       | 885/3250 [1:22:57<3:37:14,  5.51s/it]                                                       27%|       | 885/3250 [1:22:57<3:37:14,  5.51s/it] 27%|       | 886/3250 [1:23:03<3:41:25,  5.62s/it]                                                       27%|  {'loss': 1.287, 'learning_rate': 8.277293059993535e-05, 'epoch': 0.27}
{'loss': 0.8208, 'learning_rate': 8.273639082232276e-05, 'epoch': 0.27}
{'loss': 0.8613, 'learning_rate': 8.269982041813267e-05, 'epoch': 0.27}
{'loss': 0.8535, 'learning_rate': 8.26632194215786e-05, 'epoch': 0.27}
     | 886/3250 [1:23:03<3:41:25,  5.62s/it] 27%|       | 887/3250 [1:23:08<3:38:16,  5.54s/it]                                                       27%|       | 887/3250 [1:23:08<3:38:16,  5.54s/it] 27%|       | 888/3250 [1:23:14<3:36:24,  5.50s/it]                                                       27%|       | 888/3250 [1:23:14<3:36:24,  5.50s/it] 27%|       | 889/3250 [1:23:19<3:35:24,  5.47s/it]                                                       27%|       | 889/3250 [1:23:19<3:35:24,  5.47s/it] 27%|       | 890/3250 [1:23:25<3:34:23,  5.45s/it]                                                       27%|       | 890/3250 [1:23:25<3:34:23,  5.45s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.9219279885292053, 'eval_runtime': 1.3832, 'eval_samples_per_second': 8.676, 'eval_steps_per_second': 2.169, 'epoch': 0.27}
                                                       27%|       | 890/3250 [1:23:26<3:34:23,  5.45s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_4/tmp-checkpoint-890
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-890
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-890
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-890
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-890
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-890/pytorch_model.bin
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-890/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-890/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-890/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.8496, 'learning_rate': 8.262658786690262e-05, 'epoch': 0.27}
{'loss': 0.8108, 'learning_rate': 8.258992578837548e-05, 'epoch': 0.27}
{'loss': 0.8879, 'learning_rate': 8.255323322029642e-05, 'epoch': 0.27}
{'loss': 0.8851, 'learning_rate': 8.251651019699322e-05, 'epoch': 0.28}
{'loss': 0.8345, 'learning_rate': 8.247975675282218e-05, 'epoch': 0.28}
{'loss': 0.8134, 'learning_rate': 8.244297292216802e-05, 'epoch': 0.28}
 27%|       | 891/3250 [1:23:32<3:54:13,  5.96s/it]                                                       27%|       | 891/3250 [1:23:32<3:54:13,  5.96s/it] 27%|       | 892/3250 [1:23:37<3:47:21,  5.79s/it]                                                       27%|       | 892/3250 [1:23:37<3:47:21,  5.79s/it] 27%|       | 893/3250 [1:23:43<3:42:43,  5.67s/it]                                                       27%|       | 893/3250 [1:23:43<3:42:43,  5.67s/it] 28%|       | 894/3250 [1:23:48<3:39:20,  5.59s/it]                                                       28%|       | 894/3250 [1:23:48<3:39:20,  5.59s/it] 28%|       | 895/3250 [1:23:53<3:36:57,  5.53s/it]                                                       28%|       | 895/3250 [1:23:53<3:36:57,  5.53s/it] 28%|       | 896/3250 [1:23:59<3:35:25,  5.49s/it]                                                       28%|  {'loss': 0.8393, 'learning_rate': 8.240615873944391e-05, 'epoch': 0.28}
{'loss': 0.8266, 'learning_rate': 8.236931423909138e-05, 'epoch': 0.28}
{'loss': 0.8284, 'learning_rate': 8.233243945558042e-05, 'epoch': 0.28}
{'loss': 0.8412, 'learning_rate': 8.229553442340922e-05, 'epoch': 0.28}
     | 896/3250 [1:23:59<3:35:25,  5.49s/it] 28%|       | 897/3250 [1:24:04<3:34:12,  5.46s/it]                                                       28%|       | 897/3250 [1:24:04<3:34:12,  5.46s/it] 28%|       | 898/3250 [1:24:10<3:33:25,  5.44s/it]                                                       28%|       | 898/3250 [1:24:10<3:33:25,  5.44s/it] 28%|       | 899/3250 [1:24:15<3:32:29,  5.42s/it]                                                       28%|       | 899/3250 [1:24:15<3:32:29,  5.42s/it] 28%|       | 900/3250 [1:24:20<3:31:59,  5.41s/it]                                                       28%|       | 900/3250 [1:24:20<3:31:59,  5.41s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.9166622757911682, 'eval_runtime': 1.3733, 'eval_samples_per_second': 8.738, 'eval_steps_per_second': 2.185, 'epoch': 0.28}
                                                       28%|       | 900/3250 [1:24:22<3:31:59,  5.41s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_4/tmp-checkpoint-900
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-900
/projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-900
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-900
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-900
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-900/pytorch_model.bin
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-900/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-900/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-900/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.8158, 'learning_rate': 8.225859917710439e-05, 'epoch': 0.28}
{'loss': 0.8438, 'learning_rate': 8.222163375122072e-05, 'epoch': 0.28}
{'loss': 0.8384, 'learning_rate': 8.218463818034128e-05, 'epoch': 0.28}
{'loss': 0.8237, 'learning_rate': 8.214761249907732e-05, 'epoch': 0.28}
{'loss': 0.8328, 'learning_rate': 8.211055674206828e-05, 'epoch': 0.28}
{'loss': 0.8538, 'learning_rate': 8.207347094398173e-05, 'epoch': 0.28}
 28%|       | 901/3250 [1:24:27<3:51:23,  5.91s/it]                                                       28%|       | 901/3250 [1:24:27<3:51:23,  5.91s/it] 28%|       | 902/3250 [1:24:33<3:49:30,  5.86s/it]                                                       28%|       | 902/3250 [1:24:33<3:49:30,  5.86s/it] 28%|       | 903/3250 [1:24:38<3:43:39,  5.72s/it]                                                       28%|       | 903/3250 [1:24:38<3:43:39,  5.72s/it] 28%|       | 904/3250 [1:24:44<3:39:42,  5.62s/it]                                                       28%|       | 904/3250 [1:24:44<3:39:42,  5.62s/it] 28%|       | 905/3250 [1:24:49<3:36:46,  5.55s/it]                                                       28%|       | 905/3250 [1:24:49<3:36:46,  5.55s/it] 28%|       | 906/3250 [1:24:55<3:34:55,  5.50s/it]                                                       28%|  {'loss': 0.8552, 'learning_rate': 8.203635513951331e-05, 'epoch': 0.28}
{'loss': 0.7906, 'learning_rate': 8.199920936338681e-05, 'epoch': 0.28}
{'loss': 0.8912, 'learning_rate': 8.1962033650354e-05, 'epoch': 0.28}
{'loss': 0.8398, 'learning_rate': 8.192482803519465e-05, 'epoch': 0.28}
     | 906/3250 [1:24:55<3:34:55,  5.50s/it] 28%|       | 907/3250 [1:25:00<3:33:30,  5.47s/it]                                                       28%|       | 907/3250 [1:25:00<3:33:30,  5.47s/it] 28%|       | 908/3250 [1:25:05<3:32:20,  5.44s/it]                                                       28%|       | 908/3250 [1:25:05<3:32:20,  5.44s/it] 28%|       | 909/3250 [1:25:11<3:31:34,  5.42s/it]                                                       28%|       | 909/3250 [1:25:11<3:31:34,  5.42s/it] 28%|       | 910/3250 [1:25:16<3:31:05,  5.41s/it]                                                       28%|       | 910/3250 [1:25:16<3:31:05,  5.41s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.9164521098136902, 'eval_runtime': 1.3795, 'eval_samples_per_second': 8.699, 'eval_steps_per_second': 2.175, 'epoch': 0.28}
                                                       28%|       | 910/3250 [1:25:18<3:31:05,  5.41s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_4/tmp-checkpoint-910
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-910
/projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-910
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-910
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-910
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-910/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-910/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-910/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-910/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.8168, 'learning_rate': 8.188759255271654e-05, 'epoch': 0.28}
{'loss': 0.8198, 'learning_rate': 8.185032723775539e-05, 'epoch': 0.28}
{'loss': 0.7928, 'learning_rate': 8.181303212517479e-05, 'epoch': 0.28}
{'loss': 0.8563, 'learning_rate': 8.177570724986628e-05, 'epoch': 0.28}
{'loss': 0.8124, 'learning_rate': 8.173835264674916e-05, 'epoch': 0.28}
{'loss': 0.8734, 'learning_rate': 8.17009683507706e-05, 'epoch': 0.28}
 28%|       | 911/3250 [1:25:23<3:50:57,  5.92s/it]                                                       28%|       | 911/3250 [1:25:23<3:50:57,  5.92s/it] 28%|       | 912/3250 [1:25:29<3:44:29,  5.76s/it]                                                       28%|       | 912/3250 [1:25:29<3:44:29,  5.76s/it] 28%|       | 913/3250 [1:25:34<3:40:00,  5.65s/it]                                                       28%|       | 913/3250 [1:25:34<3:40:00,  5.65s/it] 28%|       | 914/3250 [1:25:39<3:36:42,  5.57s/it]                                                       28%|       | 914/3250 [1:25:39<3:36:42,  5.57s/it] 28%|       | 915/3250 [1:25:45<3:34:34,  5.51s/it]                                                       28%|       | 915/3250 [1:25:45<3:34:34,  5.51s/it] 28%|       | 916/3250 [1:25:50<3:32:58,  5.48s/it]                                                       28%|  {'loss': 1.2908, 'learning_rate': 8.166355439690553e-05, 'epoch': 0.28}
{'loss': 0.8098, 'learning_rate': 8.162611082015665e-05, 'epoch': 0.28}
{'loss': 0.8183, 'learning_rate': 8.15886376555543e-05, 'epoch': 0.28}
{'loss': 0.8534, 'learning_rate': 8.15511349381566e-05, 'epoch': 0.28}
     | 916/3250 [1:25:50<3:32:58,  5.48s/it] 28%|       | 917/3250 [1:25:56<3:31:42,  5.44s/it]                                                       28%|       | 917/3250 [1:25:56<3:31:42,  5.44s/it] 28%|       | 918/3250 [1:26:01<3:36:29,  5.57s/it]                                                       28%|       | 918/3250 [1:26:01<3:36:29,  5.57s/it] 28%|       | 919/3250 [1:26:07<3:34:11,  5.51s/it]                                                       28%|       | 919/3250 [1:26:07<3:34:11,  5.51s/it] 28%|       | 920/3250 [1:26:12<3:32:35,  5.47s/it]                                                       28%|       | 920/3250 [1:26:12<3:32:35,  5.47s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.9214993119239807, 'eval_runtime': 1.3777, 'eval_samples_per_second': 8.71, 'eval_steps_per_second': 2.178, 'epoch': 0.28}
                                                       28%|       | 920/3250 [1:26:14<3:32:35,  5.47s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_4/tmp-checkpoint-920
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-920
 /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-920
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-920
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-920
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-920/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-920/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-920/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-920/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.8424, 'learning_rate': 8.151360270304927e-05, 'epoch': 0.28}
{'loss': 0.7805, 'learning_rate': 8.14760409853456e-05, 'epoch': 0.28}
{'loss': 0.8106, 'learning_rate': 8.143844982018655e-05, 'epoch': 0.28}
{'loss': 0.9018, 'learning_rate': 8.14008292427406e-05, 'epoch': 0.28}
{'loss': 0.8293, 'learning_rate': 8.13631792882037e-05, 'epoch': 0.28}
{'loss': 0.8431, 'learning_rate': 8.132549999179933e-05, 'epoch': 0.28}
 28%|       | 921/3250 [1:26:19<3:51:54,  5.97s/it]                                                       28%|       | 921/3250 [1:26:19<3:51:54,  5.97s/it] 28%|       | 922/3250 [1:26:25<3:44:49,  5.79s/it]                                                       28%|       | 922/3250 [1:26:25<3:44:49,  5.79s/it] 28%|       | 923/3250 [1:26:30<3:39:56,  5.67s/it]                                                       28%|       | 923/3250 [1:26:30<3:39:56,  5.67s/it] 28%|       | 924/3250 [1:26:35<3:36:34,  5.59s/it]                                                       28%|       | 924/3250 [1:26:36<3:36:34,  5.59s/it] 28%|       | 925/3250 [1:26:41<3:34:08,  5.53s/it]                                                       28%|       | 925/3250 [1:26:41<3:34:08,  5.53s/it] 28%|       | 926/3250 [1:26:46<3:32:19,  5.48s/it]                                                       28%|  {'loss': 0.7742, 'learning_rate': 8.128779138877843e-05, 'epoch': 0.29}
{'loss': 0.8243, 'learning_rate': 8.12500535144193e-05, 'epoch': 0.29}
{'loss': 0.8405, 'learning_rate': 8.12122864040277e-05, 'epoch': 0.29}
{'loss': 0.8179, 'learning_rate': 8.117449009293668e-05, 'epoch': 0.29}
     | 926/3250 [1:26:46<3:32:19,  5.48s/it] 29%|       | 927/3250 [1:26:52<3:30:52,  5.45s/it]                                                       29%|       | 927/3250 [1:26:52<3:30:52,  5.45s/it] 29%|       | 928/3250 [1:26:57<3:30:08,  5.43s/it]                                                       29%|       | 928/3250 [1:26:57<3:30:08,  5.43s/it] 29%|       | 929/3250 [1:27:02<3:29:31,  5.42s/it]                                                       29%|       | 929/3250 [1:27:02<3:29:31,  5.42s/it] 29%|       | 930/3250 [1:27:08<3:29:06,  5.41s/it]                                                       29%|       | 930/3250 [1:27:08<3:29:06,  5.41s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.9131710529327393, 'eval_runtime': 1.3851, 'eval_samples_per_second': 8.664, 'eval_steps_per_second': 2.166, 'epoch': 0.29}
                                                       29%|       | 930/3250 [1:27:09<3:29:06,  5.41s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_4/tmp-checkpoint-930
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  I AM HERE AND I AM SAVING THE MODEL/projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-930
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-930

I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-930
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-930
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-930/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-930/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-930/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-930/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.7945, 'learning_rate': 8.113666461650664e-05, 'epoch': 0.29}
{'loss': 0.8091, 'learning_rate': 8.109881001012527e-05, 'epoch': 0.29}
{'loss': 0.8414, 'learning_rate': 8.106092630920749e-05, 'epoch': 0.29}
{'loss': 0.8371, 'learning_rate': 8.102301354919544e-05, 'epoch': 0.29}
{'loss': 0.8312, 'learning_rate': 8.098507176555849e-05, 'epoch': 0.29}
{'loss': 0.8278, 'learning_rate': 8.09471009937931e-05, 'epoch': 0.29}
 29%|       | 931/3250 [1:27:15<3:49:52,  5.95s/it]                                                       29%|       | 931/3250 [1:27:15<3:49:52,  5.95s/it] 29%|       | 932/3250 [1:27:20<3:43:05,  5.77s/it]                                                       29%|       | 932/3250 [1:27:20<3:43:05,  5.77s/it] 29%|       | 933/3250 [1:27:26<3:38:14,  5.65s/it]                                                       29%|       | 933/3250 [1:27:26<3:38:14,  5.65s/it] 29%|       | 934/3250 [1:27:31<3:35:00,  5.57s/it]                                                       29%|       | 934/3250 [1:27:31<3:35:00,  5.57s/it] 29%|       | 935/3250 [1:27:37<3:37:03,  5.63s/it]                                                       29%|       | 935/3250 [1:27:37<3:37:03,  5.63s/it] 29%|       | 936/3250 [1:27:42<3:34:15,  5.56s/it]                                                       29%|  {'loss': 0.8449, 'learning_rate': 8.090910126942288e-05, 'epoch': 0.29}
{'loss': 0.8213, 'learning_rate': 8.087107262799855e-05, 'epoch': 0.29}
{'loss': 0.8462, 'learning_rate': 8.083301510509784e-05, 'epoch': 0.29}
{'loss': 0.8356, 'learning_rate': 8.079492873632554e-05, 'epoch': 0.29}
     | 936/3250 [1:27:42<3:34:15,  5.56s/it] 29%|       | 937/3250 [1:27:48<3:32:07,  5.50s/it]                                                       29%|       | 937/3250 [1:27:48<3:32:07,  5.50s/it] 29%|       | 938/3250 [1:27:53<3:30:38,  5.47s/it]                                                       29%|       | 938/3250 [1:27:53<3:30:38,  5.47s/it] 29%|       | 939/3250 [1:27:58<3:29:37,  5.44s/it]                                                       29%|       | 939/3250 [1:27:58<3:29:37,  5.44s/it] 29%|       | 940/3250 [1:28:04<3:28:47,  5.42s/it]                                                       29%|       | 940/3250 [1:28:04<3:28:47,  5.42s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.9131324291229248, 'eval_runtime': 1.386, 'eval_samples_per_second': 8.658, 'eval_steps_per_second': 2.165, 'epoch': 0.29}
                                                       29%|       | 940/3250 [1:28:05<3:28:47,  5.42s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_4/tmp-checkpoint-940
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-940
/projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-940
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-940
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-940
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-940/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-940/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-940/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-940/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.8146, 'learning_rate': 8.075681355731338e-05, 'epoch': 0.29}
{'loss': 0.8012, 'learning_rate': 8.07186696037201e-05, 'epoch': 0.29}
{'loss': 0.7957, 'learning_rate': 8.06804969112313e-05, 'epoch': 0.29}
{'loss': 0.8581, 'learning_rate': 8.064229551555951e-05, 'epoch': 0.29}
{'loss': 0.7891, 'learning_rate': 8.060406545244413e-05, 'epoch': 0.29}
{'loss': 0.8946, 'learning_rate': 8.05658067576513e-05, 'epoch': 0.29}
 29%|       | 941/3250 [1:28:11<3:48:14,  5.93s/it]                                                       29%|       | 941/3250 [1:28:11<3:48:14,  5.93s/it] 29%|       | 942/3250 [1:28:16<3:41:45,  5.77s/it]                                                       29%|       | 942/3250 [1:28:16<3:41:45,  5.77s/it] 29%|       | 943/3250 [1:28:22<3:37:15,  5.65s/it]                                                       29%|       | 943/3250 [1:28:22<3:37:15,  5.65s/it] 29%|       | 944/3250 [1:28:27<3:34:06,  5.57s/it]                                                       29%|       | 944/3250 [1:28:27<3:34:06,  5.57s/it] 29%|       | 945/3250 [1:28:32<3:31:47,  5.51s/it]                                                       29%|       | 945/3250 [1:28:32<3:31:47,  5.51s/it] 29%|       | 946/3250 [1:28:38<3:30:15,  5.48s/it]                                                       29%|  {'loss': 1.3297, 'learning_rate': 8.052751946697403e-05, 'epoch': 0.29}
{'loss': 0.7525, 'learning_rate': 8.048920361623202e-05, 'epoch': 0.29}
{'loss': 0.8113, 'learning_rate': 8.045085924127177e-05, 'epoch': 0.29}
{'loss': 0.8319, 'learning_rate': 8.041248637796637e-05, 'epoch': 0.29}
     | 946/3250 [1:28:38<3:30:15,  5.48s/it] 29%|       | 947/3250 [1:28:43<3:28:53,  5.44s/it]                                                       29%|       | 947/3250 [1:28:43<3:28:53,  5.44s/it] 29%|       | 948/3250 [1:28:49<3:27:53,  5.42s/it]                                                       29%|       | 948/3250 [1:28:49<3:27:53,  5.42s/it] 29%|       | 949/3250 [1:28:54<3:27:20,  5.41s/it]                                                       29%|       | 949/3250 [1:28:54<3:27:20,  5.41s/it] 29%|       | 950/3250 [1:28:59<3:26:51,  5.40s/it]                                                       29%|       | 950/3250 [1:28:59<3:26:51,  5.40s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.916253924369812, 'eval_runtime': 1.3796, 'eval_samples_per_second': 8.698, 'eval_steps_per_second': 2.174, 'epoch': 0.29}
                                                       29%|       | 950/3250 [1:29:01<3:26:51,  5.40s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_4/tmp-checkpoint-950
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-950I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-950

I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-950
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-950
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-950/pytorch_model.bin
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-950/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-950/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-950/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.8472, 'learning_rate': 8.037408506221563e-05, 'epoch': 0.29}
{'loss': 0.8149, 'learning_rate': 8.033565532994593e-05, 'epoch': 0.29}
{'loss': 0.8093, 'learning_rate': 8.029719721711031e-05, 'epoch': 0.29}
{'loss': 0.8652, 'learning_rate': 8.025871075968828e-05, 'epoch': 0.29}
{'loss': 0.854, 'learning_rate': 8.022019599368588e-05, 'epoch': 0.29}
{'loss': 0.8136, 'learning_rate': 8.018165295513569e-05, 'epoch': 0.29}
 29%|       | 951/3250 [1:29:07<3:51:20,  6.04s/it]                                                       29%|       | 951/3250 [1:29:07<3:51:20,  6.04s/it] 29%|       | 952/3250 [1:29:12<3:43:39,  5.84s/it]                                                       29%|       | 952/3250 [1:29:12<3:43:39,  5.84s/it] 29%|       | 953/3250 [1:29:18<3:38:10,  5.70s/it]                                                       29%|       | 953/3250 [1:29:18<3:38:10,  5.70s/it] 29%|       | 954/3250 [1:29:23<3:34:17,  5.60s/it]                                                       29%|       | 954/3250 [1:29:23<3:34:17,  5.60s/it] 29%|       | 955/3250 [1:29:28<3:31:29,  5.53s/it]                                                       29%|       | 955/3250 [1:29:28<3:31:29,  5.53s/it] 29%|       | 956/3250 [1:29:34<3:29:36,  5.48s/it]                                                       29%|  {'loss': 0.7743, 'learning_rate': 8.014308168009668e-05, 'epoch': 0.29}
{'loss': 0.8295, 'learning_rate': 8.01044822046543e-05, 'epoch': 0.29}
{'loss': 0.8172, 'learning_rate': 8.006585456492029e-05, 'epoch': 0.3}
{'loss': 0.8259, 'learning_rate': 8.002719879703284e-05, 'epoch': 0.3}
     | 956/3250 [1:29:34<3:29:36,  5.48s/it] 29%|       | 957/3250 [1:29:39<3:28:09,  5.45s/it]                                                       29%|       | 957/3250 [1:29:39<3:28:09,  5.45s/it] 29%|       | 958/3250 [1:29:44<3:27:08,  5.42s/it]                                                       29%|       | 958/3250 [1:29:44<3:27:08,  5.42s/it] 30%|       | 959/3250 [1:29:50<3:26:27,  5.41s/it]                                                       30%|       | 959/3250 [1:29:50<3:26:27,  5.41s/it] 30%|       | 960/3250 [1:29:55<3:25:56,  5.40s/it]                                                       30%|       | 960/3250 [1:29:55<3:25:56,  5.40s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.9104979634284973, 'eval_runtime': 1.3799, 'eval_samples_per_second': 8.696, 'eval_steps_per_second': 2.174, 'epoch': 0.3}
                                                       30%|       | 960/3250 [1:29:57<3:25:56,  5.40s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_4/tmp-checkpoint-960
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-960I AM HERE AND I AM SAVING THE MODEL

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-960
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-960
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-960
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-960/pytorch_model.bin
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-960/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-960/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-960/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.8107, 'learning_rate': 7.99885149371564e-05, 'epoch': 0.3}
{'loss': 0.8215, 'learning_rate': 7.99498030214817e-05, 'epoch': 0.3}
{'loss': 0.8025, 'learning_rate': 7.991106308622572e-05, 'epoch': 0.3}
{'loss': 0.8303, 'learning_rate': 7.987229516763168e-05, 'epoch': 0.3}
{'loss': 0.8285, 'learning_rate': 7.983349930196896e-05, 'epoch': 0.3}
{'loss': 0.824, 'learning_rate': 7.979467552553308e-05, 'epoch': 0.3}
 30%|       | 961/3250 [1:30:02<3:45:43,  5.92s/it]                                                       30%|       | 961/3250 [1:30:02<3:45:43,  5.92s/it] 30%|       | 962/3250 [1:30:08<3:39:26,  5.75s/it]                                                       30%|       | 962/3250 [1:30:08<3:39:26,  5.75s/it] 30%|       | 963/3250 [1:30:13<3:34:59,  5.64s/it]                                                       30%|       | 963/3250 [1:30:13<3:34:59,  5.64s/it] 30%|       | 964/3250 [1:30:18<3:31:56,  5.56s/it]                                                       30%|       | 964/3250 [1:30:18<3:31:56,  5.56s/it] 30%|       | 965/3250 [1:30:24<3:29:34,  5.50s/it]                                                       30%|       | 965/3250 [1:30:24<3:29:34,  5.50s/it] 30%|       | 966/3250 [1:30:29<3:28:03,  5.47s/it]                                                       30%|  {'loss': 0.8234, 'learning_rate': 7.975582387464568e-05, 'epoch': 0.3}
{'loss': 0.8376, 'learning_rate': 7.97169443856545e-05, 'epoch': 0.3}
{'loss': 0.7901, 'learning_rate': 7.967803709493325e-05, 'epoch': 0.3}
{'loss': 0.8813, 'learning_rate': 7.963910203888177e-05, 'epoch': 0.3}
     | 966/3250 [1:30:29<3:28:03,  5.47s/it] 30%|       | 967/3250 [1:30:35<3:26:59,  5.44s/it]                                                       30%|       | 967/3250 [1:30:35<3:26:59,  5.44s/it] 30%|       | 968/3250 [1:30:40<3:30:39,  5.54s/it]                                                       30%|       | 968/3250 [1:30:40<3:30:39,  5.54s/it] 30%|       | 969/3250 [1:30:46<3:28:47,  5.49s/it]                                                       30%|       | 969/3250 [1:30:46<3:28:47,  5.49s/it] 30%|       | 970/3250 [1:30:51<3:27:11,  5.45s/it]                                                       30%|       | 970/3250 [1:30:51<3:27:11,  5.45s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.9096674919128418, 'eval_runtime': 1.3878, 'eval_samples_per_second': 8.647, 'eval_steps_per_second': 2.162, 'epoch': 0.3}
                                                       30%|       | 970/3250 [1:30:52<3:27:11,  5.45s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_4/tmp-checkpoint-970
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in   /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-970
/projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-970
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-970
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-970
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-970/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-970/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-970/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-970/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.8063, 'learning_rate': 7.960013925392576e-05, 'epoch': 0.3}
{'loss': 0.8118, 'learning_rate': 7.95611487765169e-05, 'epoch': 0.3}
{'loss': 0.8113, 'learning_rate': 7.952213064313283e-05, 'epoch': 0.3}
{'loss': 0.8041, 'learning_rate': 7.9483084890277e-05, 'epoch': 0.3}
{'loss': 0.8138, 'learning_rate': 7.944401155447871e-05, 'epoch': 0.3}
{'loss': 0.8364, 'learning_rate': 7.940491067229311e-05, 'epoch': 0.3}
 30%|       | 971/3250 [1:30:58<3:46:24,  5.96s/it]                                                       30%|       | 971/3250 [1:30:58<3:46:24,  5.96s/it] 30%|       | 972/3250 [1:31:04<3:39:37,  5.78s/it]                                                       30%|       | 972/3250 [1:31:04<3:39:37,  5.78s/it] 30%|       | 973/3250 [1:31:09<3:35:03,  5.67s/it]                                                       30%|       | 973/3250 [1:31:09<3:35:03,  5.67s/it] 30%|       | 974/3250 [1:31:14<3:31:51,  5.59s/it]                                                       30%|       | 974/3250 [1:31:14<3:31:51,  5.59s/it] 30%|       | 975/3250 [1:31:20<3:29:15,  5.52s/it]                                                       30%|       | 975/3250 [1:31:20<3:29:15,  5.52s/it] 30%|       | 976/3250 [1:31:25<3:27:33,  5.48s/it]                                                       30%|  {'loss': 0.8447, 'learning_rate': 7.936578228030105e-05, 'epoch': 0.3}
{'loss': 1.2726, 'learning_rate': 7.932662641510915e-05, 'epoch': 0.3}
{'loss': 0.8059, 'learning_rate': 7.928744311334977e-05, 'epoch': 0.3}
{'loss': 0.8223, 'learning_rate': 7.92482324116809e-05, 'epoch': 0.3}
     | 976/3250 [1:31:25<3:27:33,  5.48s/it] 30%|       | 977/3250 [1:31:30<3:26:14,  5.44s/it]                                                       30%|       | 977/3250 [1:31:30<3:26:14,  5.44s/it] 30%|       | 978/3250 [1:31:36<3:25:19,  5.42s/it]                                                       30%|       | 978/3250 [1:31:36<3:25:19,  5.42s/it] 30%|       | 979/3250 [1:31:41<3:24:38,  5.41s/it]                                                       30%|       | 979/3250 [1:31:41<3:24:38,  5.41s/it] 30%|       | 980/3250 [1:31:47<3:24:08,  5.40s/it]                                                       30%|       | 980/3250 [1:31:47<3:24:08,  5.40s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.9113647937774658, 'eval_runtime': 1.3786, 'eval_samples_per_second': 8.705, 'eval_steps_per_second': 2.176, 'epoch': 0.3}
                                                       30%|       | 980/3250 [1:31:48<3:24:08,  5.40s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_4/tmp-checkpoint-980
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-980
/projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-980
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-980
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-980
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-980/pytorch_model.binthe pytorch model path is
 /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-980/pytorch_model.bin
the pytorch model path is the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-980/pytorch_model.bin/projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-980/pytorch_model.bin

/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.8381, 'learning_rate': 7.920899434678612e-05, 'epoch': 0.3}
{'loss': 0.8252, 'learning_rate': 7.916972895537471e-05, 'epoch': 0.3}
{'loss': 0.7806, 'learning_rate': 7.913043627418144e-05, 'epoch': 0.3}
{'loss': 0.8524, 'learning_rate': 7.909111633996664e-05, 'epoch': 0.3}
{'loss': 0.8554, 'learning_rate': 7.905176918951613e-05, 'epoch': 0.3}
{'loss': 0.8114, 'learning_rate': 7.901239485964121e-05, 'epoch': 0.3}
 30%|       | 981/3250 [1:31:54<3:43:49,  5.92s/it]                                                       30%|       | 981/3250 [1:31:54<3:43:49,  5.92s/it] 30%|       | 982/3250 [1:31:59<3:37:38,  5.76s/it]                                                       30%|       | 982/3250 [1:31:59<3:37:38,  5.76s/it] 30%|       | 983/3250 [1:32:04<3:33:10,  5.64s/it]                                                       30%|       | 983/3250 [1:32:04<3:33:10,  5.64s/it] 30%|       | 984/3250 [1:32:10<3:35:27,  5.70s/it]                                                       30%|       | 984/3250 [1:32:10<3:35:27,  5.70s/it] 30%|       | 985/3250 [1:32:16<3:31:32,  5.60s/it]                                                       30%|       | 985/3250 [1:32:16<3:31:32,  5.60s/it] 30%|       | 986/3250 [1:32:21<3:30:01,  5.57s/it]                                                       30%|  {'loss': 0.7853, 'learning_rate': 7.897299338717854e-05, 'epoch': 0.3}
{'loss': 0.8204, 'learning_rate': 7.89335648089903e-05, 'epoch': 0.3}
{'loss': 0.7971, 'learning_rate': 7.889410916196389e-05, 'epoch': 0.3}
{'loss': 0.8131, 'learning_rate': 7.885462648301212e-05, 'epoch': 0.3}
     | 986/3250 [1:32:21<3:30:01,  5.57s/it] 30%|       | 987/3250 [1:32:27<3:28:09,  5.52s/it]                                                       30%|       | 987/3250 [1:32:27<3:28:09,  5.52s/it] 30%|       | 988/3250 [1:32:32<3:26:26,  5.48s/it]                                                       30%|       | 988/3250 [1:32:32<3:26:26,  5.48s/it] 30%|       | 989/3250 [1:32:37<3:25:16,  5.45s/it]                                                       30%|       | 989/3250 [1:32:37<3:25:16,  5.45s/it] 30%|       | 990/3250 [1:32:43<3:24:17,  5.42s/it]                                                       30%|       | 990/3250 [1:32:43<3:24:17,  5.42s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.909551203250885, 'eval_runtime': 1.369, 'eval_samples_per_second': 8.766, 'eval_steps_per_second': 2.191, 'epoch': 0.3}
                                                       30%|       | 990/3250 [1:32:44<3:24:17,  5.42s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_4/tmp-checkpoint-990
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-990
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-990
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-990
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-990
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-990/pytorch_model.bin
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-990/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-990/pytorch_model.binthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-990/pytorch_model.bin

/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.8058, 'learning_rate': 7.881511680907307e-05, 'epoch': 0.3}
{'loss': 0.799, 'learning_rate': 7.877558017711007e-05, 'epoch': 0.31}
{'loss': 0.8121, 'learning_rate': 7.873601662411167e-05, 'epoch': 0.31}
{'loss': 0.8258, 'learning_rate': 7.86964261870916e-05, 'epoch': 0.31}
{'loss': 0.8143, 'learning_rate': 7.865680890308879e-05, 'epoch': 0.31}
{'loss': 0.8052, 'learning_rate': 7.86171648091672e-05, 'epoch': 0.31}
 30%|       | 991/3250 [1:32:50<3:43:46,  5.94s/it]                                                       30%|       | 991/3250 [1:32:50<3:43:46,  5.94s/it] 31%|       | 992/3250 [1:32:55<3:37:07,  5.77s/it]                                                       31%|       | 992/3250 [1:32:55<3:37:07,  5.77s/it] 31%|       | 993/3250 [1:33:01<3:32:27,  5.65s/it]                                                       31%|       | 993/3250 [1:33:01<3:32:27,  5.65s/it] 31%|       | 994/3250 [1:33:06<3:29:12,  5.56s/it]                                                       31%|       | 994/3250 [1:33:06<3:29:12,  5.56s/it] 31%|       | 995/3250 [1:33:11<3:26:52,  5.50s/it]                                                       31%|       | 995/3250 [1:33:11<3:26:52,  5.50s/it] 31%|       | 996/3250 [1:33:17<3:25:17,  5.46s/it]                                                       31%|  {'loss': 0.8324, 'learning_rate': 7.857749394241593e-05, 'epoch': 0.31}
{'loss': 0.8385, 'learning_rate': 7.853779633994913e-05, 'epoch': 0.31}
{'loss': 0.7746, 'learning_rate': 7.849807203890595e-05, 'epoch': 0.31}
{'loss': 0.8721, 'learning_rate': 7.84583210764505e-05, 'epoch': 0.31}
     | 996/3250 [1:33:17<3:25:17,  5.46s/it] 31%|       | 997/3250 [1:33:22<3:24:41,  5.45s/it]                                                       31%|       | 997/3250 [1:33:22<3:24:41,  5.45s/it] 31%|       | 998/3250 [1:33:27<3:23:28,  5.42s/it]                                                       31%|       | 998/3250 [1:33:27<3:23:28,  5.42s/it] 31%|       | 999/3250 [1:33:33<3:22:49,  5.41s/it]                                                       31%|       | 999/3250 [1:33:33<3:22:49,  5.41s/it] 31%|       | 1000/3250 [1:33:39<3:26:35,  5.51s/it]                                                        31%|       | 1000/3250 [1:33:39<3:26:35,  5.51s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.9059316515922546, 'eval_runtime': 1.385, 'eval_samples_per_second': 8.664, 'eval_steps_per_second': 2.166, 'epoch': 0.31}
                                                        31%|       | 1000/3250 [1:33:40<3:26:35,  5.51s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_4/tmp-checkpoint-1000
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1000
 /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1000
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1000
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1000
the pytorch model path is the pytorch model path is/projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1000/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1000/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1000/pytorch_model.binthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1000/pytorch_model.bin

/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.8136, 'learning_rate': 7.841854348977186e-05, 'epoch': 0.31}
{'loss': 0.805, 'learning_rate': 7.837873931608401e-05, 'epoch': 0.31}
{'loss': 0.8112, 'learning_rate': 7.833890859262579e-05, 'epoch': 0.31}
{'loss': 0.7781, 'learning_rate': 7.829905135666091e-05, 'epoch': 0.31}
{'loss': 0.8244, 'learning_rate': 7.825916764547787e-05, 'epoch': 0.31}
 31%|       | 1001/3250 [1:33:46<3:45:11,  6.01s/it]                                                        31%|       | 1001/3250 [1:33:46<3:45:11,  6.01s/it] 31%|       | 1002/3250 [1:33:51<3:37:50,  5.81s/it]                                                        31%|       | 1002/3250 [1:33:51<3:37:50,  5.81s/it] 31%|       | 1003/3250 [1:33:56<3:32:41,  5.68s/it]                                                        31%|       | 1003/3250 [1:33:56<3:32:41,  5.68s/it] 31%|       | 1004/3250 [1:34:02<3:29:10,  5.59s/it]                                                        31%|       | 1004/3250 [1:34:02<3:29:10,  5.59s/it] 31%|       | 1005/3250 [1:34:07<3:26:38,  5.52s/it]                                                        31%|       | 1005/3250 [1:34:07<3:26:38,  5.52s/it] 31%|       | 1006/3250 [1:34:13<3:24:49,  5.48s/it]                                                       {'loss': 0.7963, 'learning_rate': 7.82192574963899e-05, 'epoch': 0.31}
{'loss': 0.8488, 'learning_rate': 7.817932094673501e-05, 'epoch': 0.31}
{'loss': 1.2782, 'learning_rate': 7.813935803387591e-05, 'epoch': 0.31}
{'loss': 0.7948, 'learning_rate': 7.809936879519994e-05, 'epoch': 0.31}
{'loss': 0.8005, 'learning_rate': 7.805935326811912e-05, 'epoch': 0.31}
 31%|       | 1006/3250 [1:34:13<3:24:49,  5.48s/it] 31%|       | 1007/3250 [1:34:18<3:23:34,  5.45s/it]                                                        31%|       | 1007/3250 [1:34:18<3:23:34,  5.45s/it] 31%|       | 1008/3250 [1:34:23<3:22:56,  5.43s/it]                                                        31%|       | 1008/3250 [1:34:23<3:22:56,  5.43s/it] 31%|       | 1009/3250 [1:34:29<3:22:04,  5.41s/it]                                                        31%|       | 1009/3250 [1:34:29<3:22:04,  5.41s/it] 31%|       | 1010/3250 [1:34:34<3:21:30,  5.40s/it]                                                        31%|       | 1010/3250 [1:34:34<3:21:30,  5.40s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.9056082963943481, 'eval_runtime': 1.3659, 'eval_samples_per_second': 8.785, 'eval_steps_per_second': 2.196, 'epoch': 0.31}
                                                        31%|       | 1010/3250 [1:34:35<3:21:30,  5.40s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_4/tmp-checkpoint-1010
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  I AM HERE AND I AM SAVING THE MODEL
/projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1010the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1010

I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1010
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1010
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1010/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1010/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1010/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1010/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.833, 'learning_rate': 7.801931149007001e-05, 'epoch': 0.31}
{'loss': 0.8255, 'learning_rate': 7.797924349851376e-05, 'epoch': 0.31}
{'loss': 0.7813, 'learning_rate': 7.793914933093604e-05, 'epoch': 0.31}
{'loss': 0.7979, 'learning_rate': 7.7899029024847e-05, 'epoch': 0.31}
{'loss': 0.8816, 'learning_rate': 7.785888261778124e-05, 'epoch': 0.31}
 31%|       | 1011/3250 [1:34:41<3:40:48,  5.92s/it]                                                        31%|       | 1011/3250 [1:34:41<3:40:48,  5.92s/it] 31%|       | 1012/3250 [1:34:47<3:34:37,  5.75s/it]                                                        31%|       | 1012/3250 [1:34:47<3:34:37,  5.75s/it] 31%|       | 1013/3250 [1:34:52<3:30:10,  5.64s/it]                                                        31%|       | 1013/3250 [1:34:52<3:30:10,  5.64s/it] 31%|       | 1014/3250 [1:34:57<3:27:07,  5.56s/it]                                                        31%|       | 1014/3250 [1:34:57<3:27:07,  5.56s/it] 31%|       | 1015/3250 [1:35:03<3:24:53,  5.50s/it]                                                        31%|       | 1015/3250 [1:35:03<3:24:53,  5.50s/it] 31%|      | 1016/3250 [1:35:08<3:23:17,  5.46s/it]                                                      {'loss': 0.8236, 'learning_rate': 7.781871014729781e-05, 'epoch': 0.31}
{'loss': 0.8212, 'learning_rate': 7.777851165098012e-05, 'epoch': 0.31}
{'loss': 0.759, 'learning_rate': 7.773828716643591e-05, 'epoch': 0.31}
{'loss': 0.8139, 'learning_rate': 7.769803673129727e-05, 'epoch': 0.31}
{'loss': 0.8219, 'learning_rate': 7.765776038322057e-05, 'epoch': 0.31}
  31%|      | 1016/3250 [1:35:08<3:23:17,  5.46s/it] 31%|      | 1017/3250 [1:35:14<3:28:49,  5.61s/it]                                                        31%|      | 1017/3250 [1:35:14<3:28:49,  5.61s/it] 31%|      | 1018/3250 [1:35:19<3:25:59,  5.54s/it]                                                        31%|      | 1018/3250 [1:35:19<3:25:59,  5.54s/it] 31%|      | 1019/3250 [1:35:25<3:24:09,  5.49s/it]                                                        31%|      | 1019/3250 [1:35:25<3:24:09,  5.49s/it] 31%|      | 1020/3250 [1:35:30<3:22:33,  5.45s/it]                                                        31%|      | 1020/3250 [1:35:30<3:22:33,  5.45s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.906084418296814, 'eval_runtime': 1.3679, 'eval_samples_per_second': 8.772, 'eval_steps_per_second': 2.193, 'epoch': 0.31}
                                                        31%|      | 1020/3250 [1:35:32<3:22:33,  5.45s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_4/tmp-checkpoint-1020
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1020
/projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1020
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1020
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1020
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1020/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1020/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1020/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1020/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.8013, 'learning_rate': 7.761745815988637e-05, 'epoch': 0.31}
{'loss': 0.7911, 'learning_rate': 7.757713009899949e-05, 'epoch': 0.31}
{'loss': 0.7848, 'learning_rate': 7.753677623828892e-05, 'epoch': 0.31}
{'loss': 0.8161, 'learning_rate': 7.749639661550775e-05, 'epoch': 0.32}
{'loss': 0.8206, 'learning_rate': 7.745599126843319e-05, 'epoch': 0.32}
 31%|      | 1021/3250 [1:35:37<3:41:23,  5.96s/it]                                                        31%|      | 1021/3250 [1:35:37<3:41:23,  5.96s/it] 31%|      | 1022/3250 [1:35:43<3:34:55,  5.79s/it]                                                        31%|      | 1022/3250 [1:35:43<3:34:55,  5.79s/it] 31%|      | 1023/3250 [1:35:48<3:30:14,  5.66s/it]                                                        31%|      | 1023/3250 [1:35:48<3:30:14,  5.66s/it] 32%|      | 1024/3250 [1:35:53<3:26:51,  5.58s/it]                                                        32%|      | 1024/3250 [1:35:53<3:26:51,  5.58s/it] 32%|      | 1025/3250 [1:35:59<3:24:32,  5.52s/it]                                                        32%|      | 1025/3250 [1:35:59<3:24:32,  5.52s/it] 32%|      | 1026/3250 [1:36:04<3:22:44,  5.47s/it]                                  {'loss': 0.8076, 'learning_rate': 7.741556023486654e-05, 'epoch': 0.32}
{'loss': 0.8017, 'learning_rate': 7.737510355263311e-05, 'epoch': 0.32}
{'loss': 0.8277, 'learning_rate': 7.733462125958219e-05, 'epoch': 0.32}
{'loss': 0.8016, 'learning_rate': 7.729411339358708e-05, 'epoch': 0.32}
{'loss': 0.8279, 'learning_rate': 7.725357999254492e-05, 'epoch': 0.32}
                      32%|      | 1026/3250 [1:36:04<3:22:44,  5.47s/it] 32%|      | 1027/3250 [1:36:10<3:21:45,  5.45s/it]                                                        32%|      | 1027/3250 [1:36:10<3:21:45,  5.45s/it] 32%|      | 1028/3250 [1:36:15<3:20:50,  5.42s/it]                                                        32%|      | 1028/3250 [1:36:15<3:20:50,  5.42s/it] 32%|      | 1029/3250 [1:36:20<3:20:18,  5.41s/it]                                                        32%|      | 1029/3250 [1:36:20<3:20:18,  5.41s/it] 32%|      | 1030/3250 [1:36:26<3:19:42,  5.40s/it]                                                        32%|      | 1030/3250 [1:36:26<3:19:42,  5.40s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.9055876135826111, 'eval_runtime': 1.3726, 'eval_samples_per_second': 8.742, 'eval_steps_per_second': 2.186, 'epoch': 0.32}
                                                        32%|      | 1030/3250 [1:36:27<3:19:42,  5.40s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_4/tmp-checkpoint-1030
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1030I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1030

I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1030
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1030
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1030/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1030/pytorch_model.bin
the pytorch model path is the pytorch model path is/projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1030/pytorch_model.bin /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1030/pytorch_model.bin

/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.8291, 'learning_rate': 7.721302109437685e-05, 'epoch': 0.32}
{'loss': 0.801, 'learning_rate': 7.717243673702777e-05, 'epoch': 0.32}
{'loss': 0.7843, 'learning_rate': 7.713182695846643e-05, 'epoch': 0.32}
{'loss': 0.7792, 'learning_rate': 7.709119179668538e-05, 'epoch': 0.32}
{'loss': 0.8342, 'learning_rate': 7.70505312897009e-05, 'epoch': 0.32}
 32%|      | 1031/3250 [1:36:33<3:38:13,  5.90s/it]                                                        32%|      | 1031/3250 [1:36:33<3:38:13,  5.90s/it] 32%|      | 1032/3250 [1:36:38<3:32:22,  5.75s/it]                                                        32%|      | 1032/3250 [1:36:38<3:32:22,  5.75s/it] 32%|      | 1033/3250 [1:36:44<3:31:04,  5.71s/it]                                                        32%|      | 1033/3250 [1:36:44<3:31:04,  5.71s/it] 32%|      | 1034/3250 [1:36:49<3:27:18,  5.61s/it]                                                        32%|      | 1034/3250 [1:36:49<3:27:18,  5.61s/it] 32%|      | 1035/3250 [1:36:55<3:24:34,  5.54s/it]                                                        32%|      | 1035/3250 [1:36:55<3:24:34,  5.54s/it] 32%|      | 1036/3250 [1:37:00<3:22:52,  5.50s/it]                                  {'loss': 0.7662, 'learning_rate': 7.700984547555299e-05, 'epoch': 0.32}
{'loss': 0.861, 'learning_rate': 7.696913439230534e-05, 'epoch': 0.32}
{'loss': 1.2995, 'learning_rate': 7.692839807804521e-05, 'epoch': 0.32}
{'loss': 0.7427, 'learning_rate': 7.688763657088358e-05, 'epoch': 0.32}
{'loss': 0.7936, 'learning_rate': 7.68468499089549e-05, 'epoch': 0.32}
                      32%|      | 1036/3250 [1:37:00<3:22:52,  5.50s/it] 32%|      | 1037/3250 [1:37:05<3:21:35,  5.47s/it]                                                        32%|      | 1037/3250 [1:37:05<3:21:35,  5.47s/it] 32%|      | 1038/3250 [1:37:11<3:20:30,  5.44s/it]                                                        32%|      | 1038/3250 [1:37:11<3:20:30,  5.44s/it] 32%|      | 1039/3250 [1:37:16<3:19:36,  5.42s/it]                                                        32%|      | 1039/3250 [1:37:16<3:19:36,  5.42s/it] 32%|      | 1040/3250 [1:37:21<3:19:17,  5.41s/it]                                                        32%|      | 1040/3250 [1:37:21<3:19:17,  5.41s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.9033750891685486, 'eval_runtime': 1.3766, 'eval_samples_per_second': 8.717, 'eval_steps_per_second': 2.179, 'epoch': 0.32}
                                                        32%|      | 1040/3250 [1:37:23<3:19:17,  5.41s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_4/tmp-checkpoint-1040
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1040
/projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1040
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1040
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1040
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1040/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1040/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1040/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1040/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.8242, 'learning_rate': 7.680603813041718e-05, 'epoch': 0.32}
{'loss': 0.8236, 'learning_rate': 7.676520127345197e-05, 'epoch': 0.32}
{'loss': 0.7851, 'learning_rate': 7.672433937626423e-05, 'epoch': 0.32}
{'loss': 0.7908, 'learning_rate': 7.668345247708236e-05, 'epoch': 0.32}
{'loss': 0.8379, 'learning_rate': 7.664254061415818e-05, 'epoch': 0.32}
 32%|      | 1041/3250 [1:37:29<3:37:58,  5.92s/it]                                                        32%|      | 1041/3250 [1:37:29<3:37:58,  5.92s/it] 32%|      | 1042/3250 [1:37:34<3:31:52,  5.76s/it]                                                        32%|      | 1042/3250 [1:37:34<3:31:52,  5.76s/it] 32%|      | 1043/3250 [1:37:39<3:27:35,  5.64s/it]                                                        32%|      | 1043/3250 [1:37:39<3:27:35,  5.64s/it] 32%|      | 1044/3250 [1:37:45<3:24:30,  5.56s/it]                                                        32%|      | 1044/3250 [1:37:45<3:24:30,  5.56s/it] 32%|      | 1045/3250 [1:37:50<3:22:17,  5.50s/it]                                                        32%|      | 1045/3250 [1:37:50<3:22:17,  5.50s/it] 32%|      | 1046/3250 [1:37:55<3:20:48,  5.47s/it]                                  {'loss': 0.8293, 'learning_rate': 7.660160382576683e-05, 'epoch': 0.32}
{'loss': 0.7899, 'learning_rate': 7.65606421502068e-05, 'epoch': 0.32}
{'loss': 0.7546, 'learning_rate': 7.651965562579979e-05, 'epoch': 0.32}
{'loss': 0.8096, 'learning_rate': 7.647864429089087e-05, 'epoch': 0.32}
{'loss': 0.7947, 'learning_rate': 7.64376081838482e-05, 'epoch': 0.32}
                      32%|      | 1046/3250 [1:37:55<3:20:48,  5.47s/it] 32%|      | 1047/3250 [1:38:01<3:19:42,  5.44s/it]                                                        32%|      | 1047/3250 [1:38:01<3:19:42,  5.44s/it] 32%|      | 1048/3250 [1:38:06<3:19:01,  5.42s/it]                                                        32%|      | 1048/3250 [1:38:06<3:19:01,  5.42s/it] 32%|      | 1049/3250 [1:38:12<3:18:27,  5.41s/it]                                                        32%|      | 1049/3250 [1:38:12<3:18:27,  5.41s/it] 32%|      | 1050/3250 [1:38:18<3:24:37,  5.58s/it]                                                        32%|      | 1050/3250 [1:38:18<3:24:37,  5.58s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.901720404624939, 'eval_runtime': 1.3806, 'eval_samples_per_second': 8.692, 'eval_steps_per_second': 2.173, 'epoch': 0.32}
                                                        32%|      | 1050/3250 [1:38:19<3:24:37,  5.58s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_4/tmp-checkpoint-1050
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1050
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1050
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1050
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1050
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1050/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1050/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1050/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1050/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.7963, 'learning_rate': 7.639654734306321e-05, 'epoch': 0.32}
{'loss': 0.7811, 'learning_rate': 7.635546180695038e-05, 'epoch': 0.32}
{'loss': 0.7945, 'learning_rate': 7.63143516139474e-05, 'epoch': 0.32}
{'loss': 0.7963, 'learning_rate': 7.627321680251494e-05, 'epoch': 0.32}
{'loss': 0.8131, 'learning_rate': 7.623205741113673e-05, 'epoch': 0.32}
 32%|      | 1051/3250 [1:38:25<3:41:16,  6.04s/it]                                                        32%|      | 1051/3250 [1:38:25<3:41:16,  6.04s/it] 32%|      | 1052/3250 [1:38:30<3:33:40,  5.83s/it]                                                        32%|      | 1052/3250 [1:38:30<3:33:40,  5.83s/it] 32%|      | 1053/3250 [1:38:35<3:28:22,  5.69s/it]                                                        32%|      | 1053/3250 [1:38:35<3:28:22,  5.69s/it] 32%|      | 1054/3250 [1:38:41<3:24:39,  5.59s/it]                                                        32%|      | 1054/3250 [1:38:41<3:24:39,  5.59s/it] 32%|      | 1055/3250 [1:38:46<3:21:58,  5.52s/it]                                                        32%|      | 1055/3250 [1:38:46<3:21:58,  5.52s/it] 32%|      | 1056/3250 [1:38:51<3:20:03,  5.47s/it]                                  {'loss': 0.8007, 'learning_rate': 7.61908734783195e-05, 'epoch': 0.32}
{'loss': 0.8163, 'learning_rate': 7.614966504259293e-05, 'epoch': 0.33}
{'loss': 0.7969, 'learning_rate': 7.610843214250964e-05, 'epoch': 0.33}
{'loss': 0.8126, 'learning_rate': 7.606717481664514e-05, 'epoch': 0.33}
{'loss': 0.777, 'learning_rate': 7.602589310359778e-05, 'epoch': 0.33}
                      32%|      | 1056/3250 [1:38:51<3:20:03,  5.47s/it] 33%|      | 1057/3250 [1:38:57<3:18:47,  5.44s/it]                                                        33%|      | 1057/3250 [1:38:57<3:18:47,  5.44s/it] 33%|      | 1058/3250 [1:39:02<3:17:58,  5.42s/it]                                                        33%|      | 1058/3250 [1:39:02<3:17:58,  5.42s/it] 33%|      | 1059/3250 [1:39:08<3:17:28,  5.41s/it]                                                        33%|      | 1059/3250 [1:39:08<3:17:28,  5.41s/it] 33%|      | 1060/3250 [1:39:13<3:16:57,  5.40s/it]                                                        33%|      | 1060/3250 [1:39:13<3:16:57,  5.40s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.9026168584823608, 'eval_runtime': 1.3751, 'eval_samples_per_second': 8.726, 'eval_steps_per_second': 2.182, 'epoch': 0.33}
                                                        33%|      | 1060/3250 [1:39:14<3:16:57,  5.40s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_4/tmp-checkpoint-1060
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in I AM HERE AND I AM SAVING THE MODEL 
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1060
/projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1060
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1060
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1060
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1060/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1060/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1060/pytorch_model.binthe pytorch model path is 
/projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1060/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.8641, 'learning_rate': 7.598458704198869e-05, 'epoch': 0.33}
{'loss': 0.7835, 'learning_rate': 7.594325667046186e-05, 'epoch': 0.33}
{'loss': 0.7911, 'learning_rate': 7.590190202768394e-05, 'epoch': 0.33}
{'loss': 0.7869, 'learning_rate': 7.586052315234437e-05, 'epoch': 0.33}
{'loss': 0.7888, 'learning_rate': 7.58191200831552e-05, 'epoch': 0.33}
 33%|      | 1061/3250 [1:39:20<3:35:42,  5.91s/it]                                                        33%|      | 1061/3250 [1:39:20<3:35:42,  5.91s/it] 33%|      | 1062/3250 [1:39:25<3:29:37,  5.75s/it]                                                        33%|      | 1062/3250 [1:39:25<3:29:37,  5.75s/it] 33%|      | 1063/3250 [1:39:31<3:25:22,  5.63s/it]                                                        33%|      | 1063/3250 [1:39:31<3:25:22,  5.63s/it] 33%|      | 1064/3250 [1:39:36<3:22:23,  5.56s/it]                                                        33%|      | 1064/3250 [1:39:36<3:22:23,  5.56s/it] 33%|      | 1065/3250 [1:39:42<3:20:13,  5.50s/it]                                                        33%|      | 1065/3250 [1:39:42<3:20:13,  5.50s/it] 33%|      | 1066/3250 [1:39:47<3:21:38,  5.54s/it]                                  {'loss': 0.7861, 'learning_rate': 7.577769285885109e-05, 'epoch': 0.33}
{'loss': 0.8111, 'learning_rate': 7.57362415181894e-05, 'epoch': 0.33}
{'loss': 0.8147, 'learning_rate': 7.569476609994994e-05, 'epoch': 0.33}
{'loss': 1.2529, 'learning_rate': 7.565326664293512e-05, 'epoch': 0.33}
{'loss': 0.777, 'learning_rate': 7.561174318596983e-05, 'epoch': 0.33}
                      33%|      | 1066/3250 [1:39:47<3:21:38,  5.54s/it] 33%|      | 1067/3250 [1:39:53<3:19:37,  5.49s/it]                                                        33%|      | 1067/3250 [1:39:53<3:19:37,  5.49s/it] 33%|      | 1068/3250 [1:39:58<3:17:59,  5.44s/it]                                                        33%|      | 1068/3250 [1:39:58<3:17:59,  5.44s/it] 33%|      | 1069/3250 [1:40:03<3:16:45,  5.41s/it]                                                        33%|      | 1069/3250 [1:40:03<3:16:45,  5.41s/it] 33%|      | 1070/3250 [1:40:09<3:15:58,  5.39s/it]                                                        33%|      | 1070/3250 [1:40:09<3:15:58,  5.39s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.9010512232780457, 'eval_runtime': 1.3796, 'eval_samples_per_second': 8.698, 'eval_steps_per_second': 2.175, 'epoch': 0.33}
                                                        33%|      | 1070/3250 [1:40:10<3:15:58,  5.39s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_4/tmp-checkpoint-1070
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  I AM HERE AND I AM SAVING THE MODEL
/projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1070the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1070

I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1070
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1070
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1070/pytorch_model.bin
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1070/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1070/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1070/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.8062, 'learning_rate': 7.557019576790138e-05, 'epoch': 0.33}
{'loss': 0.8108, 'learning_rate': 7.552862442759954e-05, 'epoch': 0.33}
{'loss': 0.8009, 'learning_rate': 7.548702920395638e-05, 'epoch': 0.33}
{'loss': 0.7505, 'learning_rate': 7.544541013588645e-05, 'epoch': 0.33}
{'loss': 0.8412, 'learning_rate': 7.540376726232648e-05, 'epoch': 0.33}
 33%|      | 1071/3250 [1:40:16<3:35:12,  5.93s/it]                                                        33%|      | 1071/3250 [1:40:16<3:35:12,  5.93s/it] 33%|      | 1072/3250 [1:40:21<3:29:24,  5.77s/it]                                                        33%|      | 1072/3250 [1:40:21<3:29:24,  5.77s/it] 33%|      | 1073/3250 [1:40:26<3:24:55,  5.65s/it]                                                        33%|      | 1073/3250 [1:40:26<3:24:55,  5.65s/it] 33%|      | 1074/3250 [1:40:32<3:21:34,  5.56s/it]                                                        33%|      | 1074/3250 [1:40:32<3:21:34,  5.56s/it] 33%|      | 1075/3250 [1:40:37<3:19:12,  5.50s/it]                                                        33%|      | 1075/3250 [1:40:37<3:19:12,  5.50s/it] 33%|      | 1076/3250 [1:40:43<3:17:28,  5.45s/it]                                  {'loss': 0.8247, 'learning_rate': 7.536210062223552e-05, 'epoch': 0.33}
{'loss': 0.7876, 'learning_rate': 7.532041025459488e-05, 'epoch': 0.33}
{'loss': 0.769, 'learning_rate': 7.527869619840801e-05, 'epoch': 0.33}
{'loss': 0.802, 'learning_rate': 7.523695849270061e-05, 'epoch': 0.33}
{'loss': 0.7695, 'learning_rate': 7.519519717652039e-05, 'epoch': 0.33}
                      33%|      | 1076/3250 [1:40:43<3:17:28,  5.45s/it] 33%|      | 1077/3250 [1:40:48<3:16:15,  5.42s/it]                                                        33%|      | 1077/3250 [1:40:48<3:16:15,  5.42s/it] 33%|      | 1078/3250 [1:40:53<3:15:25,  5.40s/it]                                                        33%|      | 1078/3250 [1:40:53<3:15:25,  5.40s/it] 33%|      | 1079/3250 [1:40:59<3:14:48,  5.38s/it]                                                        33%|      | 1079/3250 [1:40:59<3:14:48,  5.38s/it] 33%|      | 1080/3250 [1:41:04<3:14:21,  5.37s/it]                                                        33%|      | 1080/3250 [1:41:04<3:14:21,  5.37s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8995996713638306, 'eval_runtime': 1.3704, 'eval_samples_per_second': 8.757, 'eval_steps_per_second': 2.189, 'epoch': 0.33}
                                                        33%|      | 1080/3250 [1:41:05<3:14:21,  5.37s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_4/tmp-checkpoint-1080
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1080I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1080

I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1080
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1080
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1080/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1080/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1080/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1080/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.7927, 'learning_rate': 7.515341228893725e-05, 'epoch': 0.33}
{'loss': 0.7801, 'learning_rate': 7.511160386904306e-05, 'epoch': 0.33}
{'loss': 0.78, 'learning_rate': 7.50697719559518e-05, 'epoch': 0.33}
{'loss': 0.7958, 'learning_rate': 7.502791658879932e-05, 'epoch': 0.33}
{'loss': 0.8118, 'learning_rate': 7.498603780674352e-05, 'epoch': 0.33}
 33%|      | 1081/3250 [1:41:11<3:32:49,  5.89s/it]                                                        33%|      | 1081/3250 [1:41:11<3:32:49,  5.89s/it] 33%|      | 1082/3250 [1:41:17<3:33:38,  5.91s/it]                                                        33%|      | 1082/3250 [1:41:17<3:33:38,  5.91s/it] 33%|      | 1083/3250 [1:41:22<3:28:07,  5.76s/it]                                                        33%|      | 1083/3250 [1:41:22<3:28:07,  5.76s/it] 33%|      | 1084/3250 [1:41:28<3:23:37,  5.64s/it]                                                        33%|      | 1084/3250 [1:41:28<3:23:37,  5.64s/it] 33%|      | 1085/3250 [1:41:33<3:20:24,  5.55s/it]                                                        33%|      | 1085/3250 [1:41:33<3:20:24,  5.55s/it] 33%|      | 1086/3250 [1:41:38<3:18:07,  5.49s/it]                                  {'loss': 0.8017, 'learning_rate': 7.494413564896414e-05, 'epoch': 0.33}
{'loss': 0.7841, 'learning_rate': 7.490221015466279e-05, 'epoch': 0.33}
{'loss': 0.8143, 'learning_rate': 7.486026136306293e-05, 'epoch': 0.33}
{'loss': 0.8198, 'learning_rate': 7.481828931340983e-05, 'epoch': 0.34}
{'loss': 0.7411, 'learning_rate': 7.477629404497048e-05, 'epoch': 0.34}
                      33%|      | 1086/3250 [1:41:38<3:18:07,  5.49s/it] 33%|      | 1087/3250 [1:41:44<3:16:23,  5.45s/it]                                                        33%|      | 1087/3250 [1:41:44<3:16:23,  5.45s/it] 33%|      | 1088/3250 [1:41:49<3:15:20,  5.42s/it]                                                        33%|      | 1088/3250 [1:41:49<3:15:20,  5.42s/it] 34%|      | 1089/3250 [1:41:54<3:14:33,  5.40s/it]                                                        34%|      | 1089/3250 [1:41:55<3:14:33,  5.40s/it] 34%|      | 1090/3250 [1:42:00<3:13:48,  5.38s/it]                                                        34%|      | 1090/3250 [1:42:00<3:13:48,  5.38s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.9000586867332458, 'eval_runtime': 1.3724, 'eval_samples_per_second': 8.744, 'eval_steps_per_second': 2.186, 'epoch': 0.34}
                                                        34%|      | 1090/3250 [1:42:01<3:13:48,  5.38s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_4/tmp-checkpoint-1090
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1090I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in 
 /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1090
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1090
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1090
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1090/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1090/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1090/pytorch_model.binthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1090/pytorch_model.bin

/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.8394, 'learning_rate': 7.47342755970336e-05, 'epoch': 0.34}
{'loss': 0.7889, 'learning_rate': 7.469223400890966e-05, 'epoch': 0.34}
{'loss': 0.7786, 'learning_rate': 7.465016931993069e-05, 'epoch': 0.34}
{'loss': 0.783, 'learning_rate': 7.460808156945036e-05, 'epoch': 0.34}
{'loss': 0.7639, 'learning_rate': 7.456597079684397e-05, 'epoch': 0.34}
 34%|      | 1091/3250 [1:42:07<3:32:09,  5.90s/it]                                                        34%|      | 1091/3250 [1:42:07<3:32:09,  5.90s/it] 34%|      | 1092/3250 [1:42:12<3:26:08,  5.73s/it]                                                        34%|      | 1092/3250 [1:42:12<3:26:08,  5.73s/it] 34%|      | 1093/3250 [1:42:18<3:22:01,  5.62s/it]                                                        34%|      | 1093/3250 [1:42:18<3:22:01,  5.62s/it] 34%|      | 1094/3250 [1:42:23<3:20:04,  5.57s/it]                                                        34%|      | 1094/3250 [1:42:23<3:20:04,  5.57s/it] 34%|      | 1095/3250 [1:42:28<3:17:49,  5.51s/it]                                                        34%|      | 1095/3250 [1:42:28<3:17:49,  5.51s/it] 34%|      | 1096/3250 [1:42:34<3:16:10,  5.46s/it]                                  {'loss': 0.8153, 'learning_rate': 7.452383704150828e-05, 'epoch': 0.34}
{'loss': 0.7775, 'learning_rate': 7.44816803428616e-05, 'epoch': 0.34}
{'loss': 0.8271, 'learning_rate': 7.443950074034368e-05, 'epoch': 0.34}
{'loss': 1.2535, 'learning_rate': 7.43972982734157e-05, 'epoch': 0.34}
{'loss': 0.7809, 'learning_rate': 7.435507298156026e-05, 'epoch': 0.34}
                      34%|      | 1096/3250 [1:42:34<3:16:10,  5.46s/it] 34%|      | 1097/3250 [1:42:39<3:14:52,  5.43s/it]                                                        34%|      | 1097/3250 [1:42:39<3:14:52,  5.43s/it] 34%|      | 1098/3250 [1:42:45<3:13:58,  5.41s/it]                                                        34%|      | 1098/3250 [1:42:45<3:13:58,  5.41s/it] 34%|      | 1099/3250 [1:42:50<3:15:58,  5.47s/it]                                                        34%|      | 1099/3250 [1:42:50<3:15:58,  5.47s/it] 34%|      | 1100/3250 [1:42:55<3:14:44,  5.43s/it]                                                        34%|      | 1100/3250 [1:42:55<3:14:44,  5.43s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8976893424987793, 'eval_runtime': 1.3813, 'eval_samples_per_second': 8.688, 'eval_steps_per_second': 2.172, 'epoch': 0.34}
                                                        34%|      | 1100/3250 [1:42:57<3:14:44,  5.43s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_4/tmp-checkpoint-1100
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1100
 /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1100
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1100
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1100
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1100/pytorch_model.bin
the pytorch model path is the pytorch model path is/projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1100/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1100/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1100/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.7872, 'learning_rate': 7.431282490428129e-05, 'epoch': 0.34}
{'loss': 0.8138, 'learning_rate': 7.427055408110403e-05, 'epoch': 0.34}
{'loss': 0.8113, 'learning_rate': 7.422826055157501e-05, 'epoch': 0.34}
{'loss': 0.7429, 'learning_rate': 7.4185944355262e-05, 'epoch': 0.34}
{'loss': 0.7765, 'learning_rate': 7.414360553175397e-05, 'epoch': 0.34}
 34%|      | 1101/3250 [1:43:03<3:32:40,  5.94s/it]                                                        34%|      | 1101/3250 [1:43:03<3:32:40,  5.94s/it] 34%|      | 1102/3250 [1:43:08<3:26:15,  5.76s/it]                                                        34%|      | 1102/3250 [1:43:08<3:26:15,  5.76s/it] 34%|      | 1103/3250 [1:43:13<3:21:51,  5.64s/it]                                                        34%|      | 1103/3250 [1:43:13<3:21:51,  5.64s/it] 34%|      | 1104/3250 [1:43:19<3:18:43,  5.56s/it]                                                        34%|      | 1104/3250 [1:43:19<3:18:43,  5.56s/it] 34%|      | 1105/3250 [1:43:24<3:16:41,  5.50s/it]                                                        34%|      | 1105/3250 [1:43:24<3:16:41,  5.50s/it] 34%|      | 1106/3250 [1:43:29<3:14:56,  5.46s/it]                                  {'loss': 0.8628, 'learning_rate': 7.41012441206611e-05, 'epoch': 0.34}
{'loss': 0.7936, 'learning_rate': 7.405886016161465e-05, 'epoch': 0.34}
{'loss': 0.8051, 'learning_rate': 7.401645369426697e-05, 'epoch': 0.34}
{'loss': 0.7449, 'learning_rate': 7.397402475829152e-05, 'epoch': 0.34}
{'loss': 0.7871, 'learning_rate': 7.393157339338276e-05, 'epoch': 0.34}
                      34%|      | 1106/3250 [1:43:29<3:14:56,  5.46s/it] 34%|      | 1107/3250 [1:43:35<3:13:39,  5.42s/it]                                                        34%|      | 1107/3250 [1:43:35<3:13:39,  5.42s/it] 34%|      | 1108/3250 [1:43:40<3:12:54,  5.40s/it]                                                        34%|      | 1108/3250 [1:43:40<3:12:54,  5.40s/it] 34%|      | 1109/3250 [1:43:45<3:12:11,  5.39s/it]                                                        34%|      | 1109/3250 [1:43:45<3:12:11,  5.39s/it] 34%|      | 1110/3250 [1:43:51<3:11:44,  5.38s/it]                                                        34%|      | 1110/3250 [1:43:51<3:11:44,  5.38s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8965811133384705, 'eval_runtime': 1.372, 'eval_samples_per_second': 8.747, 'eval_steps_per_second': 2.187, 'epoch': 0.34}
                                                        34%|      | 1110/3250 [1:43:52<3:11:44,  5.38s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_4/tmp-checkpoint-1110
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in I AM HERE AND I AM SAVING THE MODEL
 the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1110
/projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1110
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1110
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1110
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1110/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1110/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1110/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1110/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.8086, 'learning_rate': 7.388909963925611e-05, 'epoch': 0.34}
{'loss': 0.7687, 'learning_rate': 7.384660353564794e-05, 'epoch': 0.34}
{'loss': 0.7663, 'learning_rate': 7.380408512231557e-05, 'epoch': 0.34}
{'loss': 0.7674, 'learning_rate': 7.376154443903713e-05, 'epoch': 0.34}
{'loss': 0.7904, 'learning_rate': 7.371898152561166e-05, 'epoch': 0.34}
 34%|      | 1111/3250 [1:43:58<3:30:05,  5.89s/it]                                                        34%|      | 1111/3250 [1:43:58<3:30:05,  5.89s/it] 34%|      | 1112/3250 [1:44:03<3:24:18,  5.73s/it]                                                        34%|      | 1112/3250 [1:44:03<3:24:18,  5.73s/it] 34%|      | 1113/3250 [1:44:09<3:20:09,  5.62s/it]                                                        34%|      | 1113/3250 [1:44:09<3:20:09,  5.62s/it] 34%|      | 1114/3250 [1:44:14<3:17:09,  5.54s/it]                                                        34%|      | 1114/3250 [1:44:14<3:17:09,  5.54s/it] 34%|      | 1115/3250 [1:44:20<3:21:27,  5.66s/it]                                                        34%|      | 1115/3250 [1:44:20<3:21:27,  5.66s/it] 34%|      | 1116/3250 [1:44:25<3:18:12,  5.57s/it]                                  {'loss': 0.7955, 'learning_rate': 7.367639642185891e-05, 'epoch': 0.34}
{'loss': 0.8007, 'learning_rate': 7.363378916761945e-05, 'epoch': 0.34}
{'loss': 0.7829, 'learning_rate': 7.359115980275455e-05, 'epoch': 0.34}
{'loss': 0.8063, 'learning_rate': 7.354850836714621e-05, 'epoch': 0.34}
{'loss': 0.7908, 'learning_rate': 7.350583490069701e-05, 'epoch': 0.34}
                      34%|      | 1116/3250 [1:44:25<3:18:12,  5.57s/it] 34%|      | 1117/3250 [1:44:31<3:15:34,  5.50s/it]                                                        34%|      | 1117/3250 [1:44:31<3:15:34,  5.50s/it] 34%|      | 1118/3250 [1:44:36<3:13:53,  5.46s/it]                                                        34%|      | 1118/3250 [1:44:36<3:13:53,  5.46s/it] 34%|      | 1119/3250 [1:44:41<3:12:40,  5.42s/it]                                                        34%|      | 1119/3250 [1:44:41<3:12:40,  5.42s/it] 34%|      | 1120/3250 [1:44:47<3:11:52,  5.40s/it]                                                        34%|      | 1120/3250 [1:44:47<3:11:52,  5.40s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8972585797309875, 'eval_runtime': 1.3716, 'eval_samples_per_second': 8.749, 'eval_steps_per_second': 2.187, 'epoch': 0.34}
                                                        34%|      | 1120/3250 [1:44:48<3:11:52,  5.40s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_4/tmp-checkpoint-1120
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1120
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1120
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1120
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1120
the pytorch model path is the pytorch model path is/projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1120/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1120/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1120/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1120/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.8047, 'learning_rate': 7.346313944333016e-05, 'epoch': 0.34}
{'loss': 0.8001, 'learning_rate': 7.342042203498951e-05, 'epoch': 0.35}
{'loss': 0.7804, 'learning_rate': 7.337768271563935e-05, 'epoch': 0.35}
{'loss': 0.7598, 'learning_rate': 7.333492152526452e-05, 'epoch': 0.35}
{'loss': 0.7611, 'learning_rate': 7.329213850387031e-05, 'epoch': 0.35}
 34%|      | 1121/3250 [1:44:54<3:30:32,  5.93s/it]                                                        34%|      | 1121/3250 [1:44:54<3:30:32,  5.93s/it] 35%|      | 1122/3250 [1:44:59<3:24:06,  5.76s/it]                                                        35%|      | 1122/3250 [1:44:59<3:24:06,  5.76s/it] 35%|      | 1123/3250 [1:45:05<3:19:48,  5.64s/it]                                                        35%|      | 1123/3250 [1:45:05<3:19:48,  5.64s/it] 35%|      | 1124/3250 [1:45:10<3:16:36,  5.55s/it]                                                        35%|      | 1124/3250 [1:45:10<3:16:36,  5.55s/it] 35%|      | 1125/3250 [1:45:15<3:14:33,  5.49s/it]                                                        35%|      | 1125/3250 [1:45:15<3:14:33,  5.49s/it] 35%|      | 1126/3250 [1:45:21<3:12:58,  5.45s/it]                                  {'loss': 0.8135, 'learning_rate': 7.324933369148243e-05, 'epoch': 0.35}
{'loss': 0.7486, 'learning_rate': 7.3206507128147e-05, 'epoch': 0.35}
{'loss': 0.8563, 'learning_rate': 7.316365885393048e-05, 'epoch': 0.35}
{'loss': 1.2845, 'learning_rate': 7.312078890891963e-05, 'epoch': 0.35}
{'loss': 0.725, 'learning_rate': 7.307789733322146e-05, 'epoch': 0.35}
                      35%|      | 1126/3250 [1:45:21<3:12:58,  5.45s/it] 35%|      | 1127/3250 [1:45:26<3:11:53,  5.42s/it]                                                        35%|      | 1127/3250 [1:45:26<3:11:53,  5.42s/it] 35%|      | 1128/3250 [1:45:31<3:11:05,  5.40s/it]                                                        35%|      | 1128/3250 [1:45:31<3:11:05,  5.40s/it] 35%|      | 1129/3250 [1:45:37<3:10:22,  5.39s/it]                                                        35%|      | 1129/3250 [1:45:37<3:10:22,  5.39s/it] 35%|      | 1130/3250 [1:45:42<3:09:46,  5.37s/it]                                                        35%|      | 1130/3250 [1:45:42<3:09:46,  5.37s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8952136039733887, 'eval_runtime': 1.3746, 'eval_samples_per_second': 8.73, 'eval_steps_per_second': 2.182, 'epoch': 0.35}
                                                        35%|      | 1130/3250 [1:45:43<3:09:46,  5.37s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_4/tmp-checkpoint-1130
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  I AM HERE AND I AM SAVING THE MODEL/projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1130
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1130

I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1130
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1130
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1130/pytorch_model.bin
the pytorch model path is the pytorch model path is/projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1130/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1130/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1130/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.7706, 'learning_rate': 7.303498416696328e-05, 'epoch': 0.35}
{'loss': 0.8002, 'learning_rate': 7.299204945029254e-05, 'epoch': 0.35}
{'loss': 0.8143, 'learning_rate': 7.294909322337689e-05, 'epoch': 0.35}
{'loss': 0.7582, 'learning_rate': 7.29061155264041e-05, 'epoch': 0.35}
{'loss': 0.7833, 'learning_rate': 7.286311639958197e-05, 'epoch': 0.35}
 35%|      | 1131/3250 [1:45:49<3:28:17,  5.90s/it]                                                        35%|      | 1131/3250 [1:45:49<3:28:17,  5.90s/it] 35%|      | 1132/3250 [1:45:55<3:24:57,  5.81s/it]                                                        35%|      | 1132/3250 [1:45:55<3:24:57,  5.81s/it] 35%|      | 1133/3250 [1:46:00<3:20:12,  5.67s/it]                                                        35%|      | 1133/3250 [1:46:00<3:20:12,  5.67s/it] 35%|      | 1134/3250 [1:46:05<3:16:39,  5.58s/it]                                                        35%|      | 1134/3250 [1:46:05<3:16:39,  5.58s/it] 35%|      | 1135/3250 [1:46:11<3:14:04,  5.51s/it]                                                        35%|      | 1135/3250 [1:46:11<3:14:04,  5.51s/it] 35%|      | 1136/3250 [1:46:16<3:12:14,  5.46s/it]                                  {'loss': 0.8247, 'learning_rate': 7.282009588313845e-05, 'epoch': 0.35}
{'loss': 0.8163, 'learning_rate': 7.277705401732143e-05, 'epoch': 0.35}
{'loss': 0.7733, 'learning_rate': 7.273399084239878e-05, 'epoch': 0.35}
{'loss': 0.7351, 'learning_rate': 7.26909063986583e-05, 'epoch': 0.35}
{'loss': 0.8072, 'learning_rate': 7.264780072640774e-05, 'epoch': 0.35}
                      35%|      | 1136/3250 [1:46:16<3:12:14,  5.46s/it] 35%|      | 1137/3250 [1:46:21<3:10:57,  5.42s/it]                                                        35%|      | 1137/3250 [1:46:21<3:10:57,  5.42s/it] 35%|      | 1138/3250 [1:46:27<3:10:04,  5.40s/it]                                                        35%|      | 1138/3250 [1:46:27<3:10:04,  5.40s/it] 35%|      | 1139/3250 [1:46:32<3:09:23,  5.38s/it]                                                        35%|      | 1139/3250 [1:46:32<3:09:23,  5.38s/it] 35%|      | 1140/3250 [1:46:38<3:09:05,  5.38s/it]                                                        35%|      | 1140/3250 [1:46:38<3:09:05,  5.38s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8935831189155579, 'eval_runtime': 1.3827, 'eval_samples_per_second': 8.679, 'eval_steps_per_second': 2.17, 'epoch': 0.35}
                                                        35%|      | 1140/3250 [1:46:39<3:09:05,  5.38s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_4/tmp-checkpoint-1140
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1140
/projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1140
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1140
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1140
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1140/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1140/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1140/pytorch_model.binthe pytorch model path is
 /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1140/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.7821, 'learning_rate': 7.260467386597466e-05, 'epoch': 0.35}
{'loss': 0.783, 'learning_rate': 7.256152585770643e-05, 'epoch': 0.35}
{'loss': 0.7688, 'learning_rate': 7.251835674197029e-05, 'epoch': 0.35}
{'loss': 0.7676, 'learning_rate': 7.24751665591531e-05, 'epoch': 0.35}
{'loss': 0.7822, 'learning_rate': 7.243195534966152e-05, 'epoch': 0.35}
 35%|      | 1141/3250 [1:46:45<3:27:24,  5.90s/it]                                                        35%|      | 1141/3250 [1:46:45<3:27:24,  5.90s/it] 35%|      | 1142/3250 [1:46:50<3:21:31,  5.74s/it]                                                        35%|      | 1142/3250 [1:46:50<3:21:31,  5.74s/it] 35%|      | 1143/3250 [1:46:55<3:17:23,  5.62s/it]                                                        35%|      | 1143/3250 [1:46:55<3:17:23,  5.62s/it] 35%|      | 1144/3250 [1:47:01<3:14:33,  5.54s/it]                                                        35%|      | 1144/3250 [1:47:01<3:14:33,  5.54s/it] 35%|      | 1145/3250 [1:47:06<3:12:28,  5.49s/it]                                                        35%|      | 1145/3250 [1:47:06<3:12:28,  5.49s/it] 35%|      | 1146/3250 [1:47:11<3:10:56,  5.45s/it]                                  {'loss': 0.7934, 'learning_rate': 7.238872315392189e-05, 'epoch': 0.35}
{'loss': 0.7924, 'learning_rate': 7.234547001238012e-05, 'epoch': 0.35}
{'loss': 0.7968, 'learning_rate': 7.230219596550176e-05, 'epoch': 0.35}
{'loss': 0.7818, 'learning_rate': 7.22589010537719e-05, 'epoch': 0.35}
{'loss': 0.7949, 'learning_rate': 7.221558531769519e-05, 'epoch': 0.35}
                      35%|      | 1146/3250 [1:47:11<3:10:56,  5.45s/it] 35%|      | 1147/3250 [1:47:17<3:09:45,  5.41s/it]                                                        35%|      | 1147/3250 [1:47:17<3:09:45,  5.41s/it] 35%|      | 1148/3250 [1:47:23<3:13:52,  5.53s/it]                                                        35%|      | 1148/3250 [1:47:23<3:13:52,  5.53s/it] 35%|      | 1149/3250 [1:47:28<3:12:26,  5.50s/it]                                                        35%|      | 1149/3250 [1:47:28<3:12:26,  5.50s/it] 35%|      | 1150/3250 [1:47:33<3:10:54,  5.45s/it]                                                        35%|      | 1150/3250 [1:47:33<3:10:54,  5.45s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8944661021232605, 'eval_runtime': 1.6097, 'eval_samples_per_second': 7.455, 'eval_steps_per_second': 1.864, 'epoch': 0.35}
                                                        35%|      | 1150/3250 [1:47:35<3:10:54,  5.45s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_4/tmp-checkpoint-1150
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1150
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1150
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1150
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1150
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1150/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1150/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1150/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1150/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.762, 'learning_rate': 7.217224879779567e-05, 'epoch': 0.35}
{'loss': 0.8293, 'learning_rate': 7.212889153461694e-05, 'epoch': 0.35}
{'loss': 0.7541, 'learning_rate': 7.20855135687219e-05, 'epoch': 0.35}
{'loss': 0.7737, 'learning_rate': 7.204211494069292e-05, 'epoch': 0.36}
{'loss': 0.7745, 'learning_rate': 7.199869569113161e-05, 'epoch': 0.36}
 35%|      | 1151/3250 [1:47:41<3:30:14,  6.01s/it]                                                        35%|      | 1151/3250 [1:47:41<3:30:14,  6.01s/it] 35%|      | 1152/3250 [1:47:46<3:23:16,  5.81s/it]                                                        35%|      | 1152/3250 [1:47:46<3:23:16,  5.81s/it] 35%|      | 1153/3250 [1:47:51<3:18:20,  5.68s/it]                                                        35%|      | 1153/3250 [1:47:51<3:18:20,  5.68s/it] 36%|      | 1154/3250 [1:47:57<3:14:55,  5.58s/it]                                                        36%|      | 1154/3250 [1:47:57<3:14:55,  5.58s/it] 36%|      | 1155/3250 [1:48:02<3:12:24,  5.51s/it]                                                        36%|      | 1155/3250 [1:48:02<3:12:24,  5.51s/it] 36%|      | 1156/3250 [1:48:07<3:10:36,  5.46s/it]                                  {'loss': 0.7864, 'learning_rate': 7.195525586065892e-05, 'epoch': 0.36}
{'loss': 0.7704, 'learning_rate': 7.191179548991507e-05, 'epoch': 0.36}
{'loss': 0.8009, 'learning_rate': 7.186831461955943e-05, 'epoch': 0.36}
{'loss': 0.8045, 'learning_rate': 7.182481329027061e-05, 'epoch': 0.36}
{'loss': 1.233, 'learning_rate': 7.178129154274636e-05, 'epoch': 0.36}
                      36%|      | 1156/3250 [1:48:07<3:10:36,  5.46s/it] 36%|      | 1157/3250 [1:48:13<3:09:25,  5.43s/it]                                                        36%|      | 1157/3250 [1:48:13<3:09:25,  5.43s/it] 36%|      | 1158/3250 [1:48:18<3:08:36,  5.41s/it]                                                        36%|      | 1158/3250 [1:48:18<3:08:36,  5.41s/it] 36%|      | 1159/3250 [1:48:23<3:07:57,  5.39s/it]                                                        36%|      | 1159/3250 [1:48:23<3:07:57,  5.39s/it] 36%|      | 1160/3250 [1:48:29<3:07:26,  5.38s/it]                                                        36%|      | 1160/3250 [1:48:29<3:07:26,  5.38s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8951882719993591, 'eval_runtime': 1.3741, 'eval_samples_per_second': 8.733, 'eval_steps_per_second': 2.183, 'epoch': 0.36}
                                                        36%|      | 1160/3250 [1:48:30<3:07:26,  5.38s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_4/tmp-checkpoint-1160
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1160
/projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1160
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1160
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1160
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1160/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1160/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1160/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1160/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.7555, 'learning_rate': 7.17377494177035e-05, 'epoch': 0.36}
{'loss': 0.7915, 'learning_rate': 7.169418695587791e-05, 'epoch': 0.36}
{'loss': 0.7955, 'learning_rate': 7.165060419802453e-05, 'epoch': 0.36}
{'loss': 0.7789, 'learning_rate': 7.160700118491728e-05, 'epoch': 0.36}
{'loss': 0.7455, 'learning_rate': 7.1563377957349e-05, 'epoch': 0.36}
 36%|      | 1161/3250 [1:48:36<3:25:09,  5.89s/it]                                                        36%|      | 1161/3250 [1:48:36<3:25:09,  5.89s/it] 36%|      | 1162/3250 [1:48:41<3:19:24,  5.73s/it]                                                        36%|      | 1162/3250 [1:48:41<3:19:24,  5.73s/it] 36%|      | 1163/3250 [1:48:47<3:15:29,  5.62s/it]                                                        36%|      | 1163/3250 [1:48:47<3:15:29,  5.62s/it] 36%|      | 1164/3250 [1:48:52<3:15:02,  5.61s/it]                                                        36%|      | 1164/3250 [1:48:52<3:15:02,  5.61s/it] 36%|      | 1165/3250 [1:48:58<3:12:20,  5.54s/it]                                                        36%|      | 1165/3250 [1:48:58<3:12:20,  5.54s/it] 36%|      | 1166/3250 [1:49:03<3:10:26,  5.48s/it]                                  {'loss': 0.8173, 'learning_rate': 7.15197345561315e-05, 'epoch': 0.36}
{'loss': 0.8231, 'learning_rate': 7.147607102209538e-05, 'epoch': 0.36}
{'loss': 0.7707, 'learning_rate': 7.143238739609016e-05, 'epoch': 0.36}
{'loss': 0.7465, 'learning_rate': 7.13886837189841e-05, 'epoch': 0.36}
{'loss': 0.7804, 'learning_rate': 7.134496003166423e-05, 'epoch': 0.36}
                      36%|      | 1166/3250 [1:49:03<3:10:26,  5.48s/it] 36%|      | 1167/3250 [1:49:08<3:09:03,  5.45s/it]                                                        36%|      | 1167/3250 [1:49:08<3:09:03,  5.45s/it] 36%|      | 1168/3250 [1:49:14<3:08:00,  5.42s/it]                                                        36%|      | 1168/3250 [1:49:14<3:08:00,  5.42s/it] 36%|      | 1169/3250 [1:49:19<3:07:32,  5.41s/it]                                                        36%|      | 1169/3250 [1:49:19<3:07:32,  5.41s/it] 36%|      | 1170/3250 [1:49:24<3:07:00,  5.39s/it]                                                        36%|      | 1170/3250 [1:49:24<3:07:00,  5.39s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8916394114494324, 'eval_runtime': 1.3785, 'eval_samples_per_second': 8.705, 'eval_steps_per_second': 2.176, 'epoch': 0.36}
                                                        36%|      | 1170/3250 [1:49:26<3:07:00,  5.39s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_4/tmp-checkpoint-1170
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1170
/projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1170
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1170
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1170
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1170/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1170/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1170/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1170/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.7655, 'learning_rate': 7.130121637503632e-05, 'epoch': 0.36}
{'loss': 0.7851, 'learning_rate': 7.125745279002482e-05, 'epoch': 0.36}
{'loss': 0.764, 'learning_rate': 7.121366931757281e-05, 'epoch': 0.36}
{'loss': 0.7618, 'learning_rate': 7.116986599864197e-05, 'epoch': 0.36}
{'loss': 0.7746, 'learning_rate': 7.112604287421256e-05, 'epoch': 0.36}
 36%|      | 1171/3250 [1:49:31<3:24:05,  5.89s/it]                                                        36%|      | 1171/3250 [1:49:31<3:24:05,  5.89s/it] 36%|      | 1172/3250 [1:49:37<3:18:27,  5.73s/it]                                                        36%|      | 1172/3250 [1:49:37<3:18:27,  5.73s/it] 36%|      | 1173/3250 [1:49:42<3:14:20,  5.61s/it]                                                        36%|      | 1173/3250 [1:49:42<3:14:20,  5.61s/it] 36%|      | 1174/3250 [1:49:47<3:11:31,  5.54s/it]                                                        36%|      | 1174/3250 [1:49:47<3:11:31,  5.54s/it] 36%|      | 1175/3250 [1:49:53<3:09:32,  5.48s/it]                                                        36%|      | 1175/3250 [1:49:53<3:09:32,  5.48s/it] 36%|      | 1176/3250 [1:49:58<3:08:03,  5.44s/it]                                  {'loss': 0.7952, 'learning_rate': 7.108219998528337e-05, 'epoch': 0.36}
{'loss': 0.7839, 'learning_rate': 7.103833737287168e-05, 'epoch': 0.36}
{'loss': 0.7659, 'learning_rate': 7.099445507801323e-05, 'epoch': 0.36}
{'loss': 0.791, 'learning_rate': 7.095055314176216e-05, 'epoch': 0.36}
{'loss': 0.8057, 'learning_rate': 7.090663160519095e-05, 'epoch': 0.36}
                      36%|      | 1176/3250 [1:49:58<3:08:03,  5.44s/it] 36%|      | 1177/3250 [1:50:04<3:06:56,  5.41s/it]                                                        36%|      | 1177/3250 [1:50:04<3:06:56,  5.41s/it] 36%|      | 1178/3250 [1:50:09<3:06:13,  5.39s/it]                                                        36%|      | 1178/3250 [1:50:09<3:06:13,  5.39s/it] 36%|      | 1179/3250 [1:50:14<3:05:47,  5.38s/it]                                                        36%|      | 1179/3250 [1:50:14<3:05:47,  5.38s/it] 36%|      | 1180/3250 [1:50:20<3:05:24,  5.37s/it]                                                        36%|      | 1180/3250 [1:50:20<3:05:24,  5.37s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8943021297454834, 'eval_runtime': 1.3688, 'eval_samples_per_second': 8.767, 'eval_steps_per_second': 2.192, 'epoch': 0.36}
                                                        36%|      | 1180/3250 [1:50:21<3:05:24,  5.37s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_4/tmp-checkpoint-1180
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1180
/projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1180
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1180
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1180
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1180/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1180/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1180/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1180/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.7291, 'learning_rate': 7.086269050939051e-05, 'epoch': 0.36}
{'loss': 0.8374, 'learning_rate': 7.081872989546998e-05, 'epoch': 0.36}
{'loss': 0.7768, 'learning_rate': 7.077474980455678e-05, 'epoch': 0.36}
{'loss': 0.7642, 'learning_rate': 7.073075027779651e-05, 'epoch': 0.36}
{'loss': 0.7526, 'learning_rate': 7.068673135635302e-05, 'epoch': 0.36}
 36%|      | 1181/3250 [1:50:27<3:29:56,  6.09s/it]                                                        36%|      | 1181/3250 [1:50:27<3:29:56,  6.09s/it] 36%|      | 1182/3250 [1:50:33<3:22:12,  5.87s/it]                                                        36%|      | 1182/3250 [1:50:33<3:22:12,  5.87s/it] 36%|      | 1183/3250 [1:50:38<3:16:46,  5.71s/it]                                                        36%|      | 1183/3250 [1:50:38<3:16:46,  5.71s/it] 36%|      | 1184/3250 [1:50:43<3:13:04,  5.61s/it]                                                        36%|      | 1184/3250 [1:50:43<3:13:04,  5.61s/it] 36%|      | 1185/3250 [1:50:49<3:10:22,  5.53s/it]                                                        36%|      | 1185/3250 [1:50:49<3:10:22,  5.53s/it] 36%|      | 1186/3250 [1:50:54<3:08:31,  5.48s/it]                                  {'loss': 0.7506, 'learning_rate': 7.06426930814083e-05, 'epoch': 0.36}
{'loss': 0.7945, 'learning_rate': 7.059863549416237e-05, 'epoch': 0.37}
{'loss': 0.7571, 'learning_rate': 7.05545586358334e-05, 'epoch': 0.37}
{'loss': 0.8137, 'learning_rate': 7.051046254765755e-05, 'epoch': 0.37}
{'loss': 1.2424, 'learning_rate': 7.046634727088898e-05, 'epoch': 0.37}
                      36%|      | 1186/3250 [1:50:54<3:08:31,  5.48s/it] 37%|      | 1187/3250 [1:50:59<3:07:16,  5.45s/it]                                                        37%|      | 1187/3250 [1:50:59<3:07:16,  5.45s/it] 37%|      | 1188/3250 [1:51:05<3:06:11,  5.42s/it]                                                        37%|      | 1188/3250 [1:51:05<3:06:11,  5.42s/it] 37%|      | 1189/3250 [1:51:10<3:05:33,  5.40s/it]                                                        37%|      | 1189/3250 [1:51:10<3:05:33,  5.40s/it] 37%|      | 1190/3250 [1:51:16<3:04:50,  5.38s/it]                                                        37%|      | 1190/3250 [1:51:16<3:04:50,  5.38s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8863117694854736, 'eval_runtime': 1.3722, 'eval_samples_per_second': 8.745, 'eval_steps_per_second': 2.186, 'epoch': 0.37}
                                                        37%|      | 1190/3250 [1:51:17<3:04:50,  5.38s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_4/tmp-checkpoint-1190
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1190I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1190

I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1190
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1190
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1190/pytorch_model.bin 
/projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1190/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1190/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1190/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.7592, 'learning_rate': 7.042221284679982e-05, 'epoch': 0.37}
{'loss': 0.7741, 'learning_rate': 7.037805931668005e-05, 'epoch': 0.37}
{'loss': 0.7881, 'learning_rate': 7.03338867218376e-05, 'epoch': 0.37}
{'loss': 0.7836, 'learning_rate': 7.02896951035982e-05, 'epoch': 0.37}
{'loss': 0.7206, 'learning_rate': 7.02454845033054e-05, 'epoch': 0.37}
 37%|      | 1191/3250 [1:51:23<3:22:21,  5.90s/it]                                                        37%|      | 1191/3250 [1:51:23<3:22:21,  5.90s/it] 37%|      | 1192/3250 [1:51:28<3:16:34,  5.73s/it]                                                        37%|      | 1192/3250 [1:51:28<3:16:34,  5.73s/it] 37%|      | 1193/3250 [1:51:33<3:12:33,  5.62s/it]                                                        37%|      | 1193/3250 [1:51:33<3:12:33,  5.62s/it] 37%|      | 1194/3250 [1:51:39<3:09:53,  5.54s/it]                                                        37%|      | 1194/3250 [1:51:39<3:09:53,  5.54s/it] 37%|      | 1195/3250 [1:51:44<3:07:50,  5.48s/it]                                                        37%|      | 1195/3250 [1:51:44<3:07:50,  5.48s/it] 37%|      | 1196/3250 [1:51:49<3:06:25,  5.45s/it]                                  {'loss': 0.7588, 'learning_rate': 7.020125496232044e-05, 'epoch': 0.37}
{'loss': 0.8404, 'learning_rate': 7.015700652202237e-05, 'epoch': 0.37}
{'loss': 0.7718, 'learning_rate': 7.01127392238079e-05, 'epoch': 0.37}
{'loss': 0.7781, 'learning_rate': 7.006845310909131e-05, 'epoch': 0.37}
{'loss': 0.7567, 'learning_rate': 7.002414821930458e-05, 'epoch': 0.37}
                      37%|      | 1196/3250 [1:51:49<3:06:25,  5.45s/it] 37%|      | 1197/3250 [1:51:55<3:07:46,  5.49s/it]                                                        37%|      | 1197/3250 [1:51:55<3:07:46,  5.49s/it] 37%|      | 1198/3250 [1:52:00<3:06:21,  5.45s/it]                                                        37%|      | 1198/3250 [1:52:00<3:06:21,  5.45s/it] 37%|      | 1199/3250 [1:52:06<3:05:17,  5.42s/it]                                                        37%|      | 1199/3250 [1:52:06<3:05:17,  5.42s/it] 37%|      | 1200/3250 [1:52:11<3:04:32,  5.40s/it]                                                        37%|      | 1200/3250 [1:52:11<3:04:32,  5.40s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8887219429016113, 'eval_runtime': 1.3844, 'eval_samples_per_second': 8.668, 'eval_steps_per_second': 2.167, 'epoch': 0.37}
                                                        37%|      | 1200/3250 [1:52:12<3:04:32,  5.40s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_4/tmp-checkpoint-1200
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1200
/projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1200
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1200
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1200
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1200/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1200/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1200/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1200/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.7413, 'learning_rate': 6.99798245958972e-05, 'epoch': 0.37}
{'loss': 0.7842, 'learning_rate': 6.993548228033618e-05, 'epoch': 0.37}
{'loss': 0.7579, 'learning_rate': 6.989112131410607e-05, 'epoch': 0.37}
{'loss': 0.7616, 'learning_rate': 6.984674173870882e-05, 'epoch': 0.37}
{'loss': 0.7513, 'learning_rate': 6.98023435956638e-05, 'epoch': 0.37}
 37%|      | 1201/3250 [1:52:18<3:22:16,  5.92s/it]                                                        37%|      | 1201/3250 [1:52:18<3:22:16,  5.92s/it] 37%|      | 1202/3250 [1:52:24<3:16:12,  5.75s/it]                                                        37%|      | 1202/3250 [1:52:24<3:16:12,  5.75s/it] 37%|      | 1203/3250 [1:52:29<3:12:05,  5.63s/it]                                                        37%|      | 1203/3250 [1:52:29<3:12:05,  5.63s/it] 37%|      | 1204/3250 [1:52:34<3:09:06,  5.55s/it]                                                        37%|      | 1204/3250 [1:52:34<3:09:06,  5.55s/it] 37%|      | 1205/3250 [1:52:40<3:06:58,  5.49s/it]                                                        37%|      | 1205/3250 [1:52:40<3:06:58,  5.49s/it] 37%|      | 1206/3250 [1:52:45<3:05:27,  5.44s/it]                                  {'loss': 0.7765, 'learning_rate': 6.975792692650777e-05, 'epoch': 0.37}
{'loss': 0.7806, 'learning_rate': 6.971349177279481e-05, 'epoch': 0.37}
{'loss': 0.7791, 'learning_rate': 6.966903817609629e-05, 'epoch': 0.37}
{'loss': 0.7605, 'learning_rate': 6.962456617800081e-05, 'epoch': 0.37}
{'loss': 0.7923, 'learning_rate': 6.958007582011426e-05, 'epoch': 0.37}
                      37%|      | 1206/3250 [1:52:45<3:05:27,  5.44s/it] 37%|      | 1207/3250 [1:52:50<3:04:26,  5.42s/it]                                                        37%|      | 1207/3250 [1:52:50<3:04:26,  5.42s/it] 37%|      | 1208/3250 [1:52:56<3:03:37,  5.40s/it]                                                        37%|      | 1208/3250 [1:52:56<3:03:37,  5.40s/it] 37%|      | 1209/3250 [1:53:01<3:03:03,  5.38s/it]                                                        37%|      | 1209/3250 [1:53:01<3:03:03,  5.38s/it] 37%|      | 1210/3250 [1:53:06<3:02:41,  5.37s/it]                                                        37%|      | 1210/3250 [1:53:06<3:02:41,  5.37s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8895358443260193, 'eval_runtime': 1.3703, 'eval_samples_per_second': 8.757, 'eval_steps_per_second': 2.189, 'epoch': 0.37}
                                                        37%|      | 1210/3250 [1:53:08<3:02:41,  5.37s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_4/tmp-checkpoint-1210
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  I AM HERE AND I AM SAVING THE MODEL/projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1210
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1210

I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1210
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1210
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1210/pytorch_model.bin
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1210/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1210/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1210/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.7659, 'learning_rate': 6.95355671440596e-05, 'epoch': 0.37}
{'loss': 0.7867, 'learning_rate': 6.949104019147703e-05, 'epoch': 0.37}
{'loss': 0.7848, 'learning_rate': 6.94464950040238e-05, 'epoch': 0.37}
{'loss': 0.7655, 'learning_rate': 6.940193162337421e-05, 'epoch': 0.37}
{'loss': 0.7434, 'learning_rate': 6.935735009121958e-05, 'epoch': 0.37}
 37%|      | 1211/3250 [1:53:14<3:21:20,  5.92s/it]                                                        37%|      | 1211/3250 [1:53:14<3:21:20,  5.92s/it] 37%|      | 1212/3250 [1:53:19<3:15:48,  5.76s/it]                                                        37%|      | 1212/3250 [1:53:19<3:15:48,  5.76s/it] 37%|      | 1213/3250 [1:53:25<3:15:53,  5.77s/it]                                                        37%|      | 1213/3250 [1:53:25<3:15:53,  5.77s/it] 37%|      | 1214/3250 [1:53:30<3:11:35,  5.65s/it]                                                        37%|      | 1214/3250 [1:53:30<3:11:35,  5.65s/it] 37%|      | 1215/3250 [1:53:35<3:08:29,  5.56s/it]                                                        37%|      | 1215/3250 [1:53:35<3:08:29,  5.56s/it] 37%|      | 1216/3250 [1:53:41<3:06:17,  5.50s/it]                                  {'loss': 0.7431, 'learning_rate': 6.931275044926828e-05, 'epoch': 0.37}
{'loss': 0.7968, 'learning_rate': 6.926813273924553e-05, 'epoch': 0.37}
{'loss': 0.7322, 'learning_rate': 6.922349700289348e-05, 'epoch': 0.37}
{'loss': 0.8273, 'learning_rate': 6.91788432819712e-05, 'epoch': 0.38}
{'loss': 1.2564, 'learning_rate': 6.91341716182545e-05, 'epoch': 0.38}
                      37%|      | 1216/3250 [1:53:41<3:06:17,  5.50s/it] 37%|      | 1217/3250 [1:53:46<3:05:02,  5.46s/it]                                                        37%|      | 1217/3250 [1:53:46<3:05:02,  5.46s/it] 37%|      | 1218/3250 [1:53:52<3:03:57,  5.43s/it]                                                        37%|      | 1218/3250 [1:53:52<3:03:57,  5.43s/it] 38%|      | 1219/3250 [1:53:57<3:03:12,  5.41s/it]                                                        38%|      | 1219/3250 [1:53:57<3:03:12,  5.41s/it] 38%|      | 1220/3250 [1:54:02<3:02:37,  5.40s/it]                                                        38%|      | 1220/3250 [1:54:02<3:02:37,  5.40s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8868855237960815, 'eval_runtime': 1.3762, 'eval_samples_per_second': 8.72, 'eval_steps_per_second': 2.18, 'epoch': 0.38}
                                                        38%|      | 1220/3250 [1:54:04<3:02:37,  5.40s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_4/tmp-checkpoint-1220
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  I AM HERE AND I AM SAVING THE MODEL
/projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1220the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1220

I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1220
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1220
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1220/pytorch_model.bin
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1220/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1220/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1220/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.7304, 'learning_rate': 6.908948205353603e-05, 'epoch': 0.38}
{'loss': 0.7531, 'learning_rate': 6.904477462962521e-05, 'epoch': 0.38}
{'loss': 0.7903, 'learning_rate': 6.900004938834809e-05, 'epoch': 0.38}
{'loss': 0.7863, 'learning_rate': 6.895530637154745e-05, 'epoch': 0.38}
{'loss': 0.7488, 'learning_rate': 6.891054562108273e-05, 'epoch': 0.38}
 38%|      | 1221/3250 [1:54:09<3:20:04,  5.92s/it]                                                        38%|      | 1221/3250 [1:54:09<3:20:04,  5.92s/it] 38%|      | 1222/3250 [1:54:15<3:14:13,  5.75s/it]                                                        38%|      | 1222/3250 [1:54:15<3:14:13,  5.75s/it] 38%|      | 1223/3250 [1:54:20<3:10:42,  5.64s/it]                                                        38%|      | 1223/3250 [1:54:20<3:10:42,  5.64s/it] 38%|      | 1224/3250 [1:54:25<3:07:44,  5.56s/it]                                                        38%|      | 1224/3250 [1:54:25<3:07:44,  5.56s/it] 38%|      | 1225/3250 [1:54:31<3:05:27,  5.49s/it]                                                        38%|      | 1225/3250 [1:54:31<3:05:27,  5.49s/it] 38%|      | 1226/3250 [1:54:36<3:03:50,  5.45s/it]                                  {'loss': 0.7552, 'learning_rate': 6.886576717882982e-05, 'epoch': 0.38}
{'loss': 0.811, 'learning_rate': 6.882097108668132e-05, 'epoch': 0.38}
{'loss': 0.8119, 'learning_rate': 6.877615738654628e-05, 'epoch': 0.38}
{'loss': 0.7669, 'learning_rate': 6.87313261203502e-05, 'epoch': 0.38}
{'loss': 0.7152, 'learning_rate': 6.868647733003502e-05, 'epoch': 0.38}
                      38%|      | 1226/3250 [1:54:36<3:03:50,  5.45s/it] 38%|      | 1227/3250 [1:54:42<3:02:48,  5.42s/it]                                                        38%|      | 1227/3250 [1:54:42<3:02:48,  5.42s/it] 38%|      | 1228/3250 [1:54:47<3:02:01,  5.40s/it]                                                        38%|      | 1228/3250 [1:54:47<3:02:01,  5.40s/it] 38%|      | 1229/3250 [1:54:52<3:01:30,  5.39s/it]                                                        38%|      | 1229/3250 [1:54:52<3:01:30,  5.39s/it] 38%|      | 1230/3250 [1:54:58<3:07:13,  5.56s/it]                                                        38%|      | 1230/3250 [1:54:58<3:07:13,  5.56s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8839094042778015, 'eval_runtime': 1.3714, 'eval_samples_per_second': 8.75, 'eval_steps_per_second': 2.187, 'epoch': 0.38}
                                                        38%|      | 1230/3250 [1:55:00<3:07:13,  5.56s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_4/tmp-checkpoint-1230
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1230
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1230
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1230
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1230
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1230/pytorch_model.bin
the pytorch model path isthe pytorch model path is  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1230/pytorch_model.bin/projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1230/pytorch_model.bin

the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1230/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.7944, 'learning_rate': 6.864161105755915e-05, 'epoch': 0.38}
{'loss': 0.7606, 'learning_rate': 6.859672734489723e-05, 'epoch': 0.38}
{'loss': 0.7712, 'learning_rate': 6.855182623404033e-05, 'epoch': 0.38}
{'loss': 0.7496, 'learning_rate': 6.850690776699573e-05, 'epoch': 0.38}
{'loss': 0.7596, 'learning_rate': 6.846197198578695e-05, 'epoch': 0.38}
 38%|      | 1231/3250 [1:55:05<3:22:10,  6.01s/it]                                                        38%|      | 1231/3250 [1:55:05<3:22:10,  6.01s/it] 38%|      | 1232/3250 [1:55:11<3:15:21,  5.81s/it]                                                        38%|      | 1232/3250 [1:55:11<3:15:21,  5.81s/it] 38%|      | 1233/3250 [1:55:16<3:10:43,  5.67s/it]                                                        38%|      | 1233/3250 [1:55:16<3:10:43,  5.67s/it] 38%|      | 1234/3250 [1:55:21<3:07:29,  5.58s/it]                                                        38%|      | 1234/3250 [1:55:21<3:07:29,  5.58s/it] 38%|      | 1235/3250 [1:55:27<3:05:12,  5.52s/it]                                                        38%|      | 1235/3250 [1:55:27<3:05:12,  5.52s/it] 38%|      | 1236/3250 [1:55:32<3:03:30,  5.47s/it]                                  {'loss': 0.7553, 'learning_rate': 6.841701893245374e-05, 'epoch': 0.38}
{'loss': 0.7791, 'learning_rate': 6.8372048649052e-05, 'epoch': 0.38}
{'loss': 0.775, 'learning_rate': 6.832706117765375e-05, 'epoch': 0.38}
{'loss': 0.7797, 'learning_rate': 6.828205656034706e-05, 'epoch': 0.38}
{'loss': 0.7655, 'learning_rate': 6.823703483923607e-05, 'epoch': 0.38}
                      38%|      | 1236/3250 [1:55:32<3:03:30,  5.47s/it] 38%|      | 1237/3250 [1:55:37<3:02:17,  5.43s/it]                                                        38%|      | 1237/3250 [1:55:37<3:02:17,  5.43s/it] 38%|      | 1238/3250 [1:55:43<3:01:24,  5.41s/it]                                                        38%|      | 1238/3250 [1:55:43<3:01:24,  5.41s/it] 38%|      | 1239/3250 [1:55:48<3:00:46,  5.39s/it]                                                        38%|      | 1239/3250 [1:55:48<3:00:46,  5.39s/it] 38%|      | 1240/3250 [1:55:53<3:00:22,  5.38s/it]                                                        38%|      | 1240/3250 [1:55:53<3:00:22,  5.38s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.886383056640625, 'eval_runtime': 1.3725, 'eval_samples_per_second': 8.743, 'eval_steps_per_second': 2.186, 'epoch': 0.38}
                                                        38%|      | 1240/3250 [1:55:55<3:00:22,  5.38s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_4/tmp-checkpoint-1240
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1240
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1240
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1240
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1240
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1240/pytorch_model.bin
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1240/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1240/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1240/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.7727, 'learning_rate': 6.819199605644094e-05, 'epoch': 0.38}
{'loss': 0.7459, 'learning_rate': 6.814694025409773e-05, 'epoch': 0.38}
{'loss': 0.8169, 'learning_rate': 6.810186747435849e-05, 'epoch': 0.38}
{'loss': 0.7374, 'learning_rate': 6.805677775939111e-05, 'epoch': 0.38}
{'loss': 0.7444, 'learning_rate': 6.801167115137934e-05, 'epoch': 0.38}
 38%|      | 1241/3250 [1:56:01<3:16:59,  5.88s/it]                                                        38%|      | 1241/3250 [1:56:01<3:16:59,  5.88s/it] 38%|      | 1242/3250 [1:56:06<3:11:41,  5.73s/it]                                                        38%|      | 1242/3250 [1:56:06<3:11:41,  5.73s/it] 38%|      | 1243/3250 [1:56:11<3:07:56,  5.62s/it]                                                        38%|      | 1243/3250 [1:56:11<3:07:56,  5.62s/it] 38%|      | 1244/3250 [1:56:17<3:05:11,  5.54s/it]                                                        38%|      | 1244/3250 [1:56:17<3:05:11,  5.54s/it] 38%|      | 1245/3250 [1:56:22<3:03:20,  5.49s/it]                                                        38%|      | 1245/3250 [1:56:22<3:03:20,  5.49s/it] 38%|      | 1246/3250 [1:56:28<3:04:42,  5.53s/it]                                  {'loss': 0.7495, 'learning_rate': 6.796654769252274e-05, 'epoch': 0.38}
{'loss': 0.7581, 'learning_rate': 6.792140742503661e-05, 'epoch': 0.38}
{'loss': 0.754, 'learning_rate': 6.7876250391152e-05, 'epoch': 0.38}
{'loss': 0.7916, 'learning_rate': 6.783107663311565e-05, 'epoch': 0.38}
{'loss': 0.7818, 'learning_rate': 6.778588619318993e-05, 'epoch': 0.38}
                      38%|      | 1246/3250 [1:56:28<3:04:42,  5.53s/it] 38%|      | 1247/3250 [1:56:33<3:02:52,  5.48s/it]                                                        38%|      | 1247/3250 [1:56:33<3:02:52,  5.48s/it] 38%|      | 1248/3250 [1:56:38<3:01:40,  5.44s/it]                                                        38%|      | 1248/3250 [1:56:38<3:01:40,  5.44s/it] 38%|      | 1249/3250 [1:56:44<3:00:50,  5.42s/it]                                                        38%|      | 1249/3250 [1:56:44<3:00:50,  5.42s/it] 38%|      | 1250/3250 [1:56:49<3:00:15,  5.41s/it]                                                        38%|      | 1250/3250 [1:56:49<3:00:15,  5.41s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8836621046066284, 'eval_runtime': 1.3825, 'eval_samples_per_second': 8.68, 'eval_steps_per_second': 2.17, 'epoch': 0.38}
                                                        38%|      | 1250/3250 [1:56:50<3:00:15,  5.41s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_4/tmp-checkpoint-1250
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1250
/projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1250
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1250
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1250
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1250/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1250/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1250/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1250/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 1.2318, 'learning_rate': 6.77406791136528e-05, 'epoch': 0.38}
{'loss': 0.7494, 'learning_rate': 6.769545543679785e-05, 'epoch': 0.39}
{'loss': 0.7846, 'learning_rate': 6.76502152049341e-05, 'epoch': 0.39}
{'loss': 0.7748, 'learning_rate': 6.760495846038614e-05, 'epoch': 0.39}
{'loss': 0.7634, 'learning_rate': 6.755968524549402e-05, 'epoch': 0.39}
 38%|      | 1251/3250 [1:56:56<3:16:26,  5.90s/it]                                                        38%|      | 1251/3250 [1:56:56<3:16:26,  5.90s/it] 39%|      | 1252/3250 [1:57:01<3:10:57,  5.73s/it]                                                        39%|      | 1252/3250 [1:57:01<3:10:57,  5.73s/it] 39%|      | 1253/3250 [1:57:07<3:07:06,  5.62s/it]                                                        39%|      | 1253/3250 [1:57:07<3:07:06,  5.62s/it] 39%|      | 1254/3250 [1:57:12<3:04:21,  5.54s/it]                                                        39%|      | 1254/3250 [1:57:12<3:04:21,  5.54s/it] 39%|      | 1255/3250 [1:57:18<3:02:35,  5.49s/it]                                                        39%|      | 1255/3250 [1:57:18<3:02:35,  5.49s/it] 39%|      | 1256/3250 [1:57:23<3:01:14,  5.45s/it]                                  {'loss': 0.7289, 'learning_rate': 6.75143956026131e-05, 'epoch': 0.39}
{'loss': 0.8067, 'learning_rate': 6.746908957411421e-05, 'epoch': 0.39}
{'loss': 0.8165, 'learning_rate': 6.742376720238346e-05, 'epoch': 0.39}
{'loss': 0.7535, 'learning_rate': 6.737842852982225e-05, 'epoch': 0.39}
{'loss': 0.7333, 'learning_rate': 6.733307359884725e-05, 'epoch': 0.39}
                      39%|      | 1256/3250 [1:57:23<3:01:14,  5.45s/it] 39%|      | 1257/3250 [1:57:28<3:00:08,  5.42s/it]                                                        39%|      | 1257/3250 [1:57:28<3:00:08,  5.42s/it] 39%|      | 1258/3250 [1:57:34<2:59:24,  5.40s/it]                                                        39%|      | 1258/3250 [1:57:34<2:59:24,  5.40s/it] 39%|      | 1259/3250 [1:57:39<2:58:50,  5.39s/it]                                                        39%|      | 1259/3250 [1:57:39<2:58:50,  5.39s/it] 39%|      | 1260/3250 [1:57:44<2:58:26,  5.38s/it]                                                        39%|      | 1260/3250 [1:57:44<2:58:26,  5.38s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8838236331939697, 'eval_runtime': 1.606, 'eval_samples_per_second': 7.472, 'eval_steps_per_second': 1.868, 'epoch': 0.39}
                                                        39%|      | 1260/3250 [1:57:46<2:58:26,  5.38s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_4/tmp-checkpoint-1260
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1260I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1260

I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1260
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1260
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1260/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1260/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1260/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1260/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.766, 'learning_rate': 6.728770245189032e-05, 'epoch': 0.39}
{'loss': 0.7459, 'learning_rate': 6.724231513139852e-05, 'epoch': 0.39}
{'loss': 0.7707, 'learning_rate': 6.719691167983401e-05, 'epoch': 0.39}
{'loss': 0.7599, 'learning_rate': 6.715149213967407e-05, 'epoch': 0.39}
{'loss': 0.7517, 'learning_rate': 6.7106056553411e-05, 'epoch': 0.39}
 39%|      | 1261/3250 [1:57:52<3:17:05,  5.95s/it]                                                        39%|      | 1261/3250 [1:57:52<3:17:05,  5.95s/it] 39%|      | 1262/3250 [1:57:57<3:11:06,  5.77s/it]                                                        39%|      | 1262/3250 [1:57:57<3:11:06,  5.77s/it] 39%|      | 1263/3250 [1:58:03<3:10:38,  5.76s/it]                                                        39%|      | 1263/3250 [1:58:03<3:10:38,  5.76s/it] 39%|      | 1264/3250 [1:58:08<3:06:28,  5.63s/it]                                                        39%|      | 1264/3250 [1:58:08<3:06:28,  5.63s/it] 39%|      | 1265/3250 [1:58:13<3:03:38,  5.55s/it]                                                        39%|      | 1265/3250 [1:58:13<3:03:38,  5.55s/it] 39%|      | 1266/3250 [1:58:19<3:01:36,  5.49s/it]                                  {'loss': 0.7569, 'learning_rate': 6.706060496355212e-05, 'epoch': 0.39}
{'loss': 0.7776, 'learning_rate': 6.701513741261976e-05, 'epoch': 0.39}
{'loss': 0.766, 'learning_rate': 6.696965394315114e-05, 'epoch': 0.39}
{'loss': 0.7693, 'learning_rate': 6.692415459769836e-05, 'epoch': 0.39}
{'loss': 0.7844, 'learning_rate': 6.687863941882841e-05, 'epoch': 0.39}
                      39%|      | 1266/3250 [1:58:19<3:01:36,  5.49s/it] 39%|      | 1267/3250 [1:58:24<3:00:16,  5.45s/it]                                                        39%|      | 1267/3250 [1:58:24<3:00:16,  5.45s/it] 39%|      | 1268/3250 [1:58:29<2:59:06,  5.42s/it]                                                        39%|      | 1268/3250 [1:58:29<2:59:06,  5.42s/it] 39%|      | 1269/3250 [1:58:35<2:58:21,  5.40s/it]                                                        39%|      | 1269/3250 [1:58:35<2:58:21,  5.40s/it] 39%|      | 1270/3250 [1:58:40<2:57:55,  5.39s/it]                                                        39%|      | 1270/3250 [1:58:40<2:57:55,  5.39s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8844748735427856, 'eval_runtime': 1.3822, 'eval_samples_per_second': 8.682, 'eval_steps_per_second': 2.17, 'epoch': 0.39}
                                                        39%|      | 1270/3250 [1:58:42<2:57:55,  5.39s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_4/tmp-checkpoint-1270
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1270
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1270
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1270
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1270
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1270/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1270/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1270/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1270/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.7822, 'learning_rate': 6.683310844912311e-05, 'epoch': 0.39}
{'loss': 0.7279, 'learning_rate': 6.6787561731179e-05, 'epoch': 0.39}
{'loss': 0.817, 'learning_rate': 6.674199930760738e-05, 'epoch': 0.39}
{'loss': 0.7569, 'learning_rate': 6.669642122103423e-05, 'epoch': 0.39}
{'loss': 0.7549, 'learning_rate': 6.665082751410023e-05, 'epoch': 0.39}
 39%|      | 1271/3250 [1:58:47<3:14:32,  5.90s/it]                                                        39%|      | 1271/3250 [1:58:47<3:14:32,  5.90s/it] 39%|      | 1272/3250 [1:58:53<3:09:05,  5.74s/it]                                                        39%|      | 1272/3250 [1:58:53<3:09:05,  5.74s/it] 39%|      | 1273/3250 [1:58:58<3:05:13,  5.62s/it]                                                        39%|      | 1273/3250 [1:58:58<3:05:13,  5.62s/it] 39%|      | 1274/3250 [1:59:03<3:02:25,  5.54s/it]                                                        39%|      | 1274/3250 [1:59:03<3:02:25,  5.54s/it] 39%|      | 1275/3250 [1:59:09<3:00:25,  5.48s/it]                                                        39%|      | 1275/3250 [1:59:09<3:00:25,  5.48s/it] 39%|      | 1276/3250 [1:59:14<2:59:09,  5.45s/it]                                  {'loss': 0.7367, 'learning_rate': 6.66052182294606e-05, 'epoch': 0.39}
{'loss': 0.7381, 'learning_rate': 6.655959340978519e-05, 'epoch': 0.39}
{'loss': 0.7784, 'learning_rate': 6.651395309775837e-05, 'epoch': 0.39}
{'loss': 0.7457, 'learning_rate': 6.646829733607896e-05, 'epoch': 0.39}
{'loss': 0.7982, 'learning_rate': 6.642262616746034e-05, 'epoch': 0.39}
                      39%|      | 1276/3250 [1:59:14<2:59:09,  5.45s/it] 39%|      | 1277/3250 [1:59:19<2:58:14,  5.42s/it]                                                        39%|      | 1277/3250 [1:59:19<2:58:14,  5.42s/it] 39%|      | 1278/3250 [1:59:25<2:57:52,  5.41s/it]                                                        39%|      | 1278/3250 [1:59:25<2:57:52,  5.41s/it] 39%|      | 1279/3250 [1:59:30<2:59:52,  5.48s/it]                                                        39%|      | 1279/3250 [1:59:30<2:59:52,  5.48s/it] 39%|      | 1280/3250 [1:59:36<2:58:34,  5.44s/it]                                                        39%|      | 1280/3250 [1:59:36<2:58:34,  5.44s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8837786912918091, 'eval_runtime': 1.3867, 'eval_samples_per_second': 8.654, 'eval_steps_per_second': 2.163, 'epoch': 0.39}
                                                        39%|      | 1280/3250 [1:59:37<2:58:34,  5.44s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_4/tmp-checkpoint-1280
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1280I AM HERE AND I AM SAVING THE MODEL

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1280
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1280
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1280
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1280/pytorch_model.bin
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1280/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1280/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1280/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 1.2253, 'learning_rate': 6.637693963463018e-05, 'epoch': 0.39}
{'loss': 0.7253, 'learning_rate': 6.633123778033061e-05, 'epoch': 0.39}
{'loss': 0.7759, 'learning_rate': 6.628552064731807e-05, 'epoch': 0.39}
{'loss': 0.7851, 'learning_rate': 6.623978827836327e-05, 'epoch': 0.4}
{'loss': 0.7706, 'learning_rate': 6.61940407162512e-05, 'epoch': 0.4}
 39%|      | 1281/3250 [1:59:43<3:14:48,  5.94s/it]                                                        39%|      | 1281/3250 [1:59:43<3:14:48,  5.94s/it] 39%|      | 1282/3250 [1:59:48<3:08:53,  5.76s/it]                                                        39%|      | 1282/3250 [1:59:48<3:08:53,  5.76s/it] 39%|      | 1283/3250 [1:59:54<3:04:41,  5.63s/it]                                                        39%|      | 1283/3250 [1:59:54<3:04:41,  5.63s/it] 40%|      | 1284/3250 [1:59:59<3:01:47,  5.55s/it]                                                        40%|      | 1284/3250 [1:59:59<3:01:47,  5.55s/it] 40%|      | 1285/3250 [2:00:04<2:59:47,  5.49s/it]                                                        40%|      | 1285/3250 [2:00:04<2:59:47,  5.49s/it] 40%|      | 1286/3250 [2:00:10<2:58:13,  5.44s/it]                                  {'loss': 0.7181, 'learning_rate': 6.614827800378108e-05, 'epoch': 0.4}
{'loss': 0.7551, 'learning_rate': 6.610250018376623e-05, 'epoch': 0.4}
{'loss': 0.8229, 'learning_rate': 6.60567072990342e-05, 'epoch': 0.4}
{'loss': 0.7661, 'learning_rate': 6.601089939242657e-05, 'epoch': 0.4}
{'loss': 0.7577, 'learning_rate': 6.5965076506799e-05, 'epoch': 0.4}
                      40%|      | 1286/3250 [2:00:10<2:58:13,  5.44s/it] 40%|      | 1287/3250 [2:00:15<2:57:10,  5.42s/it]                                                        40%|      | 1287/3250 [2:00:15<2:57:10,  5.42s/it] 40%|      | 1288/3250 [2:00:20<2:56:27,  5.40s/it]                                                        40%|      | 1288/3250 [2:00:20<2:56:27,  5.40s/it] 40%|      | 1289/3250 [2:00:26<2:55:58,  5.38s/it]                                                        40%|      | 1289/3250 [2:00:26<2:55:58,  5.38s/it] 40%|      | 1290/3250 [2:00:31<2:55:35,  5.38s/it]                                                        40%|      | 1290/3250 [2:00:31<2:55:35,  5.38s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8824965953826904, 'eval_runtime': 1.3725, 'eval_samples_per_second': 8.743, 'eval_steps_per_second': 2.186, 'epoch': 0.4}
                                                        40%|      | 1290/3250 [2:00:32<2:55:35,  5.38s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_4/tmp-checkpoint-1290
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1290/projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1290

I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1290
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1290
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1290/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1290/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1290/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1290/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.7535, 'learning_rate': 6.591923868502117e-05, 'epoch': 0.4}
{'loss': 0.7265, 'learning_rate': 6.58733859699767e-05, 'epoch': 0.4}
{'loss': 0.7606, 'learning_rate': 6.582751840456315e-05, 'epoch': 0.4}
{'loss': 0.7407, 'learning_rate': 6.578163603169202e-05, 'epoch': 0.4}
{'loss': 0.755, 'learning_rate': 6.573573889428862e-05, 'epoch': 0.4}
 40%|      | 1291/3250 [2:00:38<3:13:45,  5.93s/it]                                                        40%|      | 1291/3250 [2:00:38<3:13:45,  5.93s/it] 40%|      | 1292/3250 [2:00:44<3:08:02,  5.76s/it]                                                        40%|      | 1292/3250 [2:00:44<3:08:02,  5.76s/it] 40%|      | 1293/3250 [2:00:49<3:03:52,  5.64s/it]                                                        40%|      | 1293/3250 [2:00:49<3:03:52,  5.64s/it] 40%|      | 1294/3250 [2:00:54<3:01:00,  5.55s/it]                                                        40%|      | 1294/3250 [2:00:54<3:01:00,  5.55s/it] 40%|      | 1295/3250 [2:01:00<3:02:33,  5.60s/it]                                                        40%|      | 1295/3250 [2:01:00<3:02:33,  5.60s/it] 40%|      | 1296/3250 [2:01:05<3:00:14,  5.53s/it]                                  {'loss': 0.7369, 'learning_rate': 6.568982703529206e-05, 'epoch': 0.4}
{'loss': 0.7613, 'learning_rate': 6.564390049765528e-05, 'epoch': 0.4}
{'loss': 0.7656, 'learning_rate': 6.55979593243449e-05, 'epoch': 0.4}
{'loss': 0.7619, 'learning_rate': 6.555200355834123e-05, 'epoch': 0.4}
{'loss': 0.7463, 'learning_rate': 6.55060332426383e-05, 'epoch': 0.4}
                      40%|      | 1296/3250 [2:01:05<3:00:14,  5.53s/it] 40%|      | 1297/3250 [2:01:11<2:58:36,  5.49s/it]                                                        40%|      | 1297/3250 [2:01:11<2:58:36,  5.49s/it] 40%|      | 1298/3250 [2:01:16<2:57:28,  5.45s/it]                                                        40%|      | 1298/3250 [2:01:16<2:57:28,  5.45s/it] 40%|      | 1299/3250 [2:01:22<2:56:32,  5.43s/it]                                                        40%|      | 1299/3250 [2:01:22<2:56:32,  5.43s/it] 40%|      | 1300/3250 [2:01:27<2:55:48,  5.41s/it]                                                        40%|      | 1300/3250 [2:01:27<2:55:48,  5.41s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8833528757095337, 'eval_runtime': 1.3832, 'eval_samples_per_second': 8.675, 'eval_steps_per_second': 2.169, 'epoch': 0.4}
                                                        40%|      | 1300/3250 [2:01:28<2:55:48,  5.41s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_4/tmp-checkpoint-1300
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1300
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1300
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1300
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1300
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1300/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1300/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1300/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1300/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.7796, 'learning_rate': 6.546004842024369e-05, 'epoch': 0.4}
{'loss': 0.7418, 'learning_rate': 6.541404913417853e-05, 'epoch': 0.4}
{'loss': 0.7767, 'learning_rate': 6.536803542747756e-05, 'epoch': 0.4}
{'loss': 0.773, 'learning_rate': 6.532200734318896e-05, 'epoch': 0.4}
{'loss': 0.7483, 'learning_rate': 6.527596492437436e-05, 'epoch': 0.4}
 40%|      | 1301/3250 [2:01:34<3:13:20,  5.95s/it]                                                        40%|      | 1301/3250 [2:01:34<3:13:20,  5.95s/it] 40%|      | 1302/3250 [2:01:39<3:07:29,  5.77s/it]                                                        40%|      | 1302/3250 [2:01:39<3:07:29,  5.77s/it] 40%|      | 1303/3250 [2:01:45<3:03:21,  5.65s/it]                                                        40%|      | 1303/3250 [2:01:45<3:03:21,  5.65s/it] 40%|      | 1304/3250 [2:01:50<3:00:23,  5.56s/it]                                                        40%|      | 1304/3250 [2:01:50<3:00:23,  5.56s/it] 40%|      | 1305/3250 [2:01:56<2:58:21,  5.50s/it]                                                        40%|      | 1305/3250 [2:01:56<2:58:21,  5.50s/it] 40%|      | 1306/3250 [2:02:01<2:56:56,  5.46s/it]                                  {'loss': 0.7255, 'learning_rate': 6.52299082141088e-05, 'epoch': 0.4}
{'loss': 0.7261, 'learning_rate': 6.518383725548074e-05, 'epoch': 0.4}
{'loss': 0.7787, 'learning_rate': 6.51377520915919e-05, 'epoch': 0.4}
{'loss': 0.7188, 'learning_rate': 6.509165276555734e-05, 'epoch': 0.4}
{'loss': 0.817, 'learning_rate': 6.504553932050534e-05, 'epoch': 0.4}
                      40%|      | 1306/3250 [2:02:01<2:56:56,  5.46s/it] 40%|      | 1307/3250 [2:02:06<2:55:45,  5.43s/it]                                                        40%|      | 1307/3250 [2:02:06<2:55:45,  5.43s/it] 40%|      | 1308/3250 [2:02:12<2:55:04,  5.41s/it]                                                        40%|      | 1308/3250 [2:02:12<2:55:04,  5.41s/it] 40%|      | 1309/3250 [2:02:17<2:54:31,  5.39s/it]                                                        40%|      | 1309/3250 [2:02:17<2:54:31,  5.39s/it] 40%|      | 1310/3250 [2:02:22<2:54:09,  5.39s/it]                                                        40%|      | 1310/3250 [2:02:22<2:54:09,  5.39s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8812962174415588, 'eval_runtime': 1.3759, 'eval_samples_per_second': 8.722, 'eval_steps_per_second': 2.18, 'epoch': 0.4}
                                                        40%|      | 1310/3250 [2:02:24<2:54:09,  5.39s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_4/tmp-checkpoint-1310
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1310I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in 
 /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1310
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1310
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1310
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1310/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1310/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1310/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1310/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 1.243, 'learning_rate': 6.49994117995774e-05, 'epoch': 0.4}
{'loss': 0.7185, 'learning_rate': 6.495327024592817e-05, 'epoch': 0.4}
{'loss': 0.7338, 'learning_rate': 6.490711470272549e-05, 'epoch': 0.4}
{'loss': 0.7729, 'learning_rate': 6.486094521315022e-05, 'epoch': 0.4}
{'loss': 0.7645, 'learning_rate': 6.481476182039627e-05, 'epoch': 0.4}
 40%|      | 1311/3250 [2:02:29<3:10:24,  5.89s/it]                                                        40%|      | 1311/3250 [2:02:29<3:10:24,  5.89s/it] 40%|      | 1312/3250 [2:02:35<3:07:37,  5.81s/it]                                                        40%|      | 1312/3250 [2:02:35<3:07:37,  5.81s/it] 40%|      | 1313/3250 [2:02:40<3:03:01,  5.67s/it]                                                        40%|      | 1313/3250 [2:02:40<3:03:01,  5.67s/it] 40%|      | 1314/3250 [2:02:46<2:59:56,  5.58s/it]                                                        40%|      | 1314/3250 [2:02:46<2:59:56,  5.58s/it] 40%|      | 1315/3250 [2:02:51<2:57:45,  5.51s/it]                                                        40%|      | 1315/3250 [2:02:51<2:57:45,  5.51s/it] 40%|      | 1316/3250 [2:02:56<2:56:07,  5.46s/it]                                  {'loss': 0.7362, 'learning_rate': 6.476856456767064e-05, 'epoch': 0.4}
{'loss': 0.7351, 'learning_rate': 6.472235349819318e-05, 'epoch': 0.41}
{'loss': 0.7898, 'learning_rate': 6.467612865519674e-05, 'epoch': 0.41}
{'loss': 0.7789, 'learning_rate': 6.462989008192706e-05, 'epoch': 0.41}
{'loss': 0.7532, 'learning_rate': 6.458363782164266e-05, 'epoch': 0.41}
                      40%|      | 1316/3250 [2:02:56<2:56:07,  5.46s/it] 41%|      | 1317/3250 [2:03:02<2:54:57,  5.43s/it]                                                        41%|      | 1317/3250 [2:03:02<2:54:57,  5.43s/it] 41%|      | 1318/3250 [2:03:07<2:54:15,  5.41s/it]                                                        41%|      | 1318/3250 [2:03:07<2:54:15,  5.41s/it] 41%|      | 1319/3250 [2:03:13<2:53:33,  5.39s/it]                                                        41%|      | 1319/3250 [2:03:13<2:53:33,  5.39s/it] 41%|      | 1320/3250 [2:03:18<2:53:04,  5.38s/it]                                                        41%|      | 1320/3250 [2:03:18<2:53:04,  5.38s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8802339434623718, 'eval_runtime': 1.6148, 'eval_samples_per_second': 7.431, 'eval_steps_per_second': 1.858, 'epoch': 0.41}
                                                        41%|      | 1320/3250 [2:03:19<2:53:04,  5.38s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_4/tmp-checkpoint-1320
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1320
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1320
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1320
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1320
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1320/pytorch_model.bin
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1320/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1320/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1320/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.7045, 'learning_rate': 6.453737191761493e-05, 'epoch': 0.41}
{'loss': 0.7586, 'learning_rate': 6.449109241312803e-05, 'epoch': 0.41}
{'loss': 0.7392, 'learning_rate': 6.444479935147878e-05, 'epoch': 0.41}
{'loss': 0.7549, 'learning_rate': 6.439849277597671e-05, 'epoch': 0.41}
{'loss': 0.7352, 'learning_rate': 6.435217272994406e-05, 'epoch': 0.41}
 41%|      | 1321/3250 [2:03:25<3:12:19,  5.98s/it]                                                        41%|      | 1321/3250 [2:03:25<3:12:19,  5.98s/it] 41%|      | 1322/3250 [2:03:31<3:06:11,  5.79s/it]                                                        41%|      | 1322/3250 [2:03:31<3:06:11,  5.79s/it] 41%|      | 1323/3250 [2:03:36<3:01:51,  5.66s/it]                                                        41%|      | 1323/3250 [2:03:36<3:01:51,  5.66s/it] 41%|      | 1324/3250 [2:03:41<2:58:52,  5.57s/it]                                                        41%|      | 1324/3250 [2:03:41<2:58:52,  5.57s/it] 41%|      | 1325/3250 [2:03:47<2:56:37,  5.51s/it]                                                        41%|      | 1325/3250 [2:03:47<2:56:37,  5.51s/it] 41%|      | 1326/3250 [2:03:52<2:55:09,  5.46s/it]                                  {'loss': 0.7547, 'learning_rate': 6.430583925671558e-05, 'epoch': 0.41}
{'loss': 0.7478, 'learning_rate': 6.42594923996386e-05, 'epoch': 0.41}
{'loss': 0.7677, 'learning_rate': 6.421313220207304e-05, 'epoch': 0.41}
{'loss': 0.7636, 'learning_rate': 6.416675870739118e-05, 'epoch': 0.41}
{'loss': 0.7583, 'learning_rate': 6.412037195897785e-05, 'epoch': 0.41}
                      41%|      | 1326/3250 [2:03:52<2:55:09,  5.46s/it] 41%|      | 1327/3250 [2:03:57<2:53:56,  5.43s/it]                                                        41%|      | 1327/3250 [2:03:57<2:53:56,  5.43s/it] 41%|      | 1328/3250 [2:04:03<2:56:58,  5.52s/it]                                                        41%|      | 1328/3250 [2:04:03<2:56:58,  5.52s/it] 41%|      | 1329/3250 [2:04:08<2:55:13,  5.47s/it]                                                        41%|      | 1329/3250 [2:04:08<2:55:13,  5.47s/it] 41%|      | 1330/3250 [2:04:14<2:54:06,  5.44s/it]                                                        41%|      | 1330/3250 [2:04:14<2:54:06,  5.44s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8798027038574219, 'eval_runtime': 1.3865, 'eval_samples_per_second': 8.655, 'eval_steps_per_second': 2.164, 'epoch': 0.41}
                                                        41%|      | 1330/3250 [2:04:15<2:54:06,  5.44s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_4/tmp-checkpoint-1330
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1330
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1330
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1330
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1330
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1330/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1330/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1330/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1330/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.7595, 'learning_rate': 6.407397200023027e-05, 'epoch': 0.41}
{'loss': 0.7702, 'learning_rate': 6.402755887455792e-05, 'epoch': 0.41}
{'loss': 0.7331, 'learning_rate': 6.398113262538272e-05, 'epoch': 0.41}
{'loss': 0.8016, 'learning_rate': 6.393469329613879e-05, 'epoch': 0.41}
{'loss': 0.7321, 'learning_rate': 6.388824093027253e-05, 'epoch': 0.41}
 41%|      | 1331/3250 [2:04:21<3:11:09,  5.98s/it]                                                        41%|      | 1331/3250 [2:04:21<3:11:09,  5.98s/it] 41%|      | 1332/3250 [2:04:26<3:05:02,  5.79s/it]                                                        41%|      | 1332/3250 [2:04:26<3:05:02,  5.79s/it] 41%|      | 1333/3250 [2:04:32<3:00:49,  5.66s/it]                                                        41%|      | 1333/3250 [2:04:32<3:00:49,  5.66s/it] 41%|      | 1334/3250 [2:04:37<2:57:54,  5.57s/it]                                                        41%|      | 1334/3250 [2:04:37<2:57:54,  5.57s/it] 41%|      | 1335/3250 [2:04:43<2:55:45,  5.51s/it]                                                        41%|      | 1335/3250 [2:04:43<2:55:45,  5.51s/it] 41%|      | 1336/3250 [2:04:48<2:54:13,  5.46s/it]                                  {'loss': 0.73, 'learning_rate': 6.384177557124247e-05, 'epoch': 0.41}
{'loss': 0.7376, 'learning_rate': 6.37952972625194e-05, 'epoch': 0.41}
{'loss': 0.7446, 'learning_rate': 6.374880604758615e-05, 'epoch': 0.41}
{'loss': 0.7426, 'learning_rate': 6.370230196993763e-05, 'epoch': 0.41}
{'loss': 0.7669, 'learning_rate': 6.36557850730808e-05, 'epoch': 0.41}
                      41%|      | 1336/3250 [2:04:48<2:54:13,  5.46s/it] 41%|      | 1337/3250 [2:04:53<2:53:06,  5.43s/it]                                                        41%|      | 1337/3250 [2:04:53<2:53:06,  5.43s/it] 41%|      | 1338/3250 [2:04:59<2:52:21,  5.41s/it]                                                        41%|      | 1338/3250 [2:04:59<2:52:21,  5.41s/it] 41%|      | 1339/3250 [2:05:04<2:51:49,  5.39s/it]                                                        41%|      | 1339/3250 [2:05:04<2:51:49,  5.39s/it] 41%|      | 1340/3250 [2:05:09<2:51:21,  5.38s/it]                                                        41%|      | 1340/3250 [2:05:09<2:51:21,  5.38s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8792933821678162, 'eval_runtime': 1.3776, 'eval_samples_per_second': 8.711, 'eval_steps_per_second': 2.178, 'epoch': 0.41}
                                                        41%|      | 1340/3250 [2:05:11<2:51:21,  5.38s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_4/tmp-checkpoint-1340
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1340
/projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1340
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1340
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1340
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1340/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1340/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1340/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1340/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.7745, 'learning_rate': 6.360925540053463e-05, 'epoch': 0.41}
{'loss': 1.2105, 'learning_rate': 6.356271299582999e-05, 'epoch': 0.41}
{'loss': 0.7372, 'learning_rate': 6.351615790250973e-05, 'epoch': 0.41}
{'loss': 0.7618, 'learning_rate': 6.346959016412852e-05, 'epoch': 0.41}
{'loss': 0.7582, 'learning_rate': 6.342300982425284e-05, 'epoch': 0.41}
 41%|     | 1341/3250 [2:05:16<3:07:47,  5.90s/it]                                                        41%|     | 1341/3250 [2:05:16<3:07:47,  5.90s/it] 41%|     | 1342/3250 [2:05:22<3:02:24,  5.74s/it]                                                        41%|     | 1342/3250 [2:05:22<3:02:24,  5.74s/it] 41%|     | 1343/3250 [2:05:27<2:58:40,  5.62s/it]                                                        41%|     | 1343/3250 [2:05:27<2:58:40,  5.62s/it] 41%|     | 1344/3250 [2:05:32<2:55:58,  5.54s/it]                                                        41%|     | 1344/3250 [2:05:32<2:55:58,  5.54s/it] 41%|     | 1345/3250 [2:05:38<2:56:31,  5.56s/it]                                                        41%|     | 1345/3250 [2:05:38<2:56:31,  5.56s/it] 41%|     | 1346/3250 [2:05:43<2:54:31,  5.50s/it]            {'loss': 0.7644, 'learning_rate': 6.337641692646106e-05, 'epoch': 0.41}
{'loss': 0.7227, 'learning_rate': 6.332981151434317e-05, 'epoch': 0.41}
{'loss': 0.7846, 'learning_rate': 6.328319363150095e-05, 'epoch': 0.41}
{'loss': 0.7932, 'learning_rate': 6.323656332154786e-05, 'epoch': 0.42}
{'loss': 0.744, 'learning_rate': 6.318992062810891e-05, 'epoch': 0.42}
                                            41%|     | 1346/3250 [2:05:43<2:54:31,  5.50s/it] 41%|     | 1347/3250 [2:05:49<2:52:55,  5.45s/it]                                                        41%|     | 1347/3250 [2:05:49<2:52:55,  5.45s/it] 41%|     | 1348/3250 [2:05:54<2:51:55,  5.42s/it]                                                        41%|     | 1348/3250 [2:05:54<2:51:55,  5.42s/it] 42%|     | 1349/3250 [2:05:59<2:51:03,  5.40s/it]                                                        42%|     | 1349/3250 [2:05:59<2:51:03,  5.40s/it] 42%|     | 1350/3250 [2:06:05<2:50:37,  5.39s/it]                                                        42%|     | 1350/3250 [2:06:05<2:50:37,  5.39s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8807676434516907, 'eval_runtime': 1.3871, 'eval_samples_per_second': 8.651, 'eval_steps_per_second': 2.163, 'epoch': 0.42}
                                                        42%|     | 1350/3250 [2:06:06<2:50:37,  5.39s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_4/tmp-checkpoint-1350
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1350
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1350
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1350
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1350
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1350/pytorch_model.bin
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1350/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1350/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1350/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.717, 'learning_rate': 6.314326559482076e-05, 'epoch': 0.42}
{'loss': 0.7505, 'learning_rate': 6.30965982653316e-05, 'epoch': 0.42}
{'loss': 0.7368, 'learning_rate': 6.30499186833011e-05, 'epoch': 0.42}
{'loss': 0.7494, 'learning_rate': 6.300322689240041e-05, 'epoch': 0.42}
{'loss': 0.7431, 'learning_rate': 6.295652293631212e-05, 'epoch': 0.42}
 42%|     | 1351/3250 [2:06:12<3:08:27,  5.95s/it]                                                        42%|     | 1351/3250 [2:06:12<3:08:27,  5.95s/it] 42%|     | 1352/3250 [2:06:17<3:02:37,  5.77s/it]                                                        42%|     | 1352/3250 [2:06:17<3:02:37,  5.77s/it] 42%|     | 1353/3250 [2:06:23<2:58:30,  5.65s/it]                                                        42%|     | 1353/3250 [2:06:23<2:58:30,  5.65s/it] 42%|     | 1354/3250 [2:06:28<2:55:36,  5.56s/it]                                                        42%|     | 1354/3250 [2:06:28<2:55:36,  5.56s/it] 42%|     | 1355/3250 [2:06:34<2:53:39,  5.50s/it]                                                        42%|     | 1355/3250 [2:06:34<2:53:39,  5.50s/it] 42%|     | 1356/3250 [2:06:39<2:52:21,  5.46s/it]            {'loss': 0.7322, 'learning_rate': 6.290980685873017e-05, 'epoch': 0.42}
{'loss': 0.7431, 'learning_rate': 6.286307870335984e-05, 'epoch': 0.42}
{'loss': 0.7599, 'learning_rate': 6.281633851391777e-05, 'epoch': 0.42}
{'loss': 0.7494, 'learning_rate': 6.276958633413175e-05, 'epoch': 0.42}
{'loss': 0.7445, 'learning_rate': 6.272282220774091e-05, 'epoch': 0.42}
                                            42%|     | 1356/3250 [2:06:39<2:52:21,  5.46s/it] 42%|     | 1357/3250 [2:06:44<2:51:23,  5.43s/it]                                                        42%|     | 1357/3250 [2:06:44<2:51:23,  5.43s/it] 42%|     | 1358/3250 [2:06:50<2:50:43,  5.41s/it]                                                        42%|     | 1358/3250 [2:06:50<2:50:43,  5.41s/it] 42%|     | 1359/3250 [2:06:55<2:50:09,  5.40s/it]                                                        42%|     | 1359/3250 [2:06:55<2:50:09,  5.40s/it] 42%|     | 1360/3250 [2:07:00<2:49:45,  5.39s/it]                                                        42%|     | 1360/3250 [2:07:00<2:49:45,  5.39s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8780030012130737, 'eval_runtime': 1.6099, 'eval_samples_per_second': 7.454, 'eval_steps_per_second': 1.864, 'epoch': 0.42}
                                                        42%|     | 1360/3250 [2:07:02<2:49:45,  5.39s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_4/tmp-checkpoint-1360
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1360
/projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1360
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1360
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1360
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1360/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1360/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1360/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1360/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.761, 'learning_rate': 6.267604617849544e-05, 'epoch': 0.42}
{'loss': 0.7679, 'learning_rate': 6.262925829015676e-05, 'epoch': 0.42}
{'loss': 0.7052, 'learning_rate': 6.258245858649731e-05, 'epoch': 0.42}
{'loss': 0.7959, 'learning_rate': 6.253564711130067e-05, 'epoch': 0.42}
{'loss': 0.7275, 'learning_rate': 6.248882390836135e-05, 'epoch': 0.42}
 42%|     | 1361/3250 [2:07:08<3:11:24,  6.08s/it]                                                        42%|     | 1361/3250 [2:07:08<3:11:24,  6.08s/it] 42%|     | 1362/3250 [2:07:13<3:04:37,  5.87s/it]                                                        42%|     | 1362/3250 [2:07:13<3:04:37,  5.87s/it] 42%|     | 1363/3250 [2:07:19<3:00:00,  5.72s/it]                                                        42%|     | 1363/3250 [2:07:19<3:00:00,  5.72s/it] 42%|     | 1364/3250 [2:07:24<2:56:40,  5.62s/it]                                                        42%|     | 1364/3250 [2:07:24<2:56:40,  5.62s/it] 42%|     | 1365/3250 [2:07:30<2:54:08,  5.54s/it]                                                        42%|     | 1365/3250 [2:07:30<2:54:08,  5.54s/it] 42%|     | 1366/3250 [2:07:35<2:52:53,  5.51s/it]            {'loss': 0.7376, 'learning_rate': 6.244198902148486e-05, 'epoch': 0.42}
{'loss': 0.73, 'learning_rate': 6.239514249448767e-05, 'epoch': 0.42}
{'loss': 0.7297, 'learning_rate': 6.234828437119709e-05, 'epoch': 0.42}
{'loss': 0.7637, 'learning_rate': 6.230141469545132e-05, 'epoch': 0.42}
{'loss': 0.7236, 'learning_rate': 6.225453351109934e-05, 'epoch': 0.42}
                                            42%|     | 1366/3250 [2:07:35<2:52:53,  5.51s/it] 42%|     | 1367/3250 [2:07:40<2:51:22,  5.46s/it]                                                        42%|     | 1367/3250 [2:07:40<2:51:22,  5.46s/it] 42%|     | 1368/3250 [2:07:46<2:50:22,  5.43s/it]                                                        42%|     | 1368/3250 [2:07:46<2:50:22,  5.43s/it] 42%|     | 1369/3250 [2:07:51<2:49:37,  5.41s/it]                                                        42%|     | 1369/3250 [2:07:51<2:49:37,  5.41s/it] 42%|     | 1370/3250 [2:07:56<2:49:02,  5.39s/it]                                                        42%|     | 1370/3250 [2:07:56<2:49:02,  5.39s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8758533596992493, 'eval_runtime': 1.5741, 'eval_samples_per_second': 7.623, 'eval_steps_per_second': 1.906, 'epoch': 0.42}
                                                        42%|     | 1370/3250 [2:07:58<2:49:02,  5.39s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_4/tmp-checkpoint-1370
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1370
 /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1370
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1370
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1370
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1370/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1370/pytorch_model.bin
the pytorch model path is the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1370/pytorch_model.bin
/projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1370/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.7928, 'learning_rate': 6.220764086200094e-05, 'epoch': 0.42}
{'loss': 1.2154, 'learning_rate': 6.216073679202656e-05, 'epoch': 0.42}
{'loss': 0.7099, 'learning_rate': 6.211382134505742e-05, 'epoch': 0.42}
{'loss': 0.767, 'learning_rate': 6.206689456498529e-05, 'epoch': 0.42}
{'loss': 0.7553, 'learning_rate': 6.20199564957126e-05, 'epoch': 0.42}
 42%|     | 1371/3250 [2:08:04<3:06:49,  5.97s/it]                                                        42%|     | 1371/3250 [2:08:04<3:06:49,  5.97s/it] 42%|     | 1372/3250 [2:08:09<3:00:56,  5.78s/it]                                                        42%|     | 1372/3250 [2:08:09<3:00:56,  5.78s/it] 42%|     | 1373/3250 [2:08:14<2:57:00,  5.66s/it]                                                        42%|     | 1373/3250 [2:08:14<2:57:00,  5.66s/it] 42%|     | 1374/3250 [2:08:20<2:54:28,  5.58s/it]                                                        42%|     | 1374/3250 [2:08:20<2:54:28,  5.58s/it] 42%|     | 1375/3250 [2:08:25<2:52:15,  5.51s/it]                                                        42%|     | 1375/3250 [2:08:25<2:52:15,  5.51s/it] 42%|     | 1376/3250 [2:08:31<2:50:38,  5.46s/it]            {'loss': 0.7604, 'learning_rate': 6.197300718115234e-05, 'epoch': 0.42}
{'loss': 0.6955, 'learning_rate': 6.192604666522801e-05, 'epoch': 0.42}
{'loss': 0.7415, 'learning_rate': 6.187907499187356e-05, 'epoch': 0.42}
{'loss': 0.8132, 'learning_rate': 6.183209220503343e-05, 'epoch': 0.42}
{'loss': 0.7476, 'learning_rate': 6.178509834866244e-05, 'epoch': 0.42}
                                            42%|     | 1376/3250 [2:08:31<2:50:38,  5.46s/it] 42%|     | 1377/3250 [2:08:36<2:51:43,  5.50s/it]                                                        42%|     | 1377/3250 [2:08:36<2:51:43,  5.50s/it] 42%|     | 1378/3250 [2:08:41<2:50:15,  5.46s/it]                                                        42%|     | 1378/3250 [2:08:41<2:50:15,  5.46s/it] 42%|     | 1379/3250 [2:08:47<2:49:18,  5.43s/it]                                                        42%|     | 1379/3250 [2:08:47<2:49:18,  5.43s/it] 42%|     | 1380/3250 [2:08:52<2:48:26,  5.40s/it]                                                        42%|     | 1380/3250 [2:08:52<2:48:26,  5.40s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8786223530769348, 'eval_runtime': 1.4146, 'eval_samples_per_second': 8.483, 'eval_steps_per_second': 2.121, 'epoch': 0.42}
                                                        42%|     | 1380/3250 [2:08:54<2:48:26,  5.40s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_4/tmp-checkpoint-1380
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1380
/projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1380
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1380
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1380
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1380/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1380/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1380/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1380/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.7426, 'learning_rate': 6.173809346672574e-05, 'epoch': 0.42}
{'loss': 0.7338, 'learning_rate': 6.169107760319884e-05, 'epoch': 0.43}
{'loss': 0.7118, 'learning_rate': 6.164405080206746e-05, 'epoch': 0.43}
{'loss': 0.7567, 'learning_rate': 6.159701310732763e-05, 'epoch': 0.43}
{'loss': 0.7232, 'learning_rate': 6.154996456298552e-05, 'epoch': 0.43}
 42%|     | 1381/3250 [2:08:59<3:04:46,  5.93s/it]                                                        42%|     | 1381/3250 [2:08:59<3:04:46,  5.93s/it] 43%|     | 1382/3250 [2:09:05<2:59:07,  5.75s/it]                                                        43%|     | 1382/3250 [2:09:05<2:59:07,  5.75s/it] 43%|     | 1383/3250 [2:09:10<2:55:23,  5.64s/it]                                                        43%|     | 1383/3250 [2:09:10<2:55:23,  5.64s/it] 43%|     | 1384/3250 [2:09:15<2:53:23,  5.58s/it]                                                        43%|     | 1384/3250 [2:09:15<2:53:23,  5.58s/it] 43%|     | 1385/3250 [2:09:21<2:51:25,  5.52s/it]                                                        43%|     | 1385/3250 [2:09:21<2:51:25,  5.52s/it] 43%|     | 1386/3250 [2:09:26<2:50:00,  5.47s/it]            {'loss': 0.7381, 'learning_rate': 6.150290521305746e-05, 'epoch': 0.43}
{'loss': 0.7339, 'learning_rate': 6.145583510156989e-05, 'epoch': 0.43}
{'loss': 0.7457, 'learning_rate': 6.14087542725593e-05, 'epoch': 0.43}
{'loss': 0.7526, 'learning_rate': 6.136166277007229e-05, 'epoch': 0.43}
{'loss': 0.7571, 'learning_rate': 6.13145606381653e-05, 'epoch': 0.43}
                                            43%|     | 1386/3250 [2:09:26<2:50:00,  5.47s/it] 43%|     | 1387/3250 [2:09:32<2:49:07,  5.45s/it]                                                        43%|     | 1387/3250 [2:09:32<2:49:07,  5.45s/it] 43%|     | 1388/3250 [2:09:37<2:48:22,  5.43s/it]                                                        43%|     | 1388/3250 [2:09:37<2:48:22,  5.43s/it] 43%|     | 1389/3250 [2:09:42<2:47:46,  5.41s/it]                                                        43%|     | 1389/3250 [2:09:42<2:47:46,  5.41s/it] 43%|     | 1390/3250 [2:09:49<2:59:12,  5.78s/it]                                                        43%|     | 1390/3250 [2:09:49<2:59:12,  5.78s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.875724732875824, 'eval_runtime': 2.0336, 'eval_samples_per_second': 5.901, 'eval_steps_per_second': 1.475, 'epoch': 0.43}
                                                        43%|     | 1390/3250 [2:09:51<2:59:12,  5.78s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_4/tmp-checkpoint-1390
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1390
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1390
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1390
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1390
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1390/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1390/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1390/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1390/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.7443, 'learning_rate': 6.126744792090487e-05, 'epoch': 0.43}
{'loss': 0.7622, 'learning_rate': 6.122032466236733e-05, 'epoch': 0.43}
{'loss': 0.7295, 'learning_rate': 6.117319090663893e-05, 'epoch': 0.43}
{'loss': 0.7574, 'learning_rate': 6.112604669781572e-05, 'epoch': 0.43}
{'loss': 0.7537, 'learning_rate': 6.107889208000354e-05, 'epoch': 0.43}
 43%|     | 1391/3250 [2:09:57<3:19:43,  6.45s/it]                                                        43%|     | 1391/3250 [2:09:57<3:19:43,  6.45s/it] 43%|     | 1392/3250 [2:10:02<3:09:34,  6.12s/it]                                                        43%|     | 1392/3250 [2:10:02<3:09:34,  6.12s/it] 43%|     | 1393/3250 [2:10:08<3:02:28,  5.90s/it]                                                        43%|     | 1393/3250 [2:10:08<3:02:28,  5.90s/it] 43%|     | 1394/3250 [2:10:15<3:13:28,  6.25s/it]                                                        43%|     | 1394/3250 [2:10:15<3:13:28,  6.25s/it] 43%|     | 1395/3250 [2:10:20<3:05:12,  5.99s/it]                                                        43%|     | 1395/3250 [2:10:20<3:05:12,  5.99s/it] 43%|     | 1396/3250 [2:10:26<2:59:19,  5.80s/it]            {'loss': 0.7346, 'learning_rate': 6.103172709731793e-05, 'epoch': 0.43}
{'loss': 0.7112, 'learning_rate': 6.098455179388417e-05, 'epoch': 0.43}
{'loss': 0.715, 'learning_rate': 6.093736621383721e-05, 'epoch': 0.43}
{'loss': 0.7758, 'learning_rate': 6.089017040132155e-05, 'epoch': 0.43}
{'loss': 0.7018, 'learning_rate': 6.084296440049132e-05, 'epoch': 0.43}
                                            43%|     | 1396/3250 [2:10:26<2:59:19,  5.80s/it] 43%|     | 1397/3250 [2:10:31<2:55:11,  5.67s/it]                                                        43%|     | 1397/3250 [2:10:31<2:55:11,  5.67s/it] 43%|     | 1398/3250 [2:10:36<2:52:15,  5.58s/it]                                                        43%|     | 1398/3250 [2:10:36<2:52:15,  5.58s/it] 43%|     | 1399/3250 [2:10:42<2:50:12,  5.52s/it]                                                        43%|     | 1399/3250 [2:10:42<2:50:12,  5.52s/it] 43%|     | 1400/3250 [2:10:47<2:48:50,  5.48s/it]                                                        43%|     | 1400/3250 [2:10:47<2:48:50,  5.48s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.875272810459137, 'eval_runtime': 1.3768, 'eval_samples_per_second': 8.716, 'eval_steps_per_second': 2.179, 'epoch': 0.43}
                                                        43%|     | 1400/3250 [2:10:48<2:48:50,  5.48s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_4/tmp-checkpoint-1400
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1400I AM HERE AND I AM SAVING THE MODEL

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1400
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1400
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1400
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1400/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1400/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1400/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1400/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.8039, 'learning_rate': 6.079574825551017e-05, 'epoch': 0.43}
{'loss': 1.2155, 'learning_rate': 6.0748522010551215e-05, 'epoch': 0.43}
{'loss': 0.7056, 'learning_rate': 6.070128570979703e-05, 'epoch': 0.43}
{'loss': 0.7357, 'learning_rate': 6.0654039397439635e-05, 'epoch': 0.43}
{'loss': 0.7555, 'learning_rate': 6.060678311768035e-05, 'epoch': 0.43}
 43%|     | 1401/3250 [2:10:54<3:04:48,  6.00s/it]                                                        43%|     | 1401/3250 [2:10:54<3:04:48,  6.00s/it] 43%|     | 1402/3250 [2:11:00<2:58:41,  5.80s/it]                                                        43%|     | 1402/3250 [2:11:00<2:58:41,  5.80s/it] 43%|     | 1403/3250 [2:11:05<2:54:36,  5.67s/it]                                                        43%|     | 1403/3250 [2:11:05<2:54:36,  5.67s/it] 43%|     | 1404/3250 [2:11:10<2:51:45,  5.58s/it]                                                        43%|     | 1404/3250 [2:11:10<2:51:45,  5.58s/it] 43%|     | 1405/3250 [2:11:16<2:49:42,  5.52s/it]                                                        43%|     | 1405/3250 [2:11:16<2:49:42,  5.52s/it] 43%|     | 1406/3250 [2:11:21<2:48:18,  5.48s/it]            {'loss': 0.7393, 'learning_rate': 6.0559516914729886e-05, 'epoch': 0.43}
{'loss': 0.7198, 'learning_rate': 6.05122408328082e-05, 'epoch': 0.43}
{'loss': 0.7236, 'learning_rate': 6.0464954916144465e-05, 'epoch': 0.43}
{'loss': 0.7842, 'learning_rate': 6.0417659208977127e-05, 'epoch': 0.43}
{'loss': 0.7666, 'learning_rate': 6.0370353755553753e-05, 'epoch': 0.43}
                                            43%|     | 1406/3250 [2:11:21<2:48:18,  5.48s/it] 43%|     | 1407/3250 [2:11:26<2:47:18,  5.45s/it]                                                        43%|     | 1407/3250 [2:11:26<2:47:18,  5.45s/it] 43%|     | 1408/3250 [2:11:32<2:46:31,  5.42s/it]                                                        43%|     | 1408/3250 [2:11:32<2:46:31,  5.42s/it] 43%|     | 1409/3250 [2:11:37<2:45:57,  5.41s/it]                                                        43%|     | 1409/3250 [2:11:37<2:45:57,  5.41s/it] 43%|     | 1410/3250 [2:11:43<2:49:07,  5.52s/it]                                                        43%|     | 1410/3250 [2:11:43<2:49:07,  5.52s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8756390810012817, 'eval_runtime': 1.385, 'eval_samples_per_second': 8.664, 'eval_steps_per_second': 2.166, 'epoch': 0.43}
                                                        43%|     | 1410/3250 [2:11:44<2:49:07,  5.52s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_4/tmp-checkpoint-1410
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  I AM HERE AND I AM SAVING THE MODEL/projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1410
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1410

I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1410
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1410
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1410/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1410/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1410/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1410/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.7371, 'learning_rate': 6.0323038600131024e-05, 'epoch': 0.43}
{'loss': 0.7047, 'learning_rate': 6.027571378697468e-05, 'epoch': 0.43}
{'loss': 0.7463, 'learning_rate': 6.022837936035952e-05, 'epoch': 0.43}
{'loss': 0.7204, 'learning_rate': 6.018103536456936e-05, 'epoch': 0.44}
{'loss': 0.7381, 'learning_rate': 6.013368184389692e-05, 'epoch': 0.44}
 43%|     | 1411/3250 [2:11:50<3:03:01,  5.97s/it]                                                        43%|     | 1411/3250 [2:11:50<3:03:01,  5.97s/it] 43%|     | 1412/3250 [2:11:55<2:57:21,  5.79s/it]                                                        43%|     | 1412/3250 [2:11:55<2:57:21,  5.79s/it] 43%|     | 1413/3250 [2:12:01<2:53:13,  5.66s/it]                                                        43%|     | 1413/3250 [2:12:01<2:53:13,  5.66s/it] 44%|     | 1414/3250 [2:12:06<2:50:18,  5.57s/it]                                                        44%|     | 1414/3250 [2:12:06<2:50:18,  5.57s/it] 44%|     | 1415/3250 [2:12:11<2:48:21,  5.50s/it]                                                        44%|     | 1415/3250 [2:12:11<2:48:21,  5.50s/it] 44%|     | 1416/3250 [2:12:17<2:46:53,  5.46s/it]            {'loss': 0.717, 'learning_rate': 6.008631884264388e-05, 'epoch': 0.44}
{'loss': 0.7415, 'learning_rate': 6.003894640512073e-05, 'epoch': 0.44}
{'loss': 0.7318, 'learning_rate': 5.9991564575646855e-05, 'epoch': 0.44}
{'loss': 0.7545, 'learning_rate': 5.994417339855039e-05, 'epoch': 0.44}
{'loss': 0.7482, 'learning_rate': 5.989677291816818e-05, 'epoch': 0.44}
                                            44%|     | 1416/3250 [2:12:17<2:46:53,  5.46s/it] 44%|     | 1417/3250 [2:12:22<2:46:15,  5.44s/it]                                                        44%|     | 1417/3250 [2:12:22<2:46:15,  5.44s/it] 44%|     | 1418/3250 [2:12:28<2:45:23,  5.42s/it]                                                        44%|     | 1418/3250 [2:12:28<2:45:23,  5.42s/it] 44%|     | 1419/3250 [2:12:33<2:44:32,  5.39s/it]                                                        44%|     | 1419/3250 [2:12:33<2:44:32,  5.39s/it] 44%|     | 1420/3250 [2:12:38<2:44:00,  5.38s/it]                                                        44%|     | 1420/3250 [2:12:38<2:44:00,  5.38s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.873982846736908, 'eval_runtime': 1.3743, 'eval_samples_per_second': 8.732, 'eval_steps_per_second': 2.183, 'epoch': 0.44}
                                                        44%|     | 1420/3250 [2:12:40<2:44:00,  5.38s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_4/tmp-checkpoint-1420
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1420
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1420
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1420
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1420
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1420/pytorch_model.binthe pytorch model path is
 /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1420/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1420/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1420/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.7418, 'learning_rate': 5.984936317884584e-05, 'epoch': 0.44}
{'loss': 0.7454, 'learning_rate': 5.9801944224937644e-05, 'epoch': 0.44}
{'loss': 0.7572, 'learning_rate': 5.9754516100806423e-05, 'epoch': 0.44}
{'loss': 0.7176, 'learning_rate': 5.970707885082364e-05, 'epoch': 0.44}
{'loss': 0.7919, 'learning_rate': 5.965963251936929e-05, 'epoch': 0.44}
 44%|     | 1421/3250 [2:12:45<2:58:48,  5.87s/it]                                                        44%|     | 1421/3250 [2:12:45<2:58:48,  5.87s/it] 44%|     | 1422/3250 [2:12:51<2:54:08,  5.72s/it]                                                        44%|     | 1422/3250 [2:12:51<2:54:08,  5.72s/it] 44%|     | 1423/3250 [2:12:56<2:50:46,  5.61s/it]                                                        44%|     | 1423/3250 [2:12:56<2:50:46,  5.61s/it] 44%|     | 1424/3250 [2:13:01<2:48:23,  5.53s/it]                                                        44%|     | 1424/3250 [2:13:01<2:48:23,  5.53s/it] 44%|     | 1425/3250 [2:13:07<2:46:26,  5.47s/it]                                                        44%|     | 1425/3250 [2:13:07<2:46:26,  5.47s/it] 44%|     | 1426/3250 [2:13:12<2:45:12,  5.43s/it]            {'loss': 0.7265, 'learning_rate': 5.961217715083185e-05, 'epoch': 0.44}
{'loss': 0.7123, 'learning_rate': 5.9564712789608256e-05, 'epoch': 0.44}
{'loss': 0.727, 'learning_rate': 5.951723948010388e-05, 'epoch': 0.44}
{'loss': 0.7395, 'learning_rate': 5.946975726673241e-05, 'epoch': 0.44}
{'loss': 0.7179, 'learning_rate': 5.9422266193915924e-05, 'epoch': 0.44}
                                            44%|     | 1426/3250 [2:13:12<2:45:12,  5.43s/it] 44%|     | 1427/3250 [2:13:18<2:49:07,  5.57s/it]                                                        44%|     | 1427/3250 [2:13:18<2:49:07,  5.57s/it] 44%|     | 1428/3250 [2:13:23<2:47:18,  5.51s/it]                                                        44%|     | 1428/3250 [2:13:23<2:47:18,  5.51s/it] 44%|     | 1429/3250 [2:13:29<2:45:51,  5.47s/it]                                                        44%|     | 1429/3250 [2:13:29<2:45:51,  5.47s/it] 44%|     | 1430/3250 [2:13:34<2:44:51,  5.43s/it]                                                        44%|     | 1430/3250 [2:13:34<2:44:51,  5.43s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8724268078804016, 'eval_runtime': 1.3754, 'eval_samples_per_second': 8.725, 'eval_steps_per_second': 2.181, 'epoch': 0.44}
                                                        44%|     | 1430/3250 [2:13:35<2:44:51,  5.43s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_4/tmp-checkpoint-1430
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1430
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1430
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1430
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1430
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1430/pytorch_model.bin
the pytorch model path isthe pytorch model path is  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1430/pytorch_model.bin/projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1430/pytorch_model.bin

the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1430/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.7541, 'learning_rate': 5.937476630608475e-05, 'epoch': 0.44}
{'loss': 0.7604, 'learning_rate': 5.932725764767748e-05, 'epoch': 0.44}
{'loss': 1.1995, 'learning_rate': 5.927974026314091e-05, 'epoch': 0.44}
{'loss': 0.7297, 'learning_rate': 5.9232214196930014e-05, 'epoch': 0.44}
{'loss': 0.7515, 'learning_rate': 5.918467949350784e-05, 'epoch': 0.44}
 44%|     | 1431/3250 [2:13:41<2:59:01,  5.90s/it]                                                        44%|     | 1431/3250 [2:13:41<2:59:01,  5.90s/it] 44%|     | 1432/3250 [2:13:46<2:53:58,  5.74s/it]                                                        44%|     | 1432/3250 [2:13:46<2:53:58,  5.74s/it] 44%|     | 1433/3250 [2:13:52<2:50:13,  5.62s/it]                                                        44%|     | 1433/3250 [2:13:52<2:50:13,  5.62s/it] 44%|     | 1434/3250 [2:13:57<2:47:37,  5.54s/it]                                                        44%|     | 1434/3250 [2:13:57<2:47:37,  5.54s/it] 44%|     | 1435/3250 [2:14:02<2:45:51,  5.48s/it]                                                        44%|     | 1435/3250 [2:14:02<2:45:51,  5.48s/it] 44%|     | 1436/3250 [2:14:08<2:44:36,  5.44s/it]            {'loss': 0.7476, 'learning_rate': 5.913713619734558e-05, 'epoch': 0.44}
{'loss': 0.7436, 'learning_rate': 5.908958435292241e-05, 'epoch': 0.44}
{'loss': 0.7041, 'learning_rate': 5.904202400472553e-05, 'epoch': 0.44}
{'loss': 0.7666, 'learning_rate': 5.899445519725009e-05, 'epoch': 0.44}
{'loss': 0.7835, 'learning_rate': 5.894687797499916e-05, 'epoch': 0.44}
                                            44%|     | 1436/3250 [2:14:08<2:44:36,  5.44s/it] 44%|     | 1437/3250 [2:14:13<2:43:44,  5.42s/it]                                                        44%|     | 1437/3250 [2:14:13<2:43:44,  5.42s/it] 44%|     | 1438/3250 [2:14:18<2:43:09,  5.40s/it]                                                        44%|     | 1438/3250 [2:14:18<2:43:09,  5.40s/it] 44%|     | 1439/3250 [2:14:24<2:42:42,  5.39s/it]                                                        44%|     | 1439/3250 [2:14:24<2:42:42,  5.39s/it] 44%|     | 1440/3250 [2:14:29<2:42:13,  5.38s/it]                                                        44%|     | 1440/3250 [2:14:29<2:42:13,  5.38s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8748692274093628, 'eval_runtime': 1.3763, 'eval_samples_per_second': 8.719, 'eval_steps_per_second': 2.18, 'epoch': 0.44}
                                                        44%|     | 1440/3250 [2:14:31<2:42:13,  5.38s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_4/tmp-checkpoint-1440
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1440
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1440
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1440
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1440
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1440/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1440/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1440/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1440/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.7284, 'learning_rate': 5.889929238248368e-05, 'epoch': 0.44}
{'loss': 0.7077, 'learning_rate': 5.8851698464222416e-05, 'epoch': 0.44}
{'loss': 0.7353, 'learning_rate': 5.880409626474195e-05, 'epoch': 0.44}
{'loss': 0.7201, 'learning_rate': 5.8756485828576544e-05, 'epoch': 0.44}
{'loss': 0.7497, 'learning_rate': 5.870886720026825e-05, 'epoch': 0.44}
 44%|     | 1441/3250 [2:14:36<2:58:02,  5.91s/it]                                                        44%|     | 1441/3250 [2:14:36<2:58:02,  5.91s/it] 44%|     | 1442/3250 [2:14:42<2:52:58,  5.74s/it]                                                        44%|     | 1442/3250 [2:14:42<2:52:58,  5.74s/it] 44%|     | 1443/3250 [2:14:47<2:52:51,  5.74s/it]                                                        44%|     | 1443/3250 [2:14:47<2:52:51,  5.74s/it] 44%|     | 1444/3250 [2:14:53<2:49:15,  5.62s/it]                                                        44%|     | 1444/3250 [2:14:53<2:49:15,  5.62s/it] 44%|     | 1445/3250 [2:14:58<2:46:47,  5.54s/it]                                                        44%|     | 1445/3250 [2:14:58<2:46:47,  5.54s/it] 44%|     | 1446/3250 [2:15:03<2:45:01,  5.49s/it]            {'loss': 0.7088, 'learning_rate': 5.8661240424366735e-05, 'epoch': 0.44}
{'loss': 0.7139, 'learning_rate': 5.861360554542927e-05, 'epoch': 0.45}
{'loss': 0.7269, 'learning_rate': 5.8565962608020765e-05, 'epoch': 0.45}
{'loss': 0.7547, 'learning_rate': 5.851831165671363e-05, 'epoch': 0.45}
{'loss': 0.7439, 'learning_rate': 5.847065273608777e-05, 'epoch': 0.45}
                                            44%|     | 1446/3250 [2:15:03<2:45:01,  5.49s/it] 45%|     | 1447/3250 [2:15:09<2:43:53,  5.45s/it]                                                        45%|     | 1447/3250 [2:15:09<2:43:53,  5.45s/it] 45%|     | 1448/3250 [2:15:14<2:42:51,  5.42s/it]                                                        45%|     | 1448/3250 [2:15:14<2:42:51,  5.42s/it] 45%|     | 1449/3250 [2:15:20<2:42:13,  5.40s/it]                                                        45%|     | 1449/3250 [2:15:20<2:42:13,  5.40s/it] 45%|     | 1450/3250 [2:15:25<2:41:41,  5.39s/it]                                                        45%|     | 1450/3250 [2:15:25<2:41:41,  5.39s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8729538321495056, 'eval_runtime': 1.3826, 'eval_samples_per_second': 8.679, 'eval_steps_per_second': 2.17, 'epoch': 0.45}
                                                        45%|     | 1450/3250 [2:15:26<2:41:41,  5.39s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_4/tmp-checkpoint-1450
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1450I AM HERE AND I AM SAVING THE MODEL

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1450
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1450
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1450
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1450/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1450/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1450/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1450/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.7272, 'learning_rate': 5.8422985890730576e-05, 'epoch': 0.45}
{'loss': 0.7487, 'learning_rate': 5.837531116523682e-05, 'epoch': 0.45}
{'loss': 0.7621, 'learning_rate': 5.832762860420868e-05, 'epoch': 0.45}
{'loss': 0.6877, 'learning_rate': 5.827993825225561e-05, 'epoch': 0.45}
{'loss': 0.7857, 'learning_rate': 5.823224015399442e-05, 'epoch': 0.45}
 45%|     | 1451/3250 [2:15:32<2:57:17,  5.91s/it]                                                        45%|     | 1451/3250 [2:15:32<2:57:17,  5.91s/it] 45%|     | 1452/3250 [2:15:37<2:52:13,  5.75s/it]                                                        45%|     | 1452/3250 [2:15:37<2:52:13,  5.75s/it] 45%|     | 1453/3250 [2:15:43<2:48:34,  5.63s/it]                                                        45%|     | 1453/3250 [2:15:43<2:48:34,  5.63s/it] 45%|     | 1454/3250 [2:15:48<2:46:06,  5.55s/it]                                                        45%|     | 1454/3250 [2:15:48<2:46:06,  5.55s/it] 45%|     | 1455/3250 [2:15:53<2:44:15,  5.49s/it]                                                        45%|     | 1455/3250 [2:15:53<2:44:15,  5.49s/it] 45%|     | 1456/3250 [2:15:59<2:42:53,  5.45s/it]            {'loss': 0.7212, 'learning_rate': 5.8184534354049104e-05, 'epoch': 0.45}
{'loss': 0.7288, 'learning_rate': 5.813682089705092e-05, 'epoch': 0.45}
{'loss': 0.719, 'learning_rate': 5.808909982763825e-05, 'epoch': 0.45}
{'loss': 0.7182, 'learning_rate': 5.8041371190456595e-05, 'epoch': 0.45}
{'loss': 0.7486, 'learning_rate': 5.799363503015856e-05, 'epoch': 0.45}
                                            45%|     | 1456/3250 [2:15:59<2:42:53,  5.45s/it] 45%|     | 1457/3250 [2:16:04<2:42:00,  5.42s/it]                                                        45%|     | 1457/3250 [2:16:04<2:42:00,  5.42s/it] 45%|     | 1458/3250 [2:16:10<2:41:21,  5.40s/it]                                                        45%|     | 1458/3250 [2:16:10<2:41:21,  5.40s/it] 45%|     | 1459/3250 [2:16:15<2:45:21,  5.54s/it]                                                        45%|     | 1459/3250 [2:16:15<2:45:21,  5.54s/it] 45%|     | 1460/3250 [2:16:21<2:43:33,  5.48s/it]                                                        45%|     | 1460/3250 [2:16:21<2:43:33,  5.48s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8700420260429382, 'eval_runtime': 1.3798, 'eval_samples_per_second': 8.697, 'eval_steps_per_second': 2.174, 'epoch': 0.45}
                                                        45%|     | 1460/3250 [2:16:22<2:43:33,  5.48s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_4/tmp-checkpoint-1460
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  I AM HERE AND I AM SAVING THE MODEL
/projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1460the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1460

I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1460
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1460
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1460/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1460/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1460/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1460/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.7211, 'learning_rate': 5.794589139140381e-05, 'epoch': 0.45}
{'loss': 0.7718, 'learning_rate': 5.789814031885894e-05, 'epoch': 0.45}
{'loss': 1.2043, 'learning_rate': 5.7850381857197525e-05, 'epoch': 0.45}
{'loss': 0.7019, 'learning_rate': 5.7802616051100086e-05, 'epoch': 0.45}
{'loss': 0.7496, 'learning_rate': 5.775484294525399e-05, 'epoch': 0.45}
 45%|     | 1461/3250 [2:16:28<2:57:29,  5.95s/it]                                                        45%|     | 1461/3250 [2:16:28<2:57:29,  5.95s/it] 45%|     | 1462/3250 [2:16:33<2:51:57,  5.77s/it]                                                        45%|     | 1462/3250 [2:16:33<2:51:57,  5.77s/it] 45%|     | 1463/3250 [2:16:39<2:48:02,  5.64s/it]                                                        45%|     | 1463/3250 [2:16:39<2:48:02,  5.64s/it] 45%|     | 1464/3250 [2:16:44<2:45:24,  5.56s/it]                                                        45%|     | 1464/3250 [2:16:44<2:45:24,  5.56s/it] 45%|     | 1465/3250 [2:16:49<2:43:31,  5.50s/it]                                                        45%|     | 1465/3250 [2:16:49<2:43:31,  5.50s/it] 45%|     | 1466/3250 [2:16:55<2:42:11,  5.46s/it]            {'loss': 0.7436, 'learning_rate': 5.770706258435342e-05, 'epoch': 0.45}
{'loss': 0.7492, 'learning_rate': 5.765927501309938e-05, 'epoch': 0.45}
{'loss': 0.6817, 'learning_rate': 5.761148027619958e-05, 'epoch': 0.45}
{'loss': 0.7259, 'learning_rate': 5.756367841836847e-05, 'epoch': 0.45}
{'loss': 0.7938, 'learning_rate': 5.7515869484327155e-05, 'epoch': 0.45}
                                            45%|     | 1466/3250 [2:16:55<2:42:11,  5.46s/it] 45%|     | 1467/3250 [2:17:00<2:41:19,  5.43s/it]                                                        45%|     | 1467/3250 [2:17:00<2:41:19,  5.43s/it] 45%|     | 1468/3250 [2:17:05<2:40:32,  5.41s/it]                                                        45%|     | 1468/3250 [2:17:05<2:40:32,  5.41s/it] 45%|     | 1469/3250 [2:17:11<2:40:05,  5.39s/it]                                                        45%|     | 1469/3250 [2:17:11<2:40:05,  5.39s/it] 45%|     | 1470/3250 [2:17:16<2:39:41,  5.38s/it]                                                        45%|     | 1470/3250 [2:17:16<2:39:41,  5.38s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.873160183429718, 'eval_runtime': 1.3747, 'eval_samples_per_second': 8.729, 'eval_steps_per_second': 2.182, 'epoch': 0.45}
                                                        45%|     | 1470/3250 [2:17:17<2:39:41,  5.38s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_4/tmp-checkpoint-1470
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in   /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1470
/projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1470
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1470
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1470
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1470/pytorch_model.bin
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1470/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1470/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1470/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.7372, 'learning_rate': 5.746805351880334e-05, 'epoch': 0.45}
{'loss': 0.7305, 'learning_rate': 5.742023056653131e-05, 'epoch': 0.45}
{'loss': 0.7273, 'learning_rate': 5.73724006722519e-05, 'epoch': 0.45}
{'loss': 0.7122, 'learning_rate': 5.732456388071247e-05, 'epoch': 0.45}
{'loss': 0.7408, 'learning_rate': 5.7276720236666746e-05, 'epoch': 0.45}
 45%|     | 1471/3250 [2:17:23<2:54:44,  5.89s/it]                                                        45%|     | 1471/3250 [2:17:23<2:54:44,  5.89s/it] 45%|     | 1472/3250 [2:17:28<2:49:54,  5.73s/it]                                                        45%|     | 1472/3250 [2:17:28<2:49:54,  5.73s/it] 45%|     | 1473/3250 [2:17:34<2:46:30,  5.62s/it]                                                        45%|     | 1473/3250 [2:17:34<2:46:30,  5.62s/it] 45%|     | 1474/3250 [2:17:39<2:43:56,  5.54s/it]                                                        45%|     | 1474/3250 [2:17:39<2:43:56,  5.54s/it] 45%|     | 1475/3250 [2:17:45<2:42:13,  5.48s/it]                                                        45%|     | 1475/3250 [2:17:45<2:42:13,  5.48s/it] 45%|     | 1476/3250 [2:17:50<2:44:14,  5.55s/it]            {'loss': 0.6984, 'learning_rate': 5.722886978487496e-05, 'epoch': 0.45}
{'loss': 0.7178, 'learning_rate': 5.7181012570103656e-05, 'epoch': 0.45}
{'loss': 0.7232, 'learning_rate': 5.713314863712571e-05, 'epoch': 0.45}
{'loss': 0.7262, 'learning_rate': 5.70852780307203e-05, 'epoch': 0.46}
{'loss': 0.7445, 'learning_rate': 5.703740079567286e-05, 'epoch': 0.46}
                                            45%|     | 1476/3250 [2:17:50<2:44:14,  5.55s/it] 45%|     | 1477/3250 [2:17:56<2:42:19,  5.49s/it]                                                        45%|     | 1477/3250 [2:17:56<2:42:19,  5.49s/it] 45%|     | 1478/3250 [2:18:01<2:41:03,  5.45s/it]                                                        45%|     | 1478/3250 [2:18:01<2:41:03,  5.45s/it] 46%|     | 1479/3250 [2:18:06<2:40:04,  5.42s/it]                                                        46%|     | 1479/3250 [2:18:06<2:40:04,  5.42s/it] 46%|     | 1480/3250 [2:18:12<2:39:13,  5.40s/it]                                                        46%|     | 1480/3250 [2:18:12<2:39:13,  5.40s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8700832724571228, 'eval_runtime': 1.3722, 'eval_samples_per_second': 8.745, 'eval_steps_per_second': 2.186, 'epoch': 0.46}
                                                        46%|     | 1480/3250 [2:18:13<2:39:13,  5.40s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_4/tmp-checkpoint-1480
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1480
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1480
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1480
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1480
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1480/pytorch_model.bin
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1480/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1480/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1480/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.7478, 'learning_rate': 5.698951697677498e-05, 'epoch': 0.46}
{'loss': 0.7321, 'learning_rate': 5.694162661882444e-05, 'epoch': 0.46}
{'loss': 0.7395, 'learning_rate': 5.6893729766625146e-05, 'epoch': 0.46}
{'loss': 0.7084, 'learning_rate': 5.684582646498706e-05, 'epoch': 0.46}
{'loss': 0.7532, 'learning_rate': 5.679791675872619e-05, 'epoch': 0.46}
 46%|     | 1481/3250 [2:18:19<2:53:32,  5.89s/it]                                                        46%|     | 1481/3250 [2:18:19<2:53:32,  5.89s/it] 46%|     | 1482/3250 [2:18:24<2:48:44,  5.73s/it]                                                        46%|     | 1482/3250 [2:18:24<2:48:44,  5.73s/it] 46%|     | 1483/3250 [2:18:29<2:45:27,  5.62s/it]                                                        46%|     | 1483/3250 [2:18:29<2:45:27,  5.62s/it] 46%|     | 1484/3250 [2:18:35<2:43:07,  5.54s/it]                                                        46%|     | 1484/3250 [2:18:35<2:43:07,  5.54s/it] 46%|     | 1485/3250 [2:18:40<2:41:21,  5.49s/it]                                                        46%|     | 1485/3250 [2:18:40<2:41:21,  5.49s/it] 46%|     | 1486/3250 [2:18:45<2:39:59,  5.44s/it]            {'loss': 0.7365, 'learning_rate': 5.675000069266451e-05, 'epoch': 0.46}
{'loss': 0.715, 'learning_rate': 5.6702078311629995e-05, 'epoch': 0.46}
{'loss': 0.7008, 'learning_rate': 5.6654149660456455e-05, 'epoch': 0.46}
{'loss': 0.6968, 'learning_rate': 5.660621478398367e-05, 'epoch': 0.46}
{'loss': 0.7551, 'learning_rate': 5.655827372705712e-05, 'epoch': 0.46}
                                            46%|     | 1486/3250 [2:18:45<2:39:59,  5.44s/it] 46%|     | 1487/3250 [2:18:51<2:39:09,  5.42s/it]                                                        46%|     | 1487/3250 [2:18:51<2:39:09,  5.42s/it] 46%|     | 1488/3250 [2:18:56<2:38:33,  5.40s/it]                                                        46%|     | 1488/3250 [2:18:56<2:38:33,  5.40s/it] 46%|     | 1489/3250 [2:19:02<2:38:00,  5.38s/it]                                                        46%|     | 1489/3250 [2:19:02<2:38:00,  5.38s/it] 46%|     | 1490/3250 [2:19:07<2:37:40,  5.38s/it]                                                        46%|     | 1490/3250 [2:19:07<2:37:40,  5.38s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8716838359832764, 'eval_runtime': 1.5924, 'eval_samples_per_second': 7.536, 'eval_steps_per_second': 1.884, 'epoch': 0.46}
                                                        46%|     | 1490/3250 [2:19:08<2:37:40,  5.38s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_4/tmp-checkpoint-1490
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1490I AM HERE AND I AM SAVING THE MODEL

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1490
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1490
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1490
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1490/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1490/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1490/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1490/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.6904, 'learning_rate': 5.651032653452817e-05, 'epoch': 0.46}
{'loss': 0.7872, 'learning_rate': 5.6462373251253875e-05, 'epoch': 0.46}
{'loss': 1.2117, 'learning_rate': 5.641441392209699e-05, 'epoch': 0.46}
{'loss': 0.6993, 'learning_rate': 5.636644859192594e-05, 'epoch': 0.46}
{'loss': 0.7279, 'learning_rate': 5.6318477305614756e-05, 'epoch': 0.46}
 46%|     | 1491/3250 [2:19:14<2:54:43,  5.96s/it]                                                        46%|     | 1491/3250 [2:19:14<2:54:43,  5.96s/it] 46%|     | 1492/3250 [2:19:20<2:52:05,  5.87s/it]                                                        46%|     | 1492/3250 [2:19:20<2:52:05,  5.87s/it] 46%|     | 1493/3250 [2:19:25<2:47:26,  5.72s/it]                                                        46%|     | 1493/3250 [2:19:25<2:47:26,  5.72s/it] 46%|     | 1494/3250 [2:19:31<2:44:08,  5.61s/it]                                                        46%|     | 1494/3250 [2:19:31<2:44:08,  5.61s/it] 46%|     | 1495/3250 [2:19:36<2:41:56,  5.54s/it]                                                        46%|     | 1495/3250 [2:19:36<2:41:56,  5.54s/it] 46%|     | 1496/3250 [2:19:41<2:40:07,  5.48s/it]            {'loss': 0.7345, 'learning_rate': 5.6270500108043046e-05, 'epoch': 0.46}
{'loss': 0.7247, 'learning_rate': 5.6222517044095945e-05, 'epoch': 0.46}
{'loss': 0.707, 'learning_rate': 5.6174528158664096e-05, 'epoch': 0.46}
{'loss': 0.7107, 'learning_rate': 5.612653349664353e-05, 'epoch': 0.46}
{'loss': 0.7692, 'learning_rate': 5.6078533102935745e-05, 'epoch': 0.46}
                                            46%|     | 1496/3250 [2:19:41<2:40:07,  5.48s/it] 46%|     | 1497/3250 [2:19:47<2:38:54,  5.44s/it]                                                        46%|     | 1497/3250 [2:19:47<2:38:54,  5.44s/it] 46%|     | 1498/3250 [2:19:52<2:38:02,  5.41s/it]                                                        46%|     | 1498/3250 [2:19:52<2:38:02,  5.41s/it] 46%|     | 1499/3250 [2:19:57<2:37:21,  5.39s/it]                                                        46%|     | 1499/3250 [2:19:57<2:37:21,  5.39s/it] 46%|     | 1500/3250 [2:20:03<2:36:57,  5.38s/it]                                                        46%|     | 1500/3250 [2:20:03<2:36:57,  5.38s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8742577433586121, 'eval_runtime': 1.3829, 'eval_samples_per_second': 8.677, 'eval_steps_per_second': 2.169, 'epoch': 0.46}
                                                        46%|     | 1500/3250 [2:20:04<2:36:57,  5.38s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_4/tmp-checkpoint-1500
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1500
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1500
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1500
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1500
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1500/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1500/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1500/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1500/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.7586, 'learning_rate': 5.60305270224476e-05, 'epoch': 0.46}
{'loss': 0.7344, 'learning_rate': 5.598251530009121e-05, 'epoch': 0.46}
{'loss': 0.6835, 'learning_rate': 5.5934497980784054e-05, 'epoch': 0.46}
{'loss': 0.7359, 'learning_rate': 5.5886475109448765e-05, 'epoch': 0.46}
{'loss': 0.7265, 'learning_rate': 5.583844673101323e-05, 'epoch': 0.46}
 46%|     | 1501/3250 [2:20:10<2:51:39,  5.89s/it]                                                        46%|     | 1501/3250 [2:20:10<2:51:39,  5.89s/it] 46%|     | 1502/3250 [2:20:15<2:46:52,  5.73s/it]                                                        46%|     | 1502/3250 [2:20:15<2:46:52,  5.73s/it] 46%|     | 1503/3250 [2:20:20<2:43:33,  5.62s/it]                                                        46%|     | 1503/3250 [2:20:20<2:43:33,  5.62s/it] 46%|     | 1504/3250 [2:20:26<2:41:09,  5.54s/it]                                                        46%|     | 1504/3250 [2:20:26<2:41:09,  5.54s/it] 46%|     | 1505/3250 [2:20:31<2:39:28,  5.48s/it]                                                        46%|     | 1505/3250 [2:20:31<2:39:28,  5.48s/it] 46%|     | 1506/3250 [2:20:37<2:38:18,  5.45s/it]            {'loss': 0.7303, 'learning_rate': 5.5790412890410446e-05, 'epoch': 0.46}
{'loss': 0.7074, 'learning_rate': 5.574237363257858e-05, 'epoch': 0.46}
{'loss': 0.7224, 'learning_rate': 5.56943290024608e-05, 'epoch': 0.46}
{'loss': 0.7236, 'learning_rate': 5.564627904500533e-05, 'epoch': 0.46}
{'loss': 0.7422, 'learning_rate': 5.559822380516539e-05, 'epoch': 0.46}
                                            46%|     | 1506/3250 [2:20:37<2:38:18,  5.45s/it] 46%|     | 1507/3250 [2:20:42<2:37:31,  5.42s/it]                                                        46%|     | 1507/3250 [2:20:42<2:37:31,  5.42s/it] 46%|     | 1508/3250 [2:20:47<2:37:01,  5.41s/it]                                                        46%|     | 1508/3250 [2:20:47<2:37:01,  5.41s/it] 46%|     | 1509/3250 [2:20:53<2:39:49,  5.51s/it]                                                        46%|     | 1509/3250 [2:20:53<2:39:49,  5.51s/it] 46%|     | 1510/3250 [2:20:58<2:38:25,  5.46s/it]                                                        46%|     | 1510/3250 [2:20:58<2:38:25,  5.46s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8708364367485046, 'eval_runtime': 1.3861, 'eval_samples_per_second': 8.657, 'eval_steps_per_second': 2.164, 'epoch': 0.46}
                                                        46%|     | 1510/3250 [2:21:00<2:38:25,  5.46s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_4/tmp-checkpoint-1510
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1510
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1510
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1510
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1510
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1510/pytorch_model.binthe pytorch model path is
 /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1510/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1510/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1510/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.752, 'learning_rate': 5.5550163327899126e-05, 'epoch': 0.46}
{'loss': 0.7106, 'learning_rate': 5.550209765816958e-05, 'epoch': 0.47}
{'loss': 0.7471, 'learning_rate': 5.545402684094467e-05, 'epoch': 0.47}
{'loss': 0.7387, 'learning_rate': 5.540595092119709e-05, 'epoch': 0.47}
{'loss': 0.7047, 'learning_rate': 5.535786994390436e-05, 'epoch': 0.47}
 46%|     | 1511/3250 [2:21:05<2:52:02,  5.94s/it]                                                        46%|     | 1511/3250 [2:21:05<2:52:02,  5.94s/it] 47%|     | 1512/3250 [2:21:11<2:47:01,  5.77s/it]                                                        47%|     | 1512/3250 [2:21:11<2:47:01,  5.77s/it] 47%|     | 1513/3250 [2:21:16<2:43:30,  5.65s/it]                                                        47%|     | 1513/3250 [2:21:16<2:43:30,  5.65s/it] 47%|     | 1514/3250 [2:21:22<2:41:14,  5.57s/it]                                                        47%|     | 1514/3250 [2:21:22<2:41:14,  5.57s/it] 47%|     | 1515/3250 [2:21:27<2:39:12,  5.51s/it]                                                        47%|     | 1515/3250 [2:21:27<2:39:12,  5.51s/it] 47%|     | 1516/3250 [2:21:32<2:37:48,  5.46s/it]            {'loss': 0.7698, 'learning_rate': 5.530978395404872e-05, 'epoch': 0.47}
{'loss': 0.7137, 'learning_rate': 5.526169299661705e-05, 'epoch': 0.47}
{'loss': 0.7048, 'learning_rate': 5.521359711660094e-05, 'epoch': 0.47}
{'loss': 0.7115, 'learning_rate': 5.516549635899655e-05, 'epoch': 0.47}
{'loss': 0.728, 'learning_rate': 5.511739076880461e-05, 'epoch': 0.47}
                                            47%|     | 1516/3250 [2:21:32<2:37:48,  5.46s/it] 47%|     | 1517/3250 [2:21:38<2:36:55,  5.43s/it]                                                        47%|     | 1517/3250 [2:21:38<2:36:55,  5.43s/it] 47%|     | 1518/3250 [2:21:43<2:36:07,  5.41s/it]                                                        47%|     | 1518/3250 [2:21:43<2:36:07,  5.41s/it] 47%|     | 1519/3250 [2:21:48<2:35:35,  5.39s/it]                                                        47%|     | 1519/3250 [2:21:48<2:35:35,  5.39s/it] 47%|     | 1520/3250 [2:21:54<2:35:12,  5.38s/it]                                                        47%|     | 1520/3250 [2:21:54<2:35:12,  5.38s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8698707818984985, 'eval_runtime': 1.3702, 'eval_samples_per_second': 8.758, 'eval_steps_per_second': 2.189, 'epoch': 0.47}
                                                        47%|     | 1520/3250 [2:21:55<2:35:12,  5.38s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_4/tmp-checkpoint-1520
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1520
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1520
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1520
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1520
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1520/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1520/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1520/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1520/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.722, 'learning_rate': 5.50692803910304e-05, 'epoch': 0.47}
{'loss': 0.7497, 'learning_rate': 5.502116527068363e-05, 'epoch': 0.47}
{'loss': 0.7929, 'learning_rate': 5.497304545277846e-05, 'epoch': 0.47}
{'loss': 1.1519, 'learning_rate': 5.492492098233346e-05, 'epoch': 0.47}
{'loss': 0.7112, 'learning_rate': 5.487679190437158e-05, 'epoch': 0.47}
 47%|     | 1521/3250 [2:22:01<2:50:05,  5.90s/it]                                                        47%|     | 1521/3250 [2:22:01<2:50:05,  5.90s/it] 47%|     | 1522/3250 [2:22:06<2:45:17,  5.74s/it]                                                        47%|     | 1522/3250 [2:22:06<2:45:17,  5.74s/it] 47%|     | 1523/3250 [2:22:12<2:41:49,  5.62s/it]                                                        47%|     | 1523/3250 [2:22:12<2:41:49,  5.62s/it] 47%|     | 1524/3250 [2:22:17<2:39:24,  5.54s/it]                                                        47%|     | 1524/3250 [2:22:17<2:39:24,  5.54s/it] 47%|     | 1525/3250 [2:22:23<2:41:47,  5.63s/it]                                                        47%|     | 1525/3250 [2:22:23<2:41:47,  5.63s/it] 47%|     | 1526/3250 [2:22:28<2:39:13,  5.54s/it]            {'loss': 0.7336, 'learning_rate': 5.482865826392001e-05, 'epoch': 0.47}
{'loss': 0.7339, 'learning_rate': 5.4780520106010256e-05, 'epoch': 0.47}
{'loss': 0.7127, 'learning_rate': 5.473237747567805e-05, 'epoch': 0.47}
{'loss': 0.6887, 'learning_rate': 5.468423041796331e-05, 'epoch': 0.47}
{'loss': 0.7528, 'learning_rate': 5.463607897791006e-05, 'epoch': 0.47}
                                            47%|     | 1526/3250 [2:22:28<2:39:13,  5.54s/it] 47%|     | 1527/3250 [2:22:33<2:37:33,  5.49s/it]                                                        47%|     | 1527/3250 [2:22:33<2:37:33,  5.49s/it] 47%|     | 1528/3250 [2:22:39<2:36:23,  5.45s/it]                                                        47%|     | 1528/3250 [2:22:39<2:36:23,  5.45s/it] 47%|     | 1529/3250 [2:22:44<2:35:24,  5.42s/it]                                                        47%|     | 1529/3250 [2:22:44<2:35:24,  5.42s/it] 47%|     | 1530/3250 [2:22:49<2:34:47,  5.40s/it]                                                        47%|     | 1530/3250 [2:22:49<2:34:47,  5.40s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.872369647026062, 'eval_runtime': 1.373, 'eval_samples_per_second': 8.74, 'eval_steps_per_second': 2.185, 'epoch': 0.47}
                                                        47%|     | 1530/3250 [2:22:51<2:34:47,  5.40s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_4/tmp-checkpoint-1530
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  I AM HERE AND I AM SAVING THE MODEL/projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1530
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1530

I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1530
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1530
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1530/pytorch_model.bin
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1530/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1530/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1530/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.7575, 'learning_rate': 5.458792320056645e-05, 'epoch': 0.47}
{'loss': 0.7192, 'learning_rate': 5.45397631309847e-05, 'epoch': 0.47}
{'loss': 0.6974, 'learning_rate': 5.449159881422101e-05, 'epoch': 0.47}
{'loss': 0.7172, 'learning_rate': 5.444343029533562e-05, 'epoch': 0.47}
{'loss': 0.7071, 'learning_rate': 5.439525761939261e-05, 'epoch': 0.47}
 47%|     | 1531/3250 [2:22:57<2:49:28,  5.92s/it]                                                        47%|     | 1531/3250 [2:22:57<2:49:28,  5.92s/it] 47%|     | 1532/3250 [2:23:02<2:44:36,  5.75s/it]                                                        47%|     | 1532/3250 [2:23:02<2:44:36,  5.75s/it] 47%|     | 1533/3250 [2:23:07<2:41:03,  5.63s/it]                                                        47%|     | 1533/3250 [2:23:07<2:41:03,  5.63s/it] 47%|     | 1534/3250 [2:23:13<2:38:36,  5.55s/it]                                                        47%|     | 1534/3250 [2:23:13<2:38:36,  5.55s/it] 47%|     | 1535/3250 [2:23:18<2:36:46,  5.49s/it]                                                        47%|     | 1535/3250 [2:23:18<2:36:46,  5.49s/it] 47%|     | 1536/3250 [2:23:23<2:35:35,  5.45s/it]            {'loss': 0.7326, 'learning_rate': 5.4347080831460015e-05, 'epoch': 0.47}
{'loss': 0.7007, 'learning_rate': 5.4298899976609717e-05, 'epoch': 0.47}
{'loss': 0.6988, 'learning_rate': 5.425071509991737e-05, 'epoch': 0.47}
{'loss': 0.7343, 'learning_rate': 5.420252624646238e-05, 'epoch': 0.47}
{'loss': 0.7387, 'learning_rate': 5.415433346132793e-05, 'epoch': 0.47}
                                            47%|     | 1536/3250 [2:23:23<2:35:35,  5.45s/it] 47%|     | 1537/3250 [2:23:29<2:34:44,  5.42s/it]                                                        47%|     | 1537/3250 [2:23:29<2:34:44,  5.42s/it] 47%|     | 1538/3250 [2:23:34<2:34:05,  5.40s/it]                                                        47%|     | 1538/3250 [2:23:34<2:34:05,  5.40s/it] 47%|     | 1539/3250 [2:23:39<2:33:31,  5.38s/it]                                                        47%|     | 1539/3250 [2:23:39<2:33:31,  5.38s/it] 47%|     | 1540/3250 [2:23:45<2:32:59,  5.37s/it]                                                        47%|     | 1540/3250 [2:23:45<2:32:59,  5.37s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8684781789779663, 'eval_runtime': 1.3734, 'eval_samples_per_second': 8.737, 'eval_steps_per_second': 2.184, 'epoch': 0.47}
                                                        47%|     | 1540/3250 [2:23:46<2:32:59,  5.37s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_4/tmp-checkpoint-1540
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1540
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1540
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1540
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1540
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1540/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1540/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1540/pytorch_model.binthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1540/pytorch_model.bin

/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.7247, 'learning_rate': 5.410613678960084e-05, 'epoch': 0.47}
{'loss': 0.7215, 'learning_rate': 5.4057936276371565e-05, 'epoch': 0.47}
{'loss': 0.7342, 'learning_rate': 5.400973196673419e-05, 'epoch': 0.47}
{'loss': 0.7304, 'learning_rate': 5.3961523905786295e-05, 'epoch': 0.48}
{'loss': 0.6807, 'learning_rate': 5.3913312138629014e-05, 'epoch': 0.48}
 47%|     | 1541/3250 [2:23:52<2:50:17,  5.98s/it]                                                        47%|     | 1541/3250 [2:23:52<2:50:17,  5.98s/it] 47%|     | 1542/3250 [2:23:57<2:44:51,  5.79s/it]                                                        47%|     | 1542/3250 [2:23:57<2:44:51,  5.79s/it] 47%|     | 1543/3250 [2:24:03<2:41:04,  5.66s/it]                                                        47%|     | 1543/3250 [2:24:03<2:41:04,  5.66s/it] 48%|     | 1544/3250 [2:24:08<2:38:17,  5.57s/it]                                                        48%|     | 1544/3250 [2:24:08<2:38:17,  5.57s/it] 48%|     | 1545/3250 [2:24:14<2:36:19,  5.50s/it]                                                        48%|     | 1545/3250 [2:24:14<2:36:19,  5.50s/it] 48%|     | 1546/3250 [2:24:19<2:34:53,  5.45s/it]            {'loss': 0.7645, 'learning_rate': 5.386509671036695e-05, 'epoch': 0.48}
{'loss': 0.7073, 'learning_rate': 5.38168776661081e-05, 'epoch': 0.48}
{'loss': 0.7081, 'learning_rate': 5.376865505096385e-05, 'epoch': 0.48}
{'loss': 0.6956, 'learning_rate': 5.372042891004896e-05, 'epoch': 0.48}
{'loss': 0.7072, 'learning_rate': 5.367219928848145e-05, 'epoch': 0.48}
                                            48%|     | 1546/3250 [2:24:19<2:34:53,  5.45s/it] 48%|     | 1547/3250 [2:24:24<2:33:54,  5.42s/it]                                                        48%|     | 1547/3250 [2:24:24<2:33:54,  5.42s/it] 48%|     | 1548/3250 [2:24:30<2:33:08,  5.40s/it]                                                        48%|     | 1548/3250 [2:24:30<2:33:08,  5.40s/it] 48%|     | 1549/3250 [2:24:35<2:32:40,  5.39s/it]                                                        48%|     | 1549/3250 [2:24:35<2:32:40,  5.39s/it] 48%|     | 1550/3250 [2:24:40<2:32:17,  5.38s/it]                                                        48%|     | 1550/3250 [2:24:40<2:32:17,  5.38s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8676259517669678, 'eval_runtime': 1.3786, 'eval_samples_per_second': 8.704, 'eval_steps_per_second': 2.176, 'epoch': 0.48}
                                                        48%|     | 1550/3250 [2:24:42<2:32:17,  5.38s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_4/tmp-checkpoint-1550
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1550
/projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1550
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1550
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1550
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1550/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1550/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1550/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1550/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.7364, 'learning_rate': 5.3623966231382615e-05, 'epoch': 0.48}
{'loss': 0.7047, 'learning_rate': 5.357572978387697e-05, 'epoch': 0.48}
{'loss': 0.7638, 'learning_rate': 5.3527489991092186e-05, 'epoch': 0.48}
{'loss': 1.1961, 'learning_rate': 5.3479246898159063e-05, 'epoch': 0.48}
{'loss': 0.691, 'learning_rate': 5.3431000550211505e-05, 'epoch': 0.48}
 48%|     | 1551/3250 [2:24:47<2:47:31,  5.92s/it]                                                        48%|     | 1551/3250 [2:24:47<2:47:31,  5.92s/it] 48%|     | 1552/3250 [2:24:53<2:42:38,  5.75s/it]                                                        48%|     | 1552/3250 [2:24:53<2:42:38,  5.75s/it] 48%|     | 1553/3250 [2:24:58<2:39:14,  5.63s/it]                                                        48%|     | 1553/3250 [2:24:58<2:39:14,  5.63s/it] 48%|     | 1554/3250 [2:25:04<2:36:44,  5.55s/it]                                                        48%|     | 1554/3250 [2:25:04<2:36:44,  5.55s/it] 48%|     | 1555/3250 [2:25:09<2:35:05,  5.49s/it]                                                        48%|     | 1555/3250 [2:25:09<2:35:05,  5.49s/it] 48%|     | 1556/3250 [2:25:14<2:33:49,  5.45s/it]            {'loss': 0.7352, 'learning_rate': 5.338275099238647e-05, 'epoch': 0.48}
{'loss': 0.728, 'learning_rate': 5.333449826982385e-05, 'epoch': 0.48}
{'loss': 0.7464, 'learning_rate': 5.328624242766661e-05, 'epoch': 0.48}
{'loss': 0.6677, 'learning_rate': 5.323798351106052e-05, 'epoch': 0.48}
{'loss': 0.7196, 'learning_rate': 5.31897215651543e-05, 'epoch': 0.48}
                                            48%|     | 1556/3250 [2:25:14<2:33:49,  5.45s/it] 48%|     | 1557/3250 [2:25:20<2:32:57,  5.42s/it]                                                        48%|     | 1557/3250 [2:25:20<2:32:57,  5.42s/it] 48%|     | 1558/3250 [2:25:25<2:36:19,  5.54s/it]                                                        48%|     | 1558/3250 [2:25:25<2:36:19,  5.54s/it] 48%|     | 1559/3250 [2:25:31<2:34:34,  5.48s/it]                                                        48%|     | 1559/3250 [2:25:31<2:34:34,  5.48s/it] 48%|     | 1560/3250 [2:25:36<2:33:33,  5.45s/it]                                                        48%|     | 1560/3250 [2:25:36<2:33:33,  5.45s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8724962472915649, 'eval_runtime': 1.3744, 'eval_samples_per_second': 8.731, 'eval_steps_per_second': 2.183, 'epoch': 0.48}
                                                        48%|     | 1560/3250 [2:25:38<2:33:33,  5.45s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_4/tmp-checkpoint-1560
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1560
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1560
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1560
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1560
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1560/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1560/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1560/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1560/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.7789, 'learning_rate': 5.314145663509951e-05, 'epoch': 0.48}
{'loss': 0.7248, 'learning_rate': 5.309318876605043e-05, 'epoch': 0.48}
{'loss': 0.7127, 'learning_rate': 5.3044918003164156e-05, 'epoch': 0.48}
{'loss': 0.7158, 'learning_rate': 5.299664439160047e-05, 'epoch': 0.48}
{'loss': 0.7014, 'learning_rate': 5.294836797652182e-05, 'epoch': 0.48}
 48%|     | 1561/3250 [2:25:43<2:47:28,  5.95s/it]                                                        48%|     | 1561/3250 [2:25:43<2:47:28,  5.95s/it] 48%|     | 1562/3250 [2:25:49<2:42:17,  5.77s/it]                                                        48%|     | 1562/3250 [2:25:49<2:42:17,  5.77s/it] 48%|     | 1563/3250 [2:25:54<2:38:41,  5.64s/it]                                                        48%|     | 1563/3250 [2:25:54<2:38:41,  5.64s/it] 48%|     | 1564/3250 [2:25:59<2:36:08,  5.56s/it]                                                        48%|     | 1564/3250 [2:25:59<2:36:08,  5.56s/it] 48%|     | 1565/3250 [2:26:05<2:34:15,  5.49s/it]                                                        48%|     | 1565/3250 [2:26:05<2:34:15,  5.49s/it] 48%|     | 1566/3250 [2:26:10<2:32:52,  5.45s/it]            {'loss': 0.7388, 'learning_rate': 5.290008880309326e-05, 'epoch': 0.48}
{'loss': 0.7086, 'learning_rate': 5.2851806916482464e-05, 'epoch': 0.48}
{'loss': 0.7204, 'learning_rate': 5.2803522361859594e-05, 'epoch': 0.48}
{'loss': 0.6946, 'learning_rate': 5.275523518439735e-05, 'epoch': 0.48}
{'loss': 0.7189, 'learning_rate': 5.270694542927088e-05, 'epoch': 0.48}
                                            48%|     | 1566/3250 [2:26:10<2:32:52,  5.45s/it] 48%|     | 1567/3250 [2:26:15<2:31:58,  5.42s/it]                                                        48%|     | 1567/3250 [2:26:15<2:31:58,  5.42s/it] 48%|     | 1568/3250 [2:26:21<2:31:18,  5.40s/it]                                                        48%|     | 1568/3250 [2:26:21<2:31:18,  5.40s/it] 48%|     | 1569/3250 [2:26:26<2:30:47,  5.38s/it]                                                        48%|     | 1569/3250 [2:26:26<2:30:47,  5.38s/it] 48%|     | 1570/3250 [2:26:31<2:30:23,  5.37s/it]                                                        48%|     | 1570/3250 [2:26:31<2:30:23,  5.37s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8683958053588867, 'eval_runtime': 1.3706, 'eval_samples_per_second': 8.755, 'eval_steps_per_second': 2.189, 'epoch': 0.48}
                                                        48%|     | 1570/3250 [2:26:33<2:30:23,  5.37s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_4/tmp-checkpoint-1570
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1570
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1570
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1570
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1570
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1570/pytorch_model.bin
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1570/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1570/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1570/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.7312, 'learning_rate': 5.265865314165771e-05, 'epoch': 0.48}
{'loss': 0.7301, 'learning_rate': 5.261035836673779e-05, 'epoch': 0.48}
{'loss': 0.7158, 'learning_rate': 5.256206114969333e-05, 'epoch': 0.48}
{'loss': 0.7372, 'learning_rate': 5.251376153570889e-05, 'epoch': 0.48}
{'loss': 0.7033, 'learning_rate': 5.2465459569971224e-05, 'epoch': 0.48}
 48%|     | 1571/3250 [2:26:38<2:43:59,  5.86s/it]                                                        48%|     | 1571/3250 [2:26:38<2:43:59,  5.86s/it] 48%|     | 1572/3250 [2:26:44<2:39:36,  5.71s/it]                                                        48%|     | 1572/3250 [2:26:44<2:39:36,  5.71s/it] 48%|     | 1573/3250 [2:26:49<2:36:38,  5.60s/it]                                                        48%|     | 1573/3250 [2:26:49<2:36:38,  5.60s/it] 48%|     | 1574/3250 [2:26:55<2:37:48,  5.65s/it]                                                        48%|     | 1574/3250 [2:26:55<2:37:48,  5.65s/it] 48%|     | 1575/3250 [2:27:00<2:35:06,  5.56s/it]                                                        48%|     | 1575/3250 [2:27:00<2:35:06,  5.56s/it] 48%|     | 1576/3250 [2:27:06<2:33:15,  5.49s/it]            {'loss': 0.734, 'learning_rate': 5.2417155297669326e-05, 'epoch': 0.48}
{'loss': 0.7308, 'learning_rate': 5.236884876399429e-05, 'epoch': 0.49}
{'loss': 0.715, 'learning_rate': 5.232054001413941e-05, 'epoch': 0.49}
{'loss': 0.7027, 'learning_rate': 5.2272229093299985e-05, 'epoch': 0.49}
{'loss': 0.6848, 'learning_rate': 5.222391604667336e-05, 'epoch': 0.49}
                                            48%|     | 1576/3250 [2:27:06<2:33:15,  5.49s/it] 49%|     | 1577/3250 [2:27:11<2:31:55,  5.45s/it]                                                        49%|     | 1577/3250 [2:27:11<2:31:55,  5.45s/it] 49%|     | 1578/3250 [2:27:16<2:31:00,  5.42s/it]                                                        49%|     | 1578/3250 [2:27:16<2:31:00,  5.42s/it] 49%|     | 1579/3250 [2:27:22<2:30:17,  5.40s/it]                                                        49%|     | 1579/3250 [2:27:22<2:30:17,  5.40s/it] 49%|     | 1580/3250 [2:27:27<2:29:55,  5.39s/it]                                                        49%|     | 1580/3250 [2:27:27<2:29:55,  5.39s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8697117567062378, 'eval_runtime': 1.3814, 'eval_samples_per_second': 8.687, 'eval_steps_per_second': 2.172, 'epoch': 0.49}
                                                        49%|     | 1580/3250 [2:27:28<2:29:55,  5.39s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_4/tmp-checkpoint-1580
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1580
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1580
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1580
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1580
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1580/pytorch_model.bin
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1580/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1580/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1580/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.7388, 'learning_rate': 5.217560091945887e-05, 'epoch': 0.49}
{'loss': 0.6998, 'learning_rate': 5.212728375685781e-05, 'epoch': 0.49}
{'loss': 0.772, 'learning_rate': 5.20789646040734e-05, 'epoch': 0.49}
{'loss': 1.1995, 'learning_rate': 5.203064350631064e-05, 'epoch': 0.49}
{'loss': 0.6966, 'learning_rate': 5.198232050877645e-05, 'epoch': 0.49}
 49%|     | 1581/3250 [2:27:34<2:44:21,  5.91s/it]                                                        49%|     | 1581/3250 [2:27:34<2:44:21,  5.91s/it] 49%|     | 1582/3250 [2:27:39<2:39:40,  5.74s/it]                                                        49%|     | 1582/3250 [2:27:39<2:39:40,  5.74s/it] 49%|     | 1583/3250 [2:27:45<2:36:24,  5.63s/it]                                                        49%|     | 1583/3250 [2:27:45<2:36:24,  5.63s/it] 49%|     | 1584/3250 [2:27:50<2:33:55,  5.54s/it]                                                        49%|     | 1584/3250 [2:27:50<2:33:55,  5.54s/it] 49%|     | 1585/3250 [2:27:55<2:32:20,  5.49s/it]                                                        49%|     | 1585/3250 [2:27:55<2:32:20,  5.49s/it] 49%|     | 1586/3250 [2:28:01<2:31:04,  5.45s/it]            {'loss': 0.7282, 'learning_rate': 5.1933995656679444e-05, 'epoch': 0.49}
{'loss': 0.7251, 'learning_rate': 5.188566899523002e-05, 'epoch': 0.49}
{'loss': 0.7258, 'learning_rate': 5.183734056964027e-05, 'epoch': 0.49}
{'loss': 0.6921, 'learning_rate': 5.1789010425123894e-05, 'epoch': 0.49}
{'loss': 0.697, 'learning_rate': 5.174067860689625e-05, 'epoch': 0.49}
                                            49%|     | 1586/3250 [2:28:01<2:31:04,  5.45s/it] 49%|     | 1587/3250 [2:28:06<2:30:08,  5.42s/it]                                                        49%|     | 1587/3250 [2:28:06<2:30:08,  5.42s/it] 49%|     | 1588/3250 [2:28:12<2:29:38,  5.40s/it]                                                        49%|     | 1588/3250 [2:28:12<2:29:38,  5.40s/it] 49%|     | 1589/3250 [2:28:17<2:29:08,  5.39s/it]                                                        49%|     | 1589/3250 [2:28:17<2:29:08,  5.39s/it] 49%|     | 1590/3250 [2:28:22<2:28:53,  5.38s/it]                                                        49%|     | 1590/3250 [2:28:22<2:28:53,  5.38s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8713080883026123, 'eval_runtime': 1.3719, 'eval_samples_per_second': 8.747, 'eval_steps_per_second': 2.187, 'epoch': 0.49}
                                                        49%|     | 1590/3250 [2:28:24<2:28:53,  5.38s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_4/tmp-checkpoint-1590
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1590
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1590
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1590
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1590
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1590/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1590/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1590/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1590/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.7623, 'learning_rate': 5.1692345160174225e-05, 'epoch': 0.49}
{'loss': 0.7476, 'learning_rate': 5.164401013017627e-05, 'epoch': 0.49}
{'loss': 0.721, 'learning_rate': 5.159567356212226e-05, 'epoch': 0.49}
{'loss': 0.6724, 'learning_rate': 5.1547335501233565e-05, 'epoch': 0.49}
{'loss': 0.7238, 'learning_rate': 5.149899599273291e-05, 'epoch': 0.49}
 49%|     | 1591/3250 [2:28:30<2:46:25,  6.02s/it]                                                        49%|     | 1591/3250 [2:28:30<2:46:25,  6.02s/it] 49%|     | 1592/3250 [2:28:35<2:40:46,  5.82s/it]                                                        49%|     | 1592/3250 [2:28:35<2:40:46,  5.82s/it] 49%|     | 1593/3250 [2:28:40<2:36:47,  5.68s/it]                                                        49%|     | 1593/3250 [2:28:40<2:36:47,  5.68s/it] 49%|     | 1594/3250 [2:28:46<2:33:56,  5.58s/it]                                                        49%|     | 1594/3250 [2:28:46<2:33:56,  5.58s/it] 49%|     | 1595/3250 [2:28:51<2:31:57,  5.51s/it]                                                        49%|     | 1595/3250 [2:28:51<2:31:57,  5.51s/it] 49%|     | 1596/3250 [2:28:57<2:30:35,  5.46s/it]            {'loss': 0.7162, 'learning_rate': 5.14506550818444e-05, 'epoch': 0.49}
{'loss': 0.713, 'learning_rate': 5.140231281379345e-05, 'epoch': 0.49}
{'loss': 0.6932, 'learning_rate': 5.135396923380673e-05, 'epoch': 0.49}
{'loss': 0.7162, 'learning_rate': 5.130562438711215e-05, 'epoch': 0.49}
{'loss': 0.7035, 'learning_rate': 5.1257278318938785e-05, 'epoch': 0.49}
                                            49%|     | 1596/3250 [2:28:57<2:30:35,  5.46s/it] 49%|     | 1597/3250 [2:29:02<2:29:39,  5.43s/it]                                                        49%|     | 1597/3250 [2:29:02<2:29:39,  5.43s/it] 49%|     | 1598/3250 [2:29:07<2:29:03,  5.41s/it]                                                        49%|     | 1598/3250 [2:29:07<2:29:03,  5.41s/it] 49%|     | 1599/3250 [2:29:13<2:28:33,  5.40s/it]                                                        49%|     | 1599/3250 [2:29:13<2:28:33,  5.40s/it] 49%|     | 1600/3250 [2:29:18<2:28:09,  5.39s/it]                                                        49%|     | 1600/3250 [2:29:18<2:28:09,  5.39s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8666554093360901, 'eval_runtime': 1.3684, 'eval_samples_per_second': 8.77, 'eval_steps_per_second': 2.192, 'epoch': 0.49}
                                                        49%|     | 1600/3250 [2:29:19<2:28:09,  5.39s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_4/tmp-checkpoint-1600
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1600 /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1600

I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1600
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1600
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1600/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1600/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1600/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1600/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.7342, 'learning_rate': 5.1208931074516885e-05, 'epoch': 0.49}
{'loss': 0.7304, 'learning_rate': 5.116058269907779e-05, 'epoch': 0.49}
{'loss': 0.7153, 'learning_rate': 5.111223323785387e-05, 'epoch': 0.49}
{'loss': 0.7194, 'learning_rate': 5.106388273607855e-05, 'epoch': 0.49}
{'loss': 0.7326, 'learning_rate': 5.101553123898622e-05, 'epoch': 0.49}
 49%|     | 1601/3250 [2:29:25<2:41:25,  5.87s/it]                                                        49%|     | 1601/3250 [2:29:25<2:41:25,  5.87s/it] 49%|     | 1602/3250 [2:29:30<2:36:56,  5.71s/it]                                                        49%|     | 1602/3250 [2:29:30<2:36:56,  5.71s/it] 49%|     | 1603/3250 [2:29:36<2:34:05,  5.61s/it]                                                        49%|     | 1603/3250 [2:29:36<2:34:05,  5.61s/it] 49%|     | 1604/3250 [2:29:41<2:31:58,  5.54s/it]                                                        49%|     | 1604/3250 [2:29:41<2:31:58,  5.54s/it] 49%|     | 1605/3250 [2:29:46<2:30:27,  5.49s/it]                                                        49%|     | 1605/3250 [2:29:46<2:30:27,  5.49s/it] 49%|     | 1606/3250 [2:29:52<2:29:16,  5.45s/it]            {'loss': 0.6881, 'learning_rate': 5.096717879181217e-05, 'epoch': 0.49}
{'loss': 0.7596, 'learning_rate': 5.0918825439792604e-05, 'epoch': 0.49}
{'loss': 0.7051, 'learning_rate': 5.087047122816458e-05, 'epoch': 0.49}
{'loss': 0.7018, 'learning_rate': 5.082211620216595e-05, 'epoch': 0.5}
{'loss': 0.7008, 'learning_rate': 5.077376040703533e-05, 'epoch': 0.5}
                                            49%|     | 1606/3250 [2:29:52<2:29:16,  5.45s/it] 49%|     | 1607/3250 [2:29:58<2:31:54,  5.55s/it]                                                        49%|     | 1607/3250 [2:29:58<2:31:54,  5.55s/it] 49%|     | 1608/3250 [2:30:03<2:30:23,  5.50s/it]                                                        49%|     | 1608/3250 [2:30:03<2:30:23,  5.50s/it] 50%|     | 1609/3250 [2:30:08<2:29:10,  5.45s/it]                                                        50%|     | 1609/3250 [2:30:08<2:29:10,  5.45s/it] 50%|     | 1610/3250 [2:30:14<2:28:19,  5.43s/it]                                                        50%|     | 1610/3250 [2:30:14<2:28:19,  5.43s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.868430495262146, 'eval_runtime': 1.3809, 'eval_samples_per_second': 8.69, 'eval_steps_per_second': 2.172, 'epoch': 0.5}
                                                        50%|     | 1610/3250 [2:30:15<2:28:19,  5.43s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_4/tmp-checkpoint-1610
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in I AM HERE AND I AM SAVING THE MODEL 
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1610
/projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1610
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1610
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1610
the pytorch model path isthe pytorch model path is the pytorch model path is/projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1610/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1610/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1610/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1610/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.7227, 'learning_rate': 5.072540388801204e-05, 'epoch': 0.5}
{'loss': 0.7016, 'learning_rate': 5.0677046690336096e-05, 'epoch': 0.5}
{'loss': 0.7319, 'learning_rate': 5.0628688859248164e-05, 'epoch': 0.5}
{'loss': 0.8269, 'learning_rate': 5.0580330439989465e-05, 'epoch': 0.5}
{'loss': 1.0902, 'learning_rate': 5.0531971477801776e-05, 'epoch': 0.5}
 50%|     | 1611/3250 [2:30:21<2:41:50,  5.92s/it]                                                        50%|     | 1611/3250 [2:30:21<2:41:50,  5.92s/it] 50%|     | 1612/3250 [2:30:26<2:37:07,  5.76s/it]                                                        50%|     | 1612/3250 [2:30:26<2:37:07,  5.76s/it] 50%|     | 1613/3250 [2:30:32<2:33:54,  5.64s/it]                                                        50%|     | 1613/3250 [2:30:32<2:33:54,  5.64s/it] 50%|     | 1614/3250 [2:30:37<2:31:32,  5.56s/it]                                                        50%|     | 1614/3250 [2:30:37<2:31:32,  5.56s/it] 50%|     | 1615/3250 [2:30:42<2:29:45,  5.50s/it]                                                        50%|     | 1615/3250 [2:30:42<2:29:45,  5.50s/it] 50%|     | 1616/3250 [2:30:48<2:28:30,  5.45s/it]            {'loss': 0.7041, 'learning_rate': 5.048361201792742e-05, 'epoch': 0.5}
{'loss': 0.7244, 'learning_rate': 5.043525210560912e-05, 'epoch': 0.5}
{'loss': 0.7272, 'learning_rate': 5.0386891786090105e-05, 'epoch': 0.5}
{'loss': 0.7227, 'learning_rate': 5.0338531104613926e-05, 'epoch': 0.5}
{'loss': 0.6854, 'learning_rate': 5.029017010642447e-05, 'epoch': 0.5}
                                            50%|     | 1616/3250 [2:30:48<2:28:30,  5.45s/it] 50%|     | 1617/3250 [2:30:53<2:27:33,  5.42s/it]                                                        50%|     | 1617/3250 [2:30:53<2:27:33,  5.42s/it] 50%|     | 1618/3250 [2:30:58<2:26:58,  5.40s/it]                                                        50%|     | 1618/3250 [2:30:58<2:26:58,  5.40s/it] 50%|     | 1619/3250 [2:31:04<2:26:42,  5.40s/it]                                                        50%|     | 1619/3250 [2:31:04<2:26:42,  5.40s/it] 50%|     | 1620/3250 [2:31:09<2:26:15,  5.38s/it]                                                        50%|     | 1620/3250 [2:31:09<2:26:15,  5.38s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8711693286895752, 'eval_runtime': 1.3684, 'eval_samples_per_second': 8.769, 'eval_steps_per_second': 2.192, 'epoch': 0.5}
                                                        50%|     | 1620/3250 [2:31:10<2:26:15,  5.38s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_4/tmp-checkpoint-1620
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  I AM HERE AND I AM SAVING THE MODEL
/projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1620the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1620

I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1620
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1620
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1620/pytorch_model.bin
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1620/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1620/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1620/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.746, 'learning_rate': 5.024180883676597e-05, 'epoch': 0.5}
{'loss': 0.7545, 'learning_rate': 5.019344734088287e-05, 'epoch': 0.5}
{'loss': 0.7135, 'learning_rate': 5.014508566401982e-05, 'epoch': 0.5}
{'loss': 0.6864, 'learning_rate': 5.009672385142167e-05, 'epoch': 0.5}
{'loss': 0.7138, 'learning_rate': 5.004836194833339e-05, 'epoch': 0.5}
 50%|     | 1621/3250 [2:31:16<2:39:42,  5.88s/it]                                                        50%|     | 1621/3250 [2:31:16<2:39:42,  5.88s/it] 50%|     | 1622/3250 [2:31:21<2:35:30,  5.73s/it]                                                        50%|     | 1622/3250 [2:31:21<2:35:30,  5.73s/it] 50%|     | 1623/3250 [2:31:27<2:37:28,  5.81s/it]                                                        50%|     | 1623/3250 [2:31:27<2:37:28,  5.81s/it] 50%|     | 1624/3250 [2:31:33<2:33:51,  5.68s/it]                                                        50%|     | 1624/3250 [2:31:33<2:33:51,  5.68s/it] 50%|     | 1625/3250 [2:31:38<2:31:25,  5.59s/it]                                                        50%|     | 1625/3250 [2:31:38<2:31:25,  5.59s/it] 50%|     | 1626/3250 [2:31:44<2:29:30,  5.52s/it]            {'loss': 0.7037, 'learning_rate': 5e-05, 'epoch': 0.5}
{'loss': 0.7283, 'learning_rate': 4.995163805166662e-05, 'epoch': 0.5}
{'loss': 0.7033, 'learning_rate': 4.990327614857834e-05, 'epoch': 0.5}
{'loss': 0.692, 'learning_rate': 4.9854914335980193e-05, 'epoch': 0.5}
{'loss': 0.7052, 'learning_rate': 4.980655265911714e-05, 'epoch': 0.5}
                                            50%|     | 1626/3250 [2:31:44<2:29:30,  5.52s/it] 50%|     | 1627/3250 [2:31:49<2:28:10,  5.48s/it]                                                        50%|     | 1627/3250 [2:31:49<2:28:10,  5.48s/it] 50%|     | 1628/3250 [2:31:54<2:27:14,  5.45s/it]                                                        50%|     | 1628/3250 [2:31:54<2:27:14,  5.45s/it] 50%|     | 1629/3250 [2:32:00<2:26:29,  5.42s/it]                                                        50%|     | 1629/3250 [2:32:00<2:26:29,  5.42s/it] 50%|     | 1630/3250 [2:32:05<2:25:58,  5.41s/it]                                                        50%|     | 1630/3250 [2:32:05<2:25:58,  5.41s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8666844367980957, 'eval_runtime': 1.3695, 'eval_samples_per_second': 8.762, 'eval_steps_per_second': 2.191, 'epoch': 0.5}
                                                        50%|     | 1630/3250 [2:32:06<2:25:58,  5.41s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_4/tmp-checkpoint-1630
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  I AM HERE AND I AM SAVING THE MODEL/projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1630
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1630

I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1630
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1630
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1630/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1630/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1630/pytorch_model.binthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1630/pytorch_model.bin

/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.722, 'learning_rate': 4.975819116323403e-05, 'epoch': 0.5}
{'loss': 0.7236, 'learning_rate': 4.970982989357552e-05, 'epoch': 0.5}
{'loss': 0.7035, 'learning_rate': 4.966146889538608e-05, 'epoch': 0.5}
{'loss': 0.7224, 'learning_rate': 4.96131082139099e-05, 'epoch': 0.5}
{'loss': 0.7287, 'learning_rate': 4.9564747894390903e-05, 'epoch': 0.5}
 50%|     | 1631/3250 [2:32:12<2:39:46,  5.92s/it]                                                        50%|     | 1631/3250 [2:32:12<2:39:46,  5.92s/it] 50%|     | 1632/3250 [2:32:18<2:35:13,  5.76s/it]                                                        50%|     | 1632/3250 [2:32:18<2:35:13,  5.76s/it] 50%|     | 1633/3250 [2:32:23<2:31:58,  5.64s/it]                                                        50%|     | 1633/3250 [2:32:23<2:31:58,  5.64s/it] 50%|     | 1634/3250 [2:32:28<2:29:48,  5.56s/it]                                                        50%|     | 1634/3250 [2:32:28<2:29:48,  5.56s/it] 50%|     | 1635/3250 [2:32:34<2:28:09,  5.50s/it]                                                        50%|     | 1635/3250 [2:32:34<2:28:09,  5.50s/it] 50%|     | 1636/3250 [2:32:39<2:26:50,  5.46s/it]            {'loss': 0.6739, 'learning_rate': 4.951638798207261e-05, 'epoch': 0.5}
{'loss': 0.7633, 'learning_rate': 4.946802852219824e-05, 'epoch': 0.5}
{'loss': 0.7064, 'learning_rate': 4.941966956001056e-05, 'epoch': 0.5}
{'loss': 0.707, 'learning_rate': 4.9371311140751854e-05, 'epoch': 0.5}
{'loss': 0.6917, 'learning_rate': 4.9322953309663916e-05, 'epoch': 0.5}
                                            50%|     | 1636/3250 [2:32:39<2:26:50,  5.46s/it] 50%|     | 1637/3250 [2:32:44<2:26:00,  5.43s/it]                                                        50%|     | 1637/3250 [2:32:44<2:26:00,  5.43s/it] 50%|     | 1638/3250 [2:32:50<2:25:19,  5.41s/it]                                                        50%|     | 1638/3250 [2:32:50<2:25:19,  5.41s/it] 50%|     | 1639/3250 [2:32:55<2:24:49,  5.39s/it]                                                        50%|     | 1639/3250 [2:32:55<2:24:49,  5.39s/it] 50%|     | 1640/3250 [2:33:01<2:26:43,  5.47s/it]                                                        50%|     | 1640/3250 [2:33:01<2:26:43,  5.47s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8708637356758118, 'eval_runtime': 1.3817, 'eval_samples_per_second': 8.685, 'eval_steps_per_second': 2.171, 'epoch': 0.5}
                                                        50%|     | 1640/3250 [2:33:02<2:26:43,  5.47s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_4/tmp-checkpoint-1640
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1640I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1640

I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1640
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1640
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1640/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1640/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1640/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1640/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.6978, 'learning_rate': 4.9274596111987974e-05, 'epoch': 0.5}
{'loss': 0.7296, 'learning_rate': 4.922623959296468e-05, 'epoch': 0.51}
{'loss': 0.695, 'learning_rate': 4.9177883797834064e-05, 'epoch': 0.51}
{'loss': 0.7434, 'learning_rate': 4.912952877183543e-05, 'epoch': 0.51}
{'loss': 1.1822, 'learning_rate': 4.908117456020741e-05, 'epoch': 0.51}
 50%|     | 1641/3250 [2:33:08<2:39:32,  5.95s/it]                                                        50%|     | 1641/3250 [2:33:08<2:39:32,  5.95s/it] 51%|     | 1642/3250 [2:33:13<2:34:49,  5.78s/it]                                                        51%|     | 1642/3250 [2:33:13<2:34:49,  5.78s/it] 51%|     | 1643/3250 [2:33:19<2:31:25,  5.65s/it]                                                        51%|     | 1643/3250 [2:33:19<2:31:25,  5.65s/it] 51%|     | 1644/3250 [2:33:24<2:29:05,  5.57s/it]                                                        51%|     | 1644/3250 [2:33:24<2:29:05,  5.57s/it] 51%|     | 1645/3250 [2:33:29<2:27:18,  5.51s/it]                                                        51%|     | 1645/3250 [2:33:29<2:27:18,  5.51s/it] 51%|     | 1646/3250 [2:33:35<2:26:02,  5.46s/it]            {'loss': 0.6893, 'learning_rate': 4.903282120818785e-05, 'epoch': 0.51}
{'loss': 0.7213, 'learning_rate': 4.898446876101379e-05, 'epoch': 0.51}
{'loss': 0.7267, 'learning_rate': 4.893611726392145e-05, 'epoch': 0.51}
{'loss': 0.7326, 'learning_rate': 4.8887766762146134e-05, 'epoch': 0.51}
{'loss': 0.6582, 'learning_rate': 4.8839417300922216e-05, 'epoch': 0.51}
                                            51%|     | 1646/3250 [2:33:35<2:26:02,  5.46s/it] 51%|     | 1647/3250 [2:33:40<2:25:10,  5.43s/it]                                                        51%|     | 1647/3250 [2:33:40<2:25:10,  5.43s/it] 51%|     | 1648/3250 [2:33:45<2:24:35,  5.42s/it]                                                        51%|     | 1648/3250 [2:33:45<2:24:35,  5.42s/it] 51%|     | 1649/3250 [2:33:51<2:24:05,  5.40s/it]                                                        51%|     | 1649/3250 [2:33:51<2:24:05,  5.40s/it] 51%|     | 1650/3250 [2:33:56<2:23:37,  5.39s/it]                                                        51%|     | 1650/3250 [2:33:56<2:23:37,  5.39s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8681704998016357, 'eval_runtime': 1.3705, 'eval_samples_per_second': 8.756, 'eval_steps_per_second': 2.189, 'epoch': 0.51}
                                                        51%|     | 1650/3250 [2:33:57<2:23:37,  5.39s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_4/tmp-checkpoint-1650
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1650
/projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1650
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1650
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1650
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1650/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1650/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1650/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1650/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.7044, 'learning_rate': 4.8791068925483106e-05, 'epoch': 0.51}
{'loss': 0.7716, 'learning_rate': 4.8742721681061226e-05, 'epoch': 0.51}
{'loss': 0.709, 'learning_rate': 4.869437561288788e-05, 'epoch': 0.51}
{'loss': 0.7052, 'learning_rate': 4.8646030766193285e-05, 'epoch': 0.51}
{'loss': 0.6991, 'learning_rate': 4.859768718620656e-05, 'epoch': 0.51}
 51%|     | 1651/3250 [2:34:03<2:37:17,  5.90s/it]                                                        51%|     | 1651/3250 [2:34:03<2:37:17,  5.90s/it] 51%|     | 1652/3250 [2:34:09<2:32:56,  5.74s/it]                                                        51%|     | 1652/3250 [2:34:09<2:32:56,  5.74s/it] 51%|     | 1653/3250 [2:34:14<2:29:46,  5.63s/it]                                                        51%|     | 1653/3250 [2:34:14<2:29:46,  5.63s/it] 51%|     | 1654/3250 [2:34:19<2:27:38,  5.55s/it]                                                        51%|     | 1654/3250 [2:34:19<2:27:38,  5.55s/it] 51%|     | 1655/3250 [2:34:25<2:25:59,  5.49s/it]                                                        51%|     | 1655/3250 [2:34:25<2:25:59,  5.49s/it] 51%|     | 1656/3250 [2:34:31<2:29:53,  5.64s/it]            {'loss': 0.6915, 'learning_rate': 4.854934491815561e-05, 'epoch': 0.51}
{'loss': 0.7187, 'learning_rate': 4.8501004007267095e-05, 'epoch': 0.51}
{'loss': 0.687, 'learning_rate': 4.845266449876645e-05, 'epoch': 0.51}
{'loss': 0.6969, 'learning_rate': 4.8404326437877746e-05, 'epoch': 0.51}
{'loss': 0.6964, 'learning_rate': 4.8355989869823737e-05, 'epoch': 0.51}
                                            51%|     | 1656/3250 [2:34:31<2:29:53,  5.64s/it] 51%|     | 1657/3250 [2:34:36<2:27:32,  5.56s/it]                                                        51%|     | 1657/3250 [2:34:36<2:27:32,  5.56s/it] 51%|     | 1658/3250 [2:34:41<2:25:58,  5.50s/it]                                                        51%|     | 1658/3250 [2:34:41<2:25:58,  5.50s/it] 51%|     | 1659/3250 [2:34:47<2:24:45,  5.46s/it]                                                        51%|     | 1659/3250 [2:34:47<2:24:45,  5.46s/it] 51%|     | 1660/3250 [2:34:52<2:23:58,  5.43s/it]                                                        51%|     | 1660/3250 [2:34:52<2:23:58,  5.43s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8670073747634888, 'eval_runtime': 1.3722, 'eval_samples_per_second': 8.745, 'eval_steps_per_second': 2.186, 'epoch': 0.51}
                                                        51%|     | 1660/3250 [2:34:53<2:23:58,  5.43s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_4/tmp-checkpoint-1660
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1660
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1660
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1660
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1660
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1660/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1660/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1660/pytorch_model.binthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1660/pytorch_model.bin

/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.7154, 'learning_rate': 4.830765483982578e-05, 'epoch': 0.51}
{'loss': 0.7235, 'learning_rate': 4.8259321393103754e-05, 'epoch': 0.51}
{'loss': 0.7314, 'learning_rate': 4.821098957487611e-05, 'epoch': 0.51}
{'loss': 0.7214, 'learning_rate': 4.816265943035975e-05, 'epoch': 0.51}
{'loss': 0.7129, 'learning_rate': 4.811433100476999e-05, 'epoch': 0.51}
 51%|     | 1661/3250 [2:34:59<2:36:46,  5.92s/it]                                                        51%|     | 1661/3250 [2:34:59<2:36:46,  5.92s/it] 51%|     | 1662/3250 [2:35:05<2:32:30,  5.76s/it]                                                        51%|     | 1662/3250 [2:35:05<2:32:30,  5.76s/it] 51%|     | 1663/3250 [2:35:10<2:29:22,  5.65s/it]                                                        51%|     | 1663/3250 [2:35:10<2:29:22,  5.65s/it] 51%|     | 1664/3250 [2:35:15<2:27:20,  5.57s/it]                                                        51%|     | 1664/3250 [2:35:15<2:27:20,  5.57s/it] 51%|     | 1665/3250 [2:35:21<2:25:54,  5.52s/it]                                                        51%|     | 1665/3250 [2:35:21<2:25:54,  5.52s/it] 51%|    | 1666/3250 [2:35:26<2:24:48,  5.48s/it]          {'loss': 0.6885, 'learning_rate': 4.806600434332056e-05, 'epoch': 0.51}
{'loss': 0.7363, 'learning_rate': 4.801767949122356e-05, 'epoch': 0.51}
{'loss': 0.7168, 'learning_rate': 4.796935649368935e-05, 'epoch': 0.51}
{'loss': 0.7042, 'learning_rate': 4.7921035395926625e-05, 'epoch': 0.51}
{'loss': 0.693, 'learning_rate': 4.7872716243142194e-05, 'epoch': 0.51}
                                              51%|    | 1666/3250 [2:35:26<2:24:48,  5.48s/it] 51%|    | 1667/3250 [2:35:32<2:23:56,  5.46s/it]                                                        51%|    | 1667/3250 [2:35:32<2:23:56,  5.46s/it] 51%|    | 1668/3250 [2:35:37<2:23:18,  5.44s/it]                                                        51%|    | 1668/3250 [2:35:37<2:23:18,  5.44s/it] 51%|    | 1669/3250 [2:35:42<2:22:57,  5.43s/it]                                                        51%|    | 1669/3250 [2:35:42<2:22:57,  5.43s/it] 51%|    | 1670/3250 [2:35:48<2:22:39,  5.42s/it]                                                        51%|    | 1670/3250 [2:35:48<2:22:39,  5.42s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.868597149848938, 'eval_runtime': 1.3697, 'eval_samples_per_second': 8.761, 'eval_steps_per_second': 2.19, 'epoch': 0.51}
                                                        51%|    | 1670/3250 [2:35:49<2:22:39,  5.42s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_4/tmp-checkpoint-1670
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1670I AM HERE AND I AM SAVING THE MODEL

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1670
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1670
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1670
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1670/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1670/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1670/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1670/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.6748, 'learning_rate': 4.782439908054115e-05, 'epoch': 0.51}
{'loss': 0.7399, 'learning_rate': 4.777608395332667e-05, 'epoch': 0.51}
{'loss': 0.6817, 'learning_rate': 4.7727770906700034e-05, 'epoch': 0.51}
{'loss': 0.7583, 'learning_rate': 4.76794599858606e-05, 'epoch': 0.52}
{'loss': 1.1884, 'learning_rate': 4.763115123600571e-05, 'epoch': 0.52}
 51%|    | 1671/3250 [2:35:55<2:36:43,  5.96s/it]                                                        51%|    | 1671/3250 [2:35:55<2:36:43,  5.96s/it] 51%|    | 1672/3250 [2:36:00<2:32:17,  5.79s/it]                                                        51%|    | 1672/3250 [2:36:00<2:32:17,  5.79s/it] 51%|    | 1673/3250 [2:36:06<2:31:18,  5.76s/it]                                                        51%|    | 1673/3250 [2:36:06<2:31:18,  5.76s/it] 52%|    | 1674/3250 [2:36:11<2:28:24,  5.65s/it]                                                        52%|    | 1674/3250 [2:36:11<2:28:24,  5.65s/it] 52%|    | 1675/3250 [2:36:17<2:26:19,  5.57s/it]                                                        52%|    | 1675/3250 [2:36:17<2:26:19,  5.57s/it] 52%|    | 1676/3250 [2:36:22<2:24:48,  {'loss': 0.6914, 'learning_rate': 4.7582844702330685e-05, 'epoch': 0.52}
{'loss': 0.6988, 'learning_rate': 4.753454043002878e-05, 'epoch': 0.52}
{'loss': 0.7212, 'learning_rate': 4.748623846429112e-05, 'epoch': 0.52}
{'loss': 0.715, 'learning_rate': 4.743793885030668e-05, 'epoch': 0.52}
{'loss': 0.6708, 'learning_rate': 4.7389641633262224e-05, 'epoch': 0.52}
5.52s/it]                                                        52%|    | 1676/3250 [2:36:22<2:24:48,  5.52s/it] 52%|    | 1677/3250 [2:36:28<2:23:46,  5.48s/it]                                                        52%|    | 1677/3250 [2:36:28<2:23:46,  5.48s/it] 52%|    | 1678/3250 [2:36:33<2:23:00,  5.46s/it]                                                        52%|    | 1678/3250 [2:36:33<2:23:00,  5.46s/it] 52%|    | 1679/3250 [2:36:38<2:22:28,  5.44s/it]                                                        52%|    | 1679/3250 [2:36:38<2:22:28,  5.44s/it] 52%|    | 1680/3250 [2:36:44<2:22:04,  5.43s/it]                                                        52%|    | 1680/3250 [2:36:44<2:22:04,  5.43s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8664641976356506, 'eval_runtime': 1.3728, 'eval_samples_per_second': 8.741, 'eval_steps_per_second': 2.185, 'epoch': 0.52}
                                                        52%|    | 1680/3250 [2:36:45<2:22:04,  5.43s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_4/tmp-checkpoint-1680
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  I AM HERE AND I AM SAVING THE MODEL/projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1680
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1680

I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1680
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1680
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1680/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1680/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1680/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1680/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.6867, 'learning_rate': 4.7341346858342287e-05, 'epoch': 0.52}
{'loss': 0.7539, 'learning_rate': 4.729305457072913e-05, 'epoch': 0.52}
{'loss': 0.7287, 'learning_rate': 4.724476481560265e-05, 'epoch': 0.52}
{'loss': 0.7121, 'learning_rate': 4.7196477638140404e-05, 'epoch': 0.52}
{'loss': 0.6657, 'learning_rate': 4.714819308351755e-05, 'epoch': 0.52}
 52%|    | 1681/3250 [2:36:51<2:35:26,  5.94s/it]                                                        52%|    | 1681/3250 [2:36:51<2:35:26,  5.94s/it] 52%|    | 1682/3250 [2:36:56<2:31:08,  5.78s/it]                                                        52%|    | 1682/3250 [2:36:56<2:31:08,  5.78s/it] 52%|    | 1683/3250 [2:37:02<2:28:00,  5.67s/it]                                                        52%|    | 1683/3250 [2:37:02<2:28:00,  5.67s/it] 52%|    | 1684/3250 [2:37:07<2:25:55,  5.59s/it]                                                        52%|    | 1684/3250 [2:37:07<2:25:55,  5.59s/it] 52%|    | 1685/3250 [2:37:13<2:24:23,  5.54s/it]                                                        52%|    | 1685/3250 [2:37:13<2:24:23,  5.54s/it] 52%|    | 1686/3250 [2:37:18<2:23:22,  {'loss': 0.7073, 'learning_rate': 4.7099911196906764e-05, 'epoch': 0.52}
{'loss': 0.7021, 'learning_rate': 4.7051632023478204e-05, 'epoch': 0.52}
{'loss': 0.6968, 'learning_rate': 4.700335560839955e-05, 'epoch': 0.52}
{'loss': 0.6761, 'learning_rate': 4.695508199683586e-05, 'epoch': 0.52}
{'loss': 0.7064, 'learning_rate': 4.6906811233949585e-05, 'epoch': 0.52}
5.50s/it]                                                        52%|    | 1686/3250 [2:37:18<2:23:22,  5.50s/it] 52%|    | 1687/3250 [2:37:23<2:22:51,  5.48s/it]                                                        52%|    | 1687/3250 [2:37:23<2:22:51,  5.48s/it] 52%|    | 1688/3250 [2:37:29<2:22:12,  5.46s/it]                                                        52%|    | 1688/3250 [2:37:29<2:22:12,  5.46s/it] 52%|    | 1689/3250 [2:37:35<2:26:38,  5.64s/it]                                                        52%|    | 1689/3250 [2:37:35<2:26:38,  5.64s/it] 52%|    | 1690/3250 [2:37:40<2:24:43,  5.57s/it]                                                        52%|    | 1690/3250 [2:37:40<2:24:43,  5.57s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8661054372787476, 'eval_runtime': 1.372, 'eval_samples_per_second': 8.746, 'eval_steps_per_second': 2.187, 'epoch': 0.52}
                                                        52%|    | 1690/3250 [2:37:42<2:24:43,  5.57s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_4/tmp-checkpoint-1690
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1690I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1690

I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1690
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1690
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1690/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1690/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1690/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1690/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.707, 'learning_rate': 4.68585433649005e-05, 'epoch': 0.52}
{'loss': 0.7218, 'learning_rate': 4.68102784348457e-05, 'epoch': 0.52}
{'loss': 0.7218, 'learning_rate': 4.676201648893949e-05, 'epoch': 0.52}
{'loss': 0.6949, 'learning_rate': 4.67137575723334e-05, 'epoch': 0.52}
{'loss': 0.7195, 'learning_rate': 4.666550173017615e-05, 'epoch': 0.52}
 52%|    | 1691/3250 [2:37:47<2:36:26,  6.02s/it]                                                        52%|    | 1691/3250 [2:37:47<2:36:26,  6.02s/it] 52%|    | 1692/3250 [2:37:53<2:31:31,  5.84s/it]                                                        52%|    | 1692/3250 [2:37:53<2:31:31,  5.84s/it] 52%|    | 1693/3250 [2:37:58<2:28:01,  5.70s/it]                                                        52%|    | 1693/3250 [2:37:58<2:28:01,  5.70s/it] 52%|    | 1694/3250 [2:38:04<2:25:35,  5.61s/it]                                                        52%|    | 1694/3250 [2:38:04<2:25:35,  5.61s/it] 52%|    | 1695/3250 [2:38:09<2:23:51,  5.55s/it]                                                        52%|    | 1695/3250 [2:38:09<2:23:51,  5.55s/it] 52%|    | 1696/3250 [2:38:14<2:22:33,  {'loss': 0.71, 'learning_rate': 4.6617249007613544e-05, 'epoch': 0.52}
{'loss': 0.6726, 'learning_rate': 4.65689994497885e-05, 'epoch': 0.52}
{'loss': 0.7475, 'learning_rate': 4.652075310184094e-05, 'epoch': 0.52}
{'loss': 0.6882, 'learning_rate': 4.647251000890782e-05, 'epoch': 0.52}
{'loss': 0.6813, 'learning_rate': 4.642427021612304e-05, 'epoch': 0.52}
5.50s/it]                                                        52%|    | 1696/3250 [2:38:14<2:22:33,  5.50s/it] 52%|    | 1697/3250 [2:38:20<2:21:41,  5.47s/it]                                                        52%|    | 1697/3250 [2:38:20<2:21:41,  5.47s/it] 52%|    | 1698/3250 [2:38:25<2:21:00,  5.45s/it]                                                        52%|    | 1698/3250 [2:38:25<2:21:00,  5.45s/it] 52%|    | 1699/3250 [2:38:31<2:20:34,  5.44s/it]                                                        52%|    | 1699/3250 [2:38:31<2:20:34,  5.44s/it] 52%|    | 1700/3250 [2:38:36<2:20:11,  5.43s/it]                                                        52%|    | 1700/3250 [2:38:36<2:20:11,  5.43s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8688946962356567, 'eval_runtime': 1.3719, 'eval_samples_per_second': 8.747, 'eval_steps_per_second': 2.187, 'epoch': 0.52}
                                                        52%|    | 1700/3250 [2:38:37<2:20:11,  5.43s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_4/tmp-checkpoint-1700
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1700I AM HERE AND I AM SAVING THE MODEL

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1700
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1700
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1700
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1700/pytorch_model.bin
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1700/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1700/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1700/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.6826, 'learning_rate': 4.637603376861738e-05, 'epoch': 0.52}
{'loss': 0.7158, 'learning_rate': 4.6327800711518545e-05, 'epoch': 0.52}
{'loss': 0.6955, 'learning_rate': 4.6279571089951054e-05, 'epoch': 0.52}
{'loss': 0.7198, 'learning_rate': 4.623134494903618e-05, 'epoch': 0.52}
{'loss': 0.9156, 'learning_rate': 4.6183122333891926e-05, 'epoch': 0.52}
 52%|    | 1701/3250 [2:38:43<2:33:33,  5.95s/it]                                                        52%|    | 1701/3250 [2:38:43<2:33:33,  5.95s/it] 52%|    | 1702/3250 [2:38:49<2:29:10,  5.78s/it]                                                        52%|    | 1702/3250 [2:38:49<2:29:10,  5.78s/it] 52%|    | 1703/3250 [2:38:54<2:26:14,  5.67s/it]                                                        52%|    | 1703/3250 [2:38:54<2:26:14,  5.67s/it] 52%|    | 1704/3250 [2:38:59<2:24:06,  5.59s/it]                                                        52%|    | 1704/3250 [2:38:59<2:24:06,  5.59s/it] 52%|    | 1705/3250 [2:39:05<2:24:35,  5.62s/it]                                                        52%|    | 1705/3250 [2:39:05<2:24:35,  5.62s/it] 52%|    | 1706/3250 [2:39:10<2:22:47,  {'loss': 0.9838, 'learning_rate': 4.613490328963307e-05, 'epoch': 0.52}
{'loss': 0.6899, 'learning_rate': 4.6086687861371004e-05, 'epoch': 0.53}
{'loss': 0.7153, 'learning_rate': 4.6038476094213724e-05, 'epoch': 0.53}
{'loss': 0.7202, 'learning_rate': 4.599026803326583e-05, 'epoch': 0.53}
{'loss': 0.6996, 'learning_rate': 4.594206372362845e-05, 'epoch': 0.53}
5.55s/it]                                                        52%|    | 1706/3250 [2:39:10<2:22:47,  5.55s/it] 53%|    | 1707/3250 [2:39:16<2:21:28,  5.50s/it]                                                        53%|    | 1707/3250 [2:39:16<2:21:28,  5.50s/it] 53%|    | 1708/3250 [2:39:21<2:20:31,  5.47s/it]                                                        53%|    | 1708/3250 [2:39:21<2:20:31,  5.47s/it] 53%|    | 1709/3250 [2:39:27<2:19:56,  5.45s/it]                                                        53%|    | 1709/3250 [2:39:27<2:19:56,  5.45s/it] 53%|    | 1710/3250 [2:39:32<2:19:38,  5.44s/it]                                                        53%|    | 1710/3250 [2:39:32<2:19:38,  5.44s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8643024563789368, 'eval_runtime': 1.3728, 'eval_samples_per_second': 8.741, 'eval_steps_per_second': 2.185, 'epoch': 0.53}
                                                        53%|    | 1710/3250 [2:39:33<2:19:38,  5.44s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_4/tmp-checkpoint-1710
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  I AM HERE AND I AM SAVING THE MODEL
/projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1710the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1710

I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1710
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1710
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1710/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1710/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1710/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1710/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.6735, 'learning_rate': 4.589386321039917e-05, 'epoch': 0.53}
{'loss': 0.7425, 'learning_rate': 4.5845666538672074e-05, 'epoch': 0.53}
{'loss': 0.7438, 'learning_rate': 4.579747375353763e-05, 'epoch': 0.53}
{'loss': 0.7019, 'learning_rate': 4.574928490008264e-05, 'epoch': 0.53}
{'loss': 0.6738, 'learning_rate': 4.570110002339028e-05, 'epoch': 0.53}
 53%|    | 1711/3250 [2:39:39<2:32:33,  5.95s/it]                                                        53%|    | 1711/3250 [2:39:39<2:32:33,  5.95s/it] 53%|    | 1712/3250 [2:39:45<2:28:14,  5.78s/it]                                                        53%|    | 1712/3250 [2:39:45<2:28:14,  5.78s/it] 53%|    | 1713/3250 [2:39:50<2:25:13,  5.67s/it]                                                        53%|    | 1713/3250 [2:39:50<2:25:13,  5.67s/it] 53%|    | 1714/3250 [2:39:55<2:23:06,  5.59s/it]                                                        53%|    | 1714/3250 [2:39:55<2:23:06,  5.59s/it] 53%|    | 1715/3250 [2:40:01<2:21:34,  5.53s/it]                                                        53%|    | 1715/3250 [2:40:01<2:21:34,  5.53s/it] 53%|    | 1716/3250 [2:40:06<2:20:24,  {'loss': 0.7054, 'learning_rate': 4.5652919168539976e-05, 'epoch': 0.53}
{'loss': 0.691, 'learning_rate': 4.560474238060739e-05, 'epoch': 0.53}
{'loss': 0.7159, 'learning_rate': 4.5556569704664394e-05, 'epoch': 0.53}
{'loss': 0.6873, 'learning_rate': 4.5508401185778986e-05, 'epoch': 0.53}
{'loss': 0.6877, 'learning_rate': 4.546023686901533e-05, 'epoch': 0.53}
5.49s/it]                                                        53%|    | 1716/3250 [2:40:06<2:20:24,  5.49s/it] 53%|    | 1717/3250 [2:40:12<2:19:38,  5.47s/it]                                                        53%|    | 1717/3250 [2:40:12<2:19:38,  5.47s/it] 53%|    | 1718/3250 [2:40:17<2:19:02,  5.45s/it]                                                        53%|    | 1718/3250 [2:40:17<2:19:02,  5.45s/it] 53%|    | 1719/3250 [2:40:22<2:18:38,  5.43s/it]                                                        53%|    | 1719/3250 [2:40:22<2:18:38,  5.43s/it] 53%|    | 1720/3250 [2:40:28<2:18:16,  5.42s/it]                                                        53%|    | 1720/3250 [2:40:28<2:18:16,  5.42s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8623221516609192, 'eval_runtime': 1.3758, 'eval_samples_per_second': 8.722, 'eval_steps_per_second': 2.18, 'epoch': 0.53}
                                                        53%|    | 1720/3250 [2:40:29<2:18:16,  5.42s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_4/tmp-checkpoint-1720
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1720
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1720
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1720
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1720
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1720/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1720/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1720/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1720/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.6924, 'learning_rate': 4.541207679943357e-05, 'epoch': 0.53}
{'loss': 0.7108, 'learning_rate': 4.5363921022089974e-05, 'epoch': 0.53}
{'loss': 0.7143, 'learning_rate': 4.531576958203671e-05, 'epoch': 0.53}
{'loss': 0.6891, 'learning_rate': 4.526762252432195e-05, 'epoch': 0.53}
{'loss': 0.7145, 'learning_rate': 4.5219479893989756e-05, 'epoch': 0.53}
 53%|    | 1721/3250 [2:40:35<2:30:46,  5.92s/it]                                                        53%|    | 1721/3250 [2:40:35<2:30:46,  5.92s/it] 53%|    | 1722/3250 [2:40:41<2:29:36,  5.87s/it]                                                        53%|    | 1722/3250 [2:40:41<2:29:36,  5.87s/it] 53%|    | 1723/3250 [2:40:46<2:25:50,  5.73s/it]                                                        53%|    | 1723/3250 [2:40:46<2:25:50,  5.73s/it] 53%|    | 1724/3250 [2:40:51<2:23:17,  5.63s/it]                                                        53%|    | 1724/3250 [2:40:51<2:23:17,  5.63s/it] 53%|    | 1725/3250 [2:40:57<2:21:17,  5.56s/it]                                                        53%|    | 1725/3250 [2:40:57<2:21:17,  5.56s/it] 53%|    | 1726/3250 [2:41:02<2:19:45,  {'loss': 0.7289, 'learning_rate': 4.5171341736080004e-05, 'epoch': 0.53}
{'loss': 0.6656, 'learning_rate': 4.5123208095628424e-05, 'epoch': 0.53}
{'loss': 0.7538, 'learning_rate': 4.5075079017666547e-05, 'epoch': 0.53}
{'loss': 0.6864, 'learning_rate': 4.502695454722156e-05, 'epoch': 0.53}
{'loss': 0.6891, 'learning_rate': 4.4978834729316384e-05, 'epoch': 0.53}
5.50s/it]                                                        53%|    | 1726/3250 [2:41:02<2:19:45,  5.50s/it] 53%|    | 1727/3250 [2:41:08<2:18:38,  5.46s/it]                                                        53%|    | 1727/3250 [2:41:08<2:18:38,  5.46s/it] 53%|    | 1728/3250 [2:41:13<2:17:47,  5.43s/it]                                                        53%|    | 1728/3250 [2:41:13<2:17:47,  5.43s/it] 53%|    | 1729/3250 [2:41:18<2:17:23,  5.42s/it]                                                        53%|    | 1729/3250 [2:41:18<2:17:23,  5.42s/it] 53%|    | 1730/3250 [2:41:24<2:16:44,  5.40s/it]                                                        53%|    | 1730/3250 [2:41:24<2:16:44,  5.40s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8659586906433105, 'eval_runtime': 1.6001, 'eval_samples_per_second': 7.499, 'eval_steps_per_second': 1.875, 'epoch': 0.53}
                                                        53%|    | 1730/3250 [2:41:25<2:16:44,  5.40s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_4/tmp-checkpoint-1730
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1730
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1730
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1730
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1730
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1730/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1730/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1730/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1730/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.6856, 'learning_rate': 4.493071960896961e-05, 'epoch': 0.53}
{'loss': 0.683, 'learning_rate': 4.488260923119538e-05, 'epoch': 0.53}
{'loss': 0.7239, 'learning_rate': 4.483450364100345e-05, 'epoch': 0.53}
{'loss': 0.686, 'learning_rate': 4.478640288339907e-05, 'epoch': 0.53}
{'loss': 0.7405, 'learning_rate': 4.473830700338295e-05, 'epoch': 0.53}
 53%|    | 1731/3250 [2:41:31<2:30:57,  5.96s/it]                                                        53%|    | 1731/3250 [2:41:31<2:30:57,  5.96s/it] 53%|    | 1732/3250 [2:41:36<2:26:11,  5.78s/it]                                                        53%|    | 1732/3250 [2:41:36<2:26:11,  5.78s/it] 53%|    | 1733/3250 [2:41:42<2:22:52,  5.65s/it]                                                        53%|    | 1733/3250 [2:41:42<2:22:52,  5.65s/it] 53%|    | 1734/3250 [2:41:47<2:20:38,  5.57s/it]                                                        53%|    | 1734/3250 [2:41:47<2:20:38,  5.57s/it] 53%|    | 1735/3250 [2:41:52<2:18:57,  5.50s/it]                                                        53%|    | 1735/3250 [2:41:52<2:18:57,  5.50s/it] 53%|    | 1736/3250 [2:41:58<2:17:38,  {'loss': 1.1812, 'learning_rate': 4.4690216045951305e-05, 'epoch': 0.53}
{'loss': 0.6732, 'learning_rate': 4.4642130056095644e-05, 'epoch': 0.53}
{'loss': 0.7171, 'learning_rate': 4.4594049078802925e-05, 'epoch': 0.53}
{'loss': 0.7158, 'learning_rate': 4.454597315905535e-05, 'epoch': 0.54}
{'loss': 0.7199, 'learning_rate': 4.449790234183044e-05, 'epoch': 0.54}
5.45s/it]                                                        53%|    | 1736/3250 [2:41:58<2:17:38,  5.45s/it] 53%|    | 1737/3250 [2:42:03<2:16:46,  5.42s/it]                                                        53%|    | 1737/3250 [2:42:03<2:16:46,  5.42s/it] 53%|    | 1738/3250 [2:42:09<2:18:04,  5.48s/it]                                                        53%|    | 1738/3250 [2:42:09<2:18:04,  5.48s/it] 54%|    | 1739/3250 [2:42:14<2:17:03,  5.44s/it]                                                        54%|    | 1739/3250 [2:42:14<2:17:03,  5.44s/it] 54%|    | 1740/3250 [2:42:19<2:16:21,  5.42s/it]                                                        54%|    | 1740/3250 [2:42:19<2:16:21,  5.42s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8620589971542358, 'eval_runtime': 1.3829, 'eval_samples_per_second': 8.678, 'eval_steps_per_second': 2.169, 'epoch': 0.54}
                                                        54%|    | 1740/3250 [2:42:21<2:16:21,  5.42s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_4/tmp-checkpoint-1740
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  I AM HERE AND I AM SAVING THE MODEL
/projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1740the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1740

I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1740
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1740
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1740/pytorch_model.bin
the pytorch model path is the pytorch model path is/projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1740/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1740/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1740/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.6566, 'learning_rate': 4.4449836672100885e-05, 'epoch': 0.54}
{'loss': 0.7093, 'learning_rate': 4.4401776194834613e-05, 'epoch': 0.54}
{'loss': 0.7719, 'learning_rate': 4.435372095499468e-05, 'epoch': 0.54}
{'loss': 0.6979, 'learning_rate': 4.430567099753921e-05, 'epoch': 0.54}
{'loss': 0.6988, 'learning_rate': 4.425762636742143e-05, 'epoch': 0.54}
 54%|    | 1741/3250 [2:42:26<2:28:18,  5.90s/it]                                                        54%|    | 1741/3250 [2:42:26<2:28:18,  5.90s/it] 54%|    | 1742/3250 [2:42:32<2:24:12,  5.74s/it]                                                        54%|    | 1742/3250 [2:42:32<2:24:12,  5.74s/it] 54%|    | 1743/3250 [2:42:37<2:21:13,  5.62s/it]                                                        54%|    | 1743/3250 [2:42:37<2:21:13,  5.62s/it] 54%|    | 1744/3250 [2:42:42<2:19:06,  5.54s/it]                                                        54%|    | 1744/3250 [2:42:42<2:19:06,  5.54s/it] 54%|    | 1745/3250 [2:42:48<2:17:38,  5.49s/it]                                                        54%|    | 1745/3250 [2:42:48<2:17:38,  5.49s/it] 54%|    | 1746/3250 [2:42:53<2:16:35,  {'loss': 0.7005, 'learning_rate': 4.420958710958956e-05, 'epoch': 0.54}
{'loss': 0.6824, 'learning_rate': 4.416155326898679e-05, 'epoch': 0.54}
{'loss': 0.7173, 'learning_rate': 4.4113524890551246e-05, 'epoch': 0.54}
{'loss': 0.6743, 'learning_rate': 4.4065502019215965e-05, 'epoch': 0.54}
{'loss': 0.6961, 'learning_rate': 4.401748469990879e-05, 'epoch': 0.54}
5.45s/it]                                                        54%|    | 1746/3250 [2:42:53<2:16:35,  5.45s/it] 54%|    | 1747/3250 [2:42:59<2:15:42,  5.42s/it]                                                        54%|    | 1747/3250 [2:42:59<2:15:42,  5.42s/it] 54%|    | 1748/3250 [2:43:04<2:15:02,  5.39s/it]                                                        54%|    | 1748/3250 [2:43:04<2:15:02,  5.39s/it] 54%|    | 1749/3250 [2:43:09<2:14:40,  5.38s/it]                                                        54%|    | 1749/3250 [2:43:09<2:14:40,  5.38s/it] 54%|    | 1750/3250 [2:43:15<2:14:22,  5.37s/it]                                                        54%|    | 1750/3250 [2:43:15<2:14:22,  5.37s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8598679304122925, 'eval_runtime': 1.3706, 'eval_samples_per_second': 8.755, 'eval_steps_per_second': 2.189, 'epoch': 0.54}
                                                        54%|    | 1750/3250 [2:43:16<2:14:22,  5.37s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_4/tmp-checkpoint-1750
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in I AM HERE AND I AM SAVING THE MODEL 
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1750
/projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1750
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1750
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1750
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1750/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1750/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1750/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1750/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.6883, 'learning_rate': 4.39694729775524e-05, 'epoch': 0.54}
{'loss': 0.7072, 'learning_rate': 4.392146689706425e-05, 'epoch': 0.54}
{'loss': 0.7174, 'learning_rate': 4.387346650335649e-05, 'epoch': 0.54}
{'loss': 0.7169, 'learning_rate': 4.382547184133593e-05, 'epoch': 0.54}
{'loss': 0.7069, 'learning_rate': 4.377748295590407e-05, 'epoch': 0.54}
 54%|    | 1751/3250 [2:43:22<2:27:27,  5.90s/it]                                                        54%|    | 1751/3250 [2:43:22<2:27:27,  5.90s/it] 54%|    | 1752/3250 [2:43:27<2:23:13,  5.74s/it]                                                        54%|    | 1752/3250 [2:43:27<2:23:13,  5.74s/it] 54%|    | 1753/3250 [2:43:32<2:20:09,  5.62s/it]                                                        54%|    | 1753/3250 [2:43:32<2:20:09,  5.62s/it] 54%|    | 1754/3250 [2:43:38<2:20:56,  5.65s/it]                                                        54%|    | 1754/3250 [2:43:38<2:20:56,  5.65s/it] 54%|    | 1755/3250 [2:43:44<2:18:41,  5.57s/it]                                                        54%|    | 1755/3250 [2:43:44<2:18:41,  5.57s/it] 54%|    | 1756/3250 [2:43:49<2:17:04,  {'loss': 0.7012, 'learning_rate': 4.372949989195697e-05, 'epoch': 0.54}
{'loss': 0.6894, 'learning_rate': 4.3681522694385256e-05, 'epoch': 0.54}
{'loss': 0.7161, 'learning_rate': 4.3633551408074075e-05, 'epoch': 0.54}
{'loss': 0.703, 'learning_rate': 4.358558607790303e-05, 'epoch': 0.54}
{'loss': 0.6761, 'learning_rate': 4.3537626748746143e-05, 'epoch': 0.54}
5.50s/it]                                                        54%|    | 1756/3250 [2:43:49<2:17:04,  5.50s/it] 54%|    | 1757/3250 [2:43:54<2:15:56,  5.46s/it]                                                        54%|    | 1757/3250 [2:43:54<2:15:56,  5.46s/it] 54%|    | 1758/3250 [2:44:00<2:15:02,  5.43s/it]                                                        54%|    | 1758/3250 [2:44:00<2:15:02,  5.43s/it] 54%|    | 1759/3250 [2:44:05<2:14:32,  5.41s/it]                                                        54%|    | 1759/3250 [2:44:05<2:14:32,  5.41s/it] 54%|    | 1760/3250 [2:44:10<2:14:07,  5.40s/it]                                                        54%|    | 1760/3250 [2:44:10<2:14:07,  5.40s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8646560907363892, 'eval_runtime': 1.3823, 'eval_samples_per_second': 8.681, 'eval_steps_per_second': 2.17, 'epoch': 0.54}
                                                        54%|    | 1760/3250 [2:44:12<2:14:07,  5.40s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_4/tmp-checkpoint-1760
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1760
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1760
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1760
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1760
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1760/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1760/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1760/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1760/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.6822, 'learning_rate': 4.348967346547185e-05, 'epoch': 0.54}
{'loss': 0.6721, 'learning_rate': 4.344172627294289e-05, 'epoch': 0.54}
{'loss': 0.7289, 'learning_rate': 4.339378521601635e-05, 'epoch': 0.54}
{'loss': 0.6722, 'learning_rate': 4.334585033954355e-05, 'epoch': 0.54}
{'loss': 0.7565, 'learning_rate': 4.329792168837002e-05, 'epoch': 0.54}
 54%|    | 1761/3250 [2:44:17<2:26:29,  5.90s/it]                                                        54%|    | 1761/3250 [2:44:17<2:26:29,  5.90s/it] 54%|    | 1762/3250 [2:44:23<2:22:16,  5.74s/it]                                                        54%|    | 1762/3250 [2:44:23<2:22:16,  5.74s/it] 54%|    | 1763/3250 [2:44:28<2:19:25,  5.63s/it]                                                        54%|    | 1763/3250 [2:44:28<2:19:25,  5.63s/it] 54%|    | 1764/3250 [2:44:33<2:17:16,  5.54s/it]                                                        54%|    | 1764/3250 [2:44:33<2:17:16,  5.54s/it] 54%|    | 1765/3250 [2:44:39<2:15:51,  5.49s/it]                                                        54%|    | 1765/3250 [2:44:39<2:15:51,  5.49s/it] 54%|    | 1766/3250 [2:44:44<2:14:41,  {'loss': 1.1826, 'learning_rate': 4.3249999307335495e-05, 'epoch': 0.54}
{'loss': 0.6801, 'learning_rate': 4.320208324127383e-05, 'epoch': 0.54}
{'loss': 0.6994, 'learning_rate': 4.3154173535012946e-05, 'epoch': 0.54}
{'loss': 0.7092, 'learning_rate': 4.3106270233374845e-05, 'epoch': 0.54}
{'loss': 0.7043, 'learning_rate': 4.3058373381175574e-05, 'epoch': 0.54}
5.45s/it]                                                        54%|    | 1766/3250 [2:44:44<2:14:41,  5.45s/it] 54%|    | 1767/3250 [2:44:50<2:13:54,  5.42s/it]                                                        54%|    | 1767/3250 [2:44:50<2:13:54,  5.42s/it] 54%|    | 1768/3250 [2:44:55<2:13:25,  5.40s/it]                                                        54%|    | 1768/3250 [2:44:55<2:13:25,  5.40s/it] 54%|    | 1769/3250 [2:45:00<2:12:55,  5.39s/it]                                                        54%|    | 1769/3250 [2:45:00<2:12:55,  5.39s/it] 54%|    | 1770/3250 [2:45:06<2:12:39,  5.38s/it]                                                        54%|    | 1770/3250 [2:45:06<2:12:39,  5.38s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8597317337989807, 'eval_runtime': 1.6019, 'eval_samples_per_second': 7.491, 'eval_steps_per_second': 1.873, 'epoch': 0.54}
                                                        54%|    | 1770/3250 [2:45:07<2:12:39,  5.38s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_4/tmp-checkpoint-1770
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1770I AM HERE AND I AM SAVING THE MODEL

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1770
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1770
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1770
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1770/pytorch_model.bin
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1770/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1770/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1770/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.6753, 'learning_rate': 4.3010483023225045e-05, 'epoch': 0.54}
{'loss': 0.6826, 'learning_rate': 4.296259920432716e-05, 'epoch': 0.55}
{'loss': 0.7391, 'learning_rate': 4.2914721969279705e-05, 'epoch': 0.55}
{'loss': 0.7279, 'learning_rate': 4.28668513628743e-05, 'epoch': 0.55}
{'loss': 0.7059, 'learning_rate': 4.281898742989636e-05, 'epoch': 0.55}
 54%|    | 1771/3250 [2:45:13<2:28:56,  6.04s/it]                                                        54%|    | 1771/3250 [2:45:13<2:28:56,  6.04s/it] 55%|    | 1772/3250 [2:45:19<2:23:44,  5.83s/it]                                                        55%|    | 1772/3250 [2:45:19<2:23:44,  5.83s/it] 55%|    | 1773/3250 [2:45:24<2:20:12,  5.70s/it]                                                        55%|    | 1773/3250 [2:45:24<2:20:12,  5.70s/it] 55%|    | 1774/3250 [2:45:29<2:17:34,  5.59s/it]                                                        55%|    | 1774/3250 [2:45:29<2:17:34,  5.59s/it] 55%|    | 1775/3250 [2:45:35<2:15:44,  5.52s/it]                                                        55%|    | 1775/3250 [2:45:35<2:15:44,  5.52s/it] 55%|    | 1776/3250 [2:45:40<2:14:24,  {'loss': 0.6491, 'learning_rate': 4.277113021512505e-05, 'epoch': 0.55}
{'loss': 0.7007, 'learning_rate': 4.2723279763333265e-05, 'epoch': 0.55}
{'loss': 0.6986, 'learning_rate': 4.267543611928754e-05, 'epoch': 0.55}
{'loss': 0.7032, 'learning_rate': 4.2627599327748105e-05, 'epoch': 0.55}
{'loss': 0.6767, 'learning_rate': 4.2579769433468694e-05, 'epoch': 0.55}
5.47s/it]                                                        55%|    | 1776/3250 [2:45:40<2:14:24,  5.47s/it] 55%|    | 1777/3250 [2:45:45<2:13:23,  5.43s/it]                                                        55%|    | 1777/3250 [2:45:45<2:13:23,  5.43s/it] 55%|    | 1778/3250 [2:45:51<2:12:38,  5.41s/it]                                                        55%|    | 1778/3250 [2:45:51<2:12:38,  5.41s/it] 55%|    | 1779/3250 [2:45:56<2:12:14,  5.39s/it]                                                        55%|    | 1779/3250 [2:45:56<2:12:14,  5.39s/it] 55%|    | 1780/3250 [2:46:01<2:11:50,  5.38s/it]                                                        55%|    | 1780/3250 [2:46:01<2:11:50,  5.38s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8590574264526367, 'eval_runtime': 1.3693, 'eval_samples_per_second': 8.763, 'eval_steps_per_second': 2.191, 'epoch': 0.55}
                                                        55%|    | 1780/3250 [2:46:03<2:11:50,  5.38s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_4/tmp-checkpoint-1780
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1780
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1780
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1780
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1780
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1780/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1780/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1780/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1780/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.6963, 'learning_rate': 4.253194648119667e-05, 'epoch': 0.55}
{'loss': 0.6981, 'learning_rate': 4.2484130515672856e-05, 'epoch': 0.55}
{'loss': 0.7178, 'learning_rate': 4.243632158163152e-05, 'epoch': 0.55}
{'loss': 0.7077, 'learning_rate': 4.2388519723800415e-05, 'epoch': 0.55}
{'loss': 0.69, 'learning_rate': 4.234072498690062e-05, 'epoch': 0.55}
 55%|    | 1781/3250 [2:46:08<2:23:53,  5.88s/it]                                                        55%|    | 1781/3250 [2:46:08<2:23:53,  5.88s/it] 55%|    | 1782/3250 [2:46:14<2:19:44,  5.71s/it]                                                        55%|    | 1782/3250 [2:46:14<2:19:44,  5.71s/it] 55%|    | 1783/3250 [2:46:19<2:17:14,  5.61s/it]                                                        55%|    | 1783/3250 [2:46:19<2:17:14,  5.61s/it] 55%|    | 1784/3250 [2:46:24<2:15:05,  5.53s/it]                                                        55%|    | 1784/3250 [2:46:24<2:15:05,  5.53s/it] 55%|    | 1785/3250 [2:46:30<2:13:41,  5.48s/it]                                                        55%|    | 1785/3250 [2:46:30<2:13:41,  5.48s/it] 55%|    | 1786/3250 [2:46:35<2:12:42,  {'loss': 0.7068, 'learning_rate': 4.229293741564658e-05, 'epoch': 0.55}
{'loss': 0.7081, 'learning_rate': 4.224515705474603e-05, 'epoch': 0.55}
{'loss': 0.6787, 'learning_rate': 4.2197383948899925e-05, 'epoch': 0.55}
{'loss': 0.7534, 'learning_rate': 4.2149618142802494e-05, 'epoch': 0.55}
{'loss': 0.6762, 'learning_rate': 4.210185968114109e-05, 'epoch': 0.55}
5.44s/it]                                                        55%|    | 1786/3250 [2:46:35<2:12:42,  5.44s/it] 55%|    | 1787/3250 [2:46:41<2:14:47,  5.53s/it]                                                        55%|    | 1787/3250 [2:46:41<2:14:47,  5.53s/it] 55%|    | 1788/3250 [2:46:46<2:13:31,  5.48s/it]                                                        55%|    | 1788/3250 [2:46:46<2:13:31,  5.48s/it] 55%|    | 1789/3250 [2:46:52<2:12:22,  5.44s/it]                                                        55%|    | 1789/3250 [2:46:52<2:12:22,  5.44s/it] 55%|    | 1790/3250 [2:46:57<2:11:44,  5.41s/it]                                                        55%|    | 1790/3250 [2:46:57<2:11:44,  5.41s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8630701303482056, 'eval_runtime': 1.3815, 'eval_samples_per_second': 8.686, 'eval_steps_per_second': 2.172, 'epoch': 0.55}
                                                        55%|    | 1790/3250 [2:46:58<2:11:44,  5.41s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_4/tmp-checkpoint-1790
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1790
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1790
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1790
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1790
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1790/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1790/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1790/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1790/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.6825, 'learning_rate': 4.20541086085962e-05, 'epoch': 0.55}
{'loss': 0.6716, 'learning_rate': 4.200636496984144e-05, 'epoch': 0.55}
{'loss': 0.7108, 'learning_rate': 4.1958628809543416e-05, 'epoch': 0.55}
{'loss': 0.6849, 'learning_rate': 4.1910900172361764e-05, 'epoch': 0.55}
{'loss': 0.7085, 'learning_rate': 4.1863179102949094e-05, 'epoch': 0.55}
 55%|    | 1791/3250 [2:47:04<2:23:42,  5.91s/it]                                                        55%|    | 1791/3250 [2:47:04<2:23:42,  5.91s/it] 55%|    | 1792/3250 [2:47:09<2:19:29,  5.74s/it]                                                        55%|    | 1792/3250 [2:47:09<2:19:29,  5.74s/it] 55%|    | 1793/3250 [2:47:15<2:16:31,  5.62s/it]                                                        55%|    | 1793/3250 [2:47:15<2:16:31,  5.62s/it] 55%|    | 1794/3250 [2:47:20<2:14:48,  5.56s/it]                                                        55%|    | 1794/3250 [2:47:20<2:14:48,  5.56s/it] 55%|    | 1795/3250 [2:47:26<2:13:58,  5.52s/it]                                                        55%|    | 1795/3250 [2:47:26<2:13:58,  5.52s/it] 55%|    | 1796/3250 [2:47:31<2:12:29,  {'loss': 0.9569, 'learning_rate': 4.18154656459509e-05, 'epoch': 0.55}
{'loss': 0.9406, 'learning_rate': 4.1767759846005596e-05, 'epoch': 0.55}
{'loss': 0.6794, 'learning_rate': 4.1720061747744396e-05, 'epoch': 0.55}
{'loss': 0.7081, 'learning_rate': 4.1672371395791335e-05, 'epoch': 0.55}
{'loss': 0.7084, 'learning_rate': 4.162468883476319e-05, 'epoch': 0.55}
5.47s/it]                                                        55%|    | 1796/3250 [2:47:31<2:12:29,  5.47s/it] 55%|    | 1797/3250 [2:47:36<2:11:27,  5.43s/it]                                                        55%|    | 1797/3250 [2:47:36<2:11:27,  5.43s/it] 55%|    | 1798/3250 [2:47:42<2:10:37,  5.40s/it]                                                        55%|    | 1798/3250 [2:47:42<2:10:37,  5.40s/it] 55%|    | 1799/3250 [2:47:47<2:10:04,  5.38s/it]                                                        55%|    | 1799/3250 [2:47:47<2:10:04,  5.38s/it] 55%|    | 1800/3250 [2:47:52<2:09:49,  5.37s/it]                                                        55%|    | 1800/3250 [2:47:52<2:09:49,  5.37s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8575560450553894, 'eval_runtime': 1.5899, 'eval_samples_per_second': 7.548, 'eval_steps_per_second': 1.887, 'epoch': 0.55}
                                                        55%|    | 1800/3250 [2:47:54<2:09:49,  5.37s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_4/tmp-checkpoint-1800
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1800
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1800
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1800
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1800
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1800/pytorch_model.binthe pytorch model path is
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1800/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1800/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1800/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.6861, 'learning_rate': 4.157701410926943e-05, 'epoch': 0.55}
{'loss': 0.6583, 'learning_rate': 4.152934726391223e-05, 'epoch': 0.55}
{'loss': 0.7268, 'learning_rate': 4.148168834328638e-05, 'epoch': 0.55}
{'loss': 0.7278, 'learning_rate': 4.1434037391979266e-05, 'epoch': 0.56}
{'loss': 0.6925, 'learning_rate': 4.1386394454570745e-05, 'epoch': 0.56}
 55%|    | 1801/3250 [2:47:59<2:23:08,  5.93s/it]                                                        55%|    | 1801/3250 [2:47:59<2:23:08,  5.93s/it] 55%|    | 1802/3250 [2:48:05<2:18:47,  5.75s/it]                                                        55%|    | 1802/3250 [2:48:05<2:18:47,  5.75s/it] 55%|    | 1803/3250 [2:48:12<2:30:17,  6.23s/it]                                                        55%|    | 1803/3250 [2:48:12<2:30:17,  6.23s/it] 56%|    | 1804/3250 [2:48:18<2:27:18,  6.11s/it]                                                        56%|    | 1804/3250 [2:48:18<2:27:18,  6.11s/it] 56%|    | 1805/3250 [2:48:23<2:21:58,  5.90s/it]                                                        56%|    | 1805/3250 [2:48:23<2:21:58,  5.90s/it] 56%|    | 1806/3250 [2:48:29<2:17:54,  {'loss': 0.6634, 'learning_rate': 4.133875957563329e-05, 'epoch': 0.56}
{'loss': 0.7093, 'learning_rate': 4.129113279973177e-05, 'epoch': 0.56}
{'loss': 0.6851, 'learning_rate': 4.124351417142347e-05, 'epoch': 0.56}
{'loss': 0.7084, 'learning_rate': 4.1195903735258064e-05, 'epoch': 0.56}
{'loss': 0.6766, 'learning_rate': 4.114830153577759e-05, 'epoch': 0.56}
5.73s/it]                                                        56%|    | 1806/3250 [2:48:29<2:17:54,  5.73s/it] 56%|    | 1807/3250 [2:48:34<2:15:01,  5.61s/it]                                                        56%|    | 1807/3250 [2:48:34<2:15:01,  5.61s/it] 56%|    | 1808/3250 [2:48:39<2:12:54,  5.53s/it]                                                        56%|    | 1808/3250 [2:48:39<2:12:54,  5.53s/it] 56%|    | 1809/3250 [2:48:45<2:11:29,  5.47s/it]                                                        56%|    | 1809/3250 [2:48:45<2:11:29,  5.47s/it] 56%|    | 1810/3250 [2:48:50<2:10:27,  5.44s/it]                                                        56%|    | 1810/3250 [2:48:50<2:10:27,  5.44s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8581498861312866, 'eval_runtime': 1.5884, 'eval_samples_per_second': 7.555, 'eval_steps_per_second': 1.889, 'epoch': 0.56}
                                                        56%|    | 1810/3250 [2:48:52<2:10:27,  5.44s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_4/tmp-checkpoint-1810
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1810I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1810

I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1810
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1810
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1810/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1810/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1810/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1810/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.683, 'learning_rate': 4.110070761751633e-05, 'epoch': 0.56}
{'loss': 0.6928, 'learning_rate': 4.1053122025000843e-05, 'epoch': 0.56}
{'loss': 0.7059, 'learning_rate': 4.100554480274993e-05, 'epoch': 0.56}
{'loss': 0.7041, 'learning_rate': 4.095797599527449e-05, 'epoch': 0.56}
{'loss': 0.6866, 'learning_rate': 4.09104156470776e-05, 'epoch': 0.56}
 56%|    | 1811/3250 [2:48:58<2:24:39,  6.03s/it]                                                        56%|    | 1811/3250 [2:48:58<2:24:39,  6.03s/it] 56%|    | 1812/3250 [2:49:03<2:19:57,  5.84s/it]                                                        56%|    | 1812/3250 [2:49:03<2:19:57,  5.84s/it] 56%|    | 1813/3250 [2:49:08<2:16:40,  5.71s/it]                                                        56%|    | 1813/3250 [2:49:08<2:16:40,  5.71s/it] 56%|    | 1814/3250 [2:49:14<2:14:09,  5.61s/it]                                                        56%|    | 1814/3250 [2:49:14<2:14:09,  5.61s/it] 56%|    | 1815/3250 [2:49:19<2:12:32,  5.54s/it]                                                        56%|    | 1815/3250 [2:49:19<2:12:32,  5.54s/it] 56%|    | 1816/3250 [2:49:24<2:11:20,  {'loss': 0.6975, 'learning_rate': 4.086286380265443e-05, 'epoch': 0.56}
{'loss': 0.711, 'learning_rate': 4.081532050649216e-05, 'epoch': 0.56}
{'loss': 0.6592, 'learning_rate': 4.076778580306999e-05, 'epoch': 0.56}
{'loss': 0.7447, 'learning_rate': 4.072025973685908e-05, 'epoch': 0.56}
{'loss': 0.6798, 'learning_rate': 4.067274235232251e-05, 'epoch': 0.56}
5.50s/it]                                                        56%|    | 1816/3250 [2:49:24<2:11:20,  5.50s/it] 56%|    | 1817/3250 [2:49:30<2:10:33,  5.47s/it]                                                        56%|    | 1817/3250 [2:49:30<2:10:33,  5.47s/it] 56%|    | 1818/3250 [2:49:35<2:09:54,  5.44s/it]                                                        56%|    | 1818/3250 [2:49:35<2:09:54,  5.44s/it] 56%|    | 1819/3250 [2:49:41<2:09:43,  5.44s/it]                                                        56%|    | 1819/3250 [2:49:41<2:09:43,  5.44s/it] 56%|    | 1820/3250 [2:49:46<2:12:04,  5.54s/it]                                                        56%|    | 1820/3250 [2:49:46<2:12:04,  5.54s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8607726693153381, 'eval_runtime': 1.3813, 'eval_samples_per_second': 8.688, 'eval_steps_per_second': 2.172, 'epoch': 0.56}
                                                        56%|    | 1820/3250 [2:49:48<2:12:04,  5.54s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_4/tmp-checkpoint-1820
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1820
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1820
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1820
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1820
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1820/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1820/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1820/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1820/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.6905, 'learning_rate': 4.0625233693915264e-05, 'epoch': 0.56}
{'loss': 0.6652, 'learning_rate': 4.057773380608411e-05, 'epoch': 0.56}
{'loss': 0.6801, 'learning_rate': 4.053024273326761e-05, 'epoch': 0.56}
{'loss': 0.7085, 'learning_rate': 4.048276051989614e-05, 'epoch': 0.56}
{'loss': 0.6743, 'learning_rate': 4.0435287210391756e-05, 'epoch': 0.56}
 56%|    | 1821/3250 [2:49:54<2:23:10,  6.01s/it]                                                        56%|    | 1821/3250 [2:49:54<2:23:10,  6.01s/it] 56%|    | 1822/3250 [2:49:59<2:18:26,  5.82s/it]                                                        56%|    | 1822/3250 [2:49:59<2:18:26,  5.82s/it] 56%|    | 1823/3250 [2:50:04<2:15:11,  5.68s/it]                                                        56%|    | 1823/3250 [2:50:04<2:15:11,  5.68s/it] 56%|    | 1824/3250 [2:50:10<2:12:55,  5.59s/it]                                                        56%|    | 1824/3250 [2:50:10<2:12:55,  5.59s/it] 56%|    | 1825/3250 [2:50:15<2:11:21,  5.53s/it]                                                        56%|    | 1825/3250 [2:50:15<2:11:21,  5.53s/it] 56%|    | 1826/3250 [2:50:20<2:10:07,  {'loss': 0.7311, 'learning_rate': 4.038782284916816e-05, 'epoch': 0.56}
{'loss': 1.1754, 'learning_rate': 4.034036748063072e-05, 'epoch': 0.56}
{'loss': 0.6639, 'learning_rate': 4.029292114917638e-05, 'epoch': 0.56}
{'loss': 0.7135, 'learning_rate': 4.0245483899193595e-05, 'epoch': 0.56}
{'loss': 0.7093, 'learning_rate': 4.019805577506237e-05, 'epoch': 0.56}
5.48s/it]                                                        56%|    | 1826/3250 [2:50:20<2:10:07,  5.48s/it] 56%|    | 1827/3250 [2:50:26<2:09:10,  5.45s/it]                                                        56%|    | 1827/3250 [2:50:26<2:09:10,  5.45s/it] 56%|    | 1828/3250 [2:50:31<2:08:38,  5.43s/it]                                                        56%|    | 1828/3250 [2:50:31<2:08:38,  5.43s/it] 56%|    | 1829/3250 [2:50:37<2:08:03,  5.41s/it]                                                        56%|    | 1829/3250 [2:50:37<2:08:03,  5.41s/it] 56%|    | 1830/3250 [2:50:42<2:07:46,  5.40s/it]                                                        56%|    | 1830/3250 [2:50:42<2:07:46,  5.40s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8563528060913086, 'eval_runtime': 1.376, 'eval_samples_per_second': 8.721, 'eval_steps_per_second': 2.18, 'epoch': 0.56}
                                                        56%|    | 1830/3250 [2:50:43<2:07:46,  5.40s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_4/tmp-checkpoint-1830
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1830
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1830
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1830
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1830
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1830/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1830/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1830/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1830/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.7052, 'learning_rate': 4.0150636821154166e-05, 'epoch': 0.56}
{'loss': 0.6538, 'learning_rate': 4.010322708183183e-05, 'epoch': 0.56}
{'loss': 0.6823, 'learning_rate': 4.005582660144963e-05, 'epoch': 0.56}
{'loss': 0.7764, 'learning_rate': 4.000843542435315e-05, 'epoch': 0.56}
{'loss': 0.6876, 'learning_rate': 3.9961053594879266e-05, 'epoch': 0.56}
 56%|    | 1831/3250 [2:50:49<2:19:21,  5.89s/it]                                                        56%|    | 1831/3250 [2:50:49<2:19:21,  5.89s/it] 56%|    | 1832/3250 [2:50:54<2:15:29,  5.73s/it]                                                        56%|    | 1832/3250 [2:50:54<2:15:29,  5.73s/it] 56%|    | 1833/3250 [2:51:00<2:12:45,  5.62s/it]                                                        56%|    | 1833/3250 [2:51:00<2:12:45,  5.62s/it] 56%|    | 1834/3250 [2:51:05<2:10:51,  5.55s/it]                                                        56%|    | 1834/3250 [2:51:05<2:10:51,  5.55s/it] 56%|    | 1835/3250 [2:51:10<2:09:29,  5.49s/it]                                                        56%|    | 1835/3250 [2:51:10<2:09:29,  5.49s/it] 56%|    | 1836/3250 [2:51:16<2:08:30,  {'loss': 0.6897, 'learning_rate': 3.991368115735612e-05, 'epoch': 0.56}
{'loss': 0.6778, 'learning_rate': 3.986631815610308e-05, 'epoch': 0.57}
{'loss': 0.6787, 'learning_rate': 3.981896463543067e-05, 'epoch': 0.57}
{'loss': 0.7172, 'learning_rate': 3.977162063964049e-05, 'epoch': 0.57}
{'loss': 0.6727, 'learning_rate': 3.972428621302534e-05, 'epoch': 0.57}
5.45s/it]                                                        56%|    | 1836/3250 [2:51:16<2:08:30,  5.45s/it] 57%|    | 1837/3250 [2:51:22<2:12:02,  5.61s/it]                                                        57%|    | 1837/3250 [2:51:22<2:12:02,  5.61s/it] 57%|    | 1838/3250 [2:51:27<2:10:15,  5.53s/it]                                                        57%|    | 1838/3250 [2:51:27<2:10:15,  5.53s/it] 57%|    | 1839/3250 [2:51:33<2:09:00,  5.49s/it]                                                        57%|    | 1839/3250 [2:51:33<2:09:00,  5.49s/it] 57%|    | 1840/3250 [2:51:38<2:08:06,  5.45s/it]                                                        57%|    | 1840/3250 [2:51:38<2:08:06,  5.45s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.858584463596344, 'eval_runtime': 1.3714, 'eval_samples_per_second': 8.75, 'eval_steps_per_second': 2.188, 'epoch': 0.57}
                                                        57%|    | 1840/3250 [2:51:39<2:08:06,  5.45s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_4/tmp-checkpoint-1840
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  I AM HERE AND I AM SAVING THE MODEL/projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1840
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1840

I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1840
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1840
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1840/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1840/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1840/pytorch_model.binthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1840/pytorch_model.bin

/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.6839, 'learning_rate': 3.9676961399869e-05, 'epoch': 0.57}
{'loss': 0.6739, 'learning_rate': 3.962964624444625e-05, 'epoch': 0.57}
{'loss': 0.6883, 'learning_rate': 3.958234079102288e-05, 'epoch': 0.57}
{'loss': 0.7074, 'learning_rate': 3.953504508385554e-05, 'epoch': 0.57}
{'loss': 0.7032, 'learning_rate': 3.9487759167191815e-05, 'epoch': 0.57}
 57%|    | 1841/3250 [2:51:45<2:19:10,  5.93s/it]                                                        57%|    | 1841/3250 [2:51:45<2:19:10,  5.93s/it] 57%|    | 1842/3250 [2:51:50<2:15:07,  5.76s/it]                                                        57%|    | 1842/3250 [2:51:50<2:15:07,  5.76s/it] 57%|    | 1843/3250 [2:51:56<2:12:48,  5.66s/it]                                                        57%|    | 1843/3250 [2:51:56<2:12:48,  5.66s/it] 57%|    | 1844/3250 [2:52:01<2:10:35,  5.57s/it]                                                        57%|    | 1844/3250 [2:52:01<2:10:35,  5.57s/it] 57%|    | 1845/3250 [2:52:06<2:09:11,  5.52s/it]                                                        57%|    | 1845/3250 [2:52:06<2:09:11,  5.52s/it] 57%|    | 1846/3250 [2:52:12<2:08:04,  {'loss': 0.7029, 'learning_rate': 3.9440483085270126e-05, 'epoch': 0.57}
{'loss': 0.6872, 'learning_rate': 3.939321688231965e-05, 'epoch': 0.57}
{'loss': 0.6742, 'learning_rate': 3.934596060256037e-05, 'epoch': 0.57}
{'loss': 0.7076, 'learning_rate': 3.9298714290202977e-05, 'epoch': 0.57}
{'loss': 0.7024, 'learning_rate': 3.92514779894488e-05, 'epoch': 0.57}
5.47s/it]                                                        57%|    | 1846/3250 [2:52:12<2:08:04,  5.47s/it] 57%|    | 1847/3250 [2:52:17<2:07:14,  5.44s/it]                                                        57%|    | 1847/3250 [2:52:17<2:07:14,  5.44s/it] 57%|    | 1848/3250 [2:52:23<2:07:03,  5.44s/it]                                                        57%|    | 1848/3250 [2:52:23<2:07:03,  5.44s/it] 57%|    | 1849/3250 [2:52:28<2:06:28,  5.42s/it]                                                        57%|    | 1849/3250 [2:52:28<2:06:28,  5.42s/it] 57%|    | 1850/3250 [2:52:33<2:05:57,  5.40s/it]                                                        57%|    | 1850/3250 [2:52:33<2:05:57,  5.40s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8617576956748962, 'eval_runtime': 1.3734, 'eval_samples_per_second': 8.738, 'eval_steps_per_second': 2.184, 'epoch': 0.57}
                                                        57%|    | 1850/3250 [2:52:35<2:05:57,  5.40s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_4/tmp-checkpoint-1850
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1850I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1850

I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1850
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1850
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1850/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1850/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1850/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1850/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.6633, 'learning_rate': 3.920425174448984e-05, 'epoch': 0.57}
{'loss': 0.6793, 'learning_rate': 3.9157035599508684e-05, 'epoch': 0.57}
{'loss': 0.6455, 'learning_rate': 3.910982959867845e-05, 'epoch': 0.57}
{'loss': 0.719, 'learning_rate': 3.906263378616279e-05, 'epoch': 0.57}
{'loss': 0.6737, 'learning_rate': 3.901544820611584e-05, 'epoch': 0.57}
 57%|    | 1851/3250 [2:52:41<2:18:49,  5.95s/it]                                                        57%|    | 1851/3250 [2:52:41<2:18:49,  5.95s/it] 57%|    | 1852/3250 [2:52:46<2:14:42,  5.78s/it]                                                        57%|    | 1852/3250 [2:52:46<2:14:42,  5.78s/it] 57%|    | 1853/3250 [2:52:52<2:13:33,  5.74s/it]                                                        57%|    | 1853/3250 [2:52:52<2:13:33,  5.74s/it] 57%|    | 1854/3250 [2:52:57<2:10:52,  5.63s/it]                                                        57%|    | 1854/3250 [2:52:57<2:10:52,  5.63s/it] 57%|    | 1855/3250 [2:53:02<2:09:03,  5.55s/it]                                                        57%|    | 1855/3250 [2:53:02<2:09:03,  5.55s/it] 57%|    | 1856/3250 [2:53:08<2:07:43,  {'loss': 0.7425, 'learning_rate': 3.89682729026821e-05, 'epoch': 0.57}
{'loss': 1.1695, 'learning_rate': 3.892110791999649e-05, 'epoch': 0.57}
{'loss': 0.6812, 'learning_rate': 3.887395330218429e-05, 'epoch': 0.57}
{'loss': 0.6873, 'learning_rate': 3.882680909336108e-05, 'epoch': 0.57}
{'loss': 0.7086, 'learning_rate': 3.877967533763267e-05, 'epoch': 0.57}
5.50s/it]                                                        57%|    | 1856/3250 [2:53:08<2:07:43,  5.50s/it] 57%|    | 1857/3250 [2:53:13<2:06:40,  5.46s/it]                                                        57%|    | 1857/3250 [2:53:13<2:06:40,  5.46s/it] 57%|    | 1858/3250 [2:53:18<2:06:06,  5.44s/it]                                                        57%|    | 1858/3250 [2:53:18<2:06:06,  5.44s/it] 57%|    | 1859/3250 [2:53:24<2:05:31,  5.41s/it]                                                        57%|    | 1859/3250 [2:53:24<2:05:31,  5.41s/it] 57%|    | 1860/3250 [2:53:29<2:05:04,  5.40s/it]                                                        57%|    | 1860/3250 [2:53:29<2:05:04,  5.40s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8557683229446411, 'eval_runtime': 1.3793, 'eval_samples_per_second': 8.7, 'eval_steps_per_second': 2.175, 'epoch': 0.57}
                                                        57%|    | 1860/3250 [2:53:31<2:05:04,  5.40s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_4/tmp-checkpoint-1860
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  I AM HERE AND I AM SAVING THE MODEL/projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1860
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1860

I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1860
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1860
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1860/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1860/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1860/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1860/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.711, 'learning_rate': 3.873255207909514e-05, 'epoch': 0.57}
{'loss': 0.6569, 'learning_rate': 3.86854393618347e-05, 'epoch': 0.57}
{'loss': 0.6691, 'learning_rate': 3.863833722992774e-05, 'epoch': 0.57}
{'loss': 0.7423, 'learning_rate': 3.859124572744071e-05, 'epoch': 0.57}
{'loss': 0.717, 'learning_rate': 3.854416489843014e-05, 'epoch': 0.57}
 57%|    | 1861/3250 [2:53:36<2:16:30,  5.90s/it]                                                        57%|    | 1861/3250 [2:53:36<2:16:30,  5.90s/it] 57%|    | 1862/3250 [2:53:42<2:12:45,  5.74s/it]                                                        57%|    | 1862/3250 [2:53:42<2:12:45,  5.74s/it] 57%|    | 1863/3250 [2:53:47<2:10:07,  5.63s/it]                                                        57%|    | 1863/3250 [2:53:47<2:10:07,  5.63s/it] 57%|    | 1864/3250 [2:53:52<2:08:12,  5.55s/it]                                                        57%|    | 1864/3250 [2:53:52<2:08:12,  5.55s/it] 57%|    | 1865/3250 [2:53:58<2:06:55,  5.50s/it]                                                        57%|    | 1865/3250 [2:53:58<2:06:55,  5.50s/it] 57%|    | 1866/3250 [2:54:03<2:05:57,  {'loss': 0.6999, 'learning_rate': 3.849709478694256e-05, 'epoch': 0.57}
{'loss': 0.6389, 'learning_rate': 3.8450035437014494e-05, 'epoch': 0.57}
{'loss': 0.6986, 'learning_rate': 3.8402986892672377e-05, 'epoch': 0.57}
{'loss': 0.6922, 'learning_rate': 3.8355949197932535e-05, 'epoch': 0.58}
{'loss': 0.6951, 'learning_rate': 3.830892239680117e-05, 'epoch': 0.58}
5.46s/it]                                                        57%|    | 1866/3250 [2:54:03<2:05:57,  5.46s/it] 57%|    | 1867/3250 [2:54:08<2:05:13,  5.43s/it]                                                        57%|    | 1867/3250 [2:54:09<2:05:13,  5.43s/it] 57%|    | 1868/3250 [2:54:14<2:04:45,  5.42s/it]                                                        57%|    | 1868/3250 [2:54:14<2:04:45,  5.42s/it] 58%|    | 1869/3250 [2:54:20<2:08:31,  5.58s/it]                                                        58%|    | 1869/3250 [2:54:20<2:08:31,  5.58s/it] 58%|    | 1870/3250 [2:54:25<2:06:54,  5.52s/it]                                                        58%|    | 1870/3250 [2:54:25<2:06:54,  5.52s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8566293716430664, 'eval_runtime': 1.3727, 'eval_samples_per_second': 8.742, 'eval_steps_per_second': 2.185, 'epoch': 0.58}
                                                        58%|    | 1870/3250 [2:54:27<2:06:54,  5.52s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_4/tmp-checkpoint-1870
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1870
 /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1870
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1870
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1870
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1870/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1870/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1870/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1870/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.6832, 'learning_rate': 3.8261906533274254e-05, 'epoch': 0.58}
{'loss': 0.6958, 'learning_rate': 3.8214901651337574e-05, 'epoch': 0.58}
{'loss': 0.6914, 'learning_rate': 3.8167907794966574e-05, 'epoch': 0.58}
{'loss': 0.7, 'learning_rate': 3.812092500812646e-05, 'epoch': 0.58}
{'loss': 0.7042, 'learning_rate': 3.807395333477201e-05, 'epoch': 0.58}
 58%|    | 1871/3250 [2:54:32<2:17:21,  5.98s/it]                                                        58%|    | 1871/3250 [2:54:32<2:17:21,  5.98s/it] 58%|    | 1872/3250 [2:54:38<2:13:04,  5.79s/it]                                                        58%|    | 1872/3250 [2:54:38<2:13:04,  5.79s/it] 58%|    | 1873/3250 [2:54:43<2:10:01,  5.67s/it]                                                        58%|    | 1873/3250 [2:54:43<2:10:01,  5.67s/it] 58%|    | 1874/3250 [2:54:48<2:07:52,  5.58s/it]                                                        58%|    | 1874/3250 [2:54:48<2:07:52,  5.58s/it] 58%|    | 1875/3250 [2:54:54<2:06:18,  5.51s/it]                                                        58%|    | 1875/3250 [2:54:54<2:06:18,  5.51s/it] 58%|    | 1876/3250 [2:54:59<2:05:15,  {'loss': 0.7065, 'learning_rate': 3.802699281884767e-05, 'epoch': 0.58}
{'loss': 0.7094, 'learning_rate': 3.798004350428741e-05, 'epoch': 0.58}
{'loss': 0.6984, 'learning_rate': 3.793310543501473e-05, 'epoch': 0.58}
{'loss': 0.669, 'learning_rate': 3.7886178654942595e-05, 'epoch': 0.58}
{'loss': 0.7386, 'learning_rate': 3.7839263207973444e-05, 'epoch': 0.58}
5.47s/it]                                                        58%|    | 1876/3250 [2:54:59<2:05:15,  5.47s/it] 58%|    | 1877/3250 [2:55:04<2:04:31,  5.44s/it]                                                        58%|    | 1877/3250 [2:55:04<2:04:31,  5.44s/it] 58%|    | 1878/3250 [2:55:10<2:04:02,  5.42s/it]                                                        58%|    | 1878/3250 [2:55:10<2:04:02,  5.42s/it] 58%|    | 1879/3250 [2:55:15<2:03:31,  5.41s/it]                                                        58%|    | 1879/3250 [2:55:15<2:03:31,  5.41s/it] 58%|    | 1880/3250 [2:55:21<2:03:10,  5.39s/it]                                                        58%|    | 1880/3250 [2:55:21<2:03:10,  5.39s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8588560223579407, 'eval_runtime': 1.3711, 'eval_samples_per_second': 8.752, 'eval_steps_per_second': 2.188, 'epoch': 0.58}
                                                        58%|    | 1880/3250 [2:55:22<2:03:10,  5.39s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_4/tmp-checkpoint-1880
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1880
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1880
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1880
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1880
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1880/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1880/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1880/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1880/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.6812, 'learning_rate': 3.7792359137999064e-05, 'epoch': 0.58}
{'loss': 0.6701, 'learning_rate': 3.774546648890066e-05, 'epoch': 0.58}
{'loss': 0.6748, 'learning_rate': 3.769858530454869e-05, 'epoch': 0.58}
{'loss': 0.6984, 'learning_rate': 3.7651715628802916e-05, 'epoch': 0.58}
{'loss': 0.6867, 'learning_rate': 3.7604857505512345e-05, 'epoch': 0.58}
 58%|    | 1881/3250 [2:55:28<2:14:21,  5.89s/it]                                                        58%|    | 1881/3250 [2:55:28<2:14:21,  5.89s/it] 58%|    | 1882/3250 [2:55:33<2:10:42,  5.73s/it]                                                        58%|    | 1882/3250 [2:55:33<2:10:42,  5.73s/it] 58%|    | 1883/3250 [2:55:38<2:08:03,  5.62s/it]                                                        58%|    | 1883/3250 [2:55:38<2:08:03,  5.62s/it] 58%|    | 1884/3250 [2:55:44<2:06:20,  5.55s/it]                                                        58%|    | 1884/3250 [2:55:44<2:06:20,  5.55s/it] 58%|    | 1885/3250 [2:55:49<2:05:03,  5.50s/it]                                                        58%|    | 1885/3250 [2:55:49<2:05:03,  5.50s/it] 58%|    | 1886/3250 [2:55:55<2:05:56,  {'loss': 0.6973, 'learning_rate': 3.7558010978515143e-05, 'epoch': 0.58}
{'loss': 1.0437, 'learning_rate': 3.7511176091638653e-05, 'epoch': 0.58}
{'loss': 0.8223, 'learning_rate': 3.7464352888699333e-05, 'epoch': 0.58}
{'loss': 0.6755, 'learning_rate': 3.74175414135027e-05, 'epoch': 0.58}
{'loss': 0.7171, 'learning_rate': 3.737074170984326e-05, 'epoch': 0.58}
5.54s/it]                                                        58%|    | 1886/3250 [2:55:55<2:05:56,  5.54s/it] 58%|    | 1887/3250 [2:56:00<2:04:43,  5.49s/it]                                                        58%|    | 1887/3250 [2:56:00<2:04:43,  5.49s/it] 58%|    | 1888/3250 [2:56:06<2:03:49,  5.46s/it]                                                        58%|    | 1888/3250 [2:56:06<2:03:49,  5.46s/it] 58%|    | 1889/3250 [2:56:11<2:03:17,  5.43s/it]                                                        58%|    | 1889/3250 [2:56:11<2:03:17,  5.43s/it] 58%|    | 1890/3250 [2:56:16<2:02:41,  5.41s/it]                                                        58%|    | 1890/3250 [2:56:16<2:02:41,  5.41s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8559186458587646, 'eval_runtime': 1.3812, 'eval_samples_per_second': 8.688, 'eval_steps_per_second': 2.172, 'epoch': 0.58}
                                                        58%|    | 1890/3250 [2:56:18<2:02:41,  5.41s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_4/tmp-checkpoint-1890
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1890I AM HERE AND I AM SAVING THE MODEL

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1890
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1890
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1890
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1890/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1890/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1890/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1890/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.7026, 'learning_rate': 3.7323953821504576e-05, 'epoch': 0.58}
{'loss': 0.6894, 'learning_rate': 3.7277177792259114e-05, 'epoch': 0.58}
{'loss': 0.6539, 'learning_rate': 3.723041366586826e-05, 'epoch': 0.58}
{'loss': 0.7303, 'learning_rate': 3.7183661486082245e-05, 'epoch': 0.58}
{'loss': 0.7157, 'learning_rate': 3.7136921296640165e-05, 'epoch': 0.58}
 58%|    | 1891/3250 [2:56:23<2:14:31,  5.94s/it]                                                        58%|    | 1891/3250 [2:56:23<2:14:31,  5.94s/it] 58%|    | 1892/3250 [2:56:29<2:10:37,  5.77s/it]                                                        58%|    | 1892/3250 [2:56:29<2:10:37,  5.77s/it] 58%|    | 1893/3250 [2:56:34<2:07:50,  5.65s/it]                                                        58%|    | 1893/3250 [2:56:34<2:07:50,  5.65s/it] 58%|    | 1894/3250 [2:56:40<2:05:54,  5.57s/it]                                                        58%|    | 1894/3250 [2:56:40<2:05:54,  5.57s/it] 58%|    | 1895/3250 [2:56:45<2:04:31,  5.51s/it]                                                        58%|    | 1895/3250 [2:56:45<2:04:31,  5.51s/it] 58%|    | 1896/3250 [2:56:50<2:03:31,  {'loss': 0.6847, 'learning_rate': 3.709019314126985e-05, 'epoch': 0.58}
{'loss': 0.6513, 'learning_rate': 3.704347706368789e-05, 'epoch': 0.58}
{'loss': 0.7009, 'learning_rate': 3.69967731075996e-05, 'epoch': 0.58}
{'loss': 0.6807, 'learning_rate': 3.695008131669891e-05, 'epoch': 0.58}
{'loss': 0.6919, 'learning_rate': 3.690340173466842e-05, 'epoch': 0.58}
5.47s/it]                                                        58%|    | 1896/3250 [2:56:50<2:03:31,  5.47s/it] 58%|    | 1897/3250 [2:56:56<2:02:49,  5.45s/it]                                                        58%|    | 1897/3250 [2:56:56<2:02:49,  5.45s/it] 58%|    | 1898/3250 [2:57:01<2:02:17,  5.43s/it]                                                        58%|    | 1898/3250 [2:57:01<2:02:17,  5.43s/it] 58%|    | 1899/3250 [2:57:06<2:01:48,  5.41s/it]                                                        58%|    | 1899/3250 [2:57:06<2:01:48,  5.41s/it] 58%|    | 1900/3250 [2:57:12<2:01:30,  5.40s/it]                                                        58%|    | 1900/3250 [2:57:12<2:01:30,  5.40s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8564742803573608, 'eval_runtime': 1.3702, 'eval_samples_per_second': 8.758, 'eval_steps_per_second': 2.189, 'epoch': 0.58}
                                                        58%|    | 1900/3250 [2:57:13<2:01:30,  5.40s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_4/tmp-checkpoint-1900
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1900
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1900
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1900
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1900
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1900/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1900/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1900/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1900/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.6782, 'learning_rate': 3.685673440517924e-05, 'epoch': 0.58}
{'loss': 0.685, 'learning_rate': 3.6810079371891094e-05, 'epoch': 0.59}
{'loss': 0.6763, 'learning_rate': 3.6763436678452153e-05, 'epoch': 0.59}
{'loss': 0.7015, 'learning_rate': 3.6716806368499045e-05, 'epoch': 0.59}
{'loss': 0.6954, 'learning_rate': 3.667018848565683e-05, 'epoch': 0.59}
 58%|    | 1901/3250 [2:57:19<2:12:47,  5.91s/it]                                                        58%|    | 1901/3250 [2:57:19<2:12:47,  5.91s/it] 59%|    | 1902/3250 [2:57:25<2:13:15,  5.93s/it]                                                        59%|    | 1902/3250 [2:57:25<2:13:15,  5.93s/it] 59%|    | 1903/3250 [2:57:30<2:09:18,  5.76s/it]                                                        59%|    | 1903/3250 [2:57:30<2:09:18,  5.76s/it] 59%|    | 1904/3250 [2:57:36<2:06:32,  5.64s/it]                                                        59%|    | 1904/3250 [2:57:36<2:06:32,  5.64s/it] 59%|    | 1905/3250 [2:57:41<2:04:43,  5.56s/it]                                                        59%|    | 1905/3250 [2:57:41<2:04:43,  5.56s/it] 59%|    | 1906/3250 [2:57:46<2:03:20,  {'loss': 0.6852, 'learning_rate': 3.6623583073538966e-05, 'epoch': 0.59}
{'loss': 0.694, 'learning_rate': 3.657699017574717e-05, 'epoch': 0.59}
{'loss': 0.705, 'learning_rate': 3.653040983587151e-05, 'epoch': 0.59}
{'loss': 0.6518, 'learning_rate': 3.6483842097490287e-05, 'epoch': 0.59}
{'loss': 0.7414, 'learning_rate': 3.643728700417002e-05, 'epoch': 0.59}
5.51s/it]                                                        59%|    | 1906/3250 [2:57:46<2:03:20,  5.51s/it] 59%|    | 1907/3250 [2:57:52<2:02:23,  5.47s/it]                                                        59%|    | 1907/3250 [2:57:52<2:02:23,  5.47s/it] 59%|    | 1908/3250 [2:57:57<2:01:45,  5.44s/it]                                                        59%|    | 1908/3250 [2:57:57<2:01:45,  5.44s/it] 59%|    | 1909/3250 [2:58:03<2:01:08,  5.42s/it]                                                        59%|    | 1909/3250 [2:58:03<2:01:08,  5.42s/it] 59%|    | 1910/3250 [2:58:08<2:00:45,  5.41s/it]                                                        59%|    | 1910/3250 [2:58:08<2:00:45,  5.41s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8579741716384888, 'eval_runtime': 1.3717, 'eval_samples_per_second': 8.748, 'eval_steps_per_second': 2.187, 'epoch': 0.59}
                                                        59%|    | 1910/3250 [2:58:09<2:00:45,  5.41s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_4/tmp-checkpoint-1910
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1910I AM HERE AND I AM SAVING THE MODEL

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1910
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1910
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1910
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1910/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1910/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1910/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1910/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.6692, 'learning_rate': 3.639074459946539e-05, 'epoch': 0.59}
{'loss': 0.6792, 'learning_rate': 3.63442149269192e-05, 'epoch': 0.59}
{'loss': 0.6525, 'learning_rate': 3.629769803006239e-05, 'epoch': 0.59}
{'loss': 0.6701, 'learning_rate': 3.6251193952413865e-05, 'epoch': 0.59}
{'loss': 0.7054, 'learning_rate': 3.62047027374806e-05, 'epoch': 0.59}
 59%|    | 1911/3250 [2:58:15<2:12:41,  5.95s/it]                                                        59%|    | 1911/3250 [2:58:15<2:12:41,  5.95s/it] 59%|    | 1912/3250 [2:58:21<2:09:05,  5.79s/it]                                                        59%|    | 1912/3250 [2:58:21<2:09:05,  5.79s/it] 59%|    | 1913/3250 [2:58:26<2:06:12,  5.66s/it]                                                        59%|    | 1913/3250 [2:58:26<2:06:12,  5.66s/it] 59%|    | 1914/3250 [2:58:31<2:04:02,  5.57s/it]                                                        59%|    | 1914/3250 [2:58:31<2:04:02,  5.57s/it] 59%|    | 1915/3250 [2:58:37<2:02:36,  5.51s/it]                                                        59%|    | 1915/3250 [2:58:37<2:02:36,  5.51s/it] 59%|    | 1916/3250 [2:58:42<2:01:35,  {'loss': 0.6619, 'learning_rate': 3.6158224428757535e-05, 'epoch': 0.59}
{'loss': 0.7243, 'learning_rate': 3.611175906972749e-05, 'epoch': 0.59}
{'loss': 1.1721, 'learning_rate': 3.606530670386121e-05, 'epoch': 0.59}
{'loss': 0.663, 'learning_rate': 3.601886737461729e-05, 'epoch': 0.59}
{'loss': 0.7026, 'learning_rate': 3.597244112544208e-05, 'epoch': 0.59}
5.47s/it]                                                        59%|    | 1916/3250 [2:58:42<2:01:35,  5.47s/it] 59%|    | 1917/3250 [2:58:47<2:00:49,  5.44s/it]                                                        59%|    | 1917/3250 [2:58:47<2:00:49,  5.44s/it] 59%|    | 1918/3250 [2:58:53<2:00:10,  5.41s/it]                                                        59%|    | 1918/3250 [2:58:53<2:00:10,  5.41s/it] 59%|    | 1919/3250 [2:58:58<2:01:19,  5.47s/it]                                                        59%|    | 1919/3250 [2:58:58<2:01:19,  5.47s/it] 59%|    | 1920/3250 [2:59:04<2:00:28,  5.43s/it]                                                        59%|    | 1920/3250 [2:59:04<2:00:28,  5.43s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8560892343521118, 'eval_runtime': 1.3844, 'eval_samples_per_second': 8.668, 'eval_steps_per_second': 2.167, 'epoch': 0.59}
                                                        59%|    | 1920/3250 [2:59:05<2:00:28,  5.43s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_4/tmp-checkpoint-1920
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1920I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1920

I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1920
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1920
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1920/pytorch_model.bin
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1920/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1920/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1920/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.6955, 'learning_rate': 3.5926027999769754e-05, 'epoch': 0.59}
{'loss': 0.7049, 'learning_rate': 3.587962804102214e-05, 'epoch': 0.59}
{'loss': 0.6467, 'learning_rate': 3.5833241292608846e-05, 'epoch': 0.59}
{'loss': 0.6779, 'learning_rate': 3.5786867797926996e-05, 'epoch': 0.59}
{'loss': 0.7548, 'learning_rate': 3.5740507600361415e-05, 'epoch': 0.59}
 59%|    | 1921/3250 [2:59:11<2:11:33,  5.94s/it]                                                        59%|    | 1921/3250 [2:59:11<2:11:33,  5.94s/it] 59%|    | 1922/3250 [2:59:16<2:07:38,  5.77s/it]                                                        59%|    | 1922/3250 [2:59:16<2:07:38,  5.77s/it] 59%|    | 1923/3250 [2:59:22<2:05:03,  5.65s/it]                                                        59%|    | 1923/3250 [2:59:22<2:05:03,  5.65s/it] 59%|    | 1924/3250 [2:59:27<2:03:01,  5.57s/it]                                                        59%|    | 1924/3250 [2:59:27<2:03:01,  5.57s/it] 59%|    | 1925/3250 [2:59:32<2:01:35,  5.51s/it]                                                        59%|    | 1925/3250 [2:59:32<2:01:35,  5.51s/it] 59%|    | 1926/3250 [2:59:38<2:00:31,  {'loss': 0.67, 'learning_rate': 3.569416074328445e-05, 'epoch': 0.59}
{'loss': 0.6826, 'learning_rate': 3.5647827270055945e-05, 'epoch': 0.59}
{'loss': 0.6693, 'learning_rate': 3.560150722402329e-05, 'epoch': 0.59}
{'loss': 0.6608, 'learning_rate': 3.5555200648521236e-05, 'epoch': 0.59}
{'loss': 0.7021, 'learning_rate': 3.550890758687199e-05, 'epoch': 0.59}
5.46s/it]                                                        59%|    | 1926/3250 [2:59:38<2:00:31,  5.46s/it] 59%|    | 1927/3250 [2:59:43<1:59:50,  5.44s/it]                                                        59%|    | 1927/3250 [2:59:43<1:59:50,  5.44s/it] 59%|    | 1928/3250 [2:59:48<1:59:15,  5.41s/it]                                                        59%|    | 1928/3250 [2:59:48<1:59:15,  5.41s/it] 59%|    | 1929/3250 [2:59:54<1:59:02,  5.41s/it]                                                        59%|    | 1929/3250 [2:59:54<1:59:02,  5.41s/it] 59%|    | 1930/3250 [2:59:59<1:58:39,  5.39s/it]                                                        59%|    | 1930/3250 [2:59:59<1:58:39,  5.39s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8580170273780823, 'eval_runtime': 1.3737, 'eval_samples_per_second': 8.735, 'eval_steps_per_second': 2.184, 'epoch': 0.59}
                                                        59%|    | 1930/3250 [3:00:00<1:58:39,  5.39s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_4/tmp-checkpoint-1930
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1930
/projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1930
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1930
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1930
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1930/pytorch_model.bin
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1930/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1930/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1930/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.6733, 'learning_rate': 3.546262808238507e-05, 'epoch': 0.59}
{'loss': 0.6761, 'learning_rate': 3.5416362178357354e-05, 'epoch': 0.59}
{'loss': 0.679, 'learning_rate': 3.537010991807296e-05, 'epoch': 0.59}
{'loss': 0.6946, 'learning_rate': 3.5323871344803263e-05, 'epoch': 0.6}
{'loss': 0.6985, 'learning_rate': 3.527764650180683e-05, 'epoch': 0.6}
 59%|    | 1931/3250 [3:00:06<2:09:32,  5.89s/it]                                                        59%|    | 1931/3250 [3:00:06<2:09:32,  5.89s/it] 59%|    | 1932/3250 [3:00:12<2:05:54,  5.73s/it]                                                        59%|    | 1932/3250 [3:00:12<2:05:54,  5.73s/it] 59%|    | 1933/3250 [3:00:17<2:03:27,  5.62s/it]                                                        59%|    | 1933/3250 [3:00:17<2:03:27,  5.62s/it] 60%|    | 1934/3250 [3:00:22<2:01:36,  5.54s/it]                                                        60%|    | 1934/3250 [3:00:22<2:01:36,  5.54s/it] 60%|    | 1935/3250 [3:00:28<2:04:17,  5.67s/it]                                                        60%|    | 1935/3250 [3:00:28<2:04:17,  5.67s/it] 60%|    | 1936/3250 [3:00:34<2:02:04,  {'loss': 0.7075, 'learning_rate': 3.523143543232936e-05, 'epoch': 0.6}
{'loss': 0.6936, 'learning_rate': 3.518523817960372e-05, 'epoch': 0.6}
{'loss': 0.6897, 'learning_rate': 3.5139054786849784e-05, 'epoch': 0.6}
{'loss': 0.6777, 'learning_rate': 3.5092885297274524e-05, 'epoch': 0.6}
{'loss': 0.7068, 'learning_rate': 3.504672975407184e-05, 'epoch': 0.6}
5.57s/it]                                                        60%|    | 1936/3250 [3:00:34<2:02:04,  5.57s/it] 60%|    | 1937/3250 [3:00:39<2:00:42,  5.52s/it]                                                        60%|    | 1937/3250 [3:00:39<2:00:42,  5.52s/it] 60%|    | 1938/3250 [3:00:44<1:59:46,  5.48s/it]                                                        60%|    | 1938/3250 [3:00:44<1:59:46,  5.48s/it] 60%|    | 1939/3250 [3:00:50<1:58:57,  5.44s/it]                                                        60%|    | 1939/3250 [3:00:50<1:58:57,  5.44s/it] 60%|    | 1940/3250 [3:00:55<1:58:26,  5.42s/it]                                                        60%|    | 1940/3250 [3:00:55<1:58:26,  5.42s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8585472106933594, 'eval_runtime': 1.3733, 'eval_samples_per_second': 8.738, 'eval_steps_per_second': 2.184, 'epoch': 0.6}
                                                        60%|    | 1940/3250 [3:00:56<1:58:26,  5.42s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_4/tmp-checkpoint-1940
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1940I AM HERE AND I AM SAVING THE MODEL

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1940
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1940
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1940
the pytorch model path is the pytorch model path is/projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1940/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1940/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1940/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1940/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.6999, 'learning_rate': 3.500058820042263e-05, 'epoch': 0.6}
{'loss': 0.662, 'learning_rate': 3.4954460679494685e-05, 'epoch': 0.6}
{'loss': 0.6645, 'learning_rate': 3.490834723444268e-05, 'epoch': 0.6}
{'loss': 0.6444, 'learning_rate': 3.486224790840811e-05, 'epoch': 0.6}
{'loss': 0.7094, 'learning_rate': 3.4816162744519263e-05, 'epoch': 0.6}
 60%|    | 1941/3250 [3:01:02<2:09:10,  5.92s/it]                                                        60%|    | 1941/3250 [3:01:02<2:09:10,  5.92s/it] 60%|    | 1942/3250 [3:01:08<2:05:29,  5.76s/it]                                                        60%|    | 1942/3250 [3:01:08<2:05:29,  5.76s/it] 60%|    | 1943/3250 [3:01:13<2:02:53,  5.64s/it]                                                        60%|    | 1943/3250 [3:01:13<2:02:53,  5.64s/it] 60%|    | 1944/3250 [3:01:18<2:01:04,  5.56s/it]                                                        60%|    | 1944/3250 [3:01:18<2:01:04,  5.56s/it] 60%|    | 1945/3250 [3:01:24<1:59:45,  5.51s/it]                                                        60%|    | 1945/3250 [3:01:24<1:59:45,  5.51s/it] 60%|    | 1946/3250 [3:01:29<1:58:48,  {'loss': 0.6666, 'learning_rate': 3.4770091785891205e-05, 'epoch': 0.6}
{'loss': 0.7413, 'learning_rate': 3.472403507562566e-05, 'epoch': 0.6}
{'loss': 1.1663, 'learning_rate': 3.467799265681105e-05, 'epoch': 0.6}
{'loss': 0.6597, 'learning_rate': 3.463196457252245e-05, 'epoch': 0.6}
{'loss': 0.6858, 'learning_rate': 3.4585950865821473e-05, 'epoch': 0.6}
5.47s/it]                                                        60%|    | 1946/3250 [3:01:29<1:58:48,  5.47s/it] 60%|    | 1947/3250 [3:01:34<1:58:03,  5.44s/it]                                                        60%|    | 1947/3250 [3:01:34<1:58:03,  5.44s/it] 60%|    | 1948/3250 [3:01:40<1:57:20,  5.41s/it]                                                        60%|    | 1948/3250 [3:01:40<1:57:20,  5.41s/it] 60%|    | 1949/3250 [3:01:45<1:56:55,  5.39s/it]                                                        60%|    | 1949/3250 [3:01:45<1:56:55,  5.39s/it] 60%|    | 1950/3250 [3:01:50<1:56:34,  5.38s/it]                                                        60%|    | 1950/3250 [3:01:50<1:56:34,  5.38s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8556395769119263, 'eval_runtime': 1.3752, 'eval_samples_per_second': 8.726, 'eval_steps_per_second': 2.181, 'epoch': 0.6}
                                                        60%|    | 1950/3250 [3:01:52<1:56:34,  5.38s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_4/tmp-checkpoint-1950
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1950I AM HERE AND I AM SAVING THE MODEL

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1950
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1950
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1950
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1950/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1950/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1950/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1950/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.7033, 'learning_rate': 3.4539951579756326e-05, 'epoch': 0.6}
{'loss': 0.7039, 'learning_rate': 3.449396675736171e-05, 'epoch': 0.6}
{'loss': 0.6639, 'learning_rate': 3.444799644165876e-05, 'epoch': 0.6}
{'loss': 0.6644, 'learning_rate': 3.440204067565511e-05, 'epoch': 0.6}
{'loss': 0.7418, 'learning_rate': 3.435609950234473e-05, 'epoch': 0.6}
 60%|    | 1951/3250 [3:01:58<2:09:20,  5.97s/it]                                                        60%|    | 1951/3250 [3:01:58<2:09:20,  5.97s/it] 60%|    | 1952/3250 [3:02:03<2:05:17,  5.79s/it]                                                        60%|    | 1952/3250 [3:02:03<2:05:17,  5.79s/it] 60%|    | 1953/3250 [3:02:09<2:02:24,  5.66s/it]                                                        60%|    | 1953/3250 [3:02:09<2:02:24,  5.66s/it] 60%|    | 1954/3250 [3:02:14<2:00:19,  5.57s/it]                                                        60%|    | 1954/3250 [3:02:14<2:00:19,  5.57s/it] 60%|    | 1955/3250 [3:02:19<1:58:56,  5.51s/it]                                                        60%|    | 1955/3250 [3:02:19<1:58:56,  5.51s/it] 60%|    | 1956/3250 [3:02:25<1:57:47,  {'loss': 0.703, 'learning_rate': 3.431017296470797e-05, 'epoch': 0.6}
{'loss': 0.6923, 'learning_rate': 3.426426110571141e-05, 'epoch': 0.6}
{'loss': 0.6385, 'learning_rate': 3.4218363968308e-05, 'epoch': 0.6}
{'loss': 0.6834, 'learning_rate': 3.417248159543687e-05, 'epoch': 0.6}
{'loss': 0.6861, 'learning_rate': 3.412661403002333e-05, 'epoch': 0.6}
5.46s/it]                                                        60%|    | 1956/3250 [3:02:25<1:57:47,  5.46s/it] 60%|    | 1957/3250 [3:02:30<1:57:02,  5.43s/it]                                                        60%|    | 1957/3250 [3:02:30<1:57:02,  5.43s/it] 60%|    | 1958/3250 [3:02:35<1:56:24,  5.41s/it]                                                        60%|    | 1958/3250 [3:02:35<1:56:24,  5.41s/it] 60%|    | 1959/3250 [3:02:41<1:56:00,  5.39s/it]                                                        60%|    | 1959/3250 [3:02:41<1:56:00,  5.39s/it] 60%|    | 1960/3250 [3:02:46<1:55:42,  5.38s/it]                                                        60%|    | 1960/3250 [3:02:46<1:55:42,  5.38s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8563277721405029, 'eval_runtime': 1.3755, 'eval_samples_per_second': 8.724, 'eval_steps_per_second': 2.181, 'epoch': 0.6}
                                                        60%|    | 1960/3250 [3:02:47<1:55:42,  5.38s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_4/tmp-checkpoint-1960
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in I AM HERE AND I AM SAVING THE MODEL 
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1960
/projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1960
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1960
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1960
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1960/pytorch_model.bin
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1960/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1960/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1960/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.6851, 'learning_rate': 3.408076131497885e-05, 'epoch': 0.6}
{'loss': 0.6659, 'learning_rate': 3.403492349320101e-05, 'epoch': 0.6}
{'loss': 0.6876, 'learning_rate': 3.398910060757344e-05, 'epoch': 0.6}
{'loss': 0.6863, 'learning_rate': 3.3943292700965824e-05, 'epoch': 0.6}
{'loss': 0.6911, 'learning_rate': 3.389749981623379e-05, 'epoch': 0.6}
 60%|    | 1961/3250 [3:02:53<2:06:58,  5.91s/it]                                                        60%|    | 1961/3250 [3:02:53<2:06:58,  5.91s/it] 60%|    | 1962/3250 [3:02:59<2:03:13,  5.74s/it]                                                        60%|    | 1962/3250 [3:02:59<2:03:13,  5.74s/it] 60%|    | 1963/3250 [3:03:04<2:00:38,  5.62s/it]                                                        60%|    | 1963/3250 [3:03:04<2:00:38,  5.62s/it] 60%|    | 1964/3250 [3:03:09<1:58:45,  5.54s/it]                                                        60%|    | 1964/3250 [3:03:09<1:58:45,  5.54s/it] 60%|    | 1965/3250 [3:03:15<1:57:28,  5.48s/it]                                                        60%|    | 1965/3250 [3:03:15<1:57:28,  5.48s/it] 60%|    | 1966/3250 [3:03:20<1:56:29,  {'loss': 0.6926, 'learning_rate': 3.3851721996218947e-05, 'epoch': 0.6}
{'loss': 0.6911, 'learning_rate': 3.380595928374881e-05, 'epoch': 0.61}
{'loss': 0.6964, 'learning_rate': 3.376021172163674e-05, 'epoch': 0.61}
{'loss': 0.683, 'learning_rate': 3.371447935268194e-05, 'epoch': 0.61}
{'loss': 0.6616, 'learning_rate': 3.366876221966939e-05, 'epoch': 0.61}
5.44s/it]                                                        60%|    | 1966/3250 [3:03:20<1:56:29,  5.44s/it] 61%|    | 1967/3250 [3:03:25<1:55:50,  5.42s/it]                                                        61%|    | 1967/3250 [3:03:25<1:55:50,  5.42s/it] 61%|    | 1968/3250 [3:03:31<1:59:37,  5.60s/it]                                                        61%|    | 1968/3250 [3:03:31<1:59:37,  5.60s/it] 61%|    | 1969/3250 [3:03:37<1:57:59,  5.53s/it]                                                        61%|    | 1969/3250 [3:03:37<1:57:59,  5.53s/it] 61%|    | 1970/3250 [3:03:42<1:56:47,  5.47s/it]                                                        61%|    | 1970/3250 [3:03:42<1:56:47,  5.47s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8579500317573547, 'eval_runtime': 1.3675, 'eval_samples_per_second': 8.775, 'eval_steps_per_second': 2.194, 'epoch': 0.61}
                                                        61%|    | 1970/3250 [3:03:43<1:56:47,  5.47s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_4/tmp-checkpoint-1970
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in I AM HERE AND I AM SAVING THE MODEL
 the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1970
/projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1970
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1970
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1970
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1970/pytorch_model.bin
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1970/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1970/pytorch_model.binthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1970/pytorch_model.bin

/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.7202, 'learning_rate': 3.362306036536982e-05, 'epoch': 0.61}
{'loss': 0.6519, 'learning_rate': 3.357737383253966e-05, 'epoch': 0.61}
{'loss': 0.6634, 'learning_rate': 3.353170266392104e-05, 'epoch': 0.61}
{'loss': 0.67, 'learning_rate': 3.3486046902241664e-05, 'epoch': 0.61}
{'loss': 0.7058, 'learning_rate': 3.344040659021482e-05, 'epoch': 0.61}
 61%|    | 1971/3250 [3:03:49<2:06:29,  5.93s/it]                                                        61%|    | 1971/3250 [3:03:49<2:06:29,  5.93s/it] 61%|    | 1972/3250 [3:03:54<2:02:38,  5.76s/it]                                                        61%|    | 1972/3250 [3:03:54<2:02:38,  5.76s/it] 61%|    | 1973/3250 [3:04:00<1:59:55,  5.63s/it]                                                        61%|    | 1973/3250 [3:04:00<1:59:55,  5.63s/it] 61%|    | 1974/3250 [3:04:05<1:58:03,  5.55s/it]                                                        61%|    | 1974/3250 [3:04:05<1:58:03,  5.55s/it] 61%|    | 1975/3250 [3:04:10<1:56:34,  5.49s/it]                                                        61%|    | 1975/3250 [3:04:10<1:56:34,  5.49s/it] 61%|    | 1976/3250 [3:04:16<1:55:32,  {'loss': 0.6679, 'learning_rate': 3.339478177053941e-05, 'epoch': 0.61}
{'loss': 0.6945, 'learning_rate': 3.3349172485899785e-05, 'epoch': 0.61}
{'loss': 1.0726, 'learning_rate': 3.330357877896577e-05, 'epoch': 0.61}
{'loss': 0.7873, 'learning_rate': 3.325800069239263e-05, 'epoch': 0.61}
{'loss': 0.6622, 'learning_rate': 3.321243826882101e-05, 'epoch': 0.61}
5.44s/it]                                                        61%|    | 1976/3250 [3:04:16<1:55:32,  5.44s/it] 61%|    | 1977/3250 [3:04:21<1:54:52,  5.41s/it]                                                        61%|    | 1977/3250 [3:04:21<1:54:52,  5.41s/it] 61%|    | 1978/3250 [3:04:26<1:54:12,  5.39s/it]                                                        61%|    | 1978/3250 [3:04:26<1:54:12,  5.39s/it] 61%|    | 1979/3250 [3:04:32<1:53:45,  5.37s/it]                                                        61%|    | 1979/3250 [3:04:32<1:53:45,  5.37s/it] 61%|    | 1980/3250 [3:04:37<1:53:32,  5.36s/it]                                                        61%|    | 1980/3250 [3:04:37<1:53:32,  5.36s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8561975955963135, 'eval_runtime': 1.3653, 'eval_samples_per_second': 8.789, 'eval_steps_per_second': 2.197, 'epoch': 0.61}
                                                        61%|    | 1980/3250 [3:04:38<1:53:32,  5.36s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_4/tmp-checkpoint-1980
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1980I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1980

I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1980
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1980
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1980/pytorch_model.bin
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1980/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1980/pytorch_model.binthe pytorch model path is 
/projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1980/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.7015, 'learning_rate': 3.31668915508769e-05, 'epoch': 0.61}
{'loss': 0.6945, 'learning_rate': 3.3121360581171594e-05, 'epoch': 0.61}
{'loss': 0.6739, 'learning_rate': 3.3075845402301655e-05, 'epoch': 0.61}
{'loss': 0.6524, 'learning_rate': 3.303034605684888e-05, 'epoch': 0.61}
{'loss': 0.7283, 'learning_rate': 3.298486258738025e-05, 'epoch': 0.61}
 61%|    | 1981/3250 [3:04:44<2:04:19,  5.88s/it]                                                        61%|    | 1981/3250 [3:04:44<2:04:19,  5.88s/it] 61%|    | 1982/3250 [3:04:50<2:00:48,  5.72s/it]                                                        61%|    | 1982/3250 [3:04:50<2:00:48,  5.72s/it] 61%|    | 1983/3250 [3:04:55<1:58:23,  5.61s/it]                                                        61%|    | 1983/3250 [3:04:55<1:58:23,  5.61s/it] 61%|    | 1984/3250 [3:05:00<1:58:10,  5.60s/it]                                                        61%|    | 1984/3250 [3:05:00<1:58:10,  5.60s/it] 61%|    | 1985/3250 [3:05:06<1:56:41,  5.54s/it]                                                        61%|    | 1985/3250 [3:05:06<1:56:41,  5.54s/it] 61%|    | 1986/3250 [3:05:11<1:55:32,  {'loss': 0.7107, 'learning_rate': 3.293939503644788e-05, 'epoch': 0.61}
{'loss': 0.6717, 'learning_rate': 3.2893943446589005e-05, 'epoch': 0.61}
{'loss': 0.6484, 'learning_rate': 3.284850786032593e-05, 'epoch': 0.61}
{'loss': 0.6874, 'learning_rate': 3.2803088320165984e-05, 'epoch': 0.61}
{'loss': 0.6804, 'learning_rate': 3.275768486860149e-05, 'epoch': 0.61}
5.48s/it]                                                        61%|    | 1986/3250 [3:05:11<1:55:32,  5.48s/it] 61%|    | 1987/3250 [3:05:17<1:54:50,  5.46s/it]                                                        61%|    | 1987/3250 [3:05:17<1:54:50,  5.46s/it] 61%|    | 1988/3250 [3:05:22<1:54:13,  5.43s/it]                                                        61%|    | 1988/3250 [3:05:22<1:54:13,  5.43s/it] 61%|    | 1989/3250 [3:05:27<1:53:52,  5.42s/it]                                                        61%|    | 1989/3250 [3:05:27<1:53:52,  5.42s/it] 61%|    | 1990/3250 [3:05:33<1:53:25,  5.40s/it]                                                        61%|    | 1990/3250 [3:05:33<1:53:25,  5.40s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.858198344707489, 'eval_runtime': 1.3755, 'eval_samples_per_second': 8.724, 'eval_steps_per_second': 2.181, 'epoch': 0.61}
                                                        61%|    | 1990/3250 [3:05:34<1:53:25,  5.40s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_4/tmp-checkpoint-1990
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1990
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1990
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1990
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1990
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1990/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1990/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1990/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-1990/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.6838, 'learning_rate': 3.271229754810969e-05, 'epoch': 0.61}
{'loss': 0.6677, 'learning_rate': 3.266692640115277e-05, 'epoch': 0.61}
{'loss': 0.6775, 'learning_rate': 3.262157147017777e-05, 'epoch': 0.61}
{'loss': 0.6754, 'learning_rate': 3.257623279761656e-05, 'epoch': 0.61}
{'loss': 0.6947, 'learning_rate': 3.2530910425885806e-05, 'epoch': 0.61}
 61%|   | 1991/3250 [3:05:40<2:05:41,  5.99s/it]                                                        61%|   | 1991/3250 [3:05:40<2:05:41,  5.99s/it] 61%|   | 1992/3250 [3:05:45<2:01:45,  5.81s/it]                                                        61%|   | 1992/3250 [3:05:45<2:01:45,  5.81s/it] 61%|   | 1993/3250 [3:05:51<1:58:53,  5.67s/it]                                                        61%|   | 1993/3250 [3:05:51<1:58:53,  5.67s/it] 61%|   | 1994/3250 [3:05:56<1:56:55,  5.59s/it]                                                        61%|   | 1994/3250 [3:05:56<1:56:55,  5.59s/it] 61%|   | 1995/3250 [3:06:02<1:55:31,  5.52s/it]                                                        61%|   | 1995/3250 [3:06:02<1:55:31,  5.52s/it] 61%|   | 1996/32{'loss': 0.6906, 'learning_rate': 3.248560439738691e-05, 'epoch': 0.61}
{'loss': 0.6992, 'learning_rate': 3.244031475450599e-05, 'epoch': 0.61}
{'loss': 0.6799, 'learning_rate': 3.2395041539613855e-05, 'epoch': 0.61}
{'loss': 0.6976, 'learning_rate': 3.234978479506591e-05, 'epoch': 0.62}
{'loss': 0.6419, 'learning_rate': 3.2304544563202163e-05, 'epoch': 0.62}
50 [3:06:07<1:54:28,  5.48s/it]                                                        61%|   | 1996/3250 [3:06:07<1:54:28,  5.48s/it] 61%|   | 1997/3250 [3:06:12<1:53:33,  5.44s/it]                                                        61%|   | 1997/3250 [3:06:12<1:53:33,  5.44s/it] 61%|   | 1998/3250 [3:06:18<1:52:59,  5.42s/it]                                                        61%|   | 1998/3250 [3:06:18<1:52:59,  5.42s/it] 62%|   | 1999/3250 [3:06:23<1:52:30,  5.40s/it]                                                        62%|   | 1999/3250 [3:06:23<1:52:30,  5.40s/it] 62%|   | 2000/3250 [3:06:29<1:56:08,  5.57s/it]                                                        62%|   | 2000/3250 [3:06:29<1:56:08,  5.57s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.855830729007721, 'eval_runtime': 1.3722, 'eval_samples_per_second': 8.745, 'eval_steps_per_second': 2.186, 'epoch': 0.62}
                                                        62%|   | 2000/3250 [3:06:30<1:56:08,  5.57s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_4/tmp-checkpoint-2000
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2000
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2000
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2000
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2000
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2000/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2000/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2000/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2000/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.7343, 'learning_rate': 3.22593208863472e-05, 'epoch': 0.62}
{'loss': 0.664, 'learning_rate': 3.221411380681007e-05, 'epoch': 0.62}
{'loss': 0.6649, 'learning_rate': 3.216892336688435e-05, 'epoch': 0.62}
{'loss': 0.6602, 'learning_rate': 3.2123749608848e-05, 'epoch': 0.62}
{'loss': 0.6707, 'learning_rate': 3.207859257496339e-05, 'epoch': 0.62}
 62%|   | 2001/3250 [3:06:36<2:05:32,  6.03s/it]                                                        62%|   | 2001/3250 [3:06:36<2:05:32,  6.03s/it] 62%|   | 2002/3250 [3:06:41<2:01:12,  5.83s/it]                                                        62%|   | 2002/3250 [3:06:41<2:01:12,  5.83s/it] 62%|   | 2003/3250 [3:06:47<1:58:13,  5.69s/it]                                                        62%|   | 2003/3250 [3:06:47<1:58:13,  5.69s/it] 62%|   | 2004/3250 [3:06:52<1:56:01,  5.59s/it]                                                        62%|   | 2004/3250 [3:06:52<1:56:01,  5.59s/it] 62%|   | 2005/3250 [3:06:58<1:54:31,  5.52s/it]                                                        62%|   | 2005/3250 [3:06:58<1:54:31,  5.52s/it] 62%|   | 2006/32{'loss': 0.7078, 'learning_rate': 3.2033452307477276e-05, 'epoch': 0.62}
{'loss': 0.6534, 'learning_rate': 3.198832884862068e-05, 'epoch': 0.62}
{'loss': 0.7239, 'learning_rate': 3.194322224060891e-05, 'epoch': 0.62}
{'loss': 1.1534, 'learning_rate': 3.189813252564152e-05, 'epoch': 0.62}
{'loss': 0.6553, 'learning_rate': 3.1853059745902285e-05, 'epoch': 0.62}
50 [3:07:03<1:53:35,  5.48s/it]                                                        62%|   | 2006/3250 [3:07:03<1:53:35,  5.48s/it] 62%|   | 2007/3250 [3:07:08<1:52:47,  5.44s/it]                                                        62%|   | 2007/3250 [3:07:08<1:52:47,  5.44s/it] 62%|   | 2008/3250 [3:07:14<1:52:19,  5.43s/it]                                                        62%|   | 2008/3250 [3:07:14<1:52:19,  5.43s/it] 62%|   | 2009/3250 [3:07:19<1:51:50,  5.41s/it]                                                        62%|   | 2009/3250 [3:07:19<1:51:50,  5.41s/it] 62%|   | 2010/3250 [3:07:24<1:51:31,  5.40s/it]                                                        62%|   | 2010/3250 [3:07:24<1:51:31,  5.40s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8559732437133789, 'eval_runtime': 1.3727, 'eval_samples_per_second': 8.742, 'eval_steps_per_second': 2.185, 'epoch': 0.62}
                                                        62%|   | 2010/3250 [3:07:26<1:51:31,  5.40s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_4/tmp-checkpoint-2010
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2010I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2010

I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2010
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2010
the pytorch model path is the pytorch model path is/projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2010/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2010/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2010/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2010/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.6942, 'learning_rate': 3.180800394355908e-05, 'epoch': 0.62}
{'loss': 0.6887, 'learning_rate': 3.176296516076394e-05, 'epoch': 0.62}
{'loss': 0.6851, 'learning_rate': 3.1717943439652954e-05, 'epoch': 0.62}
{'loss': 0.6478, 'learning_rate': 3.167293882234626e-05, 'epoch': 0.62}
{'loss': 0.6667, 'learning_rate': 3.162795135094799e-05, 'epoch': 0.62}
 62%|   | 2011/3250 [3:07:32<2:02:30,  5.93s/it]                                                        62%|   | 2011/3250 [3:07:32<2:02:30,  5.93s/it] 62%|   | 2012/3250 [3:07:37<1:58:58,  5.77s/it]                                                        62%|   | 2012/3250 [3:07:37<1:58:58,  5.77s/it] 62%|   | 2013/3250 [3:07:42<1:56:30,  5.65s/it]                                                        62%|   | 2013/3250 [3:07:42<1:56:30,  5.65s/it] 62%|   | 2014/3250 [3:07:48<1:54:40,  5.57s/it]                                                        62%|   | 2014/3250 [3:07:48<1:54:40,  5.57s/it] 62%|   | 2015/3250 [3:07:53<1:53:21,  5.51s/it]                                                        62%|   | 2015/3250 [3:07:53<1:53:21,  5.51s/it] 62%|   | 2016/32{'loss': 0.763, 'learning_rate': 3.158298106754626e-05, 'epoch': 0.62}
{'loss': 0.6603, 'learning_rate': 3.1538028014213055e-05, 'epoch': 0.62}
{'loss': 0.6801, 'learning_rate': 3.149309223300428e-05, 'epoch': 0.62}
{'loss': 0.6739, 'learning_rate': 3.144817376595968e-05, 'epoch': 0.62}
{'loss': 0.6565, 'learning_rate': 3.1403272655102764e-05, 'epoch': 0.62}
50 [3:07:58<1:52:26,  5.47s/it]                                                        62%|   | 2016/3250 [3:07:58<1:52:26,  5.47s/it] 62%|   | 2017/3250 [3:08:04<1:54:06,  5.55s/it]                                                        62%|   | 2017/3250 [3:08:04<1:54:06,  5.55s/it] 62%|   | 2018/3250 [3:08:10<1:52:57,  5.50s/it]                                                        62%|   | 2018/3250 [3:08:10<1:52:57,  5.50s/it] 62%|   | 2019/3250 [3:08:15<1:52:04,  5.46s/it]                                                        62%|   | 2019/3250 [3:08:15<1:52:04,  5.46s/it] 62%|   | 2020/3250 [3:08:20<1:51:25,  5.44s/it]                                                        62%|   | 2020/3250 [3:08:20<1:51:25,  5.44s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.857349157333374, 'eval_runtime': 1.3775, 'eval_samples_per_second': 8.711, 'eval_steps_per_second': 2.178, 'epoch': 0.62}
                                                        62%|   | 2020/3250 [3:08:22<1:51:25,  5.44s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_4/tmp-checkpoint-2020
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2020
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2020
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2020
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2020
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2020/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2020/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2020/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2020/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.681, 'learning_rate': 3.135838894244086e-05, 'epoch': 0.62}
{'loss': 0.6675, 'learning_rate': 3.1313522669964976e-05, 'epoch': 0.62}
{'loss': 0.6536, 'learning_rate': 3.1268673879649815e-05, 'epoch': 0.62}
{'loss': 0.678, 'learning_rate': 3.1223842613453745e-05, 'epoch': 0.62}
{'loss': 0.6865, 'learning_rate': 3.11790289133187e-05, 'epoch': 0.62}
 62%|   | 2021/3250 [3:08:27<2:01:18,  5.92s/it]                                                        62%|   | 2021/3250 [3:08:27<2:01:18,  5.92s/it] 62%|   | 2022/3250 [3:08:33<1:57:50,  5.76s/it]                                                        62%|   | 2022/3250 [3:08:33<1:57:50,  5.76s/it] 62%|   | 2023/3250 [3:08:38<1:55:20,  5.64s/it]                                                        62%|   | 2023/3250 [3:08:38<1:55:20,  5.64s/it] 62%|   | 2024/3250 [3:08:43<1:53:32,  5.56s/it]                                                        62%|   | 2024/3250 [3:08:43<1:53:32,  5.56s/it] 62%|   | 2025/3250 [3:08:49<1:52:13,  5.50s/it]                                                        62%|   | 2025/3250 [3:08:49<1:52:13,  5.50s/it] 62%|   | 2026/32{'loss': 0.6876, 'learning_rate': 3.11342328211702e-05, 'epoch': 0.62}
{'loss': 0.6947, 'learning_rate': 3.1089454378917304e-05, 'epoch': 0.62}
{'loss': 0.696, 'learning_rate': 3.1044693628452557e-05, 'epoch': 0.62}
{'loss': 0.6835, 'learning_rate': 3.0999950611651915e-05, 'epoch': 0.62}
{'loss': 0.6618, 'learning_rate': 3.0955225370374805e-05, 'epoch': 0.62}
50 [3:08:54<1:51:21,  5.46s/it]                                                        62%|   | 2026/3250 [3:08:54<1:51:21,  5.46s/it] 62%|   | 2027/3250 [3:09:00<1:50:42,  5.43s/it]                                                        62%|   | 2027/3250 [3:09:00<1:50:42,  5.43s/it] 62%|   | 2028/3250 [3:09:05<1:50:12,  5.41s/it]                                                        62%|   | 2028/3250 [3:09:05<1:50:12,  5.41s/it] 62%|   | 2029/3250 [3:09:10<1:49:49,  5.40s/it]                                                        62%|   | 2029/3250 [3:09:10<1:49:49,  5.40s/it] 62%|   | 2030/3250 [3:09:16<1:49:27,  5.38s/it]                                                        62%|   | 2030/3250 [3:09:16<1:49:27,  5.38s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.85684734582901, 'eval_runtime': 1.3702, 'eval_samples_per_second': 8.758, 'eval_steps_per_second': 2.189, 'epoch': 0.62}
                                                        62%|   | 2030/3250 [3:09:17<1:49:27,  5.38s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_4/tmp-checkpoint-2030
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2030
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2030
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2030
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2030
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2030/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2030/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2030/pytorch_model.binthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2030/pytorch_model.bin

/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.7073, 'learning_rate': 3.091051794646398e-05, 'epoch': 0.62}
{'loss': 0.6963, 'learning_rate': 3.086582838174551e-05, 'epoch': 0.63}
{'loss': 0.6663, 'learning_rate': 3.082115671802882e-05, 'epoch': 0.63}
{'loss': 0.6599, 'learning_rate': 3.077650299710653e-05, 'epoch': 0.63}
{'loss': 0.6429, 'learning_rate': 3.073186726075449e-05, 'epoch': 0.63}
 62%|   | 2031/3250 [3:09:23<2:00:04,  5.91s/it]                                                        62%|   | 2031/3250 [3:09:23<2:00:04,  5.91s/it] 63%|   | 2032/3250 [3:09:28<1:56:38,  5.75s/it]                                                        63%|   | 2032/3250 [3:09:28<1:56:38,  5.75s/it] 63%|   | 2033/3250 [3:09:34<1:57:19,  5.78s/it]                                                        63%|   | 2033/3250 [3:09:34<1:57:19,  5.78s/it] 63%|   | 2034/3250 [3:09:39<1:54:36,  5.65s/it]                                                        63%|   | 2034/3250 [3:09:39<1:54:36,  5.65s/it] 63%|   | 2035/3250 [3:09:45<1:52:43,  5.57s/it]                                                        63%|   | 2035/3250 [3:09:45<1:52:43,  5.57s/it] 63%|   | 2036/32{'loss': 0.7075, 'learning_rate': 3.068724955073172e-05, 'epoch': 0.63}
{'loss': 0.6479, 'learning_rate': 3.0642649908780413e-05, 'epoch': 0.63}
{'loss': 0.7357, 'learning_rate': 3.05980683766258e-05, 'epoch': 0.63}
{'loss': 1.1512, 'learning_rate': 3.05535049959762e-05, 'epoch': 0.63}
{'loss': 0.6595, 'learning_rate': 3.0508959808522974e-05, 'epoch': 0.63}
50 [3:09:50<1:51:20,  5.50s/it]                                                        63%|   | 2036/3250 [3:09:50<1:51:20,  5.50s/it] 63%|   | 2037/3250 [3:09:55<1:50:24,  5.46s/it]                                                        63%|   | 2037/3250 [3:09:55<1:50:24,  5.46s/it] 63%|   | 2038/3250 [3:10:01<1:49:43,  5.43s/it]                                                        63%|   | 2038/3250 [3:10:01<1:49:43,  5.43s/it] 63%|   | 2039/3250 [3:10:06<1:49:09,  5.41s/it]                                                        63%|   | 2039/3250 [3:10:06<1:49:09,  5.41s/it] 63%|   | 2040/3250 [3:10:12<1:48:43,  5.39s/it]                                                        63%|   | 2040/3250 [3:10:12<1:48:43,  5.39s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.857048511505127, 'eval_runtime': 1.3717, 'eval_samples_per_second': 8.748, 'eval_steps_per_second': 2.187, 'epoch': 0.63}
                                                        63%|   | 2040/3250 [3:10:13<1:48:43,  5.39s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_4/tmp-checkpoint-2040
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2040
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2040
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2040
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2040
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2040/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2040/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2040/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2040/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.683, 'learning_rate': 3.0464432855940417e-05, 'epoch': 0.63}
{'loss': 0.7002, 'learning_rate': 3.0419924179885768e-05, 'epoch': 0.63}
{'loss': 0.6941, 'learning_rate': 3.03754338219992e-05, 'epoch': 0.63}
{'loss': 0.6594, 'learning_rate': 3.0330961823903735e-05, 'epoch': 0.63}
{'loss': 0.6582, 'learning_rate': 3.02865082272052e-05, 'epoch': 0.63}
 63%|   | 2041/3250 [3:10:19<1:58:26,  5.88s/it]                                                        63%|   | 2041/3250 [3:10:19<1:58:26,  5.88s/it] 63%|   | 2042/3250 [3:10:24<1:55:11,  5.72s/it]                                                        63%|   | 2042/3250 [3:10:24<1:55:11,  5.72s/it] 63%|   | 2043/3250 [3:10:29<1:52:59,  5.62s/it]                                                        63%|   | 2043/3250 [3:10:29<1:52:59,  5.62s/it] 63%|   | 2044/3250 [3:10:35<1:51:20,  5.54s/it]                                                        63%|   | 2044/3250 [3:10:35<1:51:20,  5.54s/it] 63%|   | 2045/3250 [3:10:40<1:50:07,  5.48s/it]                                                        63%|   | 2045/3250 [3:10:40<1:50:07,  5.48s/it] 63%|   | 2046/32{'loss': 0.7336, 'learning_rate': 3.024207307349224e-05, 'epoch': 0.63}
{'loss': 0.6963, 'learning_rate': 3.0197656404336206e-05, 'epoch': 0.63}
{'loss': 0.6821, 'learning_rate': 3.0153258261291195e-05, 'epoch': 0.63}
{'loss': 0.6307, 'learning_rate': 3.0108878685893947e-05, 'epoch': 0.63}
{'loss': 0.68, 'learning_rate': 3.006451771966383e-05, 'epoch': 0.63}
50 [3:10:45<1:49:19,  5.45s/it]                                                        63%|   | 2046/3250 [3:10:45<1:49:19,  5.45s/it] 63%|   | 2047/3250 [3:10:51<1:48:45,  5.42s/it]                                                        63%|   | 2047/3250 [3:10:51<1:48:45,  5.42s/it] 63%|   | 2048/3250 [3:10:56<1:48:20,  5.41s/it]                                                        63%|   | 2048/3250 [3:10:56<1:48:20,  5.41s/it] 63%|   | 2049/3250 [3:11:01<1:47:59,  5.40s/it]                                                        63%|   | 2049/3250 [3:11:01<1:47:59,  5.40s/it] 63%|   | 2050/3250 [3:11:07<1:49:58,  5.50s/it]                                                        63%|   | 2050/3250 [3:11:07<1:49:58,  5.50s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8564854264259338, 'eval_runtime': 1.377, 'eval_samples_per_second': 8.715, 'eval_steps_per_second': 2.179, 'epoch': 0.63}
                                                        63%|   | 2050/3250 [3:11:09<1:49:58,  5.50s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_4/tmp-checkpoint-2050
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2050
/projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2050
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2050
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2050
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2050/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2050/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2050/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2050/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.6773, 'learning_rate': 3.002017540410282e-05, 'epoch': 0.63}
{'loss': 0.6594, 'learning_rate': 2.9975851780695442e-05, 'epoch': 0.63}
{'loss': 0.6568, 'learning_rate': 2.9931546890908697e-05, 'epoch': 0.63}
{'loss': 0.6715, 'learning_rate': 2.988726077619211e-05, 'epoch': 0.63}
{'loss': 0.681, 'learning_rate': 2.9842993477977622e-05, 'epoch': 0.63}
 63%|   | 2051/3250 [3:11:14<1:59:25,  5.98s/it]                                                        63%|   | 2051/3250 [3:11:14<1:59:25,  5.98s/it] 63%|   | 2052/3250 [3:11:20<1:55:41,  5.79s/it]                                                        63%|   | 2052/3250 [3:11:20<1:55:41,  5.79s/it] 63%|   | 2053/3250 [3:11:25<1:52:58,  5.66s/it]                                                        63%|   | 2053/3250 [3:11:25<1:52:58,  5.66s/it] 63%|   | 2054/3250 [3:11:30<1:51:04,  5.57s/it]                                                        63%|   | 2054/3250 [3:11:30<1:51:04,  5.57s/it] 63%|   | 2055/3250 [3:11:36<1:49:35,  5.50s/it]                                                        63%|   | 2055/3250 [3:11:36<1:49:35,  5.50s/it] 63%|   | 2056/32{'loss': 0.6922, 'learning_rate': 2.9798745037679556e-05, 'epoch': 0.63}
{'loss': 0.6827, 'learning_rate': 2.9754515496694603e-05, 'epoch': 0.63}
{'loss': 0.6798, 'learning_rate': 2.9710304896401802e-05, 'epoch': 0.63}
{'loss': 0.6988, 'learning_rate': 2.966611327816241e-05, 'epoch': 0.63}
{'loss': 0.6777, 'learning_rate': 2.962194068331996e-05, 'epoch': 0.63}
50 [3:11:41<1:48:32,  5.45s/it]                                                        63%|   | 2056/3250 [3:11:41<1:48:32,  5.45s/it] 63%|   | 2057/3250 [3:11:46<1:47:43,  5.42s/it]                                                        63%|   | 2057/3250 [3:11:46<1:47:43,  5.42s/it] 63%|   | 2058/3250 [3:11:52<1:47:17,  5.40s/it]                                                        63%|   | 2058/3250 [3:11:52<1:47:17,  5.40s/it] 63%|   | 2059/3250 [3:11:57<1:46:55,  5.39s/it]                                                        63%|   | 2059/3250 [3:11:57<1:46:55,  5.39s/it] 63%|   | 2060/3250 [3:12:02<1:46:44,  5.38s/it]                                                        63%|   | 2060/3250 [3:12:02<1:46:44,  5.38s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8560724258422852, 'eval_runtime': 1.3701, 'eval_samples_per_second': 8.758, 'eval_steps_per_second': 2.19, 'epoch': 0.63}
                                                        63%|   | 2060/3250 [3:12:04<1:46:44,  5.38s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_4/tmp-checkpoint-2060
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  I AM HERE AND I AM SAVING THE MODEL/projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2060
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2060

I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2060
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2060
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2060/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2060/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2060/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2060/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.6587, 'learning_rate': 2.9577787153200197e-05, 'epoch': 0.63}
{'loss': 0.7151, 'learning_rate': 2.9533652729111026e-05, 'epoch': 0.63}
{'loss': 0.6619, 'learning_rate': 2.948953745234246e-05, 'epoch': 0.63}
{'loss': 0.6608, 'learning_rate': 2.9445441364166616e-05, 'epoch': 0.64}
{'loss': 0.6622, 'learning_rate': 2.940136450583765e-05, 'epoch': 0.64}
 63%|   | 2061/3250 [3:12:10<1:56:53,  5.90s/it]                                                        63%|   | 2061/3250 [3:12:10<1:56:53,  5.90s/it] 63%|   | 2062/3250 [3:12:15<1:53:30,  5.73s/it]                                                        63%|   | 2062/3250 [3:12:15<1:53:30,  5.73s/it] 63%|   | 2063/3250 [3:12:20<1:51:10,  5.62s/it]                                                        63%|   | 2063/3250 [3:12:20<1:51:10,  5.62s/it] 64%|   | 2064/3250 [3:12:26<1:49:30,  5.54s/it]                                                        64%|   | 2064/3250 [3:12:26<1:49:30,  5.54s/it] 64%|   | 2065/3250 [3:12:31<1:48:20,  5.49s/it]                                                        64%|   | 2065/3250 [3:12:31<1:48:20,  5.49s/it] 64%|   | 2066/32{'loss': 0.686, 'learning_rate': 2.935730691859172e-05, 'epoch': 0.64}
{'loss': 0.6765, 'learning_rate': 2.9313268643646986e-05, 'epoch': 0.64}
{'loss': 0.6923, 'learning_rate': 2.92692497222035e-05, 'epoch': 0.64}
{'loss': 1.1051, 'learning_rate': 2.9225250195443236e-05, 'epoch': 0.64}
{'loss': 0.7409, 'learning_rate': 2.9181270104530018e-05, 'epoch': 0.64}
50 [3:12:37<1:49:02,  5.53s/it]                                                        64%|   | 2066/3250 [3:12:37<1:49:02,  5.53s/it] 64%|   | 2067/3250 [3:12:42<1:47:55,  5.47s/it]                                                        64%|   | 2067/3250 [3:12:42<1:47:55,  5.47s/it] 64%|   | 2068/3250 [3:12:47<1:47:11,  5.44s/it]                                                        64%|   | 2068/3250 [3:12:47<1:47:11,  5.44s/it] 64%|   | 2069/3250 [3:12:53<1:46:34,  5.41s/it]                                                        64%|   | 2069/3250 [3:12:53<1:46:34,  5.41s/it] 64%|   | 2070/3250 [3:12:58<1:46:51,  5.43s/it]                                                        64%|   | 2070/3250 [3:12:58<1:46:51,  5.43s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8553224802017212, 'eval_runtime': 1.6401, 'eval_samples_per_second': 7.317, 'eval_steps_per_second': 1.829, 'epoch': 0.64}
                                                        64%|   | 2070/3250 [3:13:00<1:46:51,  5.43s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_4/tmp-checkpoint-2070
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2070
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2070
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2070
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2070
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2070/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2070/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2070/pytorch_model.binthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2070/pytorch_model.bin

/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.6567, 'learning_rate': 2.91373094906095e-05, 'epoch': 0.64}
{'loss': 0.6992, 'learning_rate': 2.909336839480905e-05, 'epoch': 0.64}
{'loss': 0.6887, 'learning_rate': 2.9049446858237854e-05, 'epoch': 0.64}
{'loss': 0.6762, 'learning_rate': 2.900554492198677e-05, 'epoch': 0.64}
{'loss': 0.6444, 'learning_rate': 2.8961662627128327e-05, 'epoch': 0.64}
 64%|   | 2071/3250 [3:13:06<1:59:22,  6.08s/it]                                                        64%|   | 2071/3250 [3:13:06<1:59:22,  6.08s/it] 64%|   | 2072/3250 [3:13:11<1:55:02,  5.86s/it]                                                        64%|   | 2072/3250 [3:13:11<1:55:02,  5.86s/it] 64%|   | 2073/3250 [3:13:16<1:52:03,  5.71s/it]                                                        64%|   | 2073/3250 [3:13:16<1:52:03,  5.71s/it] 64%|   | 2074/3250 [3:13:22<1:49:54,  5.61s/it]                                                        64%|   | 2074/3250 [3:13:22<1:49:54,  5.61s/it] 64%|   | 2075/3250 [3:13:27<1:48:23,  5.53s/it]                                                        64%|   | 2075/3250 [3:13:27<1:48:23,  5.53s/it] 64%|   | 2076/32{'loss': 0.7243, 'learning_rate': 2.8917800014716635e-05, 'epoch': 0.64}
{'loss': 0.7003, 'learning_rate': 2.8873957125787443e-05, 'epoch': 0.64}
{'loss': 0.672, 'learning_rate': 2.8830134001358055e-05, 'epoch': 0.64}
{'loss': 0.6422, 'learning_rate': 2.878633068242721e-05, 'epoch': 0.64}
{'loss': 0.6825, 'learning_rate': 2.8742547209975192e-05, 'epoch': 0.64}
50 [3:13:33<1:47:22,  5.49s/it]                                                        64%|   | 2076/3250 [3:13:33<1:47:22,  5.49s/it] 64%|   | 2077/3250 [3:13:38<1:46:35,  5.45s/it]                                                        64%|   | 2077/3250 [3:13:38<1:46:35,  5.45s/it] 64%|   | 2078/3250 [3:13:43<1:46:01,  5.43s/it]                                                        64%|   | 2078/3250 [3:13:43<1:46:01,  5.43s/it] 64%|   | 2079/3250 [3:13:49<1:45:35,  5.41s/it]                                                        64%|   | 2079/3250 [3:13:49<1:45:35,  5.41s/it] 64%|   | 2080/3250 [3:13:54<1:45:14,  5.40s/it]                                                        64%|   | 2080/3250 [3:13:54<1:45:14,  5.40s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8557798266410828, 'eval_runtime': 1.7124, 'eval_samples_per_second': 7.008, 'eval_steps_per_second': 1.752, 'epoch': 0.64}
                                                        64%|   | 2080/3250 [3:13:56<1:45:14,  5.40s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_4/tmp-checkpoint-2080
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2080
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2080
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2080
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2080
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2080/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2080/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2080/pytorch_model.binthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2080/pytorch_model.bin

/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.6676, 'learning_rate': 2.869878362496368e-05, 'epoch': 0.64}
{'loss': 0.6798, 'learning_rate': 2.8655039968335773e-05, 'epoch': 0.64}
{'loss': 0.6526, 'learning_rate': 2.8611316281015908e-05, 'epoch': 0.64}
{'loss': 0.6631, 'learning_rate': 2.8567612603909853e-05, 'epoch': 0.64}
{'loss': 0.6711, 'learning_rate': 2.8523928977904623e-05, 'epoch': 0.64}
 64%|   | 2081/3250 [3:14:01<1:56:47,  5.99s/it]                                                        64%|   | 2081/3250 [3:14:01<1:56:47,  5.99s/it] 64%|   | 2082/3250 [3:14:07<1:55:15,  5.92s/it]                                                        64%|   | 2082/3250 [3:14:07<1:55:15,  5.92s/it] 64%|   | 2083/3250 [3:14:13<1:51:52,  5.75s/it]                                                        64%|   | 2083/3250 [3:14:13<1:51:52,  5.75s/it] 64%|   | 2084/3250 [3:14:18<1:49:40,  5.64s/it]                                                        64%|   | 2084/3250 [3:14:18<1:49:40,  5.64s/it] 64%|   | 2085/3250 [3:14:23<1:47:58,  5.56s/it]                                                        64%|   | 2085/3250 [3:14:23<1:47:58,  5.56s/it] 64%|   | 2086/32{'loss': 0.6933, 'learning_rate': 2.848026544386851e-05, 'epoch': 0.64}
{'loss': 0.6857, 'learning_rate': 2.843662204265099e-05, 'epoch': 0.64}
{'loss': 0.6938, 'learning_rate': 2.8392998815082717e-05, 'epoch': 0.64}
{'loss': 0.6747, 'learning_rate': 2.8349395801975453e-05, 'epoch': 0.64}
{'loss': 0.6866, 'learning_rate': 2.8305813044122097e-05, 'epoch': 0.64}
50 [3:14:29<1:46:52,  5.51s/it]                                                        64%|   | 2086/3250 [3:14:29<1:46:52,  5.51s/it] 64%|   | 2087/3250 [3:14:34<1:46:00,  5.47s/it]                                                        64%|   | 2087/3250 [3:14:34<1:46:00,  5.47s/it] 64%|   | 2088/3250 [3:14:39<1:45:18,  5.44s/it]                                                        64%|   | 2088/3250 [3:14:39<1:45:18,  5.44s/it] 64%|   | 2089/3250 [3:14:45<1:44:56,  5.42s/it]                                                        64%|   | 2089/3250 [3:14:45<1:44:56,  5.42s/it] 64%|   | 2090/3250 [3:14:50<1:44:30,  5.41s/it]                                                        64%|   | 2090/3250 [3:14:50<1:44:30,  5.41s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.854439914226532, 'eval_runtime': 1.3804, 'eval_samples_per_second': 8.693, 'eval_steps_per_second': 2.173, 'epoch': 0.64}
                                                        64%|   | 2090/3250 [3:14:52<1:44:30,  5.41s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_4/tmp-checkpoint-2090
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2090
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2090
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2090
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2090
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2090/pytorch_model.bin
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2090/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2090/pytorch_model.binthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2090/pytorch_model.bin

/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.6367, 'learning_rate': 2.826225058229651e-05, 'epoch': 0.64}
{'loss': 0.7248, 'learning_rate': 2.821870845725366e-05, 'epoch': 0.64}
{'loss': 0.6477, 'learning_rate': 2.8175186709729397e-05, 'epoch': 0.64}
{'loss': 0.6629, 'learning_rate': 2.8131685380440585e-05, 'epoch': 0.64}
{'loss': 0.6532, 'learning_rate': 2.808820451008495e-05, 'epoch': 0.64}
 64%|   | 2091/3250 [3:14:57<1:54:36,  5.93s/it]                                                        64%|   | 2091/3250 [3:14:57<1:54:36,  5.93s/it] 64%|   | 2092/3250 [3:15:03<1:51:10,  5.76s/it]                                                        64%|   | 2092/3250 [3:15:03<1:51:10,  5.76s/it] 64%|   | 2093/3250 [3:15:08<1:48:47,  5.64s/it]                                                        64%|   | 2093/3250 [3:15:08<1:48:47,  5.64s/it] 64%|   | 2094/3250 [3:15:13<1:47:21,  5.57s/it]                                                        64%|   | 2094/3250 [3:15:13<1:47:21,  5.57s/it] 64%|   | 2095/3250 [3:15:19<1:46:05,  5.51s/it]                                                        64%|   | 2095/3250 [3:15:19<1:46:05,  5.51s/it] 64%|   | 2096/32{'loss': 0.6596, 'learning_rate': 2.8044744139341094e-05, 'epoch': 0.64}
{'loss': 0.6954, 'learning_rate': 2.800130430886841e-05, 'epoch': 0.65}
{'loss': 0.6508, 'learning_rate': 2.79578850593071e-05, 'epoch': 0.65}
{'loss': 0.7168, 'learning_rate': 2.7914486431278098e-05, 'epoch': 0.65}
{'loss': 1.149, 'learning_rate': 2.7871108465383066e-05, 'epoch': 0.65}
50 [3:15:24<1:45:05,  5.46s/it]                                                        64%|   | 2096/3250 [3:15:24<1:45:05,  5.46s/it] 65%|   | 2097/3250 [3:15:30<1:44:59,  5.46s/it]                                                        65%|   | 2097/3250 [3:15:30<1:44:59,  5.46s/it] 65%|   | 2098/3250 [3:15:35<1:44:19,  5.43s/it]                                                        65%|   | 2098/3250 [3:15:35<1:44:19,  5.43s/it] 65%|   | 2099/3250 [3:15:41<1:46:24,  5.55s/it]                                                        65%|   | 2099/3250 [3:15:41<1:46:24,  5.55s/it] 65%|   | 2100/3250 [3:15:46<1:45:12,  5.49s/it]                                                        65%|   | 2100/3250 [3:15:46<1:45:12,  5.49s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8549849987030029, 'eval_runtime': 1.5602, 'eval_samples_per_second': 7.691, 'eval_steps_per_second': 1.923, 'epoch': 0.65}
                                                        65%|   | 2100/3250 [3:15:48<1:45:12,  5.49s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_4/tmp-checkpoint-2100
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2100
/projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2100
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2100
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2100
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2100/pytorch_model.bin
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2100/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2100/pytorch_model.binthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2100/pytorch_model.bin

/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.6534, 'learning_rate': 2.7827751202204323e-05, 'epoch': 0.65}
{'loss': 0.6897, 'learning_rate': 2.7784414682304832e-05, 'epoch': 0.65}
{'loss': 0.6776, 'learning_rate': 2.77410989462281e-05, 'epoch': 0.65}
{'loss': 0.6847, 'learning_rate': 2.769780403449824e-05, 'epoch': 0.65}
{'loss': 0.6376, 'learning_rate': 2.765452998761988e-05, 'epoch': 0.65}
 65%|   | 2101/3250 [3:15:53<1:55:16,  6.02s/it]                                                        65%|   | 2101/3250 [3:15:53<1:55:16,  6.02s/it] 65%|   | 2102/3250 [3:15:59<1:53:01,  5.91s/it]                                                        65%|   | 2102/3250 [3:15:59<1:53:01,  5.91s/it] 65%|   | 2103/3250 [3:16:04<1:49:56,  5.75s/it]                                                        65%|   | 2103/3250 [3:16:04<1:49:56,  5.75s/it] 65%|   | 2104/3250 [3:16:10<1:47:41,  5.64s/it]                                                        65%|   | 2104/3250 [3:16:10<1:47:41,  5.64s/it] 65%|   | 2105/3250 [3:16:15<1:46:05,  5.56s/it]                                                        65%|   | 2105/3250 [3:16:15<1:46:05,  5.56s/it] 65%|   | 2106/32{'loss': 0.6623, 'learning_rate': 2.761127684607811e-05, 'epoch': 0.65}
{'loss': 0.7516, 'learning_rate': 2.7568044650338464e-05, 'epoch': 0.65}
{'loss': 0.6711, 'learning_rate': 2.752483344084692e-05, 'epoch': 0.65}
{'loss': 0.674, 'learning_rate': 2.7481643258029748e-05, 'epoch': 0.65}
{'loss': 0.6671, 'learning_rate': 2.743847414229358e-05, 'epoch': 0.65}
50 [3:16:21<1:45:31,  5.53s/it]                                                        65%|   | 2106/3250 [3:16:21<1:45:31,  5.53s/it] 65%|   | 2107/3250 [3:16:26<1:44:36,  5.49s/it]                                                        65%|   | 2107/3250 [3:16:26<1:44:36,  5.49s/it] 65%|   | 2108/3250 [3:16:31<1:43:58,  5.46s/it]                                                        65%|   | 2108/3250 [3:16:31<1:43:58,  5.46s/it] 65%|   | 2109/3250 [3:16:37<1:43:28,  5.44s/it]                                                        65%|   | 2109/3250 [3:16:37<1:43:28,  5.44s/it] 65%|   | 2110/3250 [3:16:42<1:43:03,  5.42s/it]                                                        65%|   | 2110/3250 [3:16:42<1:43:03,  5.42s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8541856408119202, 'eval_runtime': 2.7562, 'eval_samples_per_second': 4.354, 'eval_steps_per_second': 1.088, 'epoch': 0.65}
                                                        65%|   | 2110/3250 [3:16:45<1:43:03,  5.42s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_4/tmp-checkpoint-2110
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2110
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2110
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2110
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2110
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2110/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2110/pytorch_model.binthe pytorch model path is
 /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2110/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2110/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.6645, 'learning_rate': 2.7395326134025355e-05, 'epoch': 0.65}
{'loss': 0.6782, 'learning_rate': 2.7352199273592265e-05, 'epoch': 0.65}
{'loss': 0.6676, 'learning_rate': 2.7309093601341696e-05, 'epoch': 0.65}
{'loss': 0.6564, 'learning_rate': 2.7266009157601224e-05, 'epoch': 0.65}
{'loss': 0.6606, 'learning_rate': 2.722294598267859e-05, 'epoch': 0.65}
 65%|   | 2111/3250 [3:16:51<2:00:41,  6.36s/it]                                                        65%|   | 2111/3250 [3:16:51<2:00:41,  6.36s/it] 65%|   | 2112/3250 [3:16:56<1:54:56,  6.06s/it]                                                        65%|   | 2112/3250 [3:16:56<1:54:56,  6.06s/it] 65%|   | 2113/3250 [3:17:02<1:51:04,  5.86s/it]                                                        65%|   | 2113/3250 [3:17:02<1:51:04,  5.86s/it] 65%|   | 2114/3250 [3:17:07<1:48:14,  5.72s/it]                                                        65%|   | 2114/3250 [3:17:07<1:48:14,  5.72s/it] 65%|   | 2115/3250 [3:17:13<1:48:38,  5.74s/it]                                                        65%|   | 2115/3250 [3:17:13<1:48:38,  5.74s/it] 65%|   | 2116/32{'loss': 0.6931, 'learning_rate': 2.7179904116861556e-05, 'epoch': 0.65}
{'loss': 0.6919, 'learning_rate': 2.713688360041803e-05, 'epoch': 0.65}
{'loss': 0.6779, 'learning_rate': 2.7093884473595922e-05, 'epoch': 0.65}
{'loss': 0.6909, 'learning_rate': 2.705090677662311e-05, 'epoch': 0.65}
{'loss': 0.6747, 'learning_rate': 2.700795054970748e-05, 'epoch': 0.65}
50 [3:17:18<1:46:39,  5.64s/it]                                                        65%|   | 2116/3250 [3:17:18<1:46:39,  5.64s/it] 65%|   | 2117/3250 [3:17:24<1:45:35,  5.59s/it]                                                        65%|   | 2117/3250 [3:17:24<1:45:35,  5.59s/it] 65%|   | 2118/3250 [3:17:29<1:44:34,  5.54s/it]                                                        65%|   | 2118/3250 [3:17:29<1:44:34,  5.54s/it] 65%|   | 2119/3250 [3:17:34<1:43:43,  5.50s/it]                                                        65%|   | 2119/3250 [3:17:34<1:43:43,  5.50s/it] 65%|   | 2120/3250 [3:17:40<1:42:59,  5.47s/it]                                                        65%|   | 2120/3250 [3:17:40<1:42:59,  5.47s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8552232980728149, 'eval_runtime': 1.3708, 'eval_samples_per_second': 8.754, 'eval_steps_per_second': 2.188, 'epoch': 0.65}
                                                        65%|   | 2120/3250 [3:17:41<1:42:59,  5.47s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_4/tmp-checkpoint-2120
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2120
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2120
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2120
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2120
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2120/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2120/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2120/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2120/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.6579, 'learning_rate': 2.696501583303674e-05, 'epoch': 0.65}
{'loss': 0.6938, 'learning_rate': 2.692210266677855e-05, 'epoch': 0.65}
{'loss': 0.6823, 'learning_rate': 2.687921109108038e-05, 'epoch': 0.65}
{'loss': 0.6548, 'learning_rate': 2.6836341146069515e-05, 'epoch': 0.65}
{'loss': 0.6552, 'learning_rate': 2.6793492871852986e-05, 'epoch': 0.65}
 65%|   | 2121/3250 [3:17:47<1:52:11,  5.96s/it]                                                        65%|   | 2121/3250 [3:17:47<1:52:11,  5.96s/it] 65%|   | 2122/3250 [3:17:52<1:48:54,  5.79s/it]                                                        65%|   | 2122/3250 [3:17:52<1:48:54,  5.79s/it] 65%|   | 2123/3250 [3:17:58<1:46:30,  5.67s/it]                                                        65%|   | 2123/3250 [3:17:58<1:46:30,  5.67s/it] 65%|   | 2124/3250 [3:18:03<1:44:55,  5.59s/it]                                                        65%|   | 2124/3250 [3:18:03<1:44:55,  5.59s/it] 65%|   | 2125/3250 [3:18:09<1:43:41,  5.53s/it]                                                        65%|   | 2125/3250 [3:18:09<1:43:41,  5.53s/it] 65%|   | 2126/32{'loss': 0.6331, 'learning_rate': 2.6750666308517573e-05, 'epoch': 0.65}
{'loss': 0.7046, 'learning_rate': 2.670786149612972e-05, 'epoch': 0.65}
{'loss': 0.658, 'learning_rate': 2.6665078474735505e-05, 'epoch': 0.65}
{'loss': 0.72, 'learning_rate': 2.6622317284360664e-05, 'epoch': 0.66}
{'loss': 1.1559, 'learning_rate': 2.65795779650105e-05, 'epoch': 0.66}
50 [3:18:14<1:42:48,  5.49s/it]                                                        65%|   | 2126/3250 [3:18:14<1:42:48,  5.49s/it] 65%|   | 2127/3250 [3:18:19<1:42:16,  5.46s/it]                                                        65%|   | 2127/3250 [3:18:19<1:42:16,  5.46s/it] 65%|   | 2128/3250 [3:18:25<1:41:44,  5.44s/it]                                                        65%|   | 2128/3250 [3:18:25<1:41:44,  5.44s/it] 66%|   | 2129/3250 [3:18:30<1:41:29,  5.43s/it]                                                        66%|   | 2129/3250 [3:18:30<1:41:29,  5.43s/it] 66%|   | 2130/3250 [3:18:36<1:41:03,  5.41s/it]                                                        66%|   | 2130/3250 [3:18:36<1:41:03,  5.41s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8559881448745728, 'eval_runtime': 1.3645, 'eval_samples_per_second': 8.794, 'eval_steps_per_second': 2.199, 'epoch': 0.66}
                                                        66%|   | 2130/3250 [3:18:37<1:41:03,  5.41s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_4/tmp-checkpoint-2130
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  I AM HERE AND I AM SAVING THE MODEL/projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2130
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2130

I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2130
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2130
the pytorch model path is the pytorch model path is/projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2130/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2130/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2130/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2130/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.6608, 'learning_rate': 2.653686055666983e-05, 'epoch': 0.66}
{'loss': 0.6637, 'learning_rate': 2.649416509930299e-05, 'epoch': 0.66}
{'loss': 0.6956, 'learning_rate': 2.645149163285381e-05, 'epoch': 0.66}
{'loss': 0.6795, 'learning_rate': 2.6408840197245455e-05, 'epoch': 0.66}
{'loss': 0.6335, 'learning_rate': 2.6366210832380567e-05, 'epoch': 0.66}
 66%|   | 2131/3250 [3:18:43<1:50:51,  5.94s/it]                                                        66%|   | 2131/3250 [3:18:43<1:50:51,  5.94s/it] 66%|   | 2132/3250 [3:18:48<1:49:03,  5.85s/it]                                                        66%|   | 2132/3250 [3:18:48<1:49:03,  5.85s/it] 66%|   | 2133/3250 [3:18:54<1:46:20,  5.71s/it]                                                        66%|   | 2133/3250 [3:18:54<1:46:20,  5.71s/it] 66%|   | 2134/3250 [3:18:59<1:44:29,  5.62s/it]                                                        66%|   | 2134/3250 [3:18:59<1:44:29,  5.62s/it] 66%|   | 2135/3250 [3:19:04<1:43:03,  5.55s/it]                                                        66%|   | 2135/3250 [3:19:05<1:43:03,  5.55s/it] 66%|   | 2136/32{'loss': 0.6445, 'learning_rate': 2.632360357814111e-05, 'epoch': 0.66}
{'loss': 0.7275, 'learning_rate': 2.628101847438835e-05, 'epoch': 0.66}
{'loss': 0.6901, 'learning_rate': 2.6238455560962884e-05, 'epoch': 0.66}
{'loss': 0.6686, 'learning_rate': 2.619591487768444e-05, 'epoch': 0.66}
{'loss': 0.6289, 'learning_rate': 2.615339646435206e-05, 'epoch': 0.66}
50 [3:19:10<1:42:00,  5.49s/it]                                                        66%|   | 2136/3250 [3:19:10<1:42:00,  5.49s/it] 66%|   | 2137/3250 [3:19:15<1:41:19,  5.46s/it]                                                        66%|   | 2137/3250 [3:19:15<1:41:19,  5.46s/it] 66%|   | 2138/3250 [3:19:21<1:41:09,  5.46s/it]                                                        66%|   | 2138/3250 [3:19:21<1:41:09,  5.46s/it] 66%|   | 2139/3250 [3:19:26<1:40:52,  5.45s/it]                                                        66%|   | 2139/3250 [3:19:26<1:40:52,  5.45s/it] 66%|   | 2140/3250 [3:19:32<1:40:26,  5.43s/it]                                                        66%|   | 2140/3250 [3:19:32<1:40:26,  5.43s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8535143136978149, 'eval_runtime': 1.3697, 'eval_samples_per_second': 8.761, 'eval_steps_per_second': 2.19, 'epoch': 0.66}
                                                        66%|   | 2140/3250 [3:19:33<1:40:26,  5.43s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_4/tmp-checkpoint-2140
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2140
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2140
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2140
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2140
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2140/pytorch_model.bin
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2140/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2140/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2140/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.6667, 'learning_rate': 2.6110900360743896e-05, 'epoch': 0.66}
{'loss': 0.6721, 'learning_rate': 2.6068426606617252e-05, 'epoch': 0.66}
{'loss': 0.6689, 'learning_rate': 2.6025975241708478e-05, 'epoch': 0.66}
{'loss': 0.6375, 'learning_rate': 2.598354630573303e-05, 'epoch': 0.66}
{'loss': 0.6695, 'learning_rate': 2.5941139838385383e-05, 'epoch': 0.66}
 66%|   | 2141/3250 [3:19:39<1:49:38,  5.93s/it]                                                        66%|   | 2141/3250 [3:19:39<1:49:38,  5.93s/it] 66%|   | 2142/3250 [3:19:44<1:46:29,  5.77s/it]                                                        66%|   | 2142/3250 [3:19:44<1:46:29,  5.77s/it] 66%|   | 2143/3250 [3:19:49<1:44:21,  5.66s/it]                                                        66%|   | 2143/3250 [3:19:49<1:44:21,  5.66s/it] 66%|   | 2144/3250 [3:19:55<1:42:47,  5.58s/it]                                                        66%|   | 2144/3250 [3:19:55<1:42:47,  5.58s/it] 66%|   | 2145/3250 [3:20:00<1:41:38,  5.52s/it]                                                        66%|   | 2145/3250 [3:20:00<1:41:38,  5.52s/it] 66%|   | 2146/32{'loss': 0.6876, 'learning_rate': 2.589875587933892e-05, 'epoch': 0.66}
{'loss': 0.6821, 'learning_rate': 2.5856394468246036e-05, 'epoch': 0.66}
{'loss': 0.6753, 'learning_rate': 2.581405564473801e-05, 'epoch': 0.66}
{'loss': 0.6713, 'learning_rate': 2.5771739448425e-05, 'epoch': 0.66}
{'loss': 0.6879, 'learning_rate': 2.572944591889598e-05, 'epoch': 0.66}
50 [3:20:06<1:40:48,  5.48s/it]                                                        66%|   | 2146/3250 [3:20:06<1:40:48,  5.48s/it] 66%|   | 2147/3250 [3:20:11<1:40:16,  5.46s/it]                                                        66%|   | 2147/3250 [3:20:11<1:40:16,  5.46s/it] 66%|   | 2148/3250 [3:20:17<1:43:11,  5.62s/it]                                                        66%|   | 2148/3250 [3:20:17<1:43:11,  5.62s/it] 66%|   | 2149/3250 [3:20:22<1:41:52,  5.55s/it]                                                        66%|   | 2149/3250 [3:20:22<1:41:52,  5.55s/it] 66%|   | 2150/3250 [3:20:28<1:41:04,  5.51s/it]                                                        66%|   | 2150/3250 [3:20:28<1:41:04,  5.51s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.853873074054718, 'eval_runtime': 1.3729, 'eval_samples_per_second': 8.741, 'eval_steps_per_second': 2.185, 'epoch': 0.66}
                                                        66%|   | 2150/3250 [3:20:29<1:41:04,  5.51s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_4/tmp-checkpoint-2150
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2150I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2150

I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2150
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2150
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2150/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2150/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2150/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2150/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.657, 'learning_rate': 2.5687175095718723e-05, 'epoch': 0.66}
{'loss': 0.6575, 'learning_rate': 2.5644927018439748e-05, 'epoch': 0.66}
{'loss': 0.7068, 'learning_rate': 2.56027017265843e-05, 'epoch': 0.66}
{'loss': 0.6532, 'learning_rate': 2.5560499259656325e-05, 'epoch': 0.66}
{'loss': 0.6427, 'learning_rate': 2.55183196571384e-05, 'epoch': 0.66}
 66%|   | 2151/3250 [3:20:35<1:49:58,  6.00s/it]                                                        66%|   | 2151/3250 [3:20:35<1:49:58,  6.00s/it] 66%|   | 2152/3250 [3:20:40<1:46:37,  5.83s/it]                                                        66%|   | 2152/3250 [3:20:40<1:46:37,  5.83s/it] 66%|   | 2153/3250 [3:20:46<1:44:11,  5.70s/it]                                                        66%|   | 2153/3250 [3:20:46<1:44:11,  5.70s/it] 66%|   | 2154/3250 [3:20:51<1:42:28,  5.61s/it]                                                        66%|   | 2154/3250 [3:20:51<1:42:28,  5.61s/it] 66%|   | 2155/3250 [3:20:57<1:41:17,  5.55s/it]                                                        66%|   | 2155/3250 [3:20:57<1:41:17,  5.55s/it] 66%|   | 2156/32{'loss': 0.6462, 'learning_rate': 2.5476162958491727e-05, 'epoch': 0.66}
{'loss': 0.6954, 'learning_rate': 2.5434029203156035e-05, 'epoch': 0.66}
{'loss': 0.6603, 'learning_rate': 2.539191843054963e-05, 'epoch': 0.66}
{'loss': 0.6843, 'learning_rate': 2.5349830680069338e-05, 'epoch': 0.66}
{'loss': 1.2115, 'learning_rate': 2.530776599109036e-05, 'epoch': 0.66}
50 [3:21:02<1:40:25,  5.51s/it]                                                        66%|   | 2156/3250 [3:21:02<1:40:25,  5.51s/it] 66%|   | 2157/3250 [3:21:07<1:39:43,  5.47s/it]                                                        66%|   | 2157/3250 [3:21:07<1:39:43,  5.47s/it] 66%|   | 2158/3250 [3:21:13<1:39:15,  5.45s/it]                                                        66%|   | 2158/3250 [3:21:13<1:39:15,  5.45s/it] 66%|   | 2159/3250 [3:21:18<1:38:53,  5.44s/it]                                                        66%|   | 2159/3250 [3:21:18<1:38:53,  5.44s/it] 66%|   | 2160/3250 [3:21:24<1:38:36,  5.43s/it]                                                        66%|   | 2160/3250 [3:21:24<1:38:36,  5.43s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8562886118888855, 'eval_runtime': 1.3715, 'eval_samples_per_second': 8.749, 'eval_steps_per_second': 2.187, 'epoch': 0.66}
                                                        66%|   | 2160/3250 [3:21:25<1:38:36,  5.43s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_4/tmp-checkpoint-2160
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2160I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2160

I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2160
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2160
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2160/pytorch_model.bin
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2160/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2160/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2160/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.6265, 'learning_rate': 2.5265724402966407e-05, 'epoch': 0.66}
{'loss': 0.6492, 'learning_rate': 2.522370595502954e-05, 'epoch': 0.67}
{'loss': 0.6879, 'learning_rate': 2.51817106865902e-05, 'epoch': 0.67}
{'loss': 0.6882, 'learning_rate': 2.5139738636937084e-05, 'epoch': 0.67}
{'loss': 0.6653, 'learning_rate': 2.5097789845337223e-05, 'epoch': 0.67}
 66%|   | 2161/3250 [3:21:31<1:48:10,  5.96s/it]                                                        66%|   | 2161/3250 [3:21:31<1:48:10,  5.96s/it] 67%|   | 2162/3250 [3:21:36<1:45:03,  5.79s/it]                                                        67%|   | 2162/3250 [3:21:36<1:45:03,  5.79s/it] 67%|   | 2163/3250 [3:21:42<1:42:47,  5.67s/it]                                                        67%|   | 2163/3250 [3:21:42<1:42:47,  5.67s/it] 67%|   | 2164/3250 [3:21:47<1:42:37,  5.67s/it]                                                        67%|   | 2164/3250 [3:21:47<1:42:37,  5.67s/it] 67%|   | 2165/3250 [3:21:53<1:41:04,  5.59s/it]                                                        67%|   | 2165/3250 [3:21:53<1:41:04,  5.59s/it] 67%|   | 2166/32{'loss': 0.6433, 'learning_rate': 2.5055864351035868e-05, 'epoch': 0.67}
{'loss': 0.7143, 'learning_rate': 2.5013962193256473e-05, 'epoch': 0.67}
{'loss': 0.6969, 'learning_rate': 2.497208341120067e-05, 'epoch': 0.67}
{'loss': 0.6675, 'learning_rate': 2.493022804404822e-05, 'epoch': 0.67}
{'loss': 0.6368, 'learning_rate': 2.4888396130956948e-05, 'epoch': 0.67}
50 [3:21:58<1:39:52,  5.53s/it]                                                        67%|   | 2166/3250 [3:21:58<1:39:52,  5.53s/it] 67%|   | 2167/3250 [3:22:03<1:39:07,  5.49s/it]                                                        67%|   | 2167/3250 [3:22:03<1:39:07,  5.49s/it] 67%|   | 2168/3250 [3:22:09<1:38:33,  5.47s/it]                                                        67%|   | 2168/3250 [3:22:09<1:38:33,  5.47s/it] 67%|   | 2169/3250 [3:22:14<1:38:07,  5.45s/it]                                                        67%|   | 2169/3250 [3:22:14<1:38:07,  5.45s/it] 67%|   | 2170/3250 [3:22:20<1:37:48,  5.43s/it]                                                        67%|   | 2170/3250 [3:22:20<1:37:48,  5.43s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8559547662734985, 'eval_runtime': 1.4396, 'eval_samples_per_second': 8.335, 'eval_steps_per_second': 2.084, 'epoch': 0.67}
                                                        67%|   | 2170/3250 [3:22:21<1:37:48,  5.43s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_4/tmp-checkpoint-2170
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2170
/projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2170
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2170
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2170
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2170/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2170/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2170/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2170/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.6773, 'learning_rate': 2.4846587711062768e-05, 'epoch': 0.67}
{'loss': 0.6646, 'learning_rate': 2.4804802823479613e-05, 'epoch': 0.67}
{'loss': 0.6757, 'learning_rate': 2.4763041507299388e-05, 'epoch': 0.67}
{'loss': 0.663, 'learning_rate': 2.472130380159199e-05, 'epoch': 0.67}
{'loss': 0.6686, 'learning_rate': 2.4679589745405124e-05, 'epoch': 0.67}
 67%|   | 2171/3250 [3:22:27<1:47:22,  5.97s/it]                                                        67%|   | 2171/3250 [3:22:27<1:47:22,  5.97s/it] 67%|   | 2172/3250 [3:22:32<1:44:08,  5.80s/it]                                                        67%|   | 2172/3250 [3:22:32<1:44:08,  5.80s/it] 67%|   | 2173/3250 [3:22:38<1:41:57,  5.68s/it]                                                        67%|   | 2173/3250 [3:22:38<1:41:57,  5.68s/it] 67%|   | 2174/3250 [3:22:43<1:40:19,  5.59s/it]                                                        67%|   | 2174/3250 [3:22:43<1:40:19,  5.59s/it] 67%|   | 2175/3250 [3:22:48<1:39:10,  5.54s/it]                                                        67%|   | 2175/3250 [3:22:48<1:39:10,  5.54s/it] 67%|   | 2176/32{'loss': 0.6604, 'learning_rate': 2.4637899377764494e-05, 'epoch': 0.67}
{'loss': 0.6826, 'learning_rate': 2.459623273767354e-05, 'epoch': 0.67}
{'loss': 0.6714, 'learning_rate': 2.4554589864113563e-05, 'epoch': 0.67}
{'loss': 0.6786, 'learning_rate': 2.4512970796043616e-05, 'epoch': 0.67}
{'loss': 0.6736, 'learning_rate': 2.447137557240048e-05, 'epoch': 0.67}
50 [3:22:54<1:38:20,  5.49s/it]                                                        67%|   | 2176/3250 [3:22:54<1:38:20,  5.49s/it] 67%|   | 2177/3250 [3:22:59<1:37:43,  5.46s/it]                                                        67%|   | 2177/3250 [3:22:59<1:37:43,  5.46s/it] 67%|   | 2178/3250 [3:23:05<1:37:17,  5.45s/it]                                                        67%|   | 2178/3250 [3:23:05<1:37:17,  5.45s/it] 67%|   | 2179/3250 [3:23:10<1:36:53,  5.43s/it]                                                        67%|   | 2179/3250 [3:23:10<1:36:53,  5.43s/it] 67%|   | 2180/3250 [3:23:15<1:36:37,  5.42s/it]                                                        67%|   | 2180/3250 [3:23:15<1:36:37,  5.42s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8539391160011292, 'eval_runtime': 1.5962, 'eval_samples_per_second': 7.518, 'eval_steps_per_second': 1.879, 'epoch': 0.67}
                                                        67%|   | 2180/3250 [3:23:17<1:36:37,  5.42s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_4/tmp-checkpoint-2180
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2180I AM HERE AND I AM SAVING THE MODEL

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2180
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2180
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2180
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2180/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2180/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2180/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2180/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.6812, 'learning_rate': 2.442980423209864e-05, 'epoch': 0.67}
{'loss': 0.6331, 'learning_rate': 2.4388256814030187e-05, 'epoch': 0.67}
{'loss': 0.7173, 'learning_rate': 2.4346733357064888e-05, 'epoch': 0.67}
{'loss': 0.6584, 'learning_rate': 2.4305233900050074e-05, 'epoch': 0.67}
{'loss': 0.6684, 'learning_rate': 2.4263758481810617e-05, 'epoch': 0.67}
 67%|   | 2181/3250 [3:23:23<1:48:53,  6.11s/it]                                                        67%|   | 2181/3250 [3:23:23<1:48:53,  6.11s/it] 67%|   | 2182/3250 [3:23:29<1:44:56,  5.90s/it]                                                        67%|   | 2182/3250 [3:23:29<1:44:56,  5.90s/it] 67%|   | 2183/3250 [3:23:34<1:42:07,  5.74s/it]                                                        67%|   | 2183/3250 [3:23:34<1:42:07,  5.74s/it] 67%|   | 2184/3250 [3:23:39<1:40:08,  5.64s/it]                                                        67%|   | 2184/3250 [3:23:39<1:40:08,  5.64s/it] 67%|   | 2185/3250 [3:23:45<1:38:43,  5.56s/it]                                                        67%|   | 2185/3250 [3:23:45<1:38:43,  5.56s/it] 67%|   | 2186/32{'loss': 0.6506, 'learning_rate': 2.422230714114891e-05, 'epoch': 0.67}
{'loss': 0.6492, 'learning_rate': 2.418087991684483e-05, 'epoch': 0.67}
{'loss': 0.6851, 'learning_rate': 2.4139476847655634e-05, 'epoch': 0.67}
{'loss': 0.6563, 'learning_rate': 2.4098097972316046e-05, 'epoch': 0.67}
{'loss': 0.7127, 'learning_rate': 2.4056743329538138e-05, 'epoch': 0.67}
50 [3:23:50<1:37:55,  5.52s/it]                                                        67%|   | 2186/3250 [3:23:50<1:37:55,  5.52s/it] 67%|   | 2187/3250 [3:23:56<1:37:15,  5.49s/it]                                                        67%|   | 2187/3250 [3:23:56<1:37:15,  5.49s/it] 67%|   | 2188/3250 [3:24:01<1:36:41,  5.46s/it]                                                        67%|   | 2188/3250 [3:24:01<1:36:41,  5.46s/it] 67%|   | 2189/3250 [3:24:06<1:36:25,  5.45s/it]                                                        67%|   | 2189/3250 [3:24:06<1:36:25,  5.45s/it] 67%|   | 2190/3250 [3:24:12<1:36:05,  5.44s/it]                                                        67%|   | 2190/3250 [3:24:12<1:36:05,  5.44s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8561740517616272, 'eval_runtime': 1.3796, 'eval_samples_per_second': 8.698, 'eval_steps_per_second': 2.175, 'epoch': 0.67}
                                                        67%|   | 2190/3250 [3:24:13<1:36:05,  5.44s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_4/tmp-checkpoint-2190
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2190I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2190

I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2190
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2190
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2190/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2190/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2190/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2190/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 1.1491, 'learning_rate': 2.4015412958011302e-05, 'epoch': 0.67}
{'loss': 0.6598, 'learning_rate': 2.3974106896402237e-05, 'epoch': 0.67}
{'loss': 0.6809, 'learning_rate': 2.393282518335486e-05, 'epoch': 0.67}
{'loss': 0.6789, 'learning_rate': 2.3891567857490372e-05, 'epoch': 0.68}
{'loss': 0.679, 'learning_rate': 2.3850334957407087e-05, 'epoch': 0.68}
 67%|   | 2191/3250 [3:24:19<1:45:38,  5.99s/it]                                                        67%|   | 2191/3250 [3:24:19<1:45:38,  5.99s/it] 67%|   | 2192/3250 [3:24:24<1:42:24,  5.81s/it]                                                        67%|   | 2192/3250 [3:24:24<1:42:24,  5.81s/it] 67%|   | 2193/3250 [3:24:30<1:40:07,  5.68s/it]                                                        67%|   | 2193/3250 [3:24:30<1:40:07,  5.68s/it] 68%|   | 2194/3250 [3:24:35<1:38:30,  5.60s/it]                                                        68%|   | 2194/3250 [3:24:35<1:38:30,  5.60s/it] 68%|   | 2195/3250 [3:24:41<1:37:15,  5.53s/it]                                                        68%|   | 2195/3250 [3:24:41<1:37:15,  5.53s/it] 68%|   | 2196/32{'loss': 0.6303, 'learning_rate': 2.3809126521680518e-05, 'epoch': 0.68}
{'loss': 0.6626, 'learning_rate': 2.3767942588863283e-05, 'epoch': 0.68}
{'loss': 0.7482, 'learning_rate': 2.372678319748507e-05, 'epoch': 0.68}
{'loss': 0.669, 'learning_rate': 2.3685648386052617e-05, 'epoch': 0.68}
{'loss': 0.6572, 'learning_rate': 2.3644538193049625e-05, 'epoch': 0.68}
50 [3:24:46<1:36:26,  5.49s/it]                                                        68%|   | 2196/3250 [3:24:46<1:36:26,  5.49s/it] 68%|   | 2197/3250 [3:24:52<1:37:02,  5.53s/it]                                                        68%|   | 2197/3250 [3:24:52<1:37:02,  5.53s/it] 68%|   | 2198/3250 [3:24:57<1:36:13,  5.49s/it]                                                        68%|   | 2198/3250 [3:24:57<1:36:13,  5.49s/it] 68%|   | 2199/3250 [3:25:02<1:35:39,  5.46s/it]                                                        68%|   | 2199/3250 [3:25:02<1:35:39,  5.46s/it] 68%|   | 2200/3250 [3:25:08<1:35:09,  5.44s/it]                                                        68%|   | 2200/3250 [3:25:08<1:35:09,  5.44s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8543118238449097, 'eval_runtime': 1.382, 'eval_samples_per_second': 8.683, 'eval_steps_per_second': 2.171, 'epoch': 0.68}
                                                        68%|   | 2200/3250 [3:25:09<1:35:09,  5.44s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_4/tmp-checkpoint-2200
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2200
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2200
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2200
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2200
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2200/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2200/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2200/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2200/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.6665, 'learning_rate': 2.360345265693681e-05, 'epoch': 0.68}
{'loss': 0.656, 'learning_rate': 2.356239181615181e-05, 'epoch': 0.68}
{'loss': 0.668, 'learning_rate': 2.3521355709109138e-05, 'epoch': 0.68}
{'loss': 0.6562, 'learning_rate': 2.3480344374200203e-05, 'epoch': 0.68}
{'loss': 0.6499, 'learning_rate': 2.343935784979323e-05, 'epoch': 0.68}
 68%|   | 2201/3250 [3:25:15<1:44:00,  5.95s/it]                                                        68%|   | 2201/3250 [3:25:15<1:44:00,  5.95s/it] 68%|   | 2202/3250 [3:25:20<1:41:08,  5.79s/it]                                                        68%|   | 2202/3250 [3:25:20<1:41:08,  5.79s/it] 68%|   | 2203/3250 [3:25:26<1:39:00,  5.67s/it]                                                        68%|   | 2203/3250 [3:25:26<1:39:00,  5.67s/it] 68%|   | 2204/3250 [3:25:31<1:37:27,  5.59s/it]                                                        68%|   | 2204/3250 [3:25:31<1:37:27,  5.59s/it] 68%|   | 2205/3250 [3:25:37<1:36:22,  5.53s/it]                                                        68%|   | 2205/3250 [3:25:37<1:36:22,  5.53s/it] 68%|   | 2206/32{'loss': 0.652, 'learning_rate': 2.3398396174233178e-05, 'epoch': 0.68}
{'loss': 0.6771, 'learning_rate': 2.3357459385841823e-05, 'epoch': 0.68}
{'loss': 0.6866, 'learning_rate': 2.3316547522917638e-05, 'epoch': 0.68}
{'loss': 0.6645, 'learning_rate': 2.3275660623735772e-05, 'epoch': 0.68}
{'loss': 0.6908, 'learning_rate': 2.3234798726548044e-05, 'epoch': 0.68}
50 [3:25:42<1:35:39,  5.50s/it]                                                        68%|   | 2206/3250 [3:25:42<1:35:39,  5.50s/it] 68%|   | 2207/3250 [3:25:47<1:35:05,  5.47s/it]                                                        68%|   | 2207/3250 [3:25:47<1:35:05,  5.47s/it] 68%|   | 2208/3250 [3:25:53<1:34:40,  5.45s/it]                                                        68%|   | 2208/3250 [3:25:53<1:34:40,  5.45s/it] 68%|   | 2209/3250 [3:25:58<1:34:18,  5.44s/it]                                                        68%|   | 2209/3250 [3:25:58<1:34:18,  5.44s/it] 68%|   | 2210/3250 [3:26:04<1:34:08,  5.43s/it]                                                        68%|   | 2210/3250 [3:26:04<1:34:08,  5.43s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8525980710983276, 'eval_runtime': 1.3696, 'eval_samples_per_second': 8.762, 'eval_steps_per_second': 2.19, 'epoch': 0.68}
                                                        68%|   | 2210/3250 [3:26:05<1:34:08,  5.43s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_4/tmp-checkpoint-2210
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2210
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2210
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2210
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2210
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2210/pytorch_model.bin
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2210/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2210/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2210/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.6704, 'learning_rate': 2.3193961869582826e-05, 'epoch': 0.68}
{'loss': 0.6555, 'learning_rate': 2.3153150091045132e-05, 'epoch': 0.68}
{'loss': 0.692, 'learning_rate': 2.311236342911644e-05, 'epoch': 0.68}
{'loss': 0.6812, 'learning_rate': 2.3071601921954794e-05, 'epoch': 0.68}
{'loss': 0.661, 'learning_rate': 2.3030865607694675e-05, 'epoch': 0.68}
 68%|   | 2211/3250 [3:26:11<1:42:37,  5.93s/it]                                                        68%|   | 2211/3250 [3:26:11<1:42:37,  5.93s/it] 68%|   | 2212/3250 [3:26:16<1:39:47,  5.77s/it]                                                        68%|   | 2212/3250 [3:26:16<1:39:47,  5.77s/it] 68%|   | 2213/3250 [3:26:21<1:37:41,  5.65s/it]                                                        68%|   | 2213/3250 [3:26:21<1:37:41,  5.65s/it] 68%|   | 2214/3250 [3:26:28<1:39:37,  5.77s/it]                                                        68%|   | 2214/3250 [3:26:28<1:39:37,  5.77s/it] 68%|   | 2215/3250 [3:26:33<1:37:40,  5.66s/it]                                                        68%|   | 2215/3250 [3:26:33<1:37:40,  5.66s/it] 68%|   | 2216/32{'loss': 0.6463, 'learning_rate': 2.2990154524447005e-05, 'epoch': 0.68}
{'loss': 0.6315, 'learning_rate': 2.2949468710299116e-05, 'epoch': 0.68}
{'loss': 0.6889, 'learning_rate': 2.2908808203314635e-05, 'epoch': 0.68}
{'loss': 0.6413, 'learning_rate': 2.2868173041533585e-05, 'epoch': 0.68}
{'loss': 0.7263, 'learning_rate': 2.2827563262972244e-05, 'epoch': 0.68}
50 [3:26:38<1:36:18,  5.59s/it]                                                        68%|   | 2216/3250 [3:26:38<1:36:18,  5.59s/it] 68%|   | 2217/3250 [3:26:44<1:35:25,  5.54s/it]                                                        68%|   | 2217/3250 [3:26:44<1:35:25,  5.54s/it] 68%|   | 2218/3250 [3:26:49<1:34:39,  5.50s/it]                                                        68%|   | 2218/3250 [3:26:49<1:34:39,  5.50s/it] 68%|   | 2219/3250 [3:26:55<1:34:04,  5.48s/it]                                                        68%|   | 2219/3250 [3:26:55<1:34:04,  5.48s/it] 68%|   | 2220/3250 [3:27:00<1:33:42,  5.46s/it]                                                        68%|   | 2220/3250 [3:27:00<1:33:42,  5.46s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8562905192375183, 'eval_runtime': 1.372, 'eval_samples_per_second': 8.746, 'eval_steps_per_second': 2.187, 'epoch': 0.68}
                                                        68%|   | 2220/3250 [3:27:01<1:33:42,  5.46s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_4/tmp-checkpoint-2220
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2220
/projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2220
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2220
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2220
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2220/pytorch_model.bin
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2220/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2220/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2220/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 1.1502, 'learning_rate': 2.278697890562316e-05, 'epoch': 0.68}
{'loss': 0.6541, 'learning_rate': 2.274642000745507e-05, 'epoch': 0.68}
{'loss': 0.6643, 'learning_rate': 2.270588660641294e-05, 'epoch': 0.68}
{'loss': 0.6868, 'learning_rate': 2.266537874041781e-05, 'epoch': 0.68}
{'loss': 0.6795, 'learning_rate': 2.262489644736689e-05, 'epoch': 0.68}
 68%|   | 2221/3250 [3:27:07<1:42:24,  5.97s/it]                                                        68%|   | 2221/3250 [3:27:07<1:42:24,  5.97s/it] 68%|   | 2222/3250 [3:27:13<1:39:23,  5.80s/it]                                                        68%|   | 2222/3250 [3:27:13<1:39:23,  5.80s/it] 68%|   | 2223/3250 [3:27:18<1:37:18,  5.69s/it]                                                        68%|   | 2223/3250 [3:27:18<1:37:18,  5.69s/it] 68%|   | 2224/3250 [3:27:23<1:35:59,  5.61s/it]                                                        68%|   | 2224/3250 [3:27:23<1:35:59,  5.61s/it] 68%|   | 2225/3250 [3:27:29<1:34:49,  5.55s/it]                                                        68%|   | 2225/3250 [3:27:29<1:34:49,  5.55s/it] 68%|   | 2226/32{'loss': 0.6489, 'learning_rate': 2.2584439765133454e-05, 'epoch': 0.68}
{'loss': 0.6496, 'learning_rate': 2.2544008731566817e-05, 'epoch': 0.69}
{'loss': 0.7318, 'learning_rate': 2.250360338449226e-05, 'epoch': 0.69}
{'loss': 0.6896, 'learning_rate': 2.246322376171109e-05, 'epoch': 0.69}
{'loss': 0.6814, 'learning_rate': 2.242286990100052e-05, 'epoch': 0.69}
50 [3:27:34<1:33:52,  5.50s/it]                                                        68%|   | 2226/3250 [3:27:34<1:33:52,  5.50s/it] 69%|   | 2227/3250 [3:27:40<1:33:17,  5.47s/it]                                                        69%|   | 2227/3250 [3:27:40<1:33:17,  5.47s/it] 69%|   | 2228/3250 [3:27:45<1:32:49,  5.45s/it]                                                        69%|   | 2228/3250 [3:27:45<1:32:49,  5.45s/it] 69%|   | 2229/3250 [3:27:50<1:32:29,  5.44s/it]                                                        69%|   | 2229/3250 [3:27:50<1:32:29,  5.44s/it] 69%|   | 2230/3250 [3:27:56<1:34:13,  5.54s/it]                                                        69%|   | 2230/3250 [3:27:56<1:34:13,  5.54s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8535001873970032, 'eval_runtime': 1.3835, 'eval_samples_per_second': 8.674, 'eval_steps_per_second': 2.168, 'epoch': 0.69}
                                                        69%|   | 2230/3250 [3:27:58<1:34:13,  5.54s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_4/tmp-checkpoint-2230
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2230I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2230

I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2230
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2230
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2230/pytorch_model.bin
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2230/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2230/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2230/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.6099, 'learning_rate': 2.238254184011364e-05, 'epoch': 0.69}
{'loss': 0.6753, 'learning_rate': 2.2342239616779442e-05, 'epoch': 0.69}
{'loss': 0.6673, 'learning_rate': 2.2301963268702723e-05, 'epoch': 0.69}
{'loss': 0.6716, 'learning_rate': 2.226171283356409e-05, 'epoch': 0.69}
{'loss': 0.6502, 'learning_rate': 2.2221488349019903e-05, 'epoch': 0.69}
 69%|   | 2231/3250 [3:28:03<1:42:36,  6.04s/it]                                                        69%|   | 2231/3250 [3:28:03<1:42:36,  6.04s/it] 69%|   | 2232/3250 [3:28:09<1:39:14,  5.85s/it]                                                        69%|   | 2232/3250 [3:28:09<1:39:14,  5.85s/it] 69%|   | 2233/3250 [3:28:14<1:36:53,  5.72s/it]                                                        69%|   | 2233/3250 [3:28:14<1:36:53,  5.72s/it] 69%|   | 2234/3250 [3:28:20<1:35:25,  5.64s/it]                                                        69%|   | 2234/3250 [3:28:20<1:35:25,  5.64s/it] 69%|   | 2235/3250 [3:28:25<1:34:07,  5.56s/it]                                                        69%|   | 2235/3250 [3:28:25<1:34:07,  5.56s/it] 69%|   | 2236/32{'loss': 0.6577, 'learning_rate': 2.2181289852702204e-05, 'epoch': 0.69}
{'loss': 0.6704, 'learning_rate': 2.214111738221877e-05, 'epoch': 0.69}
{'loss': 0.6848, 'learning_rate': 2.210097097515301e-05, 'epoch': 0.69}
{'loss': 0.6728, 'learning_rate': 2.2060850669063963e-05, 'epoch': 0.69}
{'loss': 0.6613, 'learning_rate': 2.2020756501486233e-05, 'epoch': 0.69}
50 [3:28:31<1:33:16,  5.52s/it]                                                        69%|   | 2236/3250 [3:28:31<1:33:16,  5.52s/it] 69%|   | 2237/3250 [3:28:36<1:32:32,  5.48s/it]                                                        69%|   | 2237/3250 [3:28:36<1:32:32,  5.48s/it] 69%|   | 2238/3250 [3:28:41<1:32:06,  5.46s/it]                                                        69%|   | 2238/3250 [3:28:41<1:32:06,  5.46s/it] 69%|   | 2239/3250 [3:28:47<1:31:42,  5.44s/it]                                                        69%|   | 2239/3250 [3:28:47<1:31:42,  5.44s/it] 69%|   | 2240/3250 [3:28:52<1:31:26,  5.43s/it]                                                        69%|   | 2240/3250 [3:28:52<1:31:26,  5.43s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8516407608985901, 'eval_runtime': 1.3711, 'eval_samples_per_second': 8.752, 'eval_steps_per_second': 2.188, 'epoch': 0.69}
                                                        69%|   | 2240/3250 [3:28:54<1:31:26,  5.43s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_4/tmp-checkpoint-2240
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2240
/projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2240
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2240
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2240
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2240/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2240/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2240/pytorch_model.binthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2240/pytorch_model.bin

/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.6862, 'learning_rate': 2.1980688509929997e-05, 'epoch': 0.69}
{'loss': 0.6588, 'learning_rate': 2.194064673188089e-05, 'epoch': 0.69}
{'loss': 0.6558, 'learning_rate': 2.1900631204800054e-05, 'epoch': 0.69}
{'loss': 0.7189, 'learning_rate': 2.1860641966124114e-05, 'epoch': 0.69}
{'loss': 0.6625, 'learning_rate': 2.1820679053265e-05, 'epoch': 0.69}
 69%|   | 2241/3250 [3:28:59<1:40:16,  5.96s/it]                                                        69%|   | 2241/3250 [3:28:59<1:40:16,  5.96s/it] 69%|   | 2242/3250 [3:29:05<1:37:20,  5.79s/it]                                                        69%|   | 2242/3250 [3:29:05<1:37:20,  5.79s/it] 69%|   | 2243/3250 [3:29:10<1:35:15,  5.68s/it]                                                        69%|   | 2243/3250 [3:29:10<1:35:15,  5.68s/it] 69%|   | 2244/3250 [3:29:16<1:33:51,  5.60s/it]                                                        69%|   | 2244/3250 [3:29:16<1:33:51,  5.60s/it] 69%|   | 2245/3250 [3:29:21<1:32:49,  5.54s/it]                                                        69%|   | 2245/3250 [3:29:21<1:32:49,  5.54s/it] 69%|   | 2246/32{'loss': 0.6474, 'learning_rate': 2.1780742503610118e-05, 'epoch': 0.69}
{'loss': 0.6482, 'learning_rate': 2.1740832354522145e-05, 'epoch': 0.69}
{'loss': 0.6834, 'learning_rate': 2.1700948643339103e-05, 'epoch': 0.69}
{'loss': 0.6539, 'learning_rate': 2.1661091407374218e-05, 'epoch': 0.69}
{'loss': 0.6764, 'learning_rate': 2.1621260683916007e-05, 'epoch': 0.69}
50 [3:29:27<1:34:32,  5.65s/it]                                                        69%|   | 2246/3250 [3:29:27<1:34:32,  5.65s/it] 69%|   | 2247/3250 [3:29:32<1:33:14,  5.58s/it]                                                        69%|   | 2247/3250 [3:29:32<1:33:14,  5.58s/it] 69%|   | 2248/3250 [3:29:38<1:32:17,  5.53s/it]                                                        69%|   | 2248/3250 [3:29:38<1:32:17,  5.53s/it] 69%|   | 2249/3250 [3:29:43<1:31:37,  5.49s/it]                                                        69%|   | 2249/3250 [3:29:43<1:31:37,  5.49s/it] 69%|   | 2250/3250 [3:29:48<1:31:07,  5.47s/it]                                                        69%|   | 2250/3250 [3:29:49<1:31:07,  5.47s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8548371195793152, 'eval_runtime': 1.3704, 'eval_samples_per_second': 8.757, 'eval_steps_per_second': 2.189, 'epoch': 0.69}
                                                        69%|   | 2250/3250 [3:29:50<1:31:07,  5.47s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_4/tmp-checkpoint-2250
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2250I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2250

I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2250
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2250
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2250/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2250/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2250/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2250/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 1.1985, 'learning_rate': 2.1581456510228147e-05, 'epoch': 0.69}
{'loss': 0.6229, 'learning_rate': 2.1541678923549507e-05, 'epoch': 0.69}
{'loss': 0.6441, 'learning_rate': 2.1501927961094054e-05, 'epoch': 0.69}
{'loss': 0.6946, 'learning_rate': 2.1462203660050882e-05, 'epoch': 0.69}
{'loss': 0.6831, 'learning_rate': 2.1422506057584078e-05, 'epoch': 0.69}
 69%|   | 2251/3250 [3:29:56<1:39:27,  5.97s/it]                                                        69%|   | 2251/3250 [3:29:56<1:39:27,  5.97s/it] 69%|   | 2252/3250 [3:30:01<1:36:28,  5.80s/it]                                                        69%|   | 2252/3250 [3:30:01<1:36:28,  5.80s/it] 69%|   | 2253/3250 [3:30:06<1:34:23,  5.68s/it]                                                        69%|   | 2253/3250 [3:30:06<1:34:23,  5.68s/it] 69%|   | 2254/3250 [3:30:12<1:32:51,  5.59s/it]                                                        69%|   | 2254/3250 [3:30:12<1:32:51,  5.59s/it] 69%|   | 2255/3250 [3:30:17<1:31:53,  5.54s/it]                                                        69%|   | 2255/3250 [3:30:17<1:31:53,  5.54s/it] 69%|   | 2256/32{'loss': 0.6623, 'learning_rate': 2.1382835190832813e-05, 'epoch': 0.69}
{'loss': 0.6353, 'learning_rate': 2.134319109691122e-05, 'epoch': 0.69}
{'loss': 0.7061, 'learning_rate': 2.1303573812908385e-05, 'epoch': 0.69}
{'loss': 0.6857, 'learning_rate': 2.126398337588834e-05, 'epoch': 0.7}
{'loss': 0.6569, 'learning_rate': 2.122441982288994e-05, 'epoch': 0.7}
50 [3:30:23<1:31:07,  5.50s/it]                                                        69%|   | 2256/3250 [3:30:23<1:31:07,  5.50s/it] 69%|   | 2257/3250 [3:30:28<1:30:30,  5.47s/it]                                                        69%|   | 2257/3250 [3:30:28<1:30:30,  5.47s/it] 69%|   | 2258/3250 [3:30:33<1:30:09,  5.45s/it]                                                        69%|   | 2258/3250 [3:30:33<1:30:09,  5.45s/it] 70%|   | 2259/3250 [3:30:39<1:29:46,  5.44s/it]                                                        70%|   | 2259/3250 [3:30:39<1:29:46,  5.44s/it] 70%|   | 2260/3250 [3:30:44<1:29:29,  5.42s/it]                                                        70%|   | 2260/3250 [3:30:44<1:29:29,  5.42s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8530651926994324, 'eval_runtime': 1.3692, 'eval_samples_per_second': 8.764, 'eval_steps_per_second': 2.191, 'epoch': 0.7}
                                                        70%|   | 2260/3250 [3:30:46<1:29:29,  5.42s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_4/tmp-checkpoint-2260
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2260I AM HERE AND I AM SAVING THE MODEL

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2260
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2260
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2260
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2260/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2260/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2260/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2260/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.6332, 'learning_rate': 2.1184883190926954e-05, 'epoch': 0.7}
{'loss': 0.6811, 'learning_rate': 2.11453735169879e-05, 'epoch': 0.7}
{'loss': 0.6547, 'learning_rate': 2.110589083803613e-05, 'epoch': 0.7}
{'loss': 0.6714, 'learning_rate': 2.1066435191009715e-05, 'epoch': 0.7}
{'loss': 0.646, 'learning_rate': 2.1027006612821453e-05, 'epoch': 0.7}
 70%|   | 2261/3250 [3:30:51<1:37:46,  5.93s/it]                                                        70%|   | 2261/3250 [3:30:51<1:37:46,  5.93s/it] 70%|   | 2262/3250 [3:30:57<1:35:00,  5.77s/it]                                                        70%|   | 2262/3250 [3:30:57<1:35:00,  5.77s/it] 70%|   | 2263/3250 [3:31:03<1:34:54,  5.77s/it]                                                        70%|   | 2263/3250 [3:31:03<1:34:54,  5.77s/it] 70%|   | 2264/3250 [3:31:08<1:33:00,  5.66s/it]                                                        70%|   | 2264/3250 [3:31:08<1:33:00,  5.66s/it] 70%|   | 2265/3250 [3:31:13<1:31:36,  5.58s/it]                                                        70%|   | 2265/3250 [3:31:13<1:31:36,  5.58s/it] 70%|   | 2266/32{'loss': 0.6567, 'learning_rate': 2.0987605140358824e-05, 'epoch': 0.7}
{'loss': 0.6734, 'learning_rate': 2.0948230810483888e-05, 'epoch': 0.7}
{'loss': 0.6892, 'learning_rate': 2.0908883660033374e-05, 'epoch': 0.7}
{'loss': 0.6729, 'learning_rate': 2.0869563725818575e-05, 'epoch': 0.7}
{'loss': 0.6853, 'learning_rate': 2.08302710446253e-05, 'epoch': 0.7}
50 [3:31:19<1:30:36,  5.53s/it]                                                        70%|   | 2266/3250 [3:31:19<1:30:36,  5.53s/it] 70%|   | 2267/3250 [3:31:24<1:30:10,  5.50s/it]                                                        70%|   | 2267/3250 [3:31:24<1:30:10,  5.50s/it] 70%|   | 2268/3250 [3:31:30<1:29:30,  5.47s/it]                                                        70%|   | 2268/3250 [3:31:30<1:29:30,  5.47s/it] 70%|   | 2269/3250 [3:31:35<1:29:00,  5.44s/it]                                                        70%|   | 2269/3250 [3:31:35<1:29:00,  5.44s/it] 70%|   | 2270/3250 [3:31:40<1:28:41,  5.43s/it]                                                        70%|   | 2270/3250 [3:31:40<1:28:41,  5.43s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8505118489265442, 'eval_runtime': 1.3693, 'eval_samples_per_second': 8.763, 'eval_steps_per_second': 2.191, 'epoch': 0.7}
                                                        70%|   | 2270/3250 [3:31:42<1:28:41,  5.43s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_4/tmp-checkpoint-2270
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2270
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2270
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2270
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2270
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2270/pytorch_model.bin
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2270/pytorch_model.bin 
/projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2270/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2270/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.6638, 'learning_rate': 2.0791005653213884e-05, 'epoch': 0.7}
{'loss': 0.6858, 'learning_rate': 2.075176758831913e-05, 'epoch': 0.7}
{'loss': 0.6253, 'learning_rate': 2.0712556886650235e-05, 'epoch': 0.7}
{'loss': 0.7214, 'learning_rate': 2.067337358489085e-05, 'epoch': 0.7}
{'loss': 0.6468, 'learning_rate': 2.0634217719698955e-05, 'epoch': 0.7}
 70%|   | 2271/3250 [3:31:48<1:37:42,  5.99s/it]                                                        70%|   | 2271/3250 [3:31:48<1:37:42,  5.99s/it] 70%|   | 2272/3250 [3:31:53<1:34:46,  5.81s/it]                                                        70%|   | 2272/3250 [3:31:53<1:34:46,  5.81s/it] 70%|   | 2273/3250 [3:31:58<1:32:34,  5.68s/it]                                                        70%|   | 2273/3250 [3:31:58<1:32:34,  5.68s/it] 70%|   | 2274/3250 [3:32:04<1:31:03,  5.60s/it]                                                        70%|   | 2274/3250 [3:32:04<1:31:03,  5.60s/it] 70%|   | 2275/3250 [3:32:09<1:29:57,  5.54s/it]                                                        70%|   | 2275/3250 [3:32:09<1:29:57,  5.54s/it] 70%|   | 2276/32{'loss': 0.6727, 'learning_rate': 2.059508932770689e-05, 'epoch': 0.7}
{'loss': 0.645, 'learning_rate': 2.055598844552129e-05, 'epoch': 0.7}
{'loss': 0.6517, 'learning_rate': 2.0516915109723e-05, 'epoch': 0.7}
{'loss': 0.6799, 'learning_rate': 2.0477869356867186e-05, 'epoch': 0.7}
{'loss': 0.6545, 'learning_rate': 2.043885122348311e-05, 'epoch': 0.7}
50 [3:32:15<1:29:07,  5.49s/it]                                                        70%|   | 2276/3250 [3:32:15<1:29:07,  5.49s/it] 70%|   | 2277/3250 [3:32:20<1:28:58,  5.49s/it]                                                        70%|   | 2277/3250 [3:32:20<1:28:58,  5.49s/it] 70%|   | 2278/3250 [3:32:25<1:28:27,  5.46s/it]                                                        70%|   | 2278/3250 [3:32:25<1:28:27,  5.46s/it] 70%|   | 2279/3250 [3:32:31<1:30:36,  5.60s/it]                                                        70%|   | 2279/3250 [3:32:31<1:30:36,  5.60s/it] 70%|   | 2280/3250 [3:32:37<1:29:30,  5.54s/it]                                                        70%|   | 2280/3250 [3:32:37<1:29:30,  5.54s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8544135689735413, 'eval_runtime': 1.3661, 'eval_samples_per_second': 8.784, 'eval_steps_per_second': 2.196, 'epoch': 0.7}
                                                        70%|   | 2280/3250 [3:32:38<1:29:30,  5.54s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_4/tmp-checkpoint-2280
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2280I AM HERE AND I AM SAVING THE MODEL

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2280
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2280
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2280
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2280/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2280/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2280/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2280/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.6948, 'learning_rate': 2.0399860746074262e-05, 'epoch': 0.7}
{'loss': 1.1436, 'learning_rate': 2.0360897961118248e-05, 'epoch': 0.7}
{'loss': 0.643, 'learning_rate': 2.0321962905066748e-05, 'epoch': 0.7}
{'loss': 0.6788, 'learning_rate': 2.0283055614345532e-05, 'epoch': 0.7}
{'loss': 0.6659, 'learning_rate': 2.024417612535433e-05, 'epoch': 0.7}
 70%|   | 2281/3250 [3:32:44<1:37:15,  6.02s/it]                                                        70%|   | 2281/3250 [3:32:44<1:37:15,  6.02s/it] 70%|   | 2282/3250 [3:32:49<1:34:00,  5.83s/it]                                                        70%|   | 2282/3250 [3:32:49<1:34:00,  5.83s/it] 70%|   | 2283/3250 [3:32:55<1:31:48,  5.70s/it]                                                        70%|   | 2283/3250 [3:32:55<1:31:48,  5.70s/it] 70%|   | 2284/3250 [3:33:00<1:30:07,  5.60s/it]                                                        70%|   | 2284/3250 [3:33:00<1:30:07,  5.60s/it] 70%|   | 2285/3250 [3:33:05<1:28:57,  5.53s/it]                                                        70%|   | 2285/3250 [3:33:05<1:28:57,  5.53s/it] 70%|   | 2286/32{'loss': 0.6676, 'learning_rate': 2.020532447446693e-05, 'epoch': 0.7}
{'loss': 0.6277, 'learning_rate': 2.016650069803105e-05, 'epoch': 0.7}
{'loss': 0.7079, 'learning_rate': 2.012770483236832e-05, 'epoch': 0.7}
{'loss': 0.6986, 'learning_rate': 2.008893691377428e-05, 'epoch': 0.7}
{'loss': 0.6592, 'learning_rate': 2.005019697851832e-05, 'epoch': 0.7}
50 [3:33:11<1:28:10,  5.49s/it]                                                        70%|   | 2286/3250 [3:33:11<1:28:10,  5.49s/it] 70%|   | 2287/3250 [3:33:16<1:27:33,  5.46s/it]                                                        70%|   | 2287/3250 [3:33:16<1:27:33,  5.46s/it] 70%|   | 2288/3250 [3:33:22<1:27:19,  5.45s/it]                                                        70%|   | 2288/3250 [3:33:22<1:27:19,  5.45s/it] 70%|   | 2289/3250 [3:33:27<1:26:56,  5.43s/it]                                                        70%|   | 2289/3250 [3:33:27<1:26:56,  5.43s/it] 70%|   | 2290/3250 [3:33:32<1:26:41,  5.42s/it]                                                        70%|   | 2290/3250 [3:33:32<1:26:41,  5.42s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8523934483528137, 'eval_runtime': 1.3617, 'eval_samples_per_second': 8.813, 'eval_steps_per_second': 2.203, 'epoch': 0.7}
                                                        70%|   | 2290/3250 [3:33:34<1:26:41,  5.42s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_4/tmp-checkpoint-2290
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2290
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2290
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2290
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2290
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2290/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2290/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2290/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2290/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.6532, 'learning_rate': 2.001148506284361e-05, 'epoch': 0.7}
{'loss': 0.6596, 'learning_rate': 1.9972801202967162e-05, 'epoch': 0.71}
{'loss': 0.6503, 'learning_rate': 1.9934145435079702e-05, 'epoch': 0.71}
{'loss': 0.6583, 'learning_rate': 1.989551779534571e-05, 'epoch': 0.71}
{'loss': 0.6507, 'learning_rate': 1.985691831990333e-05, 'epoch': 0.71}
 70%|   | 2291/3250 [3:33:40<1:34:29,  5.91s/it]                                                        70%|   | 2291/3250 [3:33:40<1:34:29,  5.91s/it] 71%|   | 2292/3250 [3:33:45<1:31:50,  5.75s/it]                                                        71%|   | 2292/3250 [3:33:45<1:31:50,  5.75s/it] 71%|   | 2293/3250 [3:33:50<1:30:05,  5.65s/it]                                                        71%|   | 2293/3250 [3:33:50<1:30:05,  5.65s/it] 71%|   | 2294/3250 [3:33:56<1:28:45,  5.57s/it]                                                        71%|   | 2294/3250 [3:33:56<1:28:45,  5.57s/it] 71%|   | 2295/3250 [3:34:01<1:27:47,  5.52s/it]                                                        71%|   | 2295/3250 [3:34:01<1:27:47,  5.52s/it] 71%|   | 2296/32{'loss': 0.6459, 'learning_rate': 1.9818347044864328e-05, 'epoch': 0.71}
{'loss': 0.6614, 'learning_rate': 1.9779804006314147e-05, 'epoch': 0.71}
{'loss': 0.683, 'learning_rate': 1.9741289240311755e-05, 'epoch': 0.71}
{'loss': 0.6685, 'learning_rate': 1.9702802782889706e-05, 'epoch': 0.71}
{'loss': 0.6713, 'learning_rate': 1.9664344670054067e-05, 'epoch': 0.71}
50 [3:34:07<1:28:53,  5.59s/it]                                                        71%|   | 2296/3250 [3:34:07<1:28:53,  5.59s/it] 71%|   | 2297/3250 [3:34:12<1:27:48,  5.53s/it]                                                        71%|   | 2297/3250 [3:34:12<1:27:48,  5.53s/it] 71%|   | 2298/3250 [3:34:18<1:27:07,  5.49s/it]                                                        71%|   | 2298/3250 [3:34:18<1:27:07,  5.49s/it] 71%|   | 2299/3250 [3:34:23<1:26:28,  5.46s/it]                                                        71%|   | 2299/3250 [3:34:23<1:26:28,  5.46s/it] 71%|   | 2300/3250 [3:34:28<1:26:02,  5.43s/it]                                                        71%|   | 2300/3250 [3:34:28<1:26:02,  5.43s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8513746857643127, 'eval_runtime': 1.3693, 'eval_samples_per_second': 8.764, 'eval_steps_per_second': 2.191, 'epoch': 0.71}
                                                        71%|   | 2300/3250 [3:34:30<1:26:02,  5.43s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_4/tmp-checkpoint-2300
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2300
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2300
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2300
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2300
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2300/pytorch_model.bin
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2300/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2300/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2300/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.6703, 'learning_rate': 1.962591493778438e-05, 'epoch': 0.71}
{'loss': 0.6703, 'learning_rate': 1.9587513622033648e-05, 'epoch': 0.71}
{'loss': 0.6421, 'learning_rate': 1.9549140758728246e-05, 'epoch': 0.71}
{'loss': 0.6917, 'learning_rate': 1.951079638376798e-05, 'epoch': 0.71}
{'loss': 0.6657, 'learning_rate': 1.9472480533025984e-05, 'epoch': 0.71}
 71%|   | 2301/3250 [3:34:36<1:35:24,  6.03s/it]                                                        71%|   | 2301/3250 [3:34:36<1:35:24,  6.03s/it] 71%|   | 2302/3250 [3:34:41<1:32:18,  5.84s/it]                                                        71%|   | 2302/3250 [3:34:41<1:32:18,  5.84s/it] 71%|   | 2303/3250 [3:34:47<1:30:05,  5.71s/it]                                                        71%|   | 2303/3250 [3:34:47<1:30:05,  5.71s/it] 71%|   | 2304/3250 [3:34:52<1:28:29,  5.61s/it]                                                        71%|   | 2304/3250 [3:34:52<1:28:29,  5.61s/it] 71%|   | 2305/3250 [3:34:57<1:27:20,  5.55s/it]                                                        71%|   | 2305/3250 [3:34:57<1:27:20,  5.55s/it] 71%|   | 2306/32{'loss': 0.6492, 'learning_rate': 1.9434193242348708e-05, 'epoch': 0.71}
{'loss': 0.6489, 'learning_rate': 1.9395934547555878e-05, 'epoch': 0.71}
{'loss': 0.6187, 'learning_rate': 1.9357704484440498e-05, 'epoch': 0.71}
{'loss': 0.6887, 'learning_rate': 1.931950308876871e-05, 'epoch': 0.71}
{'loss': 0.6505, 'learning_rate': 1.9281330396279912e-05, 'epoch': 0.71}
50 [3:35:03<1:26:29,  5.50s/it]                                                        71%|   | 2306/3250 [3:35:03<1:26:29,  5.50s/it] 71%|   | 2307/3250 [3:35:08<1:25:52,  5.46s/it]                                                        71%|   | 2307/3250 [3:35:08<1:25:52,  5.46s/it] 71%|   | 2308/3250 [3:35:14<1:25:29,  5.44s/it]                                                        71%|   | 2308/3250 [3:35:14<1:25:29,  5.44s/it] 71%|   | 2309/3250 [3:35:19<1:25:08,  5.43s/it]                                                        71%|   | 2309/3250 [3:35:19<1:25:08,  5.43s/it] 71%|   | 2310/3250 [3:35:24<1:25:05,  5.43s/it]                                                        71%|   | 2310/3250 [3:35:24<1:25:05,  5.43s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8540770411491394, 'eval_runtime': 1.3688, 'eval_samples_per_second': 8.767, 'eval_steps_per_second': 2.192, 'epoch': 0.71}
                                                        71%|   | 2310/3250 [3:35:26<1:25:05,  5.43s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_4/tmp-checkpoint-2310
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2310I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2310

I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2310
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2310
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2310/pytorch_model.bin
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2310/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2310/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2310/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.7062, 'learning_rate': 1.9243186442686615e-05, 'epoch': 0.71}
{'loss': 1.1495, 'learning_rate': 1.920507126367448e-05, 'epoch': 0.71}
{'loss': 0.6581, 'learning_rate': 1.916698489490216e-05, 'epoch': 0.71}
{'loss': 0.6598, 'learning_rate': 1.9128927372001454e-05, 'epoch': 0.71}
{'loss': 0.6867, 'learning_rate': 1.9090898730577133e-05, 'epoch': 0.71}
 71%|   | 2311/3250 [3:35:31<1:32:46,  5.93s/it]                                                        71%|   | 2311/3250 [3:35:31<1:32:46,  5.93s/it] 71%|   | 2312/3250 [3:35:37<1:32:25,  5.91s/it]                                                        71%|   | 2312/3250 [3:35:37<1:32:25,  5.91s/it] 71%|   | 2313/3250 [3:35:43<1:29:52,  5.76s/it]                                                        71%|   | 2313/3250 [3:35:43<1:29:52,  5.76s/it] 71%|   | 2314/3250 [3:35:48<1:28:04,  5.65s/it]                                                        71%|   | 2314/3250 [3:35:48<1:28:04,  5.65s/it] 71%|   | 2315/3250 [3:35:54<1:26:45,  5.57s/it]                                                        71%|   | 2315/3250 [3:35:54<1:26:45,  5.57s/it] 71%|  | 2316/{'loss': 0.683, 'learning_rate': 1.905289900620692e-05, 'epoch': 0.71}
{'loss': 0.6351, 'learning_rate': 1.9014928234441525e-05, 'epoch': 0.71}
{'loss': 0.6427, 'learning_rate': 1.897698645080456e-05, 'epoch': 0.71}
{'loss': 0.7283, 'learning_rate': 1.893907369079252e-05, 'epoch': 0.71}
{'loss': 0.6882, 'learning_rate': 1.8901189989874745e-05, 'epoch': 0.71}
3250 [3:35:59<1:25:50,  5.51s/it]                                                        71%|  | 2316/3250 [3:35:59<1:25:50,  5.51s/it] 71%|  | 2317/3250 [3:36:04<1:25:10,  5.48s/it]                                                        71%|  | 2317/3250 [3:36:04<1:25:10,  5.48s/it] 71%|  | 2318/3250 [3:36:10<1:24:39,  5.45s/it]                                                        71%|  | 2318/3250 [3:36:10<1:24:39,  5.45s/it] 71%|  | 2319/3250 [3:36:15<1:24:19,  5.43s/it]                                                        71%|  | 2319/3250 [3:36:15<1:24:19,  5.43s/it] 71%|  | 2320/3250 [3:36:20<1:24:00,  5.42s/it]                                                        71%|  | 2320/3250 [3:36:20<1:24:00,  5.42s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8509541153907776, 'eval_runtime': 1.3707, 'eval_samples_per_second': 8.755, 'eval_steps_per_second': 2.189, 'epoch': 0.71}
                                                        71%|  | 2320/3250 [3:36:22<1:24:00,  5.42s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_4/tmp-checkpoint-2320
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2320I AM HERE AND I AM SAVING THE MODEL

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2320
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2320
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2320
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2320/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2320/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2320/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2320/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.6744, 'learning_rate': 1.886333538349337e-05, 'epoch': 0.71}
{'loss': 0.6125, 'learning_rate': 1.8825509907063327e-05, 'epoch': 0.71}
{'loss': 0.6701, 'learning_rate': 1.8787713595972306e-05, 'epoch': 0.71}
{'loss': 0.6696, 'learning_rate': 1.8749946485580693e-05, 'epoch': 0.72}
{'loss': 0.6568, 'learning_rate': 1.8712208611221572e-05, 'epoch': 0.72}
 71%|  | 2321/3250 [3:36:28<1:31:32,  5.91s/it]                                                        71%|  | 2321/3250 [3:36:28<1:31:32,  5.91s/it] 71%|  | 2322/3250 [3:36:33<1:28:59,  5.75s/it]                                                        71%|  | 2322/3250 [3:36:33<1:28:59,  5.75s/it] 71%|  | 2323/3250 [3:36:38<1:27:12,  5.64s/it]                                                        71%|  | 2323/3250 [3:36:38<1:27:12,  5.64s/it] 72%|  | 2324/3250 [3:36:44<1:25:54,  5.57s/it]                                                        72%|  | 2324/3250 [3:36:44<1:25:54,  5.57s/it] 72%|  | 2325/3250 [3:36:49<1:24:55,  5.51s/it]                                                        72%|  | 2325/3250 [3:36:49<1:24:55,  5.51s/it] 72%|{'loss': 0.6404, 'learning_rate': 1.8674500008200674e-05, 'epoch': 0.72}
{'loss': 0.6547, 'learning_rate': 1.8636820711796306e-05, 'epoch': 0.72}
{'loss': 0.6681, 'learning_rate': 1.8599170757259406e-05, 'epoch': 0.72}
{'loss': 0.6784, 'learning_rate': 1.856155017981345e-05, 'epoch': 0.72}
{'loss': 0.6674, 'learning_rate': 1.8523959014654407e-05, 'epoch': 0.72}
  | 2326/3250 [3:36:54<1:24:16,  5.47s/it]                                                        72%|  | 2326/3250 [3:36:54<1:24:16,  5.47s/it] 72%|  | 2327/3250 [3:37:00<1:23:46,  5.45s/it]                                                        72%|  | 2327/3250 [3:37:00<1:23:46,  5.45s/it] 72%|  | 2328/3250 [3:37:06<1:25:11,  5.54s/it]                                                        72%|  | 2328/3250 [3:37:06<1:25:11,  5.54s/it] 72%|  | 2329/3250 [3:37:11<1:24:26,  5.50s/it]                                                        72%|  | 2329/3250 [3:37:11<1:24:26,  5.50s/it] 72%|  | 2330/3250 [3:37:16<1:23:48,  5.47s/it]                                                        72%|  | 2330/3250 [3:37:16<1:23:48,  5.47s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8522170782089233, 'eval_runtime': 1.3757, 'eval_samples_per_second': 8.723, 'eval_steps_per_second': 2.181, 'epoch': 0.72}
                                                        72%|  | 2330/3250 [3:37:18<1:23:48,  5.47s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_4/tmp-checkpoint-2330
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2330I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2330

I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2330
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2330
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2330/pytorch_model.bin
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2330/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2330/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2330/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.6578, 'learning_rate': 1.8486397296950745e-05, 'epoch': 0.72}
{'loss': 0.6921, 'learning_rate': 1.8448865061843397e-05, 'epoch': 0.72}
{'loss': 0.6614, 'learning_rate': 1.8411362344445708e-05, 'epoch': 0.72}
{'loss': 0.661, 'learning_rate': 1.8373889179843374e-05, 'epoch': 0.72}
{'loss': 0.683, 'learning_rate': 1.833644560309447e-05, 'epoch': 0.72}
 72%|  | 2331/3250 [3:37:23<1:31:07,  5.95s/it]                                                        72%|  | 2331/3250 [3:37:23<1:31:07,  5.95s/it] 72%|  | 2332/3250 [3:37:29<1:28:29,  5.78s/it]                                                        72%|  | 2332/3250 [3:37:29<1:28:29,  5.78s/it] 72%|  | 2333/3250 [3:37:34<1:26:38,  5.67s/it]                                                        72%|  | 2333/3250 [3:37:34<1:26:38,  5.67s/it] 72%|  | 2334/3250 [3:37:40<1:25:12,  5.58s/it]                                                        72%|  | 2334/3250 [3:37:40<1:25:12,  5.58s/it] 72%|  | 2335/3250 [3:37:45<1:24:15,  5.53s/it]                                                        72%|  | 2335/3250 [3:37:45<1:24:15,  5.53s/it] 72%|{'loss': 0.6492, 'learning_rate': 1.8299031649229402e-05, 'epoch': 0.72}
{'loss': 0.6401, 'learning_rate': 1.8261647353250842e-05, 'epoch': 0.72}
{'loss': 0.6415, 'learning_rate': 1.8224292750133743e-05, 'epoch': 0.72}
{'loss': 0.6732, 'learning_rate': 1.8186967874825217e-05, 'epoch': 0.72}
{'loss': 0.6631, 'learning_rate': 1.8149672762244624e-05, 'epoch': 0.72}
  | 2336/3250 [3:37:50<1:23:36,  5.49s/it]                                                        72%|  | 2336/3250 [3:37:50<1:23:36,  5.49s/it] 72%|  | 2337/3250 [3:37:56<1:23:09,  5.47s/it]                                                        72%|  | 2337/3250 [3:37:56<1:23:09,  5.47s/it] 72%|  | 2338/3250 [3:38:01<1:22:45,  5.44s/it]                                                        72%|  | 2338/3250 [3:38:01<1:22:45,  5.44s/it] 72%|  | 2339/3250 [3:38:07<1:22:28,  5.43s/it]                                                        72%|  | 2339/3250 [3:38:07<1:22:28,  5.43s/it] 72%|  | 2340/3250 [3:38:12<1:22:13,  5.42s/it]                                                        72%|  | 2340/3250 [3:38:12<1:22:13,  5.42s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8545711040496826, 'eval_runtime': 1.3684, 'eval_samples_per_second': 8.77, 'eval_steps_per_second': 2.192, 'epoch': 0.72}
                                                        72%|  | 2340/3250 [3:38:13<1:22:13,  5.42s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_4/tmp-checkpoint-2340
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2340I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2340

I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2340
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2340
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2340/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2340/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2340/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2340/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.6792, 'learning_rate': 1.8112407447283465e-05, 'epoch': 0.72}
{'loss': 1.1909, 'learning_rate': 1.8075171964805354e-05, 'epoch': 0.72}
{'loss': 0.6109, 'learning_rate': 1.8037966349646e-05, 'epoch': 0.72}
{'loss': 0.6458, 'learning_rate': 1.8000790636613197e-05, 'epoch': 0.72}
{'loss': 0.6859, 'learning_rate': 1.7963644860486706e-05, 'epoch': 0.72}
 72%|  | 2341/3250 [3:38:19<1:30:12,  5.95s/it]                                                        72%|  | 2341/3250 [3:38:19<1:30:12,  5.95s/it] 72%|  | 2342/3250 [3:38:25<1:27:29,  5.78s/it]                                                        72%|  | 2342/3250 [3:38:25<1:27:29,  5.78s/it] 72%|  | 2343/3250 [3:38:30<1:25:34,  5.66s/it]                                                        72%|  | 2343/3250 [3:38:30<1:25:34,  5.66s/it] 72%|  | 2344/3250 [3:38:35<1:24:17,  5.58s/it]                                                        72%|  | 2344/3250 [3:38:35<1:24:17,  5.58s/it] 72%|  | 2345/3250 [3:38:41<1:25:33,  5.67s/it]                                                        72%|  | 2345/3250 [3:38:41<1:25:33,  5.67s/it] 72%|{'loss': 0.6905, 'learning_rate': 1.7926529056018298e-05, 'epoch': 0.72}
{'loss': 0.6417, 'learning_rate': 1.7889443257931737e-05, 'epoch': 0.72}
{'loss': 0.6535, 'learning_rate': 1.785238750092269e-05, 'epoch': 0.72}
{'loss': 0.7047, 'learning_rate': 1.7815361819658732e-05, 'epoch': 0.72}
{'loss': 0.6949, 'learning_rate': 1.777836624877929e-05, 'epoch': 0.72}
  | 2346/3250 [3:38:47<1:24:12,  5.59s/it]                                                        72%|  | 2346/3250 [3:38:47<1:24:12,  5.59s/it] 72%|  | 2347/3250 [3:38:52<1:23:12,  5.53s/it]                                                        72%|  | 2347/3250 [3:38:52<1:23:12,  5.53s/it] 72%|  | 2348/3250 [3:38:57<1:22:25,  5.48s/it]                                                        72%|  | 2348/3250 [3:38:57<1:22:25,  5.48s/it] 72%|  | 2349/3250 [3:39:03<1:21:52,  5.45s/it]                                                        72%|  | 2349/3250 [3:39:03<1:21:52,  5.45s/it] 72%|  | 2350/3250 [3:39:08<1:21:26,  5.43s/it]                                                        72%|  | 2350/3250 [3:39:08<1:21:26,  5.43s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8519141674041748, 'eval_runtime': 1.3649, 'eval_samples_per_second': 8.792, 'eval_steps_per_second': 2.198, 'epoch': 0.72}
                                                        72%|  | 2350/3250 [3:39:10<1:21:26,  5.43s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_4/tmp-checkpoint-2350
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2350I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in 
 /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2350
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2350
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2350
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2350/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2350/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2350/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2350/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.6551, 'learning_rate': 1.774140082289563e-05, 'epoch': 0.72}
{'loss': 0.624, 'learning_rate': 1.770446557659079e-05, 'epoch': 0.72}
{'loss': 0.6831, 'learning_rate': 1.76675605444196e-05, 'epoch': 0.72}
{'loss': 0.6604, 'learning_rate': 1.7630685760908622e-05, 'epoch': 0.72}
{'loss': 0.6745, 'learning_rate': 1.7593841260556103e-05, 'epoch': 0.72}
 72%|  | 2351/3250 [3:39:15<1:29:02,  5.94s/it]                                                        72%|  | 2351/3250 [3:39:15<1:29:02,  5.94s/it] 72%|  | 2352/3250 [3:39:21<1:26:32,  5.78s/it]                                                        72%|  | 2352/3250 [3:39:21<1:26:32,  5.78s/it] 72%|  | 2353/3250 [3:39:26<1:24:50,  5.67s/it]                                                        72%|  | 2353/3250 [3:39:26<1:24:50,  5.67s/it] 72%|  | 2354/3250 [3:39:32<1:23:25,  5.59s/it]                                                        72%|  | 2354/3250 [3:39:32<1:23:25,  5.59s/it] 72%|  | 2355/3250 [3:39:37<1:22:29,  5.53s/it]                                                        72%|  | 2355/3250 [3:39:37<1:22:29,  5.53s/it] 72%|{'loss': 0.6496, 'learning_rate': 1.7557027077832e-05, 'epoch': 0.72}
{'loss': 0.6526, 'learning_rate': 1.7520243247177824e-05, 'epoch': 0.73}
{'loss': 0.6639, 'learning_rate': 1.7483489803006776e-05, 'epoch': 0.73}
{'loss': 0.6792, 'learning_rate': 1.7446766779703576e-05, 'epoch': 0.73}
{'loss': 0.6704, 'learning_rate': 1.7410074211624518e-05, 'epoch': 0.73}
  | 2356/3250 [3:39:42<1:21:44,  5.49s/it]                                                        72%|  | 2356/3250 [3:39:42<1:21:44,  5.49s/it] 73%|  | 2357/3250 [3:39:48<1:21:14,  5.46s/it]                                                        73%|  | 2357/3250 [3:39:48<1:21:14,  5.46s/it] 73%|  | 2358/3250 [3:39:53<1:20:45,  5.43s/it]                                                        73%|  | 2358/3250 [3:39:53<1:20:45,  5.43s/it] 73%|  | 2359/3250 [3:39:58<1:20:28,  5.42s/it]                                                        73%|  | 2359/3250 [3:39:58<1:20:28,  5.42s/it] 73%|  | 2360/3250 [3:40:04<1:20:14,  5.41s/it]                                                        73%|  | 2360/3250 [3:40:04<1:20:14,  5.41s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8508010506629944, 'eval_runtime': 1.3723, 'eval_samples_per_second': 8.745, 'eval_steps_per_second': 2.186, 'epoch': 0.73}
                                                        73%|  | 2360/3250 [3:40:05<1:20:14,  5.41s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_4/tmp-checkpoint-2360
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2360
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2360
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2360
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2360
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2360/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2360/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2360/pytorch_model.binthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2360/pytorch_model.bin

/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.6813, 'learning_rate': 1.7373412133097372e-05, 'epoch': 0.73}
{'loss': 0.6582, 'learning_rate': 1.733678057842142e-05, 'epoch': 0.73}
{'loss': 0.6751, 'learning_rate': 1.730017958186735e-05, 'epoch': 0.73}
{'loss': 0.6286, 'learning_rate': 1.726360917767726e-05, 'epoch': 0.73}
{'loss': 0.709, 'learning_rate': 1.722706940006466e-05, 'epoch': 0.73}
 73%|  | 2361/3250 [3:40:11<1:29:39,  6.05s/it]                                                        73%|  | 2361/3250 [3:40:11<1:29:39,  6.05s/it] 73%|  | 2362/3250 [3:40:17<1:26:39,  5.85s/it]                                                        73%|  | 2362/3250 [3:40:17<1:26:39,  5.85s/it] 73%|  | 2363/3250 [3:40:22<1:24:46,  5.73s/it]                                                        73%|  | 2363/3250 [3:40:22<1:24:46,  5.73s/it] 73%|  | 2364/3250 [3:40:28<1:23:08,  5.63s/it]                                                        73%|  | 2364/3250 [3:40:28<1:23:08,  5.63s/it] 73%|  | 2365/3250 [3:40:33<1:22:00,  5.56s/it]                                                        73%|  | 2365/3250 [3:40:33<1:22:00,  5.56s/it] 73%|{'loss': 0.6344, 'learning_rate': 1.7190560283214395e-05, 'epoch': 0.73}
{'loss': 0.6507, 'learning_rate': 1.7154081861282617e-05, 'epoch': 0.73}
{'loss': 0.6425, 'learning_rate': 1.7117634168396774e-05, 'epoch': 0.73}
{'loss': 0.6538, 'learning_rate': 1.7081217238655563e-05, 'epoch': 0.73}
{'loss': 0.6751, 'learning_rate': 1.7044831106128866e-05, 'epoch': 0.73}
  | 2366/3250 [3:40:38<1:21:10,  5.51s/it]                                                        73%|  | 2366/3250 [3:40:38<1:21:10,  5.51s/it] 73%|  | 2367/3250 [3:40:44<1:20:32,  5.47s/it]                                                        73%|  | 2367/3250 [3:40:44<1:20:32,  5.47s/it] 73%|  | 2368/3250 [3:40:49<1:20:02,  5.45s/it]                                                        73%|  | 2368/3250 [3:40:49<1:20:02,  5.45s/it] 73%|  | 2369/3250 [3:40:55<1:19:41,  5.43s/it]                                                        73%|  | 2369/3250 [3:40:55<1:19:41,  5.43s/it] 73%|  | 2370/3250 [3:41:00<1:19:29,  5.42s/it]                                                        73%|  | 2370/3250 [3:41:00<1:19:29,  5.42s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8544967174530029, 'eval_runtime': 1.371, 'eval_samples_per_second': 8.753, 'eval_steps_per_second': 2.188, 'epoch': 0.73}
                                                        73%|  | 2370/3250 [3:41:01<1:19:29,  5.42s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_4/tmp-checkpoint-2370
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2370
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2370
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2370
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2370
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2370/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2370/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2370/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2370/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.658, 'learning_rate': 1.7008475804857804e-05, 'epoch': 0.73}
{'loss': 0.7005, 'learning_rate': 1.697215136885462e-05, 'epoch': 0.73}
{'loss': 1.137, 'learning_rate': 1.69358578321027e-05, 'epoch': 0.73}
{'loss': 0.6408, 'learning_rate': 1.689959522855652e-05, 'epoch': 0.73}
{'loss': 0.6849, 'learning_rate': 1.6863363592141618e-05, 'epoch': 0.73}
 73%|  | 2371/3250 [3:41:07<1:27:08,  5.95s/it]                                                        73%|  | 2371/3250 [3:41:07<1:27:08,  5.95s/it] 73%|  | 2372/3250 [3:41:13<1:24:39,  5.79s/it]                                                        73%|  | 2372/3250 [3:41:13<1:24:39,  5.79s/it] 73%|  | 2373/3250 [3:41:18<1:22:53,  5.67s/it]                                                        73%|  | 2373/3250 [3:41:18<1:22:53,  5.67s/it] 73%|  | 2374/3250 [3:41:23<1:21:46,  5.60s/it]                                                        73%|  | 2374/3250 [3:41:23<1:21:46,  5.60s/it] 73%|  | 2375/3250 [3:41:29<1:20:44,  5.54s/it]                                                        73%|  | 2375/3250 [3:41:29<1:20:44,  5.54s/it] 73%|{'loss': 0.6583, 'learning_rate': 1.6827162956754522e-05, 'epoch': 0.73}
{'loss': 0.6743, 'learning_rate': 1.6790993356262803e-05, 'epoch': 0.73}
{'loss': 0.6291, 'learning_rate': 1.675485482450499e-05, 'epoch': 0.73}
{'loss': 0.7062, 'learning_rate': 1.6718747395290552e-05, 'epoch': 0.73}
{'loss': 0.6993, 'learning_rate': 1.6682671102399805e-05, 'epoch': 0.73}
  | 2376/3250 [3:41:34<1:20:01,  5.49s/it]                                                        73%|  | 2376/3250 [3:41:34<1:20:01,  5.49s/it] 73%|  | 2377/3250 [3:41:40<1:19:28,  5.46s/it]                                                        73%|  | 2377/3250 [3:41:40<1:19:28,  5.46s/it] 73%|  | 2378/3250 [3:41:45<1:21:10,  5.58s/it]                                                        73%|  | 2378/3250 [3:41:45<1:21:10,  5.58s/it] 73%|  | 2379/3250 [3:41:51<1:20:13,  5.53s/it]                                                        73%|  | 2379/3250 [3:41:51<1:20:13,  5.53s/it] 73%|  | 2380/3250 [3:41:56<1:19:32,  5.49s/it]                                                        73%|  | 2380/3250 [3:41:56<1:19:32,  5.49s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8509733080863953, 'eval_runtime': 1.365, 'eval_samples_per_second': 8.791, 'eval_steps_per_second': 2.198, 'epoch': 0.73}
                                                        73%|  | 2380/3250 [3:41:58<1:19:32,  5.49s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_4/tmp-checkpoint-2380
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2380
/projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2380
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2380
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2380
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2380/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2380/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2380/pytorch_model.binthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2380/pytorch_model.bin

/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.6535, 'learning_rate': 1.6646625979584024e-05, 'epoch': 0.73}
{'loss': 0.6502, 'learning_rate': 1.6610612060565234e-05, 'epoch': 0.73}
{'loss': 0.6565, 'learning_rate': 1.6574629379036322e-05, 'epoch': 0.73}
{'loss': 0.6426, 'learning_rate': 1.6538677968660948e-05, 'epoch': 0.73}
{'loss': 0.6705, 'learning_rate': 1.6502757863073498e-05, 'epoch': 0.73}
 73%|  | 2381/3250 [3:42:03<1:26:45,  5.99s/it]                                                        73%|  | 2381/3250 [3:42:03<1:26:45,  5.99s/it] 73%|  | 2382/3250 [3:42:09<1:24:07,  5.81s/it]                                                        73%|  | 2382/3250 [3:42:09<1:24:07,  5.81s/it] 73%|  | 2383/3250 [3:42:14<1:22:09,  5.69s/it]                                                        73%|  | 2383/3250 [3:42:14<1:22:09,  5.69s/it] 73%|  | 2384/3250 [3:42:20<1:20:43,  5.59s/it]                                                        73%|  | 2384/3250 [3:42:20<1:20:43,  5.59s/it] 73%|  | 2385/3250 [3:42:25<1:19:45,  5.53s/it]                                                        73%|  | 2385/3250 [3:42:25<1:19:45,  5.53s/it] 73%|{'loss': 0.6552, 'learning_rate': 1.646686909587908e-05, 'epoch': 0.73}
{'loss': 0.6381, 'learning_rate': 1.6431011700653493e-05, 'epoch': 0.73}
{'loss': 0.6547, 'learning_rate': 1.639518571094315e-05, 'epoch': 0.73}
{'loss': 0.679, 'learning_rate': 1.6359391160265125e-05, 'epoch': 0.74}
{'loss': 0.6652, 'learning_rate': 1.632362808210705e-05, 'epoch': 0.74}
  | 2386/3250 [3:42:30<1:19:03,  5.49s/it]                                                        73%|  | 2386/3250 [3:42:30<1:19:03,  5.49s/it] 73%|  | 2387/3250 [3:42:36<1:18:32,  5.46s/it]                                                        73%|  | 2387/3250 [3:42:36<1:18:32,  5.46s/it] 73%|  | 2388/3250 [3:42:41<1:18:10,  5.44s/it]                                                        73%|  | 2388/3250 [3:42:41<1:18:10,  5.44s/it] 74%|  | 2389/3250 [3:42:47<1:17:52,  5.43s/it]                                                        74%|  | 2389/3250 [3:42:47<1:17:52,  5.43s/it] 74%|  | 2390/3250 [3:42:52<1:17:33,  5.41s/it]                                                        74%|  | 2390/3250 [3:42:52<1:17:33,  5.41s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8497575521469116, 'eval_runtime': 1.3641, 'eval_samples_per_second': 8.797, 'eval_steps_per_second': 2.199, 'epoch': 0.74}
                                                        74%|  | 2390/3250 [3:42:53<1:17:33,  5.41s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_4/tmp-checkpoint-2390
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2390
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2390
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2390
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2390
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2390/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2390/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2390/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2390/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.6661, 'learning_rate': 1.6287896509927135e-05, 'epoch': 0.74}
{'loss': 0.6702, 'learning_rate': 1.6252196477154095e-05, 'epoch': 0.74}
{'loss': 0.674, 'learning_rate': 1.6216528017187176e-05, 'epoch': 0.74}
{'loss': 0.6404, 'learning_rate': 1.618089116339601e-05, 'epoch': 0.74}
{'loss': 0.7086, 'learning_rate': 1.6145285949120738e-05, 'epoch': 0.74}
 74%|  | 2391/3250 [3:42:59<1:24:30,  5.90s/it]                                                        74%|  | 2391/3250 [3:42:59<1:24:30,  5.90s/it] 74%|  | 2392/3250 [3:43:04<1:22:15,  5.75s/it]                                                        74%|  | 2392/3250 [3:43:04<1:22:15,  5.75s/it] 74%|  | 2393/3250 [3:43:10<1:20:37,  5.65s/it]                                                        74%|  | 2393/3250 [3:43:10<1:20:37,  5.65s/it] 74%|  | 2394/3250 [3:43:16<1:21:07,  5.69s/it]                                                        74%|  | 2394/3250 [3:43:16<1:21:07,  5.69s/it] 74%|  | 2395/3250 [3:43:21<1:19:59,  5.61s/it]                                                        74%|  | 2395/3250 [3:43:21<1:19:59,  5.61s/it] 74%|{'loss': 0.6662, 'learning_rate': 1.6109712407671867e-05, 'epoch': 0.74}
{'loss': 0.6502, 'learning_rate': 1.6074170572330256e-05, 'epoch': 0.74}
{'loss': 0.6435, 'learning_rate': 1.6038660476347135e-05, 'epoch': 0.74}
{'loss': 0.621, 'learning_rate': 1.600318215294402e-05, 'epoch': 0.74}
{'loss': 0.6906, 'learning_rate': 1.596773563531273e-05, 'epoch': 0.74}
  | 2396/3250 [3:43:26<1:18:55,  5.55s/it]                                                        74%|  | 2396/3250 [3:43:26<1:18:55,  5.55s/it] 74%|  | 2397/3250 [3:43:32<1:18:10,  5.50s/it]                                                        74%|  | 2397/3250 [3:43:32<1:18:10,  5.50s/it] 74%|  | 2398/3250 [3:43:37<1:17:39,  5.47s/it]                                                        74%|  | 2398/3250 [3:43:37<1:17:39,  5.47s/it] 74%|  | 2399/3250 [3:43:43<1:17:16,  5.45s/it]                                                        74%|  | 2399/3250 [3:43:43<1:17:16,  5.45s/it] 74%|  | 2400/3250 [3:43:48<1:16:57,  5.43s/it]                                                        74%|  | 2400/3250 [3:43:48<1:16:57,  5.43s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8528534770011902, 'eval_runtime': 1.3694, 'eval_samples_per_second': 8.763, 'eval_steps_per_second': 2.191, 'epoch': 0.74}
                                                        74%|  | 2400/3250 [3:43:49<1:16:57,  5.43s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_4/tmp-checkpoint-2400
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2400
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2400
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2400
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2400
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2400/pytorch_model.bin
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2400/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2400/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2400/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.6382, 'learning_rate': 1.5932320956615265e-05, 'epoch': 0.74}
{'loss': 0.7121, 'learning_rate': 1.5896938149983908e-05, 'epoch': 0.74}
{'loss': 1.1497, 'learning_rate': 1.586158724852108e-05, 'epoch': 0.74}
{'loss': 0.6562, 'learning_rate': 1.582626828529938e-05, 'epoch': 0.74}
{'loss': 0.6502, 'learning_rate': 1.5790981293361516e-05, 'epoch': 0.74}
 74%|  | 2401/3250 [3:43:55<1:24:21,  5.96s/it]                                                        74%|  | 2401/3250 [3:43:55<1:24:21,  5.96s/it] 74%|  | 2402/3250 [3:44:01<1:21:50,  5.79s/it]                                                        74%|  | 2402/3250 [3:44:01<1:21:50,  5.79s/it] 74%|  | 2403/3250 [3:44:06<1:20:00,  5.67s/it]                                                        74%|  | 2403/3250 [3:44:06<1:20:00,  5.67s/it] 74%|  | 2404/3250 [3:44:11<1:18:45,  5.59s/it]                                                        74%|  | 2404/3250 [3:44:11<1:18:45,  5.59s/it] 74%|  | 2405/3250 [3:44:17<1:17:51,  5.53s/it]                                                        74%|  | 2405/3250 [3:44:17<1:17:51,  5.53s/it] 74%|{'loss': 0.68, 'learning_rate': 1.5755726305720266e-05, 'epoch': 0.74}
{'loss': 0.6706, 'learning_rate': 1.5720503355358495e-05, 'epoch': 0.74}
{'loss': 0.6257, 'learning_rate': 1.5685312475229085e-05, 'epoch': 0.74}
{'loss': 0.6357, 'learning_rate': 1.5650153698254916e-05, 'epoch': 0.74}
{'loss': 0.721, 'learning_rate': 1.561502705732883e-05, 'epoch': 0.74}
  | 2406/3250 [3:44:22<1:17:12,  5.49s/it]                                                        74%|  | 2406/3250 [3:44:22<1:17:12,  5.49s/it] 74%|  | 2407/3250 [3:44:28<1:16:43,  5.46s/it]                                                        74%|  | 2407/3250 [3:44:28<1:16:43,  5.46s/it] 74%|  | 2408/3250 [3:44:33<1:16:22,  5.44s/it]                                                        74%|  | 2408/3250 [3:44:33<1:16:22,  5.44s/it] 74%|  | 2409/3250 [3:44:38<1:16:05,  5.43s/it]                                                        74%|  | 2409/3250 [3:44:38<1:16:05,  5.43s/it] 74%|  | 2410/3250 [3:44:44<1:17:59,  5.57s/it]                                                        74%|  | 2410/3250 [3:44:44<1:17:59,  5.57s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.850342869758606, 'eval_runtime': 1.3714, 'eval_samples_per_second': 8.75, 'eval_steps_per_second': 2.187, 'epoch': 0.74}
                                                        74%|  | 2410/3250 [3:44:46<1:17:59,  5.57s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_4/tmp-checkpoint-2410
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2410
/projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2410
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2410
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2410
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2410/pytorch_model.bin
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2410/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2410/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2410/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.6763, 'learning_rate': 1.557993258531362e-05, 'epoch': 0.74}
{'loss': 0.6625, 'learning_rate': 1.5544870315041943e-05, 'epoch': 0.74}
{'loss': 0.6194, 'learning_rate': 1.550984027931639e-05, 'epoch': 0.74}
{'loss': 0.6597, 'learning_rate': 1.547484251090932e-05, 'epoch': 0.74}
{'loss': 0.6707, 'learning_rate': 1.5439877042562973e-05, 'epoch': 0.74}
 74%|  | 2411/3250 [3:44:51<1:24:14,  6.02s/it]                                                        74%|  | 2411/3250 [3:44:51<1:24:14,  6.02s/it] 74%|  | 2412/3250 [3:44:57<1:21:32,  5.84s/it]                                                        74%|  | 2412/3250 [3:44:57<1:21:32,  5.84s/it] 74%|  | 2413/3250 [3:45:02<1:19:29,  5.70s/it]                                                        74%|  | 2413/3250 [3:45:02<1:19:29,  5.70s/it] 74%|  | 2414/3250 [3:45:07<1:18:05,  5.60s/it]                                                        74%|  | 2414/3250 [3:45:07<1:18:05,  5.60s/it] 74%|  | 2415/3250 [3:45:13<1:17:02,  5.54s/it]                                                        74%|  | 2415/3250 [3:45:13<1:17:02,  5.54s/it] 74%|{'loss': 0.6523, 'learning_rate': 1.5404943906989334e-05, 'epoch': 0.74}
{'loss': 0.6436, 'learning_rate': 1.5370043136870148e-05, 'epoch': 0.74}
{'loss': 0.6544, 'learning_rate': 1.5335174764856908e-05, 'epoch': 0.74}
{'loss': 0.666, 'learning_rate': 1.5300338823570725e-05, 'epoch': 0.74}
{'loss': 0.6786, 'learning_rate': 1.526553534560244e-05, 'epoch': 0.74}
  | 2416/3250 [3:45:18<1:16:18,  5.49s/it]                                                        74%|  | 2416/3250 [3:45:18<1:16:18,  5.49s/it] 74%|  | 2417/3250 [3:45:24<1:15:43,  5.45s/it]                                                        74%|  | 2417/3250 [3:45:24<1:15:43,  5.45s/it] 74%|  | 2418/3250 [3:45:29<1:15:21,  5.43s/it]                                                        74%|  | 2418/3250 [3:45:29<1:15:21,  5.43s/it] 74%|  | 2419/3250 [3:45:34<1:15:05,  5.42s/it]                                                        74%|  | 2419/3250 [3:45:34<1:15:05,  5.42s/it] 74%|  | 2420/3250 [3:45:40<1:14:47,  5.41s/it]                                                        74%|  | 2420/3250 [3:45:40<1:14:47,  5.41s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8503013253211975, 'eval_runtime': 1.3634, 'eval_samples_per_second': 8.801, 'eval_steps_per_second': 2.2, 'epoch': 0.74}
                                                        74%|  | 2420/3250 [3:45:41<1:14:47,  5.41s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_4/tmp-checkpoint-2420
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2420
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2420
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2420
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2420
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2420/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2420/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2420/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2420/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.6606, 'learning_rate': 1.5230764363512485e-05, 'epoch': 0.74}
{'loss': 0.6526, 'learning_rate': 1.5196025909830913e-05, 'epoch': 0.75}
{'loss': 0.6869, 'learning_rate': 1.5161320017057346e-05, 'epoch': 0.75}
{'loss': 0.6583, 'learning_rate': 1.5126646717660897e-05, 'epoch': 0.75}
{'loss': 0.6728, 'learning_rate': 1.5092006044080242e-05, 'epoch': 0.75}
 74%|  | 2421/3250 [3:45:47<1:21:45,  5.92s/it]                                                        74%|  | 2421/3250 [3:45:47<1:21:45,  5.92s/it] 75%|  | 2422/3250 [3:45:52<1:19:32,  5.76s/it]                                                        75%|  | 2422/3250 [3:45:52<1:19:32,  5.76s/it] 75%|  | 2423/3250 [3:45:58<1:17:59,  5.66s/it]                                                        75%|  | 2423/3250 [3:45:58<1:17:59,  5.66s/it] 75%|  | 2424/3250 [3:46:03<1:16:50,  5.58s/it]                                                        75%|  | 2424/3250 [3:46:03<1:16:50,  5.58s/it] 75%|  | 2425/3250 [3:46:08<1:16:00,  5.53s/it]                                                        75%|  | 2425/3250 [3:46:08<1:16:00,  5.53s/it] 75%|{'loss': 0.6679, 'learning_rate': 1.5057398028723513e-05, 'epoch': 0.75}
{'loss': 0.6484, 'learning_rate': 1.5022822703968281e-05, 'epoch': 0.75}
{'loss': 0.6324, 'learning_rate': 1.498828010216155e-05, 'epoch': 0.75}
{'loss': 0.6425, 'learning_rate': 1.4953770255619714e-05, 'epoch': 0.75}
{'loss': 0.6667, 'learning_rate': 1.4919293196628492e-05, 'epoch': 0.75}
  | 2426/3250 [3:46:14<1:15:22,  5.49s/it]                                                        75%|  | 2426/3250 [3:46:14<1:15:22,  5.49s/it] 75%|  | 2427/3250 [3:46:20<1:16:38,  5.59s/it]                                                        75%|  | 2427/3250 [3:46:20<1:16:38,  5.59s/it] 75%|  | 2428/3250 [3:46:25<1:15:49,  5.53s/it]                                                        75%|  | 2428/3250 [3:46:25<1:15:49,  5.53s/it] 75%|  | 2429/3250 [3:46:31<1:15:15,  5.50s/it]                                                        75%|  | 2429/3250 [3:46:31<1:15:15,  5.50s/it] 75%|  | 2430/3250 [3:46:36<1:14:47,  5.47s/it]                                                        75%|  | 2430/3250 [3:46:36<1:14:47,  5.47s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8526031970977783, 'eval_runtime': 1.3804, 'eval_samples_per_second': 8.693, 'eval_steps_per_second': 2.173, 'epoch': 0.75}
                                                        75%|  | 2430/3250 [3:46:37<1:14:47,  5.47s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_4/tmp-checkpoint-2430
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2430
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2430
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2430
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2430
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2430/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2430/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2430/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2430/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.6487, 'learning_rate': 1.4884848957442931e-05, 'epoch': 0.75}
{'loss': 0.6765, 'learning_rate': 1.4850437570287406e-05, 'epoch': 0.75}
{'loss': 1.1901, 'learning_rate': 1.4816059067355536e-05, 'epoch': 0.75}
{'loss': 0.6161, 'learning_rate': 1.4781713480810184e-05, 'epoch': 0.75}
{'loss': 0.6331, 'learning_rate': 1.4747400842783404e-05, 'epoch': 0.75}
 75%|  | 2431/3250 [3:46:43<1:21:49,  5.99s/it]                                                        75%|  | 2431/3250 [3:46:43<1:21:49,  5.99s/it] 75%|  | 2432/3250 [3:46:49<1:19:20,  5.82s/it]                                                        75%|  | 2432/3250 [3:46:49<1:19:20,  5.82s/it] 75%|  | 2433/3250 [3:46:54<1:17:32,  5.69s/it]                                                        75%|  | 2433/3250 [3:46:54<1:17:32,  5.69s/it] 75%|  | 2434/3250 [3:46:59<1:16:17,  5.61s/it]                                                        75%|  | 2434/3250 [3:46:59<1:16:17,  5.61s/it] 75%|  | 2435/3250 [3:47:05<1:15:19,  5.55s/it]                                                        75%|  | 2435/3250 [3:47:05<1:15:19,  5.55s/it] 75%|{'loss': 0.6876, 'learning_rate': 1.4713121185376461e-05, 'epoch': 0.75}
{'loss': 0.6866, 'learning_rate': 1.4678874540659694e-05, 'epoch': 0.75}
{'loss': 0.6456, 'learning_rate': 1.4644660940672627e-05, 'epoch': 0.75}
{'loss': 0.6468, 'learning_rate': 1.4610480417423839e-05, 'epoch': 0.75}
{'loss': 0.6901, 'learning_rate': 1.4576333002890969e-05, 'epoch': 0.75}
  | 2436/3250 [3:47:10<1:14:36,  5.50s/it]                                                        75%|  | 2436/3250 [3:47:10<1:14:36,  5.50s/it] 75%|  | 2437/3250 [3:47:16<1:14:09,  5.47s/it]                                                        75%|  | 2437/3250 [3:47:16<1:14:09,  5.47s/it] 75%|  | 2438/3250 [3:47:21<1:13:50,  5.46s/it]                                                        75%|  | 2438/3250 [3:47:21<1:13:50,  5.46s/it] 75%|  | 2439/3250 [3:47:26<1:13:28,  5.44s/it]                                                        75%|  | 2439/3250 [3:47:26<1:13:28,  5.44s/it] 75%|  | 2440/3250 [3:47:32<1:13:14,  5.42s/it]                                                        75%|  | 2440/3250 [3:47:32<1:13:14,  5.42s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8509618043899536, 'eval_runtime': 1.3707, 'eval_samples_per_second': 8.755, 'eval_steps_per_second': 2.189, 'epoch': 0.75}
                                                        75%|  | 2440/3250 [3:47:33<1:13:14,  5.42s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_4/tmp-checkpoint-2440
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2440
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2440
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2440
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2440
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2440/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2440/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2440/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2440/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.6931, 'learning_rate': 1.454221872902069e-05, 'epoch': 0.75}
{'loss': 0.6531, 'learning_rate': 1.450813762772863e-05, 'epoch': 0.75}
{'loss': 0.6258, 'learning_rate': 1.4474089730899437e-05, 'epoch': 0.75}
{'loss': 0.6754, 'learning_rate': 1.444007507038666e-05, 'epoch': 0.75}
{'loss': 0.6587, 'learning_rate': 1.4406093678012766e-05, 'epoch': 0.75}
 75%|  | 2441/3250 [3:47:39<1:20:06,  5.94s/it]                                                        75%|  | 2441/3250 [3:47:39<1:20:06,  5.94s/it] 75%|  | 2442/3250 [3:47:44<1:17:48,  5.78s/it]                                                        75%|  | 2442/3250 [3:47:44<1:17:48,  5.78s/it] 75%|  | 2443/3250 [3:47:50<1:18:43,  5.85s/it]                                                        75%|  | 2443/3250 [3:47:50<1:18:43,  5.85s/it] 75%|  | 2444/3250 [3:47:56<1:16:51,  5.72s/it]                                                        75%|  | 2444/3250 [3:47:56<1:16:51,  5.72s/it] 75%|  | 2445/3250 [3:48:01<1:15:33,  5.63s/it]                                                        75%|  | 2445/3250 [3:48:01<1:15:33,  5.63s/it] 75%|{'loss': 0.6708, 'learning_rate': 1.4372145585569097e-05, 'epoch': 0.75}
{'loss': 0.6451, 'learning_rate': 1.4338230824815852e-05, 'epoch': 0.75}
{'loss': 0.6352, 'learning_rate': 1.4304349427482028e-05, 'epoch': 0.75}
{'loss': 0.654, 'learning_rate': 1.4270501425265386e-05, 'epoch': 0.75}
{'loss': 0.6737, 'learning_rate': 1.4236686849832498e-05, 'epoch': 0.75}
  | 2446/3250 [3:48:07<1:14:32,  5.56s/it]                                                        75%|  | 2446/3250 [3:48:07<1:14:32,  5.56s/it] 75%|  | 2447/3250 [3:48:12<1:13:47,  5.51s/it]                                                        75%|  | 2447/3250 [3:48:12<1:13:47,  5.51s/it] 75%|  | 2448/3250 [3:48:17<1:13:17,  5.48s/it]                                                        75%|  | 2448/3250 [3:48:17<1:13:17,  5.48s/it] 75%|  | 2449/3250 [3:48:23<1:12:54,  5.46s/it]                                                        75%|  | 2449/3250 [3:48:23<1:12:54,  5.46s/it] 75%|  | 2450/3250 [3:48:28<1:12:35,  5.44s/it]                                                        75%|  | 2450/3250 [3:48:28<1:12:35,  5.44s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8504911661148071, 'eval_runtime': 1.3645, 'eval_samples_per_second': 8.795, 'eval_steps_per_second': 2.199, 'epoch': 0.75}
                                                        75%|  | 2450/3250 [3:48:30<1:12:35,  5.44s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_4/tmp-checkpoint-2450
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2450
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2450
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2450
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2450
the pytorch model path is the pytorch model path is/projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2450/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2450/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2450/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2450/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.6687, 'learning_rate': 1.4202905732818633e-05, 'epoch': 0.75}
{'loss': 0.6757, 'learning_rate': 1.4169158105827768e-05, 'epoch': 0.75}
{'loss': 0.6539, 'learning_rate': 1.4135444000432541e-05, 'epoch': 0.75}
{'loss': 0.6691, 'learning_rate': 1.410176344817425e-05, 'epoch': 0.76}
{'loss': 0.6248, 'learning_rate': 1.4068116480562754e-05, 'epoch': 0.76}
 75%|  | 2451/3250 [3:48:35<1:18:53,  5.92s/it]                                                        75%|  | 2451/3250 [3:48:35<1:18:53,  5.92s/it] 75%|  | 2452/3250 [3:48:41<1:16:39,  5.76s/it]                                                        75%|  | 2452/3250 [3:48:41<1:16:39,  5.76s/it] 75%|  | 2453/3250 [3:48:46<1:15:07,  5.66s/it]                                                        75%|  | 2453/3250 [3:48:46<1:15:07,  5.66s/it] 76%|  | 2454/3250 [3:48:51<1:13:56,  5.57s/it]                                                        76%|  | 2454/3250 [3:48:51<1:13:56,  5.57s/it] 76%|  | 2455/3250 [3:48:57<1:13:02,  5.51s/it]                                                        76%|  | 2455/3250 [3:48:57<1:13:02,  5.51s/it] 76%|{'loss': 0.7061, 'learning_rate': 1.4034503129076531e-05, 'epoch': 0.76}
{'loss': 0.631, 'learning_rate': 1.4000923425162604e-05, 'epoch': 0.76}
{'loss': 0.6528, 'learning_rate': 1.3967377400236515e-05, 'epoch': 0.76}
{'loss': 0.6355, 'learning_rate': 1.3933865085682312e-05, 'epoch': 0.76}
{'loss': 0.6371, 'learning_rate': 1.3900386512852454e-05, 'epoch': 0.76}
  | 2456/3250 [3:49:02<1:12:24,  5.47s/it]                                                        76%|  | 2456/3250 [3:49:02<1:12:24,  5.47s/it] 76%|  | 2457/3250 [3:49:08<1:11:59,  5.45s/it]                                                        76%|  | 2457/3250 [3:49:08<1:11:59,  5.45s/it] 76%|  | 2458/3250 [3:49:13<1:11:39,  5.43s/it]                                                        76%|  | 2458/3250 [3:49:13<1:11:39,  5.43s/it] 76%|  | 2459/3250 [3:49:19<1:12:26,  5.50s/it]                                                        76%|  | 2459/3250 [3:49:19<1:12:26,  5.50s/it] 76%|  | 2460/3250 [3:49:24<1:11:57,  5.46s/it]                                                        76%|  | 2460/3250 [3:49:24<1:11:57,  5.46s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8515250086784363, 'eval_runtime': 1.3722, 'eval_samples_per_second': 8.745, 'eval_steps_per_second': 2.186, 'epoch': 0.76}
                                                        76%|  | 2460/3250 [3:49:25<1:11:57,  5.46s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_4/tmp-checkpoint-2460
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2460I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2460

I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2460
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2460
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2460/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2460/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2460/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2460/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.672, 'learning_rate': 1.3866941713067888e-05, 'epoch': 0.76}
{'loss': 0.6641, 'learning_rate': 1.3833530717617937e-05, 'epoch': 0.76}
{'loss': 0.6805, 'learning_rate': 1.3800153557760315e-05, 'epoch': 0.76}
{'loss': 1.1385, 'learning_rate': 1.376681026472108e-05, 'epoch': 0.76}
{'loss': 0.6454, 'learning_rate': 1.3733500869694572e-05, 'epoch': 0.76}
 76%|  | 2461/3250 [3:49:31<1:18:34,  5.98s/it]                                                        76%|  | 2461/3250 [3:49:31<1:18:34,  5.98s/it] 76%|  | 2462/3250 [3:49:37<1:16:11,  5.80s/it]                                                        76%|  | 2462/3250 [3:49:37<1:16:11,  5.80s/it] 76%|  | 2463/3250 [3:49:42<1:14:29,  5.68s/it]                                                        76%|  | 2463/3250 [3:49:42<1:14:29,  5.68s/it] 76%|  | 2464/3250 [3:49:47<1:13:10,  5.59s/it]                                                        76%|  | 2464/3250 [3:49:47<1:13:10,  5.59s/it] 76%|  | 2465/3250 [3:49:53<1:12:22,  5.53s/it]                                                        76%|  | 2465/3250 [3:49:53<1:12:22,  5.53s/it] 76%|{'loss': 0.685, 'learning_rate': 1.3700225403843469e-05, 'epoch': 0.76}
{'loss': 0.6687, 'learning_rate': 1.3666983898298657e-05, 'epoch': 0.76}
{'loss': 0.6675, 'learning_rate': 1.3633776384159285e-05, 'epoch': 0.76}
{'loss': 0.6281, 'learning_rate': 1.3600602892492693e-05, 'epoch': 0.76}
{'loss': 0.7084, 'learning_rate': 1.3567463454334389e-05, 'epoch': 0.76}
  | 2466/3250 [3:49:58<1:11:43,  5.49s/it]                                                        76%|  | 2466/3250 [3:49:58<1:11:43,  5.49s/it] 76%|  | 2467/3250 [3:50:04<1:11:15,  5.46s/it]                                                        76%|  | 2467/3250 [3:50:04<1:11:15,  5.46s/it] 76%|  | 2468/3250 [3:50:09<1:10:53,  5.44s/it]                                                        76%|  | 2468/3250 [3:50:09<1:10:53,  5.44s/it] 76%|  | 2469/3250 [3:50:14<1:10:36,  5.42s/it]                                                        76%|  | 2469/3250 [3:50:14<1:10:36,  5.42s/it] 76%|  | 2470/3250 [3:50:20<1:10:21,  5.41s/it]                                                        76%|  | 2470/3250 [3:50:20<1:10:21,  5.41s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8505251407623291, 'eval_runtime': 1.3616, 'eval_samples_per_second': 8.813, 'eval_steps_per_second': 2.203, 'epoch': 0.76}
                                                        76%|  | 2470/3250 [3:50:21<1:10:21,  5.41s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_4/tmp-checkpoint-2470
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2470
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2470
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2470
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2470
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2470/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2470/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2470/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2470/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.7031, 'learning_rate': 1.3534358100688016e-05, 'epoch': 0.76}
{'loss': 0.653, 'learning_rate': 1.3501286862525358e-05, 'epoch': 0.76}
{'loss': 0.6541, 'learning_rate': 1.3468249770786223e-05, 'epoch': 0.76}
{'loss': 0.6441, 'learning_rate': 1.3435246856378525e-05, 'epoch': 0.76}
{'loss': 0.6484, 'learning_rate': 1.34022781501782e-05, 'epoch': 0.76}
 76%|  | 2471/3250 [3:50:27<1:16:34,  5.90s/it]                                                        76%|  | 2471/3250 [3:50:27<1:16:34,  5.90s/it] 76%|  | 2472/3250 [3:50:32<1:14:30,  5.75s/it]                                                        76%|  | 2472/3250 [3:50:32<1:14:30,  5.75s/it] 76%|  | 2473/3250 [3:50:37<1:12:59,  5.64s/it]                                                        76%|  | 2473/3250 [3:50:37<1:12:59,  5.64s/it] 76%|  | 2474/3250 [3:50:43<1:11:54,  5.56s/it]                                                        76%|  | 2474/3250 [3:50:43<1:11:54,  5.56s/it] 76%|  | 2475/3250 [3:50:48<1:11:09,  5.51s/it]                                                        76%|  | 2475/3250 [3:50:48<1:11:09,  5.51s/it] 76%|{'loss': 0.6612, 'learning_rate': 1.3369343683029151e-05, 'epoch': 0.76}
{'loss': 0.6609, 'learning_rate': 1.3336443485743294e-05, 'epoch': 0.76}
{'loss': 0.6516, 'learning_rate': 1.3303577589100418e-05, 'epoch': 0.76}
{'loss': 0.6601, 'learning_rate': 1.327074602384828e-05, 'epoch': 0.76}
{'loss': 0.6922, 'learning_rate': 1.3237948820702495e-05, 'epoch': 0.76}
  | 2476/3250 [3:50:54<1:12:05,  5.59s/it]                                                        76%|  | 2476/3250 [3:50:54<1:12:05,  5.59s/it] 76%|  | 2477/3250 [3:50:59<1:11:15,  5.53s/it]                                                        76%|  | 2477/3250 [3:50:59<1:11:15,  5.53s/it] 76%|  | 2478/3250 [3:51:05<1:10:33,  5.48s/it]                                                        76%|  | 2478/3250 [3:51:05<1:10:33,  5.48s/it] 76%|  | 2479/3250 [3:51:10<1:10:01,  5.45s/it]                                                        76%|  | 2479/3250 [3:51:10<1:10:01,  5.45s/it] 76%|  | 2480/3250 [3:51:16<1:09:44,  5.43s/it]                                                        76%|  | 2480/3250 [3:51:16<1:09:44,  5.43s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8518147468566895, 'eval_runtime': 1.5921, 'eval_samples_per_second': 7.537, 'eval_steps_per_second': 1.884, 'epoch': 0.76}
                                                        76%|  | 2480/3250 [3:51:17<1:09:44,  5.43s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_4/tmp-checkpoint-2480
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2480
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2480
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2480
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2480
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2480/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2480/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2480/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2480/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.6443, 'learning_rate': 1.3205186010346548e-05, 'epoch': 0.76}
{'loss': 0.6773, 'learning_rate': 1.3172457623431706e-05, 'epoch': 0.76}
{'loss': 0.6826, 'learning_rate': 1.3139763690577073e-05, 'epoch': 0.76}
{'loss': 0.667, 'learning_rate': 1.3107104242369517e-05, 'epoch': 0.76}
{'loss': 0.6404, 'learning_rate': 1.3074479309363608e-05, 'epoch': 0.76}
 76%|  | 2481/3250 [3:51:23<1:17:12,  6.02s/it]                                                        76%|  | 2481/3250 [3:51:23<1:17:12,  6.02s/it] 76%|  | 2482/3250 [3:51:28<1:14:40,  5.83s/it]                                                        76%|  | 2482/3250 [3:51:28<1:14:40,  5.83s/it] 76%|  | 2483/3250 [3:51:34<1:12:54,  5.70s/it]                                                        76%|  | 2483/3250 [3:51:34<1:12:54,  5.70s/it] 76%|  | 2484/3250 [3:51:39<1:11:35,  5.61s/it]                                                        76%|  | 2484/3250 [3:51:39<1:11:35,  5.61s/it] 76%|  | 2485/3250 [3:51:45<1:10:38,  5.54s/it]                                                        76%|  | 2485/3250 [3:51:45<1:10:38,  5.54s/it] 76%|{'loss': 0.7109, 'learning_rate': 1.3041888922081657e-05, 'epoch': 0.76}
{'loss': 0.6635, 'learning_rate': 1.300933311101365e-05, 'epoch': 0.77}
{'loss': 0.6543, 'learning_rate': 1.2976811906617225e-05, 'epoch': 0.77}
{'loss': 0.643, 'learning_rate': 1.2944325339317637e-05, 'epoch': 0.77}
{'loss': 0.6198, 'learning_rate': 1.2911873439507765e-05, 'epoch': 0.77}
  | 2486/3250 [3:51:50<1:09:59,  5.50s/it]                                                        76%|  | 2486/3250 [3:51:50<1:09:59,  5.50s/it] 77%|  | 2487/3250 [3:51:55<1:09:27,  5.46s/it]                                                        77%|  | 2487/3250 [3:51:55<1:09:27,  5.46s/it] 77%|  | 2488/3250 [3:52:01<1:09:07,  5.44s/it]                                                        77%|  | 2488/3250 [3:52:01<1:09:07,  5.44s/it] 77%|  | 2489/3250 [3:52:06<1:08:51,  5.43s/it]                                                        77%|  | 2489/3250 [3:52:06<1:08:51,  5.43s/it] 77%|  | 2490/3250 [3:52:11<1:08:36,  5.42s/it]                                                        77%|  | 2490/3250 [3:52:11<1:08:36,  5.42s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8510029315948486, 'eval_runtime': 1.3599, 'eval_samples_per_second': 8.824, 'eval_steps_per_second': 2.206, 'epoch': 0.77}
                                                        77%|  | 2490/3250 [3:52:13<1:08:36,  5.42s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_4/tmp-checkpoint-2490
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2490
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2490
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2490
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2490
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2490/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2490/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2490/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2490/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.692, 'learning_rate': 1.2879456237547988e-05, 'epoch': 0.77}
{'loss': 0.6365, 'learning_rate': 1.2847073763766287e-05, 'epoch': 0.77}
{'loss': 0.7007, 'learning_rate': 1.2814726048458137e-05, 'epoch': 0.77}
{'loss': 1.1439, 'learning_rate': 1.2782413121886483e-05, 'epoch': 0.77}
{'loss': 0.6385, 'learning_rate': 1.2750135014281729e-05, 'epoch': 0.77}
 77%|  | 2491/3250 [3:52:19<1:14:47,  5.91s/it]                                                        77%|  | 2491/3250 [3:52:19<1:14:47,  5.91s/it] 77%|  | 2492/3250 [3:52:24<1:14:04,  5.86s/it]                                                        77%|  | 2492/3250 [3:52:24<1:14:04,  5.86s/it] 77%|  | 2493/3250 [3:52:30<1:12:16,  5.73s/it]                                                        77%|  | 2493/3250 [3:52:30<1:12:16,  5.73s/it] 77%|  | 2494/3250 [3:52:35<1:10:51,  5.62s/it]                                                        77%|  | 2494/3250 [3:52:35<1:10:51,  5.62s/it] 77%|  | 2495/3250 [3:52:40<1:09:51,  5.55s/it]                                                        77%|  | 2495/3250 [3:52:40<1:09:51,  5.55s/it] 77%|{'loss': 0.6629, 'learning_rate': 1.2717891755841722e-05, 'epoch': 0.77}
{'loss': 0.6873, 'learning_rate': 1.268568337673166e-05, 'epoch': 0.77}
{'loss': 0.6671, 'learning_rate': 1.2653509907084171e-05, 'epoch': 0.77}
{'loss': 0.6417, 'learning_rate': 1.2621371376999152e-05, 'epoch': 0.77}
{'loss': 0.6329, 'learning_rate': 1.2589267816543876e-05, 'epoch': 0.77}
  | 2496/3250 [3:52:46<1:09:06,  5.50s/it]                                                        77%|  | 2496/3250 [3:52:46<1:09:06,  5.50s/it] 77%|  | 2497/3250 [3:52:51<1:08:37,  5.47s/it]                                                        77%|  | 2497/3250 [3:52:51<1:08:37,  5.47s/it] 77%|  | 2498/3250 [3:52:57<1:08:18,  5.45s/it]                                                        77%|  | 2498/3250 [3:52:57<1:08:18,  5.45s/it] 77%|  | 2499/3250 [3:53:02<1:07:59,  5.43s/it]                                                        77%|  | 2499/3250 [3:53:02<1:07:59,  5.43s/it] 77%|  | 2500/3250 [3:53:07<1:07:44,  5.42s/it]                                                        77%|  | 2500/3250 [3:53:07<1:07:44,  5.42s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8509313464164734, 'eval_runtime': 1.6362, 'eval_samples_per_second': 7.334, 'eval_steps_per_second': 1.834, 'epoch': 0.77}
                                                        77%|  | 2500/3250 [3:53:09<1:07:44,  5.42s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_4/tmp-checkpoint-2500
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2500
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2500
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2500
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2500
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2500/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2500/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2500/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2500/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.7263, 'learning_rate': 1.2557199255752867e-05, 'epoch': 0.77}
{'loss': 0.6674, 'learning_rate': 1.2525165724627936e-05, 'epoch': 0.77}
{'loss': 0.6561, 'learning_rate': 1.249316725313806e-05, 'epoch': 0.77}
{'loss': 0.6109, 'learning_rate': 1.2461203871219473e-05, 'epoch': 0.77}
{'loss': 0.6649, 'learning_rate': 1.242927560877557e-05, 'epoch': 0.77}
 77%|  | 2501/3250 [3:53:15<1:15:20,  6.04s/it]                                                        77%|  | 2501/3250 [3:53:15<1:15:20,  6.04s/it] 77%|  | 2502/3250 [3:53:20<1:12:57,  5.85s/it]                                                        77%|  | 2502/3250 [3:53:20<1:12:57,  5.85s/it] 77%|  | 2503/3250 [3:53:26<1:11:13,  5.72s/it]                                                        77%|  | 2503/3250 [3:53:26<1:11:13,  5.72s/it] 77%|  | 2504/3250 [3:53:31<1:09:51,  5.62s/it]                                                        77%|  | 2504/3250 [3:53:31<1:09:51,  5.62s/it] 77%|  | 2505/3250 [3:53:36<1:08:44,  5.54s/it]                                                        77%|  | 2505/3250 [3:53:36<1:08:44,  5.54s/it] 77%|{'loss': 0.6656, 'learning_rate': 1.2397382495676874e-05, 'epoch': 0.77}
{'loss': 0.649, 'learning_rate': 1.2365524561761039e-05, 'epoch': 0.77}
{'loss': 0.6439, 'learning_rate': 1.2333701836832812e-05, 'epoch': 0.77}
{'loss': 0.6469, 'learning_rate': 1.2301914350663957e-05, 'epoch': 0.77}
{'loss': 0.6607, 'learning_rate': 1.2270162132993323e-05, 'epoch': 0.77}
  | 2506/3250 [3:53:42<1:07:58,  5.48s/it]                                                        77%|  | 2506/3250 [3:53:42<1:07:58,  5.48s/it] 77%|  | 2507/3250 [3:53:47<1:07:26,  5.45s/it]                                                        77%|  | 2507/3250 [3:53:47<1:07:26,  5.45s/it] 77%|  | 2508/3250 [3:53:53<1:06:59,  5.42s/it]                                                        77%|  | 2508/3250 [3:53:53<1:06:59,  5.42s/it] 77%|  | 2509/3250 [3:53:58<1:08:08,  5.52s/it]                                                        77%|  | 2509/3250 [3:53:58<1:08:08,  5.52s/it] 77%|  | 2510/3250 [3:54:04<1:07:30,  5.47s/it]                                                        77%|  | 2510/3250 [3:54:04<1:07:30,  5.47s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8512681126594543, 'eval_runtime': 1.3773, 'eval_samples_per_second': 8.713, 'eval_steps_per_second': 2.178, 'epoch': 0.77}
                                                        77%|  | 2510/3250 [3:54:05<1:07:30,  5.47s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_4/tmp-checkpoint-2510
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2510
/projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2510
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2510
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2510
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2510/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2510/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2510/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2510/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.6775, 'learning_rate': 1.223844521352674e-05, 'epoch': 0.77}
{'loss': 0.6568, 'learning_rate': 1.2206763621937023e-05, 'epoch': 0.77}
{'loss': 0.655, 'learning_rate': 1.2175117387863916e-05, 'epoch': 0.77}
{'loss': 0.6893, 'learning_rate': 1.2143506540914128e-05, 'epoch': 0.77}
{'loss': 0.6534, 'learning_rate': 1.2111931110661212e-05, 'epoch': 0.77}
 77%|  | 2511/3250 [3:54:11<1:13:22,  5.96s/it]                                                        77%|  | 2511/3250 [3:54:11<1:13:22,  5.96s/it] 77%|  | 2512/3250 [3:54:16<1:11:06,  5.78s/it]                                                        77%|  | 2512/3250 [3:54:16<1:11:06,  5.78s/it] 77%|  | 2513/3250 [3:54:21<1:09:29,  5.66s/it]                                                        77%|  | 2513/3250 [3:54:21<1:09:29,  5.66s/it] 77%|  | 2514/3250 [3:54:27<1:08:24,  5.58s/it]                                                        77%|  | 2514/3250 [3:54:27<1:08:24,  5.58s/it] 77%|  | 2515/3250 [3:54:32<1:07:32,  5.51s/it]                                                        77%|  | 2515/3250 [3:54:32<1:07:32,  5.51s/it] 77%|{'loss': 0.677, 'learning_rate': 1.2080391126645596e-05, 'epoch': 0.77}
{'loss': 0.6745, 'learning_rate': 1.2048886618374566e-05, 'epoch': 0.77}
{'loss': 0.6451, 'learning_rate': 1.2017417615322219e-05, 'epoch': 0.77}
{'loss': 0.6317, 'learning_rate': 1.1985984146929413e-05, 'epoch': 0.78}
{'loss': 0.6392, 'learning_rate': 1.1954586242603783e-05, 'epoch': 0.78}
  | 2516/3250 [3:54:38<1:06:58,  5.48s/it]                                                        77%|  | 2516/3250 [3:54:38<1:06:58,  5.48s/it] 77%|  | 2517/3250 [3:54:43<1:06:30,  5.44s/it]                                                        77%|  | 2517/3250 [3:54:43<1:06:30,  5.44s/it] 77%|  | 2518/3250 [3:54:48<1:06:10,  5.42s/it]                                                        77%|  | 2518/3250 [3:54:48<1:06:10,  5.42s/it] 78%|  | 2519/3250 [3:54:54<1:05:52,  5.41s/it]                                                        78%|  | 2519/3250 [3:54:54<1:05:52,  5.41s/it] 78%|  | 2520/3250 [3:54:59<1:05:40,  5.40s/it]                                                        78%|  | 2520/3250 [3:54:59<1:05:40,  5.40s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8511155247688293, 'eval_runtime': 1.5867, 'eval_samples_per_second': 7.563, 'eval_steps_per_second': 1.891, 'epoch': 0.78}
                                                        78%|  | 2520/3250 [3:55:01<1:05:40,  5.40s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_4/tmp-checkpoint-2520
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  I AM HERE AND I AM SAVING THE MODEL
/projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2520the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2520

I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2520
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2520
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2520/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2520/pytorch_model.binthe pytorch model path is
 /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2520/pytorch_model.binthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2520/pytorch_model.bin

/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.6687, 'learning_rate': 1.1923223931719695e-05, 'epoch': 0.78}
{'loss': 0.6514, 'learning_rate': 1.1891897243618182e-05, 'epoch': 0.78}
{'loss': 0.6738, 'learning_rate': 1.1860606207606978e-05, 'epoch': 0.78}
{'loss': 1.1864, 'learning_rate': 1.1829350852960464e-05, 'epoch': 0.78}
{'loss': 0.6167, 'learning_rate': 1.1798131208919627e-05, 'epoch': 0.78}
 78%|  | 2521/3250 [3:55:06<1:12:25,  5.96s/it]                                                        78%|  | 2521/3250 [3:55:06<1:12:25,  5.96s/it] 78%|  | 2522/3250 [3:55:12<1:10:11,  5.78s/it]                                                        78%|  | 2522/3250 [3:55:12<1:10:11,  5.78s/it] 78%|  | 2523/3250 [3:55:17<1:08:36,  5.66s/it]                                                        78%|  | 2523/3250 [3:55:17<1:08:36,  5.66s/it] 78%|  | 2524/3250 [3:55:23<1:07:27,  5.58s/it]                                                        78%|  | 2524/3250 [3:55:23<1:07:27,  5.58s/it] 78%|  | 2525/3250 [3:55:28<1:07:31,  5.59s/it]                                                        78%|  | 2525/3250 [3:55:28<1:07:31,  5.59s/it] 78%|{'loss': 0.6329, 'learning_rate': 1.176694730469206e-05, 'epoch': 0.78}
{'loss': 0.6773, 'learning_rate': 1.1735799169451888e-05, 'epoch': 0.78}
{'loss': 0.6777, 'learning_rate': 1.1704686832339812e-05, 'epoch': 0.78}
{'loss': 0.6469, 'learning_rate': 1.1673610322463014e-05, 'epoch': 0.78}
{'loss': 0.643, 'learning_rate': 1.164256966889517e-05, 'epoch': 0.78}
  | 2526/3250 [3:55:33<1:06:36,  5.52s/it]                                                        78%|  | 2526/3250 [3:55:33<1:06:36,  5.52s/it] 78%|  | 2527/3250 [3:55:39<1:05:58,  5.47s/it]                                                        78%|  | 2527/3250 [3:55:39<1:05:58,  5.47s/it] 78%|  | 2528/3250 [3:55:44<1:05:30,  5.44s/it]                                                        78%|  | 2528/3250 [3:55:44<1:05:30,  5.44s/it] 78%|  | 2529/3250 [3:55:50<1:05:11,  5.42s/it]                                                        78%|  | 2529/3250 [3:55:50<1:05:11,  5.42s/it] 78%|  | 2530/3250 [3:55:55<1:04:52,  5.41s/it]                                                        78%|  | 2530/3250 [3:55:55<1:04:52,  5.41s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8512702584266663, 'eval_runtime': 1.3747, 'eval_samples_per_second': 8.729, 'eval_steps_per_second': 2.182, 'epoch': 0.78}
                                                        78%|  | 2530/3250 [3:55:56<1:04:52,  5.41s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_4/tmp-checkpoint-2530
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2530I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2530

I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2530
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2530
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2530/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2530/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2530/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2530/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.6884, 'learning_rate': 1.1611564900676408e-05, 'epoch': 0.78}
{'loss': 0.6798, 'learning_rate': 1.1580596046813302e-05, 'epoch': 0.78}
{'loss': 0.6375, 'learning_rate': 1.1549663136278788e-05, 'epoch': 0.78}
{'loss': 0.6116, 'learning_rate': 1.1518766198012187e-05, 'epoch': 0.78}
{'loss': 0.6661, 'learning_rate': 1.148790526091918e-05, 'epoch': 0.78}
 78%|  | 2531/3250 [3:56:02<1:11:24,  5.96s/it]                                                        78%|  | 2531/3250 [3:56:02<1:11:24,  5.96s/it] 78%|  | 2532/3250 [3:56:08<1:09:13,  5.79s/it]                                                        78%|  | 2532/3250 [3:56:08<1:09:13,  5.79s/it] 78%|  | 2533/3250 [3:56:13<1:07:42,  5.67s/it]                                                        78%|  | 2533/3250 [3:56:13<1:07:42,  5.67s/it] 78%|  | 2534/3250 [3:56:18<1:06:32,  5.58s/it]                                                        78%|  | 2534/3250 [3:56:18<1:06:32,  5.58s/it] 78%|  | 2535/3250 [3:56:24<1:05:44,  5.52s/it]                                                        78%|  | 2535/3250 [3:56:24<1:05:44,  5.52s/it] 78%|{'loss': 0.6539, 'learning_rate': 1.1457080353871769e-05, 'epoch': 0.78}
{'loss': 0.6607, 'learning_rate': 1.1426291505708236e-05, 'epoch': 0.78}
{'loss': 0.6434, 'learning_rate': 1.1395538745233131e-05, 'epoch': 0.78}
{'loss': 0.6523, 'learning_rate': 1.136482210121726e-05, 'epoch': 0.78}
{'loss': 0.667, 'learning_rate': 1.1334141602397597e-05, 'epoch': 0.78}
  | 2536/3250 [3:56:29<1:05:08,  5.47s/it]                                                        78%|  | 2536/3250 [3:56:29<1:05:08,  5.47s/it] 78%|  | 2537/3250 [3:56:34<1:04:40,  5.44s/it]                                                        78%|  | 2537/3250 [3:56:34<1:04:40,  5.44s/it] 78%|  | 2538/3250 [3:56:40<1:04:18,  5.42s/it]                                                        78%|  | 2538/3250 [3:56:40<1:04:18,  5.42s/it] 78%|  | 2539/3250 [3:56:45<1:04:04,  5.41s/it]                                                        78%|  | 2539/3250 [3:56:45<1:04:04,  5.41s/it] 78%|  | 2540/3250 [3:56:51<1:03:52,  5.40s/it]                                                        78%|  | 2540/3250 [3:56:51<1:03:52,  5.40s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8512294292449951, 'eval_runtime': 1.3668, 'eval_samples_per_second': 8.779, 'eval_steps_per_second': 2.195, 'epoch': 0.78}
                                                        78%|  | 2540/3250 [3:56:52<1:03:52,  5.40s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_4/tmp-checkpoint-2540
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2540
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2540
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2540
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2540
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2540/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2540/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2540/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2540/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.6766, 'learning_rate': 1.1303497277477337e-05, 'epoch': 0.78}
{'loss': 0.6667, 'learning_rate': 1.127288915512582e-05, 'epoch': 0.78}
{'loss': 0.6762, 'learning_rate': 1.1242317263978525e-05, 'epoch': 0.78}
{'loss': 0.6588, 'learning_rate': 1.1211781632637041e-05, 'epoch': 0.78}
{'loss': 0.6806, 'learning_rate': 1.1181282289668993e-05, 'epoch': 0.78}
 78%|  | 2541/3250 [3:56:58<1:11:00,  6.01s/it]                                                        78%|  | 2541/3250 [3:56:58<1:11:00,  6.01s/it] 78%|  | 2542/3250 [3:57:03<1:08:33,  5.81s/it]                                                        78%|  | 2542/3250 [3:57:03<1:08:33,  5.81s/it] 78%|  | 2543/3250 [3:57:09<1:06:52,  5.68s/it]                                                        78%|  | 2543/3250 [3:57:09<1:06:52,  5.68s/it] 78%|  | 2544/3250 [3:57:14<1:05:42,  5.58s/it]                                                        78%|  | 2544/3250 [3:57:14<1:05:42,  5.58s/it] 78%|  | 2545/3250 [3:57:19<1:04:51,  5.52s/it]                                                        78%|  | 2545/3250 [3:57:19<1:04:51,  5.52s/it] 78%|{'loss': 0.6279, 'learning_rate': 1.1150819263608097e-05, 'epoch': 0.78}
{'loss': 0.704, 'learning_rate': 1.112039258295408e-05, 'epoch': 0.78}
{'loss': 0.6392, 'learning_rate': 1.109000227617269e-05, 'epoch': 0.78}
{'loss': 0.6515, 'learning_rate': 1.1059648371695585e-05, 'epoch': 0.78}
{'loss': 0.638, 'learning_rate': 1.102933089792042e-05, 'epoch': 0.78}
  | 2546/3250 [3:57:25<1:04:13,  5.47s/it]                                                        78%|  | 2546/3250 [3:57:25<1:04:13,  5.47s/it] 78%|  | 2547/3250 [3:57:30<1:03:42,  5.44s/it]                                                        78%|  | 2547/3250 [3:57:30<1:03:42,  5.44s/it] 78%|  | 2548/3250 [3:57:36<1:03:20,  5.41s/it]                                                        78%|  | 2548/3250 [3:57:36<1:03:20,  5.41s/it] 78%|  | 2549/3250 [3:57:41<1:03:04,  5.40s/it]                                                        78%|  | 2549/3250 [3:57:41<1:03:04,  5.40s/it] 78%|  | 2550/3250 [3:57:46<1:02:52,  5.39s/it]                                                        78%|  | 2550/3250 [3:57:46<1:02:52,  5.39s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8496423363685608, 'eval_runtime': 1.5922, 'eval_samples_per_second': 7.537, 'eval_steps_per_second': 1.884, 'epoch': 0.78}
                                                        78%|  | 2550/3250 [3:57:48<1:02:52,  5.39s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_4/tmp-checkpoint-2550
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2550
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2550
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2550
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2550
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2550/pytorch_model.bin
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2550/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2550/pytorch_model.binthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2550/pytorch_model.bin

/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.6356, 'learning_rate': 1.0999049883210771e-05, 'epoch': 0.78}
{'loss': 0.6696, 'learning_rate': 1.0968805355896044e-05, 'epoch': 0.79}
{'loss': 0.6515, 'learning_rate': 1.0938597344271579e-05, 'epoch': 0.79}
{'loss': 0.6885, 'learning_rate': 1.090842587659851e-05, 'epoch': 0.79}
{'loss': 1.1299, 'learning_rate': 1.087829098110381e-05, 'epoch': 0.79}
 78%|  | 2551/3250 [3:57:54<1:09:39,  5.98s/it]                                                        78%|  | 2551/3250 [3:57:54<1:09:39,  5.98s/it] 79%|  | 2552/3250 [3:57:59<1:07:26,  5.80s/it]                                                        79%|  | 2552/3250 [3:57:59<1:07:26,  5.80s/it] 79%|  | 2553/3250 [3:58:04<1:05:51,  5.67s/it]                                                        79%|  | 2553/3250 [3:58:04<1:05:51,  5.67s/it] 79%|  | 2554/3250 [3:58:10<1:04:46,  5.58s/it]                                                        79%|  | 2554/3250 [3:58:10<1:04:46,  5.58s/it] 79%|  | 2555/3250 [3:58:15<1:03:53,  5.52s/it]                                                        79%|  | 2555/3250 [3:58:15<1:03:53,  5.52s/it] 79%|{'loss': 0.6439, 'learning_rate': 1.0848192685980219e-05, 'epoch': 0.79}
{'loss': 0.6737, 'learning_rate': 1.0818131019386252e-05, 'epoch': 0.79}
{'loss': 0.6641, 'learning_rate': 1.0788106009446119e-05, 'epoch': 0.79}
{'loss': 0.6627, 'learning_rate': 1.0758117684249769e-05, 'epoch': 0.79}
{'loss': 0.6362, 'learning_rate': 1.0728166071852835e-05, 'epoch': 0.79}
  | 2556/3250 [3:58:20<1:03:15,  5.47s/it]                                                        79%|  | 2556/3250 [3:58:20<1:03:15,  5.47s/it] 79%|  | 2557/3250 [3:58:26<1:02:49,  5.44s/it]                                                        79%|  | 2557/3250 [3:58:26<1:02:49,  5.44s/it] 79%|  | 2558/3250 [3:58:31<1:03:20,  5.49s/it]                                                        79%|  | 2558/3250 [3:58:31<1:03:20,  5.49s/it] 79%|  | 2559/3250 [3:58:37<1:02:50,  5.46s/it]                                                        79%|  | 2559/3250 [3:58:37<1:02:50,  5.46s/it] 79%|  | 2560/3250 [3:58:42<1:02:26,  5.43s/it]                                                        79%|  | 2560/3250 [3:58:42<1:02:26,  5.43s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8514178395271301, 'eval_runtime': 1.3788, 'eval_samples_per_second': 8.703, 'eval_steps_per_second': 2.176, 'epoch': 0.79}
                                                        79%|  | 2560/3250 [3:58:44<1:02:26,  5.43s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_4/tmp-checkpoint-2560
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2560
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2560
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2560
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2560
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2560/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2560/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2560/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2560/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.6958, 'learning_rate': 1.0698251200276583e-05, 'epoch': 0.79}
{'loss': 0.6989, 'learning_rate': 1.0668373097507923e-05, 'epoch': 0.79}
{'loss': 0.6526, 'learning_rate': 1.063853179149934e-05, 'epoch': 0.79}
{'loss': 0.6455, 'learning_rate': 1.060872731016892e-05, 'epoch': 0.79}
{'loss': 0.6448, 'learning_rate': 1.0578959681400297e-05, 'epoch': 0.79}
 79%|  | 2561/3250 [3:58:49<1:08:10,  5.94s/it]                                                        79%|  | 2561/3250 [3:58:49<1:08:10,  5.94s/it] 79%|  | 2562/3250 [3:58:55<1:06:09,  5.77s/it]                                                        79%|  | 2562/3250 [3:58:55<1:06:09,  5.77s/it] 79%|  | 2563/3250 [3:59:00<1:04:41,  5.65s/it]                                                        79%|  | 2563/3250 [3:59:00<1:04:41,  5.65s/it] 79%|  | 2564/3250 [3:59:05<1:03:36,  5.56s/it]                                                        79%|  | 2564/3250 [3:59:05<1:03:36,  5.56s/it] 79%|  | 2565/3250 [3:59:11<1:02:52,  5.51s/it]                                                        79%|  | 2565/3250 [3:59:11<1:02:52,  5.51s/it] 79%|{'loss': 0.6528, 'learning_rate': 1.05492289330426e-05, 'epoch': 0.79}
{'loss': 0.6523, 'learning_rate': 1.0519535092910482e-05, 'epoch': 0.79}
{'loss': 0.6524, 'learning_rate': 1.0489878188784063e-05, 'epoch': 0.79}
{'loss': 0.6405, 'learning_rate': 1.0460258248408911e-05, 'epoch': 0.79}
{'loss': 0.6568, 'learning_rate': 1.0430675299495973e-05, 'epoch': 0.79}
  | 2566/3250 [3:59:16<1:02:19,  5.47s/it]                                                        79%|  | 2566/3250 [3:59:16<1:02:19,  5.47s/it] 79%|  | 2567/3250 [3:59:22<1:01:53,  5.44s/it]                                                        79%|  | 2567/3250 [3:59:22<1:01:53,  5.44s/it] 79%|  | 2568/3250 [3:59:27<1:01:36,  5.42s/it]                                                        79%|  | 2568/3250 [3:59:27<1:01:36,  5.42s/it] 79%|  | 2569/3250 [3:59:32<1:01:21,  5.41s/it]                                                        79%|  | 2569/3250 [3:59:32<1:01:21,  5.41s/it] 79%|  | 2570/3250 [3:59:38<1:01:06,  5.39s/it]                                                        79%|  | 2570/3250 [3:59:38<1:01:06,  5.39s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8511298298835754, 'eval_runtime': 1.3665, 'eval_samples_per_second': 8.782, 'eval_steps_per_second': 2.195, 'epoch': 0.79}
                                                        79%|  | 2570/3250 [3:59:39<1:01:06,  5.39s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_4/tmp-checkpoint-2570
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2570
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2570
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2570
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2570
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2570/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2570/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2570/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2570/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.6836, 'learning_rate': 1.040112936972164e-05, 'epoch': 0.79}
{'loss': 0.6468, 'learning_rate': 1.0371620486727651e-05, 'epoch': 0.79}
{'loss': 0.6629, 'learning_rate': 1.0342148678121073e-05, 'epoch': 0.79}
{'loss': 0.6706, 'learning_rate': 1.0312713971474307e-05, 'epoch': 0.79}
{'loss': 0.6569, 'learning_rate': 1.0283316394325054e-05, 'epoch': 0.79}
 79%|  | 2571/3250 [3:59:45<1:07:48,  5.99s/it]                                                        79%|  | 2571/3250 [3:59:45<1:07:48,  5.99s/it] 79%|  | 2572/3250 [3:59:50<1:05:34,  5.80s/it]                                                        79%|  | 2572/3250 [3:59:50<1:05:34,  5.80s/it] 79%|  | 2573/3250 [3:59:56<1:03:59,  5.67s/it]                                                        79%|  | 2573/3250 [3:59:56<1:03:59,  5.67s/it] 79%|  | 2574/3250 [4:00:02<1:04:13,  5.70s/it]                                                        79%|  | 2574/3250 [4:00:02<1:04:13,  5.70s/it] 79%|  | 2575/3250 [4:00:07<1:02:59,  5.60s/it]                                                        79%|  | 2575/3250 [4:00:07<1:02:59,  5.60s/it] 79%|{'loss': 0.6306, 'learning_rate': 1.0253955974176215e-05, 'epoch': 0.79}
{'loss': 0.7033, 'learning_rate': 1.0224632738496003e-05, 'epoch': 0.79}
{'loss': 0.6501, 'learning_rate': 1.0195346714717813e-05, 'epoch': 0.79}
{'loss': 0.6315, 'learning_rate': 1.0166097930240215e-05, 'epoch': 0.79}
{'loss': 0.633, 'learning_rate': 1.0136886412426972e-05, 'epoch': 0.79}
  | 2576/3250 [4:00:12<1:02:08,  5.53s/it]                                                        79%|  | 2576/3250 [4:00:12<1:02:08,  5.53s/it] 79%|  | 2577/3250 [4:00:18<1:01:30,  5.48s/it]                                                        79%|  | 2577/3250 [4:00:18<1:01:30,  5.48s/it] 79%|  | 2578/3250 [4:00:23<1:01:01,  5.45s/it]                                                        79%|  | 2578/3250 [4:00:23<1:01:01,  5.45s/it] 79%|  | 2579/3250 [4:00:28<1:00:37,  5.42s/it]                                                        79%|  | 2579/3250 [4:00:28<1:00:37,  5.42s/it] 79%|  | 2580/3250 [4:00:34<1:00:22,  5.41s/it]                                                        79%|  | 2580/3250 [4:00:34<1:00:22,  5.41s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8512319326400757, 'eval_runtime': 1.3727, 'eval_samples_per_second': 8.742, 'eval_steps_per_second': 2.185, 'epoch': 0.79}
                                                        79%|  | 2580/3250 [4:00:35<1:00:22,  5.41s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_4/tmp-checkpoint-2580
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2580
/projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2580
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2580
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2580
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2580/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2580/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2580/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2580/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.631, 'learning_rate': 1.0107712188606933e-05, 'epoch': 0.79}
{'loss': 0.6848, 'learning_rate': 1.0078575286074116e-05, 'epoch': 0.79}
{'loss': 0.6307, 'learning_rate': 1.004947573208756e-05, 'epoch': 0.79}
{'loss': 0.6921, 'learning_rate': 1.002041355387141e-05, 'epoch': 0.8}
{'loss': 1.1469, 'learning_rate': 9.991388778614824e-06, 'epoch': 0.8}
 79%|  | 2581/3250 [4:00:41<1:06:01,  5.92s/it]                                                        79%|  | 2581/3250 [4:00:41<1:06:01,  5.92s/it] 79%|  | 2582/3250 [4:00:46<1:04:06,  5.76s/it]                                                        79%|  | 2582/3250 [4:00:46<1:04:06,  5.76s/it] 79%|  | 2583/3250 [4:00:52<1:02:44,  5.64s/it]                                                        79%|  | 2583/3250 [4:00:52<1:02:44,  5.64s/it] 80%|  | 2584/3250 [4:00:57<1:01:43,  5.56s/it]                                                        80%|  | 2584/3250 [4:00:57<1:01:43,  5.56s/it] 80%|  | 2585/3250 [4:01:02<1:00:57,  5.50s/it]                                                        80%|  | 2585/3250 [4:01:02<1:00:57,  5.50s/it] 80%|{'loss': 0.629, 'learning_rate': 9.962401433471985e-06, 'epoch': 0.8}
{'loss': 0.6567, 'learning_rate': 9.933451545562044e-06, 'epoch': 0.8}
{'loss': 0.6798, 'learning_rate': 9.904539141969093e-06, 'epoch': 0.8}
{'loss': 0.6617, 'learning_rate': 9.875664249742183e-06, 'epoch': 0.8}
{'loss': 0.6345, 'learning_rate': 9.84682689589526e-06, 'epoch': 0.8}
  | 2586/3250 [4:01:08<1:00:26,  5.46s/it]                                                        80%|  | 2586/3250 [4:01:08<1:00:26,  5.46s/it] 80%|  | 2587/3250 [4:01:13<1:00:04,  5.44s/it]                                                        80%|  | 2587/3250 [4:01:13<1:00:04,  5.44s/it] 80%|  | 2588/3250 [4:01:18<59:45,  5.42s/it]                                                        80%|  | 2588/3250 [4:01:18<59:45,  5.42s/it] 80%|  | 2589/3250 [4:01:24<59:31,  5.40s/it]                                                      80%|  | 2589/3250 [4:01:24<59:31,  5.40s/it] 80%|  | 2590/3250 [4:01:29<59:19,  5.39s/it]                                                      80%|  | 2590/3250 [4:01:29<59:19,  5.39s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8502619862556458, 'eval_runtime': 1.5978, 'eval_samples_per_second': 7.51, 'eval_steps_per_second': 1.878, 'epoch': 0.8}
                                                      80%|  | 2590/3250 [4:01:31<59:19,  5.39s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_4/tmp-checkpoint-2590
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2590
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2590
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2590
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2590
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2590/pytorch_model.bin
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2590/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2590/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2590/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.6383, 'learning_rate': 9.818027107407151e-06, 'epoch': 0.8}
{'loss': 0.726, 'learning_rate': 9.789264911221546e-06, 'epoch': 0.8}
{'loss': 0.6669, 'learning_rate': 9.760540334246965e-06, 'epoch': 0.8}
{'loss': 0.6546, 'learning_rate': 9.731853403356705e-06, 'epoch': 0.8}
{'loss': 0.6088, 'learning_rate': 9.703204145388879e-06, 'epoch': 0.8}
 80%|  | 2591/3250 [4:01:37<1:06:46,  6.08s/it]                                                        80%|  | 2591/3250 [4:01:37<1:06:46,  6.08s/it] 80%|  | 2592/3250 [4:01:42<1:04:22,  5.87s/it]                                                        80%|  | 2592/3250 [4:01:42<1:04:22,  5.87s/it] 80%|  | 2593/3250 [4:01:48<1:02:39,  5.72s/it]                                                        80%|  | 2593/3250 [4:01:48<1:02:39,  5.72s/it] 80%|  | 2594/3250 [4:01:53<1:01:25,  5.62s/it]                                                        80%|  | 2594/3250 [4:01:53<1:01:25,  5.62s/it] 80%|  | 2595/3250 [4:01:58<1:00:32,  5.55s/it]                                                        80%|  | 2595/3250 [4:01:58<1:00:32,  5.55s/it] 80%|{'loss': 0.6546, 'learning_rate': 9.674592587146336e-06, 'epoch': 0.8}
{'loss': 0.6727, 'learning_rate': 9.646018755396664e-06, 'epoch': 0.8}
{'loss': 0.6439, 'learning_rate': 9.617482676872164e-06, 'epoch': 0.8}
{'loss': 0.6443, 'learning_rate': 9.588984378269783e-06, 'epoch': 0.8}
{'loss': 0.6461, 'learning_rate': 9.560523886251171e-06, 'epoch': 0.8}
  | 2596/3250 [4:02:04<59:54,  5.50s/it]                                                        80%|  | 2596/3250 [4:02:04<59:54,  5.50s/it] 80%|  | 2597/3250 [4:02:09<59:22,  5.46s/it]                                                      80%|  | 2597/3250 [4:02:09<59:22,  5.46s/it] 80%|  | 2598/3250 [4:02:15<59:01,  5.43s/it]                                                      80%|  | 2598/3250 [4:02:15<59:01,  5.43s/it] 80%|  | 2599/3250 [4:02:20<58:42,  5.41s/it]                                                      80%|  | 2599/3250 [4:02:20<58:42,  5.41s/it] 80%|  | 2600/3250 [4:02:25<58:27,  5.40s/it]                                                      80%|  | 2600/3250 [4:02:25<58:27,  5.40s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8493253588676453, 'eval_runtime': 1.3729, 'eval_samples_per_second': 8.74, 'eval_steps_per_second': 2.185, 'epoch': 0.8}
                                                      80%|  | 2600/3250 [4:02:27<58:27,  5.40s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_4/tmp-checkpoint-2600
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2600
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2600
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2600
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2600
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2600/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2600/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2600/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2600/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.6661, 'learning_rate': 9.532101227442552e-06, 'epoch': 0.8}
{'loss': 0.6821, 'learning_rate': 9.5037164284348e-06, 'epoch': 0.8}
{'loss': 0.6529, 'learning_rate': 9.475369515783355e-06, 'epoch': 0.8}
{'loss': 0.6734, 'learning_rate': 9.447060516008211e-06, 'epoch': 0.8}
{'loss': 0.6763, 'learning_rate': 9.418789455593906e-06, 'epoch': 0.8}
 80%|  | 2601/3250 [4:02:32<1:04:02,  5.92s/it]                                                        80%|  | 2601/3250 [4:02:32<1:04:02,  5.92s/it] 80%|  | 2602/3250 [4:02:38<1:02:06,  5.75s/it]                                                        80%|  | 2602/3250 [4:02:38<1:02:06,  5.75s/it] 80%|  | 2603/3250 [4:02:43<1:00:43,  5.63s/it]                                                        80%|  | 2603/3250 [4:02:43<1:00:43,  5.63s/it] 80%|  | 2604/3250 [4:02:48<59:46,  5.55s/it]                                                        80%|  | 2604/3250 [4:02:48<59:46,  5.55s/it] 80%|  | 2605/3250 [4:02:54<59:05,  5.50s/it]                                                      80%|  | 2605/3250 [4:02:54<59:05,  5.50s/it] 80%|{'loss': 0.6495, 'learning_rate': 9.39055636098945e-06, 'epoch': 0.8}
{'loss': 0.6719, 'learning_rate': 9.362361258608365e-06, 'epoch': 0.8}
{'loss': 0.6677, 'learning_rate': 9.334204174828614e-06, 'epoch': 0.8}
{'loss': 0.6331, 'learning_rate': 9.30608513599261e-06, 'epoch': 0.8}
{'loss': 0.6321, 'learning_rate': 9.27800416840715e-06, 'epoch': 0.8}
  | 2606/3250 [4:02:59<58:33,  5.46s/it]                                                      80%|  | 2606/3250 [4:02:59<58:33,  5.46s/it] 80%|  | 2607/3250 [4:03:05<59:25,  5.55s/it]                                                      80%|  | 2607/3250 [4:03:05<59:25,  5.55s/it] 80%|  | 2608/3250 [4:03:10<58:43,  5.49s/it]                                                      80%|  | 2608/3250 [4:03:10<58:43,  5.49s/it] 80%|  | 2609/3250 [4:03:16<58:14,  5.45s/it]                                                      80%|  | 2609/3250 [4:03:16<58:14,  5.45s/it] 80%|  | 2610/3250 [4:03:21<57:50,  5.42s/it]                                                      80%|  | 2610/3250 [4:03:21<57:50,  5.42s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8503414988517761, 'eval_runtime': 1.3727, 'eval_samples_per_second': 8.742, 'eval_steps_per_second': 2.185, 'epoch': 0.8}
                                                      80%|  | 2610/3250 [4:03:22<57:50,  5.42s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_4/tmp-checkpoint-2610
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2610I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2610

I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2610
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2610
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2610/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2610/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2610/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2610/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.6356, 'learning_rate': 9.249961298343435e-06, 'epoch': 0.8}
{'loss': 0.6738, 'learning_rate': 9.221956552036992e-06, 'epoch': 0.8}
{'loss': 0.6445, 'learning_rate': 9.193989955687715e-06, 'epoch': 0.8}
{'loss': 0.68, 'learning_rate': 9.166061535459798e-06, 'epoch': 0.8}
{'loss': 1.1862, 'learning_rate': 9.138171317481697e-06, 'epoch': 0.8}
 80%|  | 2611/3250 [4:03:28<1:03:53,  6.00s/it]                                                        80%|  | 2611/3250 [4:03:28<1:03:53,  6.00s/it] 80%|  | 2612/3250 [4:03:34<1:01:46,  5.81s/it]                                                        80%|  | 2612/3250 [4:03:34<1:01:46,  5.81s/it] 80%|  | 2613/3250 [4:03:39<1:00:15,  5.68s/it]                                                        80%|  | 2613/3250 [4:03:39<1:00:15,  5.68s/it] 80%|  | 2614/3250 [4:03:44<59:10,  5.58s/it]                                                        80%|  | 2614/3250 [4:03:44<59:10,  5.58s/it] 80%|  | 2615/3250 [4:03:50<58:19,  5.51s/it]                                                      80%|  | 2615/3250 [4:03:50<58:19,  5.51s/it] 80%|{'loss': 0.6071, 'learning_rate': 9.11031932784618e-06, 'epoch': 0.8}
{'loss': 0.6306, 'learning_rate': 9.082505592610174e-06, 'epoch': 0.81}
{'loss': 0.667, 'learning_rate': 9.054730137794886e-06, 'epoch': 0.81}
{'loss': 0.6719, 'learning_rate': 9.026992989385669e-06, 'epoch': 0.81}
{'loss': 0.6425, 'learning_rate': 8.999294173332058e-06, 'epoch': 0.81}
  | 2616/3250 [4:03:55<57:45,  5.47s/it]                                                      80%|  | 2616/3250 [4:03:55<57:45,  5.47s/it] 81%|  | 2617/3250 [4:04:01<57:18,  5.43s/it]                                                      81%|  | 2617/3250 [4:04:01<57:18,  5.43s/it] 81%|  | 2618/3250 [4:04:06<56:59,  5.41s/it]                                                      81%|  | 2618/3250 [4:04:06<56:59,  5.41s/it] 81%|  | 2619/3250 [4:04:11<56:47,  5.40s/it]                                                      81%|  | 2619/3250 [4:04:11<56:47,  5.40s/it] 81%|  | 2620/3250 [4:04:17<56:36,  5.39s/it]                                                      81%|  | 2620/3250 [4:04:17<56:36,  5.39s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8516800403594971, 'eval_runtime': 1.5976, 'eval_samples_per_second': 7.511, 'eval_steps_per_second': 1.878, 'epoch': 0.81}
                                                      81%|  | 2620/3250 [4:04:18<56:36,  5.39s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_4/tmp-checkpoint-2620
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2620
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2620
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2620
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2620
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2620/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2620/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2620/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2620/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.6453, 'learning_rate': 8.971633715547717e-06, 'epoch': 0.81}
{'loss': 0.6896, 'learning_rate': 8.944011641910432e-06, 'epoch': 0.81}
{'loss': 0.6992, 'learning_rate': 8.916427978262082e-06, 'epoch': 0.81}
{'loss': 0.6346, 'learning_rate': 8.888882750408578e-06, 'epoch': 0.81}
{'loss': 0.6165, 'learning_rate': 8.861375984119918e-06, 'epoch': 0.81}
 81%|  | 2621/3250 [4:04:24<1:02:31,  5.96s/it]                                                        81%|  | 2621/3250 [4:04:24<1:02:31,  5.96s/it] 81%|  | 2622/3250 [4:04:29<1:00:31,  5.78s/it]                                                        81%|  | 2622/3250 [4:04:29<1:00:31,  5.78s/it] 81%|  | 2623/3250 [4:04:35<1:00:15,  5.77s/it]                                                        81%|  | 2623/3250 [4:04:35<1:00:15,  5.77s/it] 81%|  | 2624/3250 [4:04:40<58:56,  5.65s/it]                                                        81%|  | 2624/3250 [4:04:40<58:56,  5.65s/it] 81%|  | 2625/3250 [4:04:46<57:55,  5.56s/it]                                                      81%|  | 2625/3250 [4:04:46<57:55,  5.56s/it] 81%|{'loss': 0.6745, 'learning_rate': 8.83390770513009e-06, 'epoch': 0.81}
{'loss': 0.6505, 'learning_rate': 8.80647793913708e-06, 'epoch': 0.81}
{'loss': 0.6562, 'learning_rate': 8.779086711802847e-06, 'epoch': 0.81}
{'loss': 0.6359, 'learning_rate': 8.751734048753306e-06, 'epoch': 0.81}
{'loss': 0.6438, 'learning_rate': 8.724419975578257e-06, 'epoch': 0.81}
  | 2626/3250 [4:04:51<57:14,  5.50s/it]                                                      81%|  | 2626/3250 [4:04:51<57:14,  5.50s/it] 81%|  | 2627/3250 [4:04:57<56:43,  5.46s/it]                                                      81%|  | 2627/3250 [4:04:57<56:43,  5.46s/it] 81%|  | 2628/3250 [4:05:02<56:20,  5.43s/it]                                                      81%|  | 2628/3250 [4:05:02<56:20,  5.43s/it] 81%|  | 2629/3250 [4:05:07<56:02,  5.42s/it]                                                      81%|  | 2629/3250 [4:05:07<56:02,  5.42s/it] 81%|  | 2630/3250 [4:05:13<55:45,  5.40s/it]                                                      81%|  | 2630/3250 [4:05:13<55:45,  5.40s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8491368293762207, 'eval_runtime': 1.3708, 'eval_samples_per_second': 8.754, 'eval_steps_per_second': 2.188, 'epoch': 0.81}
                                                      81%|  | 2630/3250 [4:05:14<55:45,  5.40s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_4/tmp-checkpoint-2630
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2630
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2630
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2630
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2630
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2630/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2630/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2630/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2630/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.6584, 'learning_rate': 8.697144517831435e-06, 'epoch': 0.81}
{'loss': 0.6814, 'learning_rate': 8.669907701030428e-06, 'epoch': 0.81}
{'loss': 0.6568, 'learning_rate': 8.64270955065669e-06, 'epoch': 0.81}
{'loss': 0.6823, 'learning_rate': 8.615550092155478e-06, 'epoch': 0.81}
{'loss': 0.6517, 'learning_rate': 8.588429350935857e-06, 'epoch': 0.81}
 81%|  | 2631/3250 [4:05:20<1:00:47,  5.89s/it]                                                        81%|  | 2631/3250 [4:05:20<1:00:47,  5.89s/it] 81%|  | 2632/3250 [4:05:25<59:04,  5.74s/it]                                                        81%|  | 2632/3250 [4:05:25<59:04,  5.74s/it] 81%|  | 2633/3250 [4:05:30<57:48,  5.62s/it]                                                      81%|  | 2633/3250 [4:05:30<57:48,  5.62s/it] 81%|  | 2634/3250 [4:05:36<56:54,  5.54s/it]                                                      81%|  | 2634/3250 [4:05:36<56:54,  5.54s/it] 81%|  | 2635/3250 [4:05:41<56:17,  5.49s/it]                                                      81%|  | 2635/3250 [4:05:41<56:17,  5.49s/it] 81%|  | 2636/32{'loss': 0.6705, 'learning_rate': 8.561347352370703e-06, 'epoch': 0.81}
{'loss': 0.6234, 'learning_rate': 8.534304121796582e-06, 'epoch': 0.81}
{'loss': 0.7046, 'learning_rate': 8.507299684513848e-06, 'epoch': 0.81}
{'loss': 0.6451, 'learning_rate': 8.480334065786532e-06, 'epoch': 0.81}
{'loss': 0.649, 'learning_rate': 8.45340729084237e-06, 'epoch': 0.81}
50 [4:05:46<55:49,  5.46s/it]                                                      81%|  | 2636/3250 [4:05:46<55:49,  5.46s/it] 81%|  | 2637/3250 [4:05:52<55:27,  5.43s/it]                                                      81%|  | 2637/3250 [4:05:52<55:27,  5.43s/it] 81%|  | 2638/3250 [4:05:57<55:10,  5.41s/it]                                                      81%|  | 2638/3250 [4:05:57<55:10,  5.41s/it] 81%|  | 2639/3250 [4:06:03<54:55,  5.39s/it]                                                      81%|  | 2639/3250 [4:06:03<54:55,  5.39s/it] 81%|  | 2640/3250 [4:06:08<55:30,  5.46s/it]                                                      81%|  | 2640/3250 [4:06:08<55:30,  5.46s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8493925929069519, 'eval_runtime': 1.3715, 'eval_samples_per_second': 8.749, 'eval_steps_per_second': 2.187, 'epoch': 0.81}
                                                      81%|  | 2640/3250 [4:06:10<55:30,  5.46s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_4/tmp-checkpoint-2640
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  I AM HERE AND I AM SAVING THE MODEL/projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2640
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2640

I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2640
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2640
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2640/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2640/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2640/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2640/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.6441, 'learning_rate': 8.426519384872733e-06, 'epoch': 0.81}
{'loss': 0.6306, 'learning_rate': 8.399670373032664e-06, 'epoch': 0.81}
{'loss': 0.659, 'learning_rate': 8.372860280440759e-06, 'epoch': 0.81}
{'loss': 0.667, 'learning_rate': 8.346089132179257e-06, 'epoch': 0.81}
{'loss': 0.6796, 'learning_rate': 8.319356953293944e-06, 'epoch': 0.81}
 81%| | 2641/3250 [4:06:15<1:00:18,  5.94s/it]                                                        81%| | 2641/3250 [4:06:15<1:00:18,  5.94s/it] 81%| | 2642/3250 [4:06:21<58:26,  5.77s/it]                                                        81%| | 2642/3250 [4:06:21<58:26,  5.77s/it] 81%| | 2643/3250 [4:06:26<57:08,  5.65s/it]                                                      81%| | 2643/3250 [4:06:26<57:08,  5.65s/it] 81%| | 2644/3250 [4:06:31<56:12,  5.57s/it]                                                      81%| | 2644/3250 [4:06:31<56:12,  5.57s/it] 81%| | 2645/3250 [4:06:37<55:31,  5.51s/it]                                                      81%| | 2645/3250 [4:06:37<55:31,  5.51s/it] 81%|{'loss': 1.1243, 'learning_rate': 8.292663768794145e-06, 'epoch': 0.81}
{'loss': 0.6403, 'learning_rate': 8.266009603652724e-06, 'epoch': 0.81}
{'loss': 0.6779, 'learning_rate': 8.239394482805996e-06, 'epoch': 0.81}
{'loss': 0.6631, 'learning_rate': 8.21281843115379e-06, 'epoch': 0.82}
{'loss': 0.6571, 'learning_rate': 8.186281473559381e-06, 'epoch': 0.82}
 | 2646/3250 [4:06:42<54:58,  5.46s/it]                                                      81%| | 2646/3250 [4:06:42<54:58,  5.46s/it] 81%| | 2647/3250 [4:06:47<54:36,  5.43s/it]                                                      81%| | 2647/3250 [4:06:47<54:36,  5.43s/it] 81%| | 2648/3250 [4:06:53<54:18,  5.41s/it]                                                      81%| | 2648/3250 [4:06:53<54:18,  5.41s/it] 82%| | 2649/3250 [4:06:58<54:05,  5.40s/it]                                                      82%| | 2649/3250 [4:06:58<54:05,  5.40s/it] 82%| | 2650/3250 [4:07:04<53:58,  5.40s/it]                                                      82%| | 2650/3250 [4:07:04<53:58,  5.40s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8507262468338013, 'eval_runtime': 1.3703, 'eval_samples_per_second': 8.757, 'eval_steps_per_second': 2.189, 'epoch': 0.82}
                                                      82%| | 2650/3250 [4:07:05<53:58,  5.40s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_4/tmp-checkpoint-2650
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2650I AM HERE AND I AM SAVING THE MODEL

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2650
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2650
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2650
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2650/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2650/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2650/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2650/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.639, 'learning_rate': 8.159783634849427e-06, 'epoch': 0.82}
{'loss': 0.6903, 'learning_rate': 8.13332493981404e-06, 'epoch': 0.82}
{'loss': 0.692, 'learning_rate': 8.106905413206689e-06, 'epoch': 0.82}
{'loss': 0.6455, 'learning_rate': 8.080525079744212e-06, 'epoch': 0.82}
{'loss': 0.6477, 'learning_rate': 8.054183964106738e-06, 'epoch': 0.82}
 82%| | 2651/3250 [4:07:11<58:54,  5.90s/it]                                                      82%| | 2651/3250 [4:07:11<58:54,  5.90s/it] 82%| | 2652/3250 [4:07:16<57:14,  5.74s/it]                                                      82%| | 2652/3250 [4:07:16<57:14,  5.74s/it] 82%| | 2653/3250 [4:07:21<56:03,  5.63s/it]                                                      82%| | 2653/3250 [4:07:21<56:03,  5.63s/it] 82%| | 2654/3250 [4:07:27<55:10,  5.55s/it]                                                      82%| | 2654/3250 [4:07:27<55:10,  5.55s/it] 82%| | 2655/3250 [4:07:32<54:34,  5.50s/it]                                                      82%| | 2655/3250 [4:07:32<54:34,  5.50s/it] 82%|{'loss': 0.6498, 'learning_rate': 8.02788209093775e-06, 'epoch': 0.82}
{'loss': 0.6456, 'learning_rate': 8.001619484844009e-06, 'epoch': 0.82}
{'loss': 0.6538, 'learning_rate': 7.975396170395521e-06, 'epoch': 0.82}
{'loss': 0.6382, 'learning_rate': 7.949212172125565e-06, 'epoch': 0.82}
{'loss': 0.6336, 'learning_rate': 7.923067514530613e-06, 'epoch': 0.82}
 | 2656/3250 [4:07:38<55:54,  5.65s/it]                                                      82%| | 2656/3250 [4:07:38<55:54,  5.65s/it] 82%| | 2657/3250 [4:07:44<55:01,  5.57s/it]                                                      82%| | 2657/3250 [4:07:44<55:01,  5.57s/it] 82%| | 2658/3250 [4:07:49<54:22,  5.51s/it]                                                      82%| | 2658/3250 [4:07:49<54:22,  5.51s/it] 82%| | 2659/3250 [4:07:54<53:52,  5.47s/it]                                                      82%| | 2659/3250 [4:07:54<53:52,  5.47s/it] 82%| | 2660/3250 [4:08:00<53:30,  5.44s/it]                                                      82%| | 2660/3250 [4:08:00<53:30,  5.44s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8507149815559387, 'eval_runtime': 1.3682, 'eval_samples_per_second': 8.771, 'eval_steps_per_second': 2.193, 'epoch': 0.82}
                                                      82%| | 2660/3250 [4:08:01<53:30,  5.44s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_4/tmp-checkpoint-2660
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2660
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2660
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2660
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2660
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2660/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2660/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2660/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2660/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.6545, 'learning_rate': 7.89696222207032e-06, 'epoch': 0.82}
{'loss': 0.6821, 'learning_rate': 7.870896319167548e-06, 'epoch': 0.82}
{'loss': 0.6454, 'learning_rate': 7.844869830208273e-06, 'epoch': 0.82}
{'loss': 0.6529, 'learning_rate': 7.818882779541631e-06, 'epoch': 0.82}
{'loss': 0.6723, 'learning_rate': 7.79293519147985e-06, 'epoch': 0.82}
 82%| | 2661/3250 [4:08:07<58:55,  6.00s/it]                                                      82%| | 2661/3250 [4:08:07<58:55,  6.00s/it] 82%| | 2662/3250 [4:08:12<56:58,  5.81s/it]                                                      82%| | 2662/3250 [4:08:12<56:58,  5.81s/it] 82%| | 2663/3250 [4:08:18<55:32,  5.68s/it]                                                      82%| | 2663/3250 [4:08:18<55:32,  5.68s/it] 82%| | 2664/3250 [4:08:23<54:33,  5.59s/it]                                                      82%| | 2664/3250 [4:08:23<54:33,  5.59s/it] 82%| | 2665/3250 [4:08:28<53:51,  5.52s/it]                                                      82%| | 2665/3250 [4:08:28<53:51,  5.52s/it] 82%|{'loss': 0.6604, 'learning_rate': 7.767027090298207e-06, 'epoch': 0.82}
{'loss': 0.6255, 'learning_rate': 7.74115850023509e-06, 'epoch': 0.82}
{'loss': 0.706, 'learning_rate': 7.715329445491876e-06, 'epoch': 0.82}
{'loss': 0.652, 'learning_rate': 7.689539950232977e-06, 'epoch': 0.82}
{'loss': 0.6348, 'learning_rate': 7.663790038585793e-06, 'epoch': 0.82}
 | 2666/3250 [4:08:34<53:20,  5.48s/it]                                                      82%| | 2666/3250 [4:08:34<53:20,  5.48s/it] 82%| | 2667/3250 [4:08:39<52:55,  5.45s/it]                                                      82%| | 2667/3250 [4:08:39<52:55,  5.45s/it] 82%| | 2668/3250 [4:08:45<52:38,  5.43s/it]                                                      82%| | 2668/3250 [4:08:45<52:38,  5.43s/it] 82%| | 2669/3250 [4:08:50<52:24,  5.41s/it]                                                      82%| | 2669/3250 [4:08:50<52:24,  5.41s/it] 82%| | 2670/3250 [4:08:55<52:12,  5.40s/it]                                                      82%| | 2670/3250 [4:08:55<52:12,  5.40s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8497490286827087, 'eval_runtime': 1.3674, 'eval_samples_per_second': 8.776, 'eval_steps_per_second': 2.194, 'epoch': 0.82}
                                                      82%| | 2670/3250 [4:08:57<52:12,  5.40s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_4/tmp-checkpoint-2670
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  I AM HERE AND I AM SAVING THE MODEL
/projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2670the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2670

I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2670
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2670
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2670/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2670/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2670/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2670/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.6461, 'learning_rate': 7.638079734640703e-06, 'epoch': 0.82}
{'loss': 0.6308, 'learning_rate': 7.612409062451015e-06, 'epoch': 0.82}
{'loss': 0.6761, 'learning_rate': 7.58677804603295e-06, 'epoch': 0.82}
{'loss': 0.6346, 'learning_rate': 7.561186709365653e-06, 'epoch': 0.82}
{'loss': 0.6976, 'learning_rate': 7.5356350763911345e-06, 'epoch': 0.82}
 82%| | 2671/3250 [4:09:03<57:23,  5.95s/it]                                                      82%| | 2671/3250 [4:09:03<57:23,  5.95s/it] 82%| | 2672/3250 [4:09:08<55:35,  5.77s/it]                                                      82%| | 2672/3250 [4:09:08<55:35,  5.77s/it] 82%| | 2673/3250 [4:09:14<55:09,  5.74s/it]                                                      82%| | 2673/3250 [4:09:14<55:09,  5.74s/it] 82%| | 2674/3250 [4:09:19<54:02,  5.63s/it]                                                      82%| | 2674/3250 [4:09:19<54:02,  5.63s/it] 82%| | 2675/3250 [4:09:24<53:13,  5.55s/it]                                                      82%| | 2675/3250 [4:09:24<53:13,  5.55s/it] 82%|{'loss': 1.1456, 'learning_rate': 7.510123171014255e-06, 'epoch': 0.82}
{'loss': 0.6367, 'learning_rate': 7.484651017102728e-06, 'epoch': 0.82}
{'loss': 0.652, 'learning_rate': 7.459218638487064e-06, 'epoch': 0.82}
{'loss': 0.6827, 'learning_rate': 7.4338260589605415e-06, 'epoch': 0.82}
{'loss': 0.6613, 'learning_rate': 7.408473302279234e-06, 'epoch': 0.82}
 | 2676/3250 [4:09:30<52:37,  5.50s/it]                                                      82%| | 2676/3250 [4:09:30<52:37,  5.50s/it] 82%| | 2677/3250 [4:09:35<52:10,  5.46s/it]                                                      82%| | 2677/3250 [4:09:35<52:10,  5.46s/it] 82%| | 2678/3250 [4:09:40<51:51,  5.44s/it]                                                      82%| | 2678/3250 [4:09:40<51:51,  5.44s/it] 82%| | 2679/3250 [4:09:46<51:36,  5.42s/it]                                                      82%| | 2679/3250 [4:09:46<51:36,  5.42s/it] 82%| | 2680/3250 [4:09:51<51:23,  5.41s/it]                                                      82%| | 2680/3250 [4:09:51<51:23,  5.41s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8509460091590881, 'eval_runtime': 1.3702, 'eval_samples_per_second': 8.758, 'eval_steps_per_second': 2.189, 'epoch': 0.82}
                                                      82%| | 2680/3250 [4:09:53<51:23,  5.41s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_4/tmp-checkpoint-2680
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2680
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2680
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2680
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2680
the pytorch model path isthe pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2680/pytorch_model.bin 
/projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2680/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2680/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2680/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.627, 'learning_rate': 7.383160392161953e-06, 'epoch': 0.82}
{'loss': 0.6342, 'learning_rate': 7.357887352290227e-06, 'epoch': 0.83}
{'loss': 0.7188, 'learning_rate': 7.332654206308298e-06, 'epoch': 0.83}
{'loss': 0.6608, 'learning_rate': 7.307460977823044e-06, 'epoch': 0.83}
{'loss': 0.6531, 'learning_rate': 7.282307690404055e-06, 'epoch': 0.83}
 82%| | 2681/3250 [4:09:58<56:02,  5.91s/it]                                                      82%| | 2681/3250 [4:09:58<56:02,  5.91s/it] 83%| | 2682/3250 [4:10:04<54:22,  5.74s/it]                                                      83%| | 2682/3250 [4:10:04<54:22,  5.74s/it] 83%| | 2683/3250 [4:10:09<53:15,  5.64s/it]                                                      83%| | 2683/3250 [4:10:09<53:15,  5.64s/it] 83%| | 2684/3250 [4:10:14<52:25,  5.56s/it]                                                      83%| | 2684/3250 [4:10:14<52:25,  5.56s/it] 83%| | 2685/3250 [4:10:20<51:49,  5.50s/it]                                                      83%| | 2685/3250 [4:10:20<51:49,  5.50s/it] 83%|{'loss': 0.6102, 'learning_rate': 7.257194367583503e-06, 'epoch': 0.83}
{'loss': 0.6553, 'learning_rate': 7.232121032856193e-06, 'epoch': 0.83}
{'loss': 0.6692, 'learning_rate': 7.207087709679533e-06, 'epoch': 0.83}
{'loss': 0.6342, 'learning_rate': 7.182094421473479e-06, 'epoch': 0.83}
{'loss': 0.629, 'learning_rate': 7.157141191620548e-06, 'epoch': 0.83}
 | 2686/3250 [4:10:25<51:19,  5.46s/it]                                                      83%| | 2686/3250 [4:10:25<51:19,  5.46s/it] 83%| | 2687/3250 [4:10:31<51:01,  5.44s/it]                                                      83%| | 2687/3250 [4:10:31<51:01,  5.44s/it] 83%| | 2688/3250 [4:10:36<50:45,  5.42s/it]                                                      83%| | 2688/3250 [4:10:36<50:45,  5.42s/it] 83%| | 2689/3250 [4:10:42<52:12,  5.58s/it]                                                      83%| | 2689/3250 [4:10:42<52:12,  5.58s/it] 83%| | 2690/3250 [4:10:49<56:28,  6.05s/it]                                                      83%| | 2690/3250 [4:10:49<56:28,  6.05s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8501042127609253, 'eval_runtime': 1.3747, 'eval_samples_per_second': 8.729, 'eval_steps_per_second': 2.182, 'epoch': 0.83}
                                                      83%| | 2690/3250 [4:10:50<56:28,  6.05s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_4/tmp-checkpoint-2690
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2690I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2690

I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2690
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2690
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2690/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2690/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2690/pytorch_model.binthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2690/pytorch_model.bin

/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.6566, 'learning_rate': 7.13222804346575e-06, 'epoch': 0.83}
{'loss': 0.6539, 'learning_rate': 7.107355000316624e-06, 'epoch': 0.83}
{'loss': 0.6806, 'learning_rate': 7.082522085443183e-06, 'epoch': 0.83}
{'loss': 0.6615, 'learning_rate': 7.0577293220778996e-06, 'epoch': 0.83}
{'loss': 0.6687, 'learning_rate': 7.032976733415675e-06, 'epoch': 0.83}
 83%| | 2691/3250 [4:10:56<59:35,  6.40s/it]                                                      83%| | 2691/3250 [4:10:56<59:35,  6.40s/it] 83%| | 2692/3250 [4:11:02<56:36,  6.09s/it]                                                      83%| | 2692/3250 [4:11:02<56:36,  6.09s/it] 83%| | 2693/3250 [4:11:07<54:29,  5.87s/it]                                                      83%| | 2693/3250 [4:11:07<54:29,  5.87s/it] 83%| | 2694/3250 [4:11:12<52:53,  5.71s/it]                                                      83%| | 2694/3250 [4:11:12<52:53,  5.71s/it] 83%| | 2695/3250 [4:11:18<51:48,  5.60s/it]                                                      83%| | 2695/3250 [4:11:18<51:48,  5.60s/it] 83%|{'loss': 0.6674, 'learning_rate': 7.0082643426138405e-06, 'epoch': 0.83}
{'loss': 0.6419, 'learning_rate': 6.983592172792086e-06, 'epoch': 0.83}
{'loss': 0.6668, 'learning_rate': 6.958960247032514e-06, 'epoch': 0.83}
{'loss': 0.6577, 'learning_rate': 6.934368588379553e-06, 'epoch': 0.83}
{'loss': 0.6276, 'learning_rate': 6.909817219839959e-06, 'epoch': 0.83}
 | 2696/3250 [4:11:23<51:03,  5.53s/it]                                                      83%| | 2696/3250 [4:11:23<51:03,  5.53s/it] 83%| | 2697/3250 [4:11:28<50:29,  5.48s/it]                                                      83%| | 2697/3250 [4:11:28<50:29,  5.48s/it] 83%| | 2698/3250 [4:11:34<50:05,  5.44s/it]                                                      83%| | 2698/3250 [4:11:34<50:05,  5.44s/it] 83%| | 2699/3250 [4:11:39<49:45,  5.42s/it]                                                      83%| | 2699/3250 [4:11:39<49:45,  5.42s/it] 83%| | 2700/3250 [4:11:44<49:29,  5.40s/it]                                                      83%| | 2700/3250 [4:11:44<49:29,  5.40s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8501230478286743, 'eval_runtime': 1.3634, 'eval_samples_per_second': 8.801, 'eval_steps_per_second': 2.2, 'epoch': 0.83}
                                                      83%| | 2700/3250 [4:11:46<49:29,  5.40s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_4/tmp-checkpoint-2700
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2700/projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2700

I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2700
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2700
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2700/pytorch_model.bin 
/projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2700/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2700/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2700/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.629, 'learning_rate': 6.8853061643828156e-06, 'epoch': 0.83}
{'loss': 0.6353, 'learning_rate': 6.860835444939456e-06, 'epoch': 0.83}
{'loss': 0.6675, 'learning_rate': 6.8364050844035245e-06, 'epoch': 0.83}
{'loss': 0.6423, 'learning_rate': 6.812015105630842e-06, 'epoch': 0.83}
{'loss': 0.6847, 'learning_rate': 6.787665531439513e-06, 'epoch': 0.83}
 83%| | 2701/3250 [4:11:52<54:23,  5.94s/it]                                                      83%| | 2701/3250 [4:11:52<54:23,  5.94s/it] 83%| | 2702/3250 [4:11:57<52:42,  5.77s/it]                                                      83%| | 2702/3250 [4:11:57<52:42,  5.77s/it] 83%| | 2703/3250 [4:12:02<51:28,  5.65s/it]                                                      83%| | 2703/3250 [4:12:02<51:28,  5.65s/it] 83%| | 2704/3250 [4:12:08<50:35,  5.56s/it]                                                      83%| | 2704/3250 [4:12:08<50:35,  5.56s/it] 83%| | 2705/3250 [4:12:13<50:40,  5.58s/it]                                                      83%| | 2705/3250 [4:12:13<50:40,  5.58s/it] 83%|{'loss': 1.1792, 'learning_rate': 6.763356384609809e-06, 'epoch': 0.83}
{'loss': 0.605, 'learning_rate': 6.739087687884188e-06, 'epoch': 0.83}
{'loss': 0.6304, 'learning_rate': 6.714859463967283e-06, 'epoch': 0.83}
{'loss': 0.6693, 'learning_rate': 6.690671735525811e-06, 'epoch': 0.83}
{'loss': 0.6652, 'learning_rate': 6.666524525188656e-06, 'epoch': 0.83}
 | 2706/3250 [4:12:19<49:56,  5.51s/it]                                                      83%| | 2706/3250 [4:12:19<49:56,  5.51s/it] 83%| | 2707/3250 [4:12:24<49:24,  5.46s/it]                                                      83%| | 2707/3250 [4:12:24<49:24,  5.46s/it] 83%| | 2708/3250 [4:12:29<49:00,  5.43s/it]                                                      83%| | 2708/3250 [4:12:29<49:00,  5.43s/it] 83%| | 2709/3250 [4:12:35<48:45,  5.41s/it]                                                      83%| | 2709/3250 [4:12:35<48:45,  5.41s/it] 83%| | 2710/3250 [4:12:40<48:32,  5.39s/it]                                                      83%| | 2710/3250 [4:12:40<48:32,  5.39s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8504809737205505, 'eval_runtime': 1.3847, 'eval_samples_per_second': 8.666, 'eval_steps_per_second': 2.167, 'epoch': 0.83}
                                                      83%| | 2710/3250 [4:12:41<48:32,  5.39s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_4/tmp-checkpoint-2710
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  I AM HERE AND I AM SAVING THE MODEL
/projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2710the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2710

I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2710
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2710
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2710/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2710/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2710/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2710/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.6389, 'learning_rate': 6.642417855546768e-06, 'epoch': 0.83}
{'loss': 0.6402, 'learning_rate': 6.6183517491531844e-06, 'epoch': 0.83}
{'loss': 0.6848, 'learning_rate': 6.594326228522979e-06, 'epoch': 0.83}
{'loss': 0.6835, 'learning_rate': 6.570341316133271e-06, 'epoch': 0.84}
{'loss': 0.6548, 'learning_rate': 6.546397034423163e-06, 'epoch': 0.84}
 83%| | 2711/3250 [4:12:47<53:02,  5.90s/it]                                                      83%| | 2711/3250 [4:12:47<53:02,  5.90s/it] 83%| | 2712/3250 [4:12:53<51:26,  5.74s/it]                                                      83%| | 2712/3250 [4:12:53<51:26,  5.74s/it] 83%| | 2713/3250 [4:12:58<50:18,  5.62s/it]                                                      83%| | 2713/3250 [4:12:58<50:18,  5.62s/it] 84%| | 2714/3250 [4:13:03<49:28,  5.54s/it]                                                      84%| | 2714/3250 [4:13:03<49:28,  5.54s/it] 84%| | 2715/3250 [4:13:09<48:56,  5.49s/it]                                                      84%| | 2715/3250 [4:13:09<48:56,  5.49s/it] 84%|{'loss': 0.6088, 'learning_rate': 6.522493405793778e-06, 'epoch': 0.84}
{'loss': 0.6722, 'learning_rate': 6.498630452608179e-06, 'epoch': 0.84}
{'loss': 0.6568, 'learning_rate': 6.474808197191401e-06, 'epoch': 0.84}
{'loss': 0.6634, 'learning_rate': 6.4510266618303724e-06, 'epoch': 0.84}
{'loss': 0.6317, 'learning_rate': 6.427285868773947e-06, 'epoch': 0.84}
 | 2716/3250 [4:13:14<48:28,  5.45s/it]                                                      84%| | 2716/3250 [4:13:14<48:28,  5.45s/it] 84%| | 2717/3250 [4:13:19<48:08,  5.42s/it]                                                      84%| | 2717/3250 [4:13:19<48:08,  5.42s/it] 84%| | 2718/3250 [4:13:25<47:50,  5.40s/it]                                                      84%| | 2718/3250 [4:13:25<47:50,  5.40s/it] 84%| | 2719/3250 [4:13:30<47:37,  5.38s/it]                                                      84%| | 2719/3250 [4:13:30<47:37,  5.38s/it] 84%| | 2720/3250 [4:13:35<47:26,  5.37s/it]                                                      84%| | 2720/3250 [4:13:35<47:26,  5.37s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8497971296310425, 'eval_runtime': 1.3639, 'eval_samples_per_second': 8.798, 'eval_steps_per_second': 2.2, 'epoch': 0.84}
                                                      84%| | 2720/3250 [4:13:37<47:26,  5.37s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_4/tmp-checkpoint-2720
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2720
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2720
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2720
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2720
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2720/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2720/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2720/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2720/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.6413, 'learning_rate': 6.403585840232873e-06, 'epoch': 0.84}
{'loss': 0.654, 'learning_rate': 6.379926598379726e-06, 'epoch': 0.84}
{'loss': 0.6855, 'learning_rate': 6.356308165348951e-06, 'epoch': 0.84}
{'loss': 0.6638, 'learning_rate': 6.332730563236805e-06, 'epoch': 0.84}
{'loss': 0.6574, 'learning_rate': 6.3091938141013495e-06, 'epoch': 0.84}
 84%| | 2721/3250 [4:13:43<52:06,  5.91s/it]                                                      84%| | 2721/3250 [4:13:43<52:06,  5.91s/it] 84%| | 2722/3250 [4:13:48<52:06,  5.92s/it]                                                      84%| | 2722/3250 [4:13:48<52:06,  5.92s/it] 84%| | 2723/3250 [4:13:54<50:32,  5.75s/it]                                                      84%| | 2723/3250 [4:13:54<50:32,  5.75s/it] 84%| | 2724/3250 [4:13:59<49:23,  5.63s/it]                                                      84%| | 2724/3250 [4:13:59<49:23,  5.63s/it] 84%| | 2725/3250 [4:14:05<48:34,  5.55s/it]                                                      84%| | 2725/3250 [4:14:05<48:34,  5.55s/it] 84%|{'loss': 0.6559, 'learning_rate': 6.285697939962437e-06, 'epoch': 0.84}
{'loss': 0.6703, 'learning_rate': 6.262242962801645e-06, 'epoch': 0.84}
{'loss': 0.6217, 'learning_rate': 6.238828904562316e-06, 'epoch': 0.84}
{'loss': 0.7005, 'learning_rate': 6.2154557871495156e-06, 'epoch': 0.84}
{'loss': 0.6317, 'learning_rate': 6.192123632429986e-06, 'epoch': 0.84}
 | 2726/3250 [4:14:10<48:00,  5.50s/it]                                                      84%| | 2726/3250 [4:14:10<48:00,  5.50s/it] 84%| | 2727/3250 [4:14:15<47:32,  5.45s/it]                                                      84%| | 2727/3250 [4:14:15<47:32,  5.45s/it] 84%| | 2728/3250 [4:14:21<47:10,  5.42s/it]                                                      84%| | 2728/3250 [4:14:21<47:10,  5.42s/it] 84%| | 2729/3250 [4:14:26<46:53,  5.40s/it]                                                      84%| | 2729/3250 [4:14:26<46:53,  5.40s/it] 84%| | 2730/3250 [4:14:31<46:43,  5.39s/it]                                                      84%| | 2730/3250 [4:14:31<46:43,  5.39s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8492501974105835, 'eval_runtime': 1.3613, 'eval_samples_per_second': 8.815, 'eval_steps_per_second': 2.204, 'epoch': 0.84}
                                                      84%| | 2730/3250 [4:14:33<46:43,  5.39s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_4/tmp-checkpoint-2730
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2730
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2730
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2730
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2730
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2730/pytorch_model.bin
the pytorch model path is the pytorch model path is/projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2730/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2730/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2730/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.6533, 'learning_rate': 6.168832462232171e-06, 'epoch': 0.84}
{'loss': 0.6359, 'learning_rate': 6.145582298346153e-06, 'epoch': 0.84}
{'loss': 0.6381, 'learning_rate': 6.1223731625236534e-06, 'epoch': 0.84}
{'loss': 0.6622, 'learning_rate': 6.099205076478004e-06, 'epoch': 0.84}
{'loss': 0.6763, 'learning_rate': 6.076078061884166e-06, 'epoch': 0.84}
 84%| | 2731/3250 [4:14:38<51:09,  5.91s/it]                                                      84%| | 2731/3250 [4:14:38<51:09,  5.91s/it] 84%| | 2732/3250 [4:14:44<49:37,  5.75s/it]                                                      84%| | 2732/3250 [4:14:44<49:37,  5.75s/it] 84%| | 2733/3250 [4:14:49<48:30,  5.63s/it]                                                      84%| | 2733/3250 [4:14:49<48:30,  5.63s/it] 84%| | 2734/3250 [4:14:55<47:41,  5.54s/it]                                                      84%| | 2734/3250 [4:14:55<47:41,  5.54s/it] 84%| | 2735/3250 [4:15:00<47:10,  5.50s/it]                                                      84%| | 2735/3250 [4:15:00<47:10,  5.50s/it] 84%|{'loss': 0.6762, 'learning_rate': 6.052992140378627e-06, 'epoch': 0.84}
{'loss': 1.1258, 'learning_rate': 6.02994733355946e-06, 'epoch': 0.84}
{'loss': 0.6428, 'learning_rate': 6.006943662986275e-06, 'epoch': 0.84}
{'loss': 0.6724, 'learning_rate': 5.98398115018019e-06, 'epoch': 0.84}
{'loss': 0.6534, 'learning_rate': 5.961059816623799e-06, 'epoch': 0.84}
 | 2736/3250 [4:15:05<46:43,  5.45s/it]                                                      84%| | 2736/3250 [4:15:05<46:43,  5.45s/it] 84%| | 2737/3250 [4:15:11<46:22,  5.42s/it]                                                      84%| | 2737/3250 [4:15:11<46:22,  5.42s/it] 84%| | 2738/3250 [4:15:16<46:46,  5.48s/it]                                                      84%| | 2738/3250 [4:15:16<46:46,  5.48s/it] 84%| | 2739/3250 [4:15:22<46:19,  5.44s/it]                                                      84%| | 2739/3250 [4:15:22<46:19,  5.44s/it] 84%| | 2740/3250 [4:15:27<46:01,  5.41s/it]                                                      84%| | 2740/3250 [4:15:27<46:01,  5.41s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8513241410255432, 'eval_runtime': 1.3687, 'eval_samples_per_second': 8.768, 'eval_steps_per_second': 2.192, 'epoch': 0.84}
                                                      84%| | 2740/3250 [4:15:28<46:01,  5.41s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_4/tmp-checkpoint-2740
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2740
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2740
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2740
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2740
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2740/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2740/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2740/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2740/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.652, 'learning_rate': 5.9381796837612025e-06, 'epoch': 0.84}
{'loss': 0.6131, 'learning_rate': 5.91534077299794e-06, 'epoch': 0.84}
{'loss': 0.6958, 'learning_rate': 5.892543105700987e-06, 'epoch': 0.84}
{'loss': 0.677, 'learning_rate': 5.8697867031987485e-06, 'epoch': 0.84}
{'loss': 0.6507, 'learning_rate': 5.8470715867809774e-06, 'epoch': 0.84}
 84%| | 2741/3250 [4:15:34<50:11,  5.92s/it]                                                      84%| | 2741/3250 [4:15:34<50:11,  5.92s/it] 84%| | 2742/3250 [4:15:39<48:38,  5.75s/it]                                                      84%| | 2742/3250 [4:15:39<48:38,  5.75s/it] 84%| | 2743/3250 [4:15:45<47:33,  5.63s/it]                                                      84%| | 2743/3250 [4:15:45<47:33,  5.63s/it] 84%| | 2744/3250 [4:15:50<46:45,  5.54s/it]                                                      84%| | 2744/3250 [4:15:50<46:45,  5.54s/it] 84%| | 2745/3250 [4:15:55<46:10,  5.49s/it]                                                      84%| | 2745/3250 [4:15:55<46:10,  5.49s/it] 84%|{'loss': 0.6471, 'learning_rate': 5.824397777698859e-06, 'epoch': 0.84}
{'loss': 0.6447, 'learning_rate': 5.801765297164891e-06, 'epoch': 0.85}
{'loss': 0.6326, 'learning_rate': 5.779174166352935e-06, 'epoch': 0.85}
{'loss': 0.6585, 'learning_rate': 5.756624406398159e-06, 'epoch': 0.85}
{'loss': 0.6393, 'learning_rate': 5.734116038397019e-06, 'epoch': 0.85}
 | 2746/3250 [4:16:01<45:44,  5.45s/it]                                                      84%| | 2746/3250 [4:16:01<45:44,  5.45s/it] 85%| | 2747/3250 [4:16:06<45:23,  5.42s/it]                                                      85%| | 2747/3250 [4:16:06<45:23,  5.42s/it] 85%| | 2748/3250 [4:16:11<45:07,  5.39s/it]                                                      85%| | 2748/3250 [4:16:11<45:07,  5.39s/it] 85%| | 2749/3250 [4:16:17<44:55,  5.38s/it]                                                      85%| | 2749/3250 [4:16:17<44:55,  5.38s/it] 85%| | 2750/3250 [4:16:22<44:44,  5.37s/it]                                                      85%| | 2750/3250 [4:16:22<44:44,  5.37s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8494967818260193, 'eval_runtime': 1.3636, 'eval_samples_per_second': 8.8, 'eval_steps_per_second': 2.2, 'epoch': 0.85}
                                                      85%| | 2750/3250 [4:16:24<44:44,  5.37s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_4/tmp-checkpoint-2750
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2750I AM HERE AND I AM SAVING THE MODEL

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2750
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2750
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2750
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2750/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2750/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2750/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2750/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.6256, 'learning_rate': 5.711649083407256e-06, 'epoch': 0.85}
{'loss': 0.6605, 'learning_rate': 5.689223562447843e-06, 'epoch': 0.85}
{'loss': 0.6808, 'learning_rate': 5.666839496499022e-06, 'epoch': 0.85}
{'loss': 0.6371, 'learning_rate': 5.644496906502233e-06, 'epoch': 0.85}
{'loss': 0.664, 'learning_rate': 5.622195813360126e-06, 'epoch': 0.85}
 85%| | 2751/3250 [4:16:29<48:59,  5.89s/it]                                                      85%| | 2751/3250 [4:16:29<48:59,  5.89s/it] 85%| | 2752/3250 [4:16:35<47:34,  5.73s/it]                                                      85%| | 2752/3250 [4:16:35<47:34,  5.73s/it] 85%| | 2753/3250 [4:16:40<46:31,  5.62s/it]                                                      85%| | 2753/3250 [4:16:40<46:31,  5.62s/it] 85%| | 2754/3250 [4:16:45<45:44,  5.53s/it]                                                      85%| | 2754/3250 [4:16:45<45:44,  5.53s/it] 85%| | 2755/3250 [4:16:51<46:44,  5.67s/it]                                                      85%| | 2755/3250 [4:16:51<46:44,  5.67s/it] 85%|{'loss': 0.6607, 'learning_rate': 5.599936237936515e-06, 'epoch': 0.85}
{'loss': 0.6578, 'learning_rate': 5.577718201056392e-06, 'epoch': 0.85}
{'loss': 0.6148, 'learning_rate': 5.5555417235058526e-06, 'epoch': 0.85}
{'loss': 0.6998, 'learning_rate': 5.533406826032133e-06, 'epoch': 0.85}
{'loss': 0.6495, 'learning_rate': 5.5113135293435815e-06, 'epoch': 0.85}
 | 2756/3250 [4:16:57<45:54,  5.58s/it]                                                      85%| | 2756/3250 [4:16:57<45:54,  5.58s/it] 85%| | 2757/3250 [4:17:02<45:18,  5.51s/it]                                                      85%| | 2757/3250 [4:17:02<45:18,  5.51s/it] 85%| | 2758/3250 [4:17:07<44:49,  5.47s/it]                                                      85%| | 2758/3250 [4:17:07<44:49,  5.47s/it] 85%| | 2759/3250 [4:17:13<44:27,  5.43s/it]                                                      85%| | 2759/3250 [4:17:13<44:27,  5.43s/it] 85%| | 2760/3250 [4:17:18<44:13,  5.42s/it]                                                      85%| | 2760/3250 [4:17:18<44:13,  5.42s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8500832915306091, 'eval_runtime': 1.4625, 'eval_samples_per_second': 8.205, 'eval_steps_per_second': 2.051, 'epoch': 0.85}
                                                      85%| | 2760/3250 [4:17:20<44:13,  5.42s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_4/tmp-checkpoint-2760
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2760
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2760
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2760
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2760
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2760/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2760/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2760/pytorch_model.binthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2760/pytorch_model.bin

/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.628, 'learning_rate': 5.489261854109606e-06, 'epoch': 0.85}
{'loss': 0.6291, 'learning_rate': 5.467251820960701e-06, 'epoch': 0.85}
{'loss': 0.6248, 'learning_rate': 5.4452834504883535e-06, 'epoch': 0.85}
{'loss': 0.6817, 'learning_rate': 5.423356763245119e-06, 'epoch': 0.85}
{'loss': 0.633, 'learning_rate': 5.401471779744538e-06, 'epoch': 0.85}
 85%| | 2761/3250 [4:17:25<48:37,  5.97s/it]                                                      85%| | 2761/3250 [4:17:25<48:37,  5.97s/it] 85%| | 2762/3250 [4:17:31<47:00,  5.78s/it]                                                      85%| | 2762/3250 [4:17:31<47:00,  5.78s/it] 85%| | 2763/3250 [4:17:36<45:52,  5.65s/it]                                                      85%| | 2763/3250 [4:17:36<45:52,  5.65s/it] 85%| | 2764/3250 [4:17:41<45:03,  5.56s/it]                                                      85%| | 2764/3250 [4:17:41<45:03,  5.56s/it] 85%| | 2765/3250 [4:17:47<44:31,  5.51s/it]                                                      85%| | 2765/3250 [4:17:47<44:31,  5.51s/it] 85%|{'loss': 0.6914, 'learning_rate': 5.3796285204611495e-06, 'epoch': 0.85}
{'loss': 1.1475, 'learning_rate': 5.357827005830435e-06, 'epoch': 0.85}
{'loss': 0.6242, 'learning_rate': 5.336067256248844e-06, 'epoch': 0.85}
{'loss': 0.6576, 'learning_rate': 5.314349292073739e-06, 'epoch': 0.85}
{'loss': 0.6763, 'learning_rate': 5.292673133623371e-06, 'epoch': 0.85}
 | 2766/3250 [4:17:52<44:04,  5.46s/it]                                                      85%| | 2766/3250 [4:17:52<44:04,  5.46s/it] 85%| | 2767/3250 [4:17:57<43:41,  5.43s/it]                                                      85%| | 2767/3250 [4:17:57<43:41,  5.43s/it] 85%| | 2768/3250 [4:18:03<43:24,  5.40s/it]                                                      85%| | 2768/3250 [4:18:03<43:24,  5.40s/it] 85%| | 2769/3250 [4:18:08<43:17,  5.40s/it]                                                      85%| | 2769/3250 [4:18:08<43:17,  5.40s/it] 85%| | 2770/3250 [4:18:14<43:03,  5.38s/it]                                                      85%| | 2770/3250 [4:18:14<43:03,  5.38s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8513168692588806, 'eval_runtime': 1.5287, 'eval_samples_per_second': 7.85, 'eval_steps_per_second': 1.962, 'epoch': 0.85}
                                                      85%| | 2770/3250 [4:18:15<43:03,  5.38s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_4/tmp-checkpoint-2770
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2770
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2770
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2770
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2770
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2770/pytorch_model.bin
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2770/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2770/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2770/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.6629, 'learning_rate': 5.271038801176919e-06, 'epoch': 0.85}
{'loss': 0.6258, 'learning_rate': 5.249446314974416e-06, 'epoch': 0.85}
{'loss': 0.6345, 'learning_rate': 5.227895695216739e-06, 'epoch': 0.85}
{'loss': 0.7222, 'learning_rate': 5.206386962065602e-06, 'epoch': 0.85}
{'loss': 0.6567, 'learning_rate': 5.184920135643539e-06, 'epoch': 0.85}
 85%| | 2771/3250 [4:18:21<48:03,  6.02s/it]                                                      85%| | 2771/3250 [4:18:21<48:03,  6.02s/it] 85%| | 2772/3250 [4:18:26<46:22,  5.82s/it]                                                      85%| | 2772/3250 [4:18:26<46:22,  5.82s/it] 85%| | 2773/3250 [4:18:32<45:09,  5.68s/it]                                                      85%| | 2773/3250 [4:18:32<45:09,  5.68s/it] 85%| | 2774/3250 [4:18:37<44:17,  5.58s/it]                                                      85%| | 2774/3250 [4:18:37<44:17,  5.58s/it] 85%| | 2775/3250 [4:18:42<43:38,  5.51s/it]                                                      85%| | 2775/3250 [4:18:42<43:38,  5.51s/it] 85%|{'loss': 0.6517, 'learning_rate': 5.163495236033855e-06, 'epoch': 0.85}
{'loss': 0.6077, 'learning_rate': 5.142112283280653e-06, 'epoch': 0.85}
{'loss': 0.6561, 'learning_rate': 5.120771297388788e-06, 'epoch': 0.85}
{'loss': 0.6771, 'learning_rate': 5.099472298323843e-06, 'epoch': 0.86}
{'loss': 0.6497, 'learning_rate': 5.078215306012135e-06, 'epoch': 0.86}
 | 2776/3250 [4:18:48<43:11,  5.47s/it]                                                      85%| | 2776/3250 [4:18:48<43:11,  5.47s/it] 85%| | 2777/3250 [4:18:53<42:47,  5.43s/it]                                                      85%| | 2777/3250 [4:18:53<42:47,  5.43s/it] 85%| | 2778/3250 [4:18:59<42:33,  5.41s/it]                                                      85%| | 2778/3250 [4:18:59<42:33,  5.41s/it] 86%| | 2779/3250 [4:19:04<42:19,  5.39s/it]                                                      86%| | 2779/3250 [4:19:04<42:19,  5.39s/it] 86%| | 2780/3250 [4:19:09<42:07,  5.38s/it]                                                      86%| | 2780/3250 [4:19:09<42:07,  5.38s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8503348231315613, 'eval_runtime': 1.3811, 'eval_samples_per_second': 8.689, 'eval_steps_per_second': 2.172, 'epoch': 0.86}
                                                      86%| | 2780/3250 [4:19:11<42:07,  5.38s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_4/tmp-checkpoint-2780
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2780I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2780

I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2780
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2780
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2780/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2780/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2780/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2780/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.639, 'learning_rate': 5.057000340340678e-06, 'epoch': 0.86}
{'loss': 0.6447, 'learning_rate': 5.035827421157146e-06, 'epoch': 0.86}
{'loss': 0.6591, 'learning_rate': 5.014696568269905e-06, 'epoch': 0.86}
{'loss': 0.6701, 'learning_rate': 4.993607801447958e-06, 'epoch': 0.86}
{'loss': 0.6633, 'learning_rate': 4.9725611404209285e-06, 'epoch': 0.86}
 86%| | 2781/3250 [4:19:17<46:26,  5.94s/it]                                                      86%| | 2781/3250 [4:19:17<46:26,  5.94s/it] 86%| | 2782/3250 [4:19:22<45:04,  5.78s/it]                                                      86%| | 2782/3250 [4:19:22<45:04,  5.78s/it] 86%| | 2783/3250 [4:19:27<44:04,  5.66s/it]                                                      86%| | 2783/3250 [4:19:27<44:04,  5.66s/it] 86%| | 2784/3250 [4:19:33<43:21,  5.58s/it]                                                      86%| | 2784/3250 [4:19:33<43:21,  5.58s/it] 86%| | 2785/3250 [4:19:38<42:49,  5.53s/it]                                                      86%| | 2785/3250 [4:19:38<42:49,  5.53s/it] 86%|{'loss': 0.6454, 'learning_rate': 4.951556604879048e-06, 'epoch': 0.86}
{'loss': 0.6735, 'learning_rate': 4.930594214473144e-06, 'epoch': 0.86}
{'loss': 0.6383, 'learning_rate': 4.909673988814601e-06, 'epoch': 0.86}
{'loss': 0.6617, 'learning_rate': 4.888795947475372e-06, 'epoch': 0.86}
{'loss': 0.6665, 'learning_rate': 4.86796010998794e-06, 'epoch': 0.86}
 | 2786/3250 [4:19:44<42:28,  5.49s/it]                                                      86%| | 2786/3250 [4:19:44<42:28,  5.49s/it] 86%| | 2787/3250 [4:19:50<43:44,  5.67s/it]                                                      86%| | 2787/3250 [4:19:50<43:44,  5.67s/it] 86%| | 2788/3250 [4:19:55<43:05,  5.60s/it]                                                      86%| | 2788/3250 [4:19:55<43:05,  5.60s/it] 86%| | 2789/3250 [4:20:00<42:37,  5.55s/it]                                                      86%| | 2789/3250 [4:20:00<42:37,  5.55s/it] 86%| | 2790/3250 [4:20:06<42:14,  5.51s/it]                                                      86%| | 2790/3250 [4:20:06<42:14,  5.51s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8497267961502075, 'eval_runtime': 1.8667, 'eval_samples_per_second': 6.429, 'eval_steps_per_second': 1.607, 'epoch': 0.86}
                                                      86%| | 2790/3250 [4:20:08<42:14,  5.51s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_4/tmp-checkpoint-2790
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2790
/projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2790
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2790
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2790
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2790/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2790/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2790/pytorch_model.binthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2790/pytorch_model.bin

/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.6495, 'learning_rate': 4.847166495845301e-06, 'epoch': 0.86}
{'loss': 0.6308, 'learning_rate': 4.826415124500955e-06, 'epoch': 0.86}
{'loss': 0.6277, 'learning_rate': 4.805706015368883e-06, 'epoch': 0.86}
{'loss': 0.6676, 'learning_rate': 4.785039187823503e-06, 'epoch': 0.86}
{'loss': 0.6459, 'learning_rate': 4.764414661199707e-06, 'epoch': 0.86}
 86%| | 2791/3250 [4:20:14<47:14,  6.18s/it]                                                      86%| | 2791/3250 [4:20:14<47:14,  6.18s/it] 86%| | 2792/3250 [4:20:19<45:20,  5.94s/it]                                                      86%| | 2792/3250 [4:20:19<45:20,  5.94s/it] 86%| | 2793/3250 [4:20:24<44:02,  5.78s/it]                                                      86%| | 2793/3250 [4:20:24<44:02,  5.78s/it] 86%| | 2794/3250 [4:20:30<43:05,  5.67s/it]                                                      86%| | 2794/3250 [4:20:30<43:05,  5.67s/it] 86%| | 2795/3250 [4:20:35<42:26,  5.60s/it]                                                      86%| | 2795/3250 [4:20:35<42:26,  5.60s/it] 86%|{'loss': 0.7035, 'learning_rate': 4.743832454792796e-06, 'epoch': 0.86}
{'loss': 1.1597, 'learning_rate': 4.723292587858485e-06, 'epoch': 0.86}
{'loss': 0.6091, 'learning_rate': 4.702795079612876e-06, 'epoch': 0.86}
{'loss': 0.6391, 'learning_rate': 4.682339949232456e-06, 'epoch': 0.86}
{'loss': 0.6652, 'learning_rate': 4.661927215854028e-06, 'epoch': 0.86}
 | 2796/3250 [4:20:41<41:57,  5.55s/it]                                                      86%| | 2796/3250 [4:20:41<41:57,  5.55s/it] 86%| | 2797/3250 [4:20:46<41:29,  5.50s/it]                                                      86%| | 2797/3250 [4:20:46<41:29,  5.50s/it] 86%| | 2798/3250 [4:20:51<41:11,  5.47s/it]                                                      86%| | 2798/3250 [4:20:51<41:11,  5.47s/it] 86%| | 2799/3250 [4:20:57<40:59,  5.45s/it]                                                      86%| | 2799/3250 [4:20:57<40:59,  5.45s/it] 86%| | 2800/3250 [4:21:02<40:48,  5.44s/it]                                                      86%| | 2800/3250 [4:21:02<40:48,  5.44s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8510037064552307, 'eval_runtime': 1.4052, 'eval_samples_per_second': 8.54, 'eval_steps_per_second': 2.135, 'epoch': 0.86}
                                                      86%| | 2800/3250 [4:21:04<40:48,  5.44s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_4/tmp-checkpoint-2800
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2800
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2800
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2800
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2800
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2800/pytorch_model.bin
the pytorch model path is the pytorch model path is/projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2800/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2800/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2800/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.6759, 'learning_rate': 4.641556898574762e-06, 'epoch': 0.86}
{'loss': 0.6414, 'learning_rate': 4.621229016452156e-06, 'epoch': 0.86}
{'loss': 0.634, 'learning_rate': 4.600943588503959e-06, 'epoch': 0.86}
{'loss': 0.691, 'learning_rate': 4.580700633708246e-06, 'epoch': 0.86}
{'loss': 0.6921, 'learning_rate': 4.5605001710033454e-06, 'epoch': 0.86}
 86%| | 2801/3250 [4:21:10<44:43,  5.98s/it]                                                      86%| | 2801/3250 [4:21:10<44:43,  5.98s/it] 86%| | 2802/3250 [4:21:15<43:17,  5.80s/it]                                                      86%| | 2802/3250 [4:21:15<43:17,  5.80s/it] 86%| | 2803/3250 [4:21:20<42:18,  5.68s/it]                                                      86%| | 2803/3250 [4:21:20<42:18,  5.68s/it] 86%| | 2804/3250 [4:21:26<42:08,  5.67s/it]                                                      86%| | 2804/3250 [4:21:26<42:08,  5.67s/it] 86%| | 2805/3250 [4:21:31<41:23,  5.58s/it]                                                      86%| | 2805/3250 [4:21:31<41:23,  5.58s/it] 86%|{'loss': 0.6393, 'learning_rate': 4.540342219287836e-06, 'epoch': 0.86}
{'loss': 0.6052, 'learning_rate': 4.520226797420501e-06, 'epoch': 0.86}
{'loss': 0.6744, 'learning_rate': 4.500153924220357e-06, 'epoch': 0.86}
{'loss': 0.6402, 'learning_rate': 4.48012361846662e-06, 'epoch': 0.86}
{'loss': 0.6518, 'learning_rate': 4.46013589889866e-06, 'epoch': 0.86}
 | 2806/3250 [4:21:37<40:52,  5.52s/it]                                                      86%| | 2806/3250 [4:21:37<40:52,  5.52s/it] 86%| | 2807/3250 [4:21:42<40:29,  5.49s/it]                                                      86%| | 2807/3250 [4:21:42<40:29,  5.49s/it] 86%| | 2808/3250 [4:21:48<41:43,  5.66s/it]                                                      86%| | 2808/3250 [4:21:48<41:43,  5.66s/it] 86%| | 2809/3250 [4:21:54<41:11,  5.60s/it]                                                      86%| | 2809/3250 [4:21:54<41:11,  5.60s/it] 86%| | 2810/3250 [4:21:59<40:39,  5.55s/it]                                                      86%| | 2810/3250 [4:21:59<40:39,  5.55s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8494723439216614, 'eval_runtime': 1.5524, 'eval_samples_per_second': 7.73, 'eval_steps_per_second': 1.932, 'epoch': 0.86}
                                                      86%| | 2810/3250 [4:22:01<40:39,  5.55s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_4/tmp-checkpoint-2810
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2810
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2810
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2810
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2810
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2810/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2810/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2810/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2810/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.6376, 'learning_rate': 4.4401907842160306e-06, 'epoch': 0.86}
{'loss': 0.6384, 'learning_rate': 4.420288293078395e-06, 'epoch': 0.87}
{'loss': 0.6435, 'learning_rate': 4.4004284441055645e-06, 'epoch': 0.87}
{'loss': 0.6766, 'learning_rate': 4.380611255877448e-06, 'epoch': 0.87}
{'loss': 0.6588, 'learning_rate': 4.360836746934055e-06, 'epoch': 0.87}
 86%| | 2811/3250 [4:22:06<44:27,  6.08s/it]                                                      86%| | 2811/3250 [4:22:06<44:27,  6.08s/it] 87%| | 2812/3250 [4:22:12<43:07,  5.91s/it]                                                      87%| | 2812/3250 [4:22:12<43:07,  5.91s/it] 87%| | 2813/3250 [4:22:17<41:51,  5.75s/it]                                                      87%| | 2813/3250 [4:22:17<41:51,  5.75s/it] 87%| | 2814/3250 [4:22:23<40:57,  5.64s/it]                                                      87%| | 2814/3250 [4:22:23<40:57,  5.64s/it] 87%| | 2815/3250 [4:22:28<40:17,  5.56s/it]                                                      87%| | 2815/3250 [4:22:28<40:17,  5.56s/it] 87%|{'loss': 0.6588, 'learning_rate': 4.341104935775442e-06, 'epoch': 0.87}
{'loss': 0.6518, 'learning_rate': 4.321415840861748e-06, 'epoch': 0.87}
{'loss': 0.6697, 'learning_rate': 4.301769480613116e-06, 'epoch': 0.87}
{'loss': 0.6187, 'learning_rate': 4.282165873409743e-06, 'epoch': 0.87}
{'loss': 0.711, 'learning_rate': 4.262605037591799e-06, 'epoch': 0.87}
 | 2816/3250 [4:22:33<39:49,  5.51s/it]                                                      87%| | 2816/3250 [4:22:33<39:49,  5.51s/it] 87%| | 2817/3250 [4:22:39<39:30,  5.48s/it]                                                      87%| | 2817/3250 [4:22:39<39:30,  5.48s/it] 87%| | 2818/3250 [4:22:44<39:14,  5.45s/it]                                                      87%| | 2818/3250 [4:22:44<39:14,  5.45s/it] 87%| | 2819/3250 [4:22:50<39:02,  5.44s/it]                                                      87%| | 2819/3250 [4:22:50<39:02,  5.44s/it] 87%| | 2820/3250 [4:22:57<44:07,  6.16s/it]                                                      87%| | 2820/3250 [4:22:57<44:07,  6.16s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8493319749832153, 'eval_runtime': 1.3733, 'eval_samples_per_second': 8.738, 'eval_steps_per_second': 2.185, 'epoch': 0.87}
                                                      87%| | 2820/3250 [4:22:59<44:07,  6.16s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_4/tmp-checkpoint-2820
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2820I AM HERE AND I AM SAVING THE MODEL

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2820
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2820
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2820
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2820/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2820/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2820/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2820/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.6418, 'learning_rate': 4.243086991459455e-06, 'epoch': 0.87}
{'loss': 0.6485, 'learning_rate': 4.223611753272849e-06, 'epoch': 0.87}
{'loss': 0.6328, 'learning_rate': 4.2041793412520734e-06, 'epoch': 0.87}
{'loss': 0.6318, 'learning_rate': 4.1847897735771464e-06, 'epoch': 0.87}
{'loss': 0.6528, 'learning_rate': 4.165443068387998e-06, 'epoch': 0.87}
 87%| | 2821/3250 [4:23:05<46:01,  6.44s/it]                                                      87%| | 2821/3250 [4:23:05<46:01,  6.44s/it] 87%| | 2822/3250 [4:23:10<43:43,  6.13s/it]                                                      87%| | 2822/3250 [4:23:10<43:43,  6.13s/it] 87%| | 2823/3250 [4:23:15<42:04,  5.91s/it]                                                      87%| | 2823/3250 [4:23:15<42:04,  5.91s/it] 87%| | 2824/3250 [4:23:21<40:53,  5.76s/it]                                                      87%| | 2824/3250 [4:23:21<40:53,  5.76s/it] 87%| | 2825/3250 [4:23:26<40:04,  5.66s/it]                                                      87%| | 2825/3250 [4:23:26<40:04,  5.66s/it] 87%|{'loss': 0.6738, 'learning_rate': 4.146139243784475e-06, 'epoch': 0.87}
{'loss': 0.6823, 'learning_rate': 4.126878317826294e-06, 'epoch': 0.87}
{'loss': 1.1121, 'learning_rate': 4.1076603085330405e-06, 'epoch': 0.87}
{'loss': 0.6434, 'learning_rate': 4.088485233884165e-06, 'epoch': 0.87}
{'loss': 0.6737, 'learning_rate': 4.069353111818913e-06, 'epoch': 0.87}
 | 2826/3250 [4:23:32<39:27,  5.58s/it]                                                      87%| | 2826/3250 [4:23:32<39:27,  5.58s/it] 87%| | 2827/3250 [4:23:37<38:59,  5.53s/it]                                                      87%| | 2827/3250 [4:23:37<38:59,  5.53s/it] 87%| | 2828/3250 [4:23:42<38:35,  5.49s/it]                                                      87%| | 2828/3250 [4:23:42<38:35,  5.49s/it] 87%| | 2829/3250 [4:23:48<38:18,  5.46s/it]                                                      87%| | 2829/3250 [4:23:48<38:18,  5.46s/it] 87%| | 2830/3250 [4:23:53<38:05,  5.44s/it]                                                      87%| | 2830/3250 [4:23:53<38:05,  5.44s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8500683307647705, 'eval_runtime': 1.373, 'eval_samples_per_second': 8.74, 'eval_steps_per_second': 2.185, 'epoch': 0.87}
                                                      87%| | 2830/3250 [4:23:55<38:05,  5.44s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_4/tmp-checkpoint-2830
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2830I AM HERE AND I AM SAVING THE MODEL

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2830
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2830
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2830
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2830/pytorch_model.bin
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2830/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2830/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2830/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.6488, 'learning_rate': 4.050263960236384e-06, 'epoch': 0.87}
{'loss': 0.6643, 'learning_rate': 4.031217796995457e-06, 'epoch': 0.87}
{'loss': 0.6331, 'learning_rate': 4.012214639914796e-06, 'epoch': 0.87}
{'loss': 0.6934, 'learning_rate': 3.9932545067728366e-06, 'epoch': 0.87}
{'loss': 0.6913, 'learning_rate': 3.97433741530776e-06, 'epoch': 0.87}
 87%| | 2831/3250 [4:24:00<41:30,  5.94s/it]                                                      87%| | 2831/3250 [4:24:00<41:30,  5.94s/it] 87%| | 2832/3250 [4:24:06<40:16,  5.78s/it]                                                      87%| | 2832/3250 [4:24:06<40:16,  5.78s/it] 87%| | 2833/3250 [4:24:11<39:23,  5.67s/it]                                                      87%| | 2833/3250 [4:24:11<39:23,  5.67s/it] 87%| | 2834/3250 [4:24:16<38:44,  5.59s/it]                                                      87%| | 2834/3250 [4:24:16<38:44,  5.59s/it] 87%| | 2835/3250 [4:24:22<38:16,  5.53s/it]                                                      87%| | 2835/3250 [4:24:22<38:16,  5.53s/it] 87%|{'loss': 0.6552, 'learning_rate': 3.955463383217478e-06, 'epoch': 0.87}
{'loss': 0.6392, 'learning_rate': 3.936632428159609e-06, 'epoch': 0.87}
{'loss': 0.6469, 'learning_rate': 3.917844567751483e-06, 'epoch': 0.87}
{'loss': 0.6407, 'learning_rate': 3.899099819570112e-06, 'epoch': 0.87}
{'loss': 0.6568, 'learning_rate': 3.8803982011521685e-06, 'epoch': 0.87}
 | 2836/3250 [4:24:27<37:54,  5.49s/it]                                                      87%| | 2836/3250 [4:24:27<37:54,  5.49s/it] 87%| | 2837/3250 [4:24:33<38:24,  5.58s/it]                                                      87%| | 2837/3250 [4:24:33<38:24,  5.58s/it] 87%| | 2838/3250 [4:24:38<37:55,  5.52s/it]                                                      87%| | 2838/3250 [4:24:38<37:55,  5.52s/it] 87%| | 2839/3250 [4:24:44<37:34,  5.48s/it]                                                      87%| | 2839/3250 [4:24:44<37:34,  5.48s/it] 87%| | 2840/3250 [4:24:49<37:19,  5.46s/it]                                                      87%| | 2840/3250 [4:24:49<37:19,  5.46s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8493562936782837, 'eval_runtime': 1.373, 'eval_samples_per_second': 8.74, 'eval_steps_per_second': 2.185, 'epoch': 0.87}
                                                      87%| | 2840/3250 [4:24:51<37:19,  5.46s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_4/tmp-checkpoint-2840
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2840
/projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2840
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2840
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2840
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2840/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2840/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2840/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2840/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.6496, 'learning_rate': 3.861739729993991e-06, 'epoch': 0.87}
{'loss': 0.6312, 'learning_rate': 3.843124423551536e-06, 'epoch': 0.87}
{'loss': 0.6459, 'learning_rate': 3.824552299240364e-06, 'epoch': 0.87}
{'loss': 0.6729, 'learning_rate': 3.8060233744356633e-06, 'epoch': 0.88}
{'loss': 0.6469, 'learning_rate': 3.7875376664722016e-06, 'epoch': 0.88}
 87%| | 2841/3250 [4:24:56<40:36,  5.96s/it]                                                      87%| | 2841/3250 [4:24:56<40:36,  5.96s/it] 87%| | 2842/3250 [4:25:02<39:21,  5.79s/it]                                                      87%| | 2842/3250 [4:25:02<39:21,  5.79s/it] 87%| | 2843/3250 [4:25:07<38:29,  5.67s/it]                                                      87%| | 2843/3250 [4:25:07<38:29,  5.67s/it] 88%| | 2844/3250 [4:25:13<37:49,  5.59s/it]                                                      88%| | 2844/3250 [4:25:13<37:49,  5.59s/it] 88%| | 2845/3250 [4:25:18<37:19,  5.53s/it]                                                      88%| | 2845/3250 [4:25:18<37:19,  5.53s/it] 88%|{'loss': 0.6524, 'learning_rate': 3.7690951926443007e-06, 'epoch': 0.88}
{'loss': 0.6678, 'learning_rate': 3.750695970205853e-06, 'epoch': 0.88}
{'loss': 0.6591, 'learning_rate': 3.732340016370267e-06, 'epoch': 0.88}
{'loss': 0.6182, 'learning_rate': 3.7140273483104838e-06, 'epoch': 0.88}
{'loss': 0.7071, 'learning_rate': 3.6957579831589538e-06, 'epoch': 0.88}
 | 2846/3250 [4:25:23<36:58,  5.49s/it]                                                      88%| | 2846/3250 [4:25:23<36:58,  5.49s/it] 88%| | 2847/3250 [4:25:29<36:42,  5.47s/it]                                                      88%| | 2847/3250 [4:25:29<36:42,  5.47s/it] 88%| | 2848/3250 [4:25:34<36:32,  5.45s/it]                                                      88%| | 2848/3250 [4:25:34<36:32,  5.45s/it] 88%| | 2849/3250 [4:25:40<36:21,  5.44s/it]                                                      88%| | 2849/3250 [4:25:40<36:21,  5.44s/it] 88%| | 2850/3250 [4:25:45<36:13,  5.43s/it]                                                      88%| | 2850/3250 [4:25:45<36:13,  5.43s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8494071960449219, 'eval_runtime': 1.3784, 'eval_samples_per_second': 8.705, 'eval_steps_per_second': 2.176, 'epoch': 0.88}
                                                      88%| | 2850/3250 [4:25:46<36:13,  5.43s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_4/tmp-checkpoint-2850
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2850I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2850

I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2850
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2850
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2850/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2850/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2850/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2850/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.6642, 'learning_rate': 3.6775319380076e-06, 'epoch': 0.88}
{'loss': 0.6401, 'learning_rate': 3.659349229907827e-06, 'epoch': 0.88}
{'loss': 0.6355, 'learning_rate': 3.641209875870505e-06, 'epoch': 0.88}
{'loss': 0.6214, 'learning_rate': 3.62311389286592e-06, 'epoch': 0.88}
{'loss': 0.6758, 'learning_rate': 3.6050612978237862e-06, 'epoch': 0.88}
 88%| | 2851/3250 [4:25:52<39:42,  5.97s/it]                                                      88%| | 2851/3250 [4:25:52<39:42,  5.97s/it] 88%| | 2852/3250 [4:25:58<38:28,  5.80s/it]                                                      88%| | 2852/3250 [4:25:58<38:28,  5.80s/it] 88%| | 2853/3250 [4:26:04<38:36,  5.84s/it]                                                      88%| | 2853/3250 [4:26:04<38:36,  5.84s/it] 88%| | 2854/3250 [4:26:09<37:38,  5.70s/it]                                                      88%| | 2854/3250 [4:26:09<37:38,  5.70s/it] 88%| | 2855/3250 [4:26:14<36:55,  5.61s/it]                                                      88%| | 2855/3250 [4:26:14<36:55,  5.61s/it] 88%|{'loss': 0.6304, 'learning_rate': 3.5870521076332486e-06, 'epoch': 0.88}
{'loss': 0.6939, 'learning_rate': 3.5690863391428296e-06, 'epoch': 0.88}
{'loss': 1.1387, 'learning_rate': 3.551164009160429e-06, 'epoch': 0.88}
{'loss': 0.6311, 'learning_rate': 3.533285134453307e-06, 'epoch': 0.88}
{'loss': 0.6512, 'learning_rate': 3.5154497317480774e-06, 'epoch': 0.88}
 | 2856/3250 [4:26:20<36:25,  5.55s/it]                                                      88%| | 2856/3250 [4:26:20<36:25,  5.55s/it] 88%| | 2857/3250 [4:26:25<36:00,  5.50s/it]                                                      88%| | 2857/3250 [4:26:25<36:00,  5.50s/it] 88%| | 2858/3250 [4:26:31<35:41,  5.46s/it]                                                      88%| | 2858/3250 [4:26:31<35:41,  5.46s/it] 88%| | 2859/3250 [4:26:36<35:26,  5.44s/it]                                                      88%| | 2859/3250 [4:26:36<35:26,  5.44s/it] 88%| | 2860/3250 [4:26:41<35:13,  5.42s/it]                                                      88%| | 2860/3250 [4:26:41<35:13,  5.42s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8505300879478455, 'eval_runtime': 1.3723, 'eval_samples_per_second': 8.744, 'eval_steps_per_second': 2.186, 'epoch': 0.88}
                                                      88%| | 2860/3250 [4:26:43<35:13,  5.42s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_4/tmp-checkpoint-2860
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2860
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2860
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2860
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2860
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2860/pytorch_model.bin
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2860/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2860/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2860/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.6829, 'learning_rate': 3.497657817730665e-06, 'epoch': 0.88}
{'loss': 0.6608, 'learning_rate': 3.4799094090463226e-06, 'epoch': 0.88}
{'loss': 0.6244, 'learning_rate': 3.462204522299606e-06, 'epoch': 0.88}
{'loss': 0.6322, 'learning_rate': 3.4445431740543434e-06, 'epoch': 0.88}
{'loss': 0.7185, 'learning_rate': 3.4269253808336455e-06, 'epoch': 0.88}
 88%| | 2861/3250 [4:26:48<38:22,  5.92s/it]                                                      88%| | 2861/3250 [4:26:48<38:22,  5.92s/it] 88%| | 2862/3250 [4:26:54<37:14,  5.76s/it]                                                      88%| | 2862/3250 [4:26:54<37:14,  5.76s/it] 88%| | 2863/3250 [4:26:59<36:25,  5.65s/it]                                                      88%| | 2863/3250 [4:26:59<36:25,  5.65s/it] 88%| | 2864/3250 [4:27:05<35:50,  5.57s/it]                                                      88%| | 2864/3250 [4:27:05<35:50,  5.57s/it] 88%| | 2865/3250 [4:27:10<35:23,  5.52s/it]                                                      88%| | 2865/3250 [4:27:10<35:23,  5.52s/it] 88%|{'loss': 0.652, 'learning_rate': 3.4093511591198445e-06, 'epoch': 0.88}
{'loss': 0.6532, 'learning_rate': 3.391820525354539e-06, 'epoch': 0.88}
{'loss': 0.6022, 'learning_rate': 3.374333495938542e-06, 'epoch': 0.88}
{'loss': 0.6604, 'learning_rate': 3.3568900872318564e-06, 'epoch': 0.88}
{'loss': 0.6658, 'learning_rate': 3.3394903155537115e-06, 'epoch': 0.88}
 | 2866/3250 [4:27:15<35:02,  5.48s/it]                                                      88%| | 2866/3250 [4:27:15<35:02,  5.48s/it] 88%| | 2867/3250 [4:27:21<34:47,  5.45s/it]                                                      88%| | 2867/3250 [4:27:21<34:47,  5.45s/it] 88%| | 2868/3250 [4:27:26<34:39,  5.44s/it]                                                      88%| | 2868/3250 [4:27:26<34:39,  5.44s/it] 88%| | 2869/3250 [4:27:32<35:12,  5.54s/it]                                                      88%| | 2869/3250 [4:27:32<35:12,  5.54s/it] 88%| | 2870/3250 [4:27:37<34:49,  5.50s/it]                                                      88%| | 2870/3250 [4:27:37<34:49,  5.50s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8495454788208008, 'eval_runtime': 1.3856, 'eval_samples_per_second': 8.66, 'eval_steps_per_second': 2.165, 'epoch': 0.88}
                                                      88%| | 2870/3250 [4:27:39<34:49,  5.50s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_4/tmp-checkpoint-2870
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  I AM HERE AND I AM SAVING THE MODEL
/projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2870the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2870

I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2870
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2870
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2870/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2870/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2870/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2870/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.6392, 'learning_rate': 3.322134197182464e-06, 'epoch': 0.88}
{'loss': 0.628, 'learning_rate': 3.3048217483556744e-06, 'epoch': 0.88}
{'loss': 0.6515, 'learning_rate': 3.2875529852700147e-06, 'epoch': 0.88}
{'loss': 0.6649, 'learning_rate': 3.270327924081301e-06, 'epoch': 0.88}
{'loss': 0.6708, 'learning_rate': 3.253146580904476e-06, 'epoch': 0.88}
 88%| | 2871/3250 [4:27:44<37:59,  6.01s/it]                                                      88%| | 2871/3250 [4:27:44<37:59,  6.01s/it] 88%| | 2872/3250 [4:27:50<36:42,  5.83s/it]                                                      88%| | 2872/3250 [4:27:50<36:42,  5.83s/it] 88%| | 2873/3250 [4:27:55<35:50,  5.70s/it]                                                      88%| | 2873/3250 [4:27:55<35:50,  5.70s/it] 88%| | 2874/3250 [4:28:01<35:13,  5.62s/it]                                                      88%| | 2874/3250 [4:28:01<35:13,  5.62s/it] 88%| | 2875/3250 [4:28:06<34:42,  5.55s/it]                                                      88%| | 2875/3250 [4:28:06<34:42,  5.55s/it] 88%|{'loss': 0.668, 'learning_rate': 3.2360089718135587e-06, 'epoch': 0.88}
{'loss': 0.6487, 'learning_rate': 3.2189151128416695e-06, 'epoch': 0.89}
{'loss': 0.673, 'learning_rate': 3.201865019981004e-06, 'epoch': 0.89}
{'loss': 0.6399, 'learning_rate': 3.184858709182775e-06, 'epoch': 0.89}
{'loss': 0.664, 'learning_rate': 3.167896196357284e-06, 'epoch': 0.89}
 | 2876/3250 [4:28:12<34:20,  5.51s/it]                                                      88%| | 2876/3250 [4:28:12<34:20,  5.51s/it] 89%| | 2877/3250 [4:28:17<34:05,  5.48s/it]                                                      89%| | 2877/3250 [4:28:17<34:05,  5.48s/it] 89%| | 2878/3250 [4:28:22<33:51,  5.46s/it]                                                      89%| | 2878/3250 [4:28:22<33:51,  5.46s/it] 89%| | 2879/3250 [4:28:28<33:41,  5.45s/it]                                                      89%| | 2879/3250 [4:28:28<33:41,  5.45s/it] 89%| | 2880/3250 [4:28:33<33:32,  5.44s/it]                                                      89%| | 2880/3250 [4:28:33<33:32,  5.44s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.848702073097229, 'eval_runtime': 1.376, 'eval_samples_per_second': 8.721, 'eval_steps_per_second': 2.18, 'epoch': 0.89}
                                                      89%| | 2880/3250 [4:28:35<33:32,  5.44s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_4/tmp-checkpoint-2880
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2880I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2880

I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2880
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2880
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2880/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2880/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2880/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2880/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.6666, 'learning_rate': 3.1509774973738193e-06, 'epoch': 0.89}
{'loss': 0.6395, 'learning_rate': 3.134102628060698e-06, 'epoch': 0.89}
{'loss': 0.6342, 'learning_rate': 3.117271604205241e-06, 'epoch': 0.89}
{'loss': 0.6253, 'learning_rate': 3.1004844415537194e-06, 'epoch': 0.89}
{'loss': 0.679, 'learning_rate': 3.0837411558113984e-06, 'epoch': 0.89}
 89%| | 2881/3250 [4:28:40<36:31,  5.94s/it]                                                      89%| | 2881/3250 [4:28:40<36:31,  5.94s/it] 89%| | 2882/3250 [4:28:46<35:22,  5.77s/it]                                                      89%| | 2882/3250 [4:28:46<35:22,  5.77s/it] 89%| | 2883/3250 [4:28:51<34:31,  5.64s/it]                                                      89%| | 2883/3250 [4:28:51<34:31,  5.64s/it] 89%| | 2884/3250 [4:28:56<33:56,  5.56s/it]                                                      89%| | 2884/3250 [4:28:56<33:56,  5.56s/it] 89%| | 2885/3250 [4:29:02<33:29,  5.51s/it]                                                      89%| | 2885/3250 [4:29:02<33:29,  5.51s/it] 89%|{'loss': 0.636, 'learning_rate': 3.067041762642475e-06, 'epoch': 0.89}
{'loss': 0.6938, 'learning_rate': 3.050386277670103e-06, 'epoch': 0.89}
{'loss': 1.157, 'learning_rate': 3.033774716476329e-06, 'epoch': 0.89}
{'loss': 0.6155, 'learning_rate': 3.017207094602126e-06, 'epoch': 0.89}
{'loss': 0.6206, 'learning_rate': 3.000683427547374e-06, 'epoch': 0.89}
 | 2886/3250 [4:29:08<34:07,  5.63s/it]                                                      89%| | 2886/3250 [4:29:08<34:07,  5.63s/it] 89%| | 2887/3250 [4:29:13<33:34,  5.55s/it]                                                      89%| | 2887/3250 [4:29:13<33:34,  5.55s/it] 89%| | 2888/3250 [4:29:18<33:11,  5.50s/it]                                                      89%| | 2888/3250 [4:29:18<33:11,  5.50s/it] 89%| | 2889/3250 [4:29:24<32:51,  5.46s/it]                                                      89%| | 2889/3250 [4:29:24<32:51,  5.46s/it] 89%| | 2890/3250 [4:29:29<32:37,  5.44s/it]                                                      89%| | 2890/3250 [4:29:29<32:37,  5.44s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8495388627052307, 'eval_runtime': 1.3806, 'eval_samples_per_second': 8.692, 'eval_steps_per_second': 2.173, 'epoch': 0.89}
                                                      89%| | 2890/3250 [4:29:31<32:37,  5.44s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_4/tmp-checkpoint-2890
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2890
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2890
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2890
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2890
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2890/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2890/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2890/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2890/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.6601, 'learning_rate': 2.9842037307707906e-06, 'epoch': 0.89}
{'loss': 0.6703, 'learning_rate': 2.9677680196899925e-06, 'epoch': 0.89}
{'loss': 0.626, 'learning_rate': 2.9513763096814305e-06, 'epoch': 0.89}
{'loss': 0.6341, 'learning_rate': 2.935028616080393e-06, 'epoch': 0.89}
{'loss': 0.6959, 'learning_rate': 2.918724954180985e-06, 'epoch': 0.89}
 89%| | 2891/3250 [4:29:36<35:33,  5.94s/it]                                                      89%| | 2891/3250 [4:29:36<35:33,  5.94s/it] 89%| | 2892/3250 [4:29:42<34:27,  5.78s/it]                                                      89%| | 2892/3250 [4:29:42<34:27,  5.78s/it] 89%| | 2893/3250 [4:29:47<33:40,  5.66s/it]                                                      89%| | 2893/3250 [4:29:47<33:40,  5.66s/it] 89%| | 2894/3250 [4:29:52<33:05,  5.58s/it]                                                      89%| | 2894/3250 [4:29:52<33:05,  5.58s/it] 89%| | 2895/3250 [4:29:58<32:38,  5.52s/it]                                                      89%| | 2895/3250 [4:29:58<32:38,  5.52s/it] 89%|{'loss': 0.6857, 'learning_rate': 2.9024653392361324e-06, 'epoch': 0.89}
{'loss': 0.6353, 'learning_rate': 2.886249786457523e-06, 'epoch': 0.89}
{'loss': 0.6134, 'learning_rate': 2.8700783110156503e-06, 'epoch': 0.89}
{'loss': 0.6606, 'learning_rate': 2.8539509280397614e-06, 'epoch': 0.89}
{'loss': 0.6442, 'learning_rate': 2.8378676526178482e-06, 'epoch': 0.89}
 | 2896/3250 [4:30:03<32:17,  5.47s/it]                                                      89%| | 2896/3250 [4:30:03<32:17,  5.47s/it] 89%| | 2897/3250 [4:30:09<32:01,  5.44s/it]                                                      89%| | 2897/3250 [4:30:09<32:01,  5.44s/it] 89%| | 2898/3250 [4:30:14<32:14,  5.50s/it]                                                      89%| | 2898/3250 [4:30:14<32:14,  5.50s/it] 89%| | 2899/3250 [4:30:20<31:58,  5.47s/it]                                                      89%| | 2899/3250 [4:30:20<31:58,  5.47s/it] 89%| | 2900/3250 [4:30:25<31:44,  5.44s/it]                                                      89%| | 2900/3250 [4:30:25<31:44,  5.44s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.850501537322998, 'eval_runtime': 1.3728, 'eval_samples_per_second': 8.741, 'eval_steps_per_second': 2.185, 'epoch': 0.89}
                                                      89%| | 2900/3250 [4:30:26<31:44,  5.44s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_4/tmp-checkpoint-2900
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2900
/projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2900
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2900
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2900
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2900/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2900/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2900/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2900/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.6472, 'learning_rate': 2.8218284997966527e-06, 'epoch': 0.89}
{'loss': 0.6301, 'learning_rate': 2.8058334845816213e-06, 'epoch': 0.89}
{'loss': 0.6468, 'learning_rate': 2.789882621936912e-06, 'epoch': 0.89}
{'loss': 0.6493, 'learning_rate': 2.7739759267853827e-06, 'epoch': 0.89}
{'loss': 0.6801, 'learning_rate': 2.758113414008551e-06, 'epoch': 0.89}
 89%| | 2901/3250 [4:30:32<34:36,  5.95s/it]                                                      89%| | 2901/3250 [4:30:32<34:36,  5.95s/it] 89%| | 2902/3250 [4:30:38<34:09,  5.89s/it]                                                      89%| | 2902/3250 [4:30:38<34:09,  5.89s/it] 89%| | 2903/3250 [4:30:43<33:10,  5.74s/it]                                                      89%| | 2903/3250 [4:30:43<33:10,  5.74s/it] 89%| | 2904/3250 [4:30:49<32:26,  5.63s/it]                                                      89%| | 2904/3250 [4:30:49<32:26,  5.63s/it] 89%| | 2905/3250 [4:30:54<31:54,  5.55s/it]                                                      89%| | 2905/3250 [4:30:54<31:54,  5.55s/it] 89%|{'loss': 0.6539, 'learning_rate': 2.7422950984466233e-06, 'epoch': 0.89}
{'loss': 0.6561, 'learning_rate': 2.7265209948984514e-06, 'epoch': 0.89}
{'loss': 0.6479, 'learning_rate': 2.7107911181215197e-06, 'epoch': 0.89}
{'loss': 0.6666, 'learning_rate': 2.695105482831928e-06, 'epoch': 0.9}
{'loss': 0.6146, 'learning_rate': 2.6794641037043988e-06, 'epoch': 0.9}
 | 2906/3250 [4:30:59<31:30,  5.50s/it]                                                      89%| | 2906/3250 [4:30:59<31:30,  5.50s/it] 89%| | 2907/3250 [4:31:05<31:13,  5.46s/it]                                                      89%| | 2907/3250 [4:31:05<31:13,  5.46s/it] 89%| | 2908/3250 [4:31:10<31:00,  5.44s/it]                                                      89%| | 2908/3250 [4:31:10<31:00,  5.44s/it] 90%| | 2909/3250 [4:31:16<30:49,  5.42s/it]                                                      90%| | 2909/3250 [4:31:16<30:49,  5.42s/it] 90%| | 2910/3250 [4:31:21<30:40,  5.41s/it]                                                      90%| | 2910/3250 [4:31:21<30:40,  5.41s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8483707904815674, 'eval_runtime': 1.3917, 'eval_samples_per_second': 8.622, 'eval_steps_per_second': 2.156, 'epoch': 0.9}
                                                      90%| | 2910/3250 [4:31:22<30:40,  5.41s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_4/tmp-checkpoint-2910
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2910
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2910
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2910
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2910
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2910/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2910/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2910/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2910/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.6973, 'learning_rate': 2.6638669953722496e-06, 'epoch': 0.9}
{'loss': 0.63, 'learning_rate': 2.648314172427374e-06, 'epoch': 0.9}
{'loss': 0.6472, 'learning_rate': 2.6328056494202445e-06, 'epoch': 0.9}
{'loss': 0.6319, 'learning_rate': 2.6173414408598827e-06, 'epoch': 0.9}
{'loss': 0.6296, 'learning_rate': 2.6019215612138383e-06, 'epoch': 0.9}
 90%| | 2911/3250 [4:31:28<33:29,  5.93s/it]                                                      90%| | 2911/3250 [4:31:28<33:29,  5.93s/it] 90%| | 2912/3250 [4:31:33<32:28,  5.76s/it]                                                      90%| | 2912/3250 [4:31:33<32:28,  5.76s/it] 90%| | 2913/3250 [4:31:39<31:42,  5.65s/it]                                                      90%| | 2913/3250 [4:31:39<31:42,  5.65s/it] 90%| | 2914/3250 [4:31:44<31:10,  5.57s/it]                                                      90%| | 2914/3250 [4:31:44<31:10,  5.57s/it] 90%| | 2915/3250 [4:31:50<30:46,  5.51s/it]                                                      90%| | 2915/3250 [4:31:50<30:46,  5.51s/it] 90%|{'loss': 0.655, 'learning_rate': 2.5865460249082097e-06, 'epoch': 0.9}
{'loss': 0.6773, 'learning_rate': 2.571214846327602e-06, 'epoch': 0.9}
{'loss': 0.6725, 'learning_rate': 2.5559280398151253e-06, 'epoch': 0.9}
{'loss': 1.1222, 'learning_rate': 2.5406856196723672e-06, 'epoch': 0.9}
{'loss': 0.6446, 'learning_rate': 2.525487600159404e-06, 'epoch': 0.9}
 | 2916/3250 [4:31:55<30:25,  5.47s/it]                                                      90%| | 2916/3250 [4:31:55<30:25,  5.47s/it] 90%| | 2917/3250 [4:32:00<30:12,  5.44s/it]                                                      90%| | 2917/3250 [4:32:00<30:12,  5.44s/it] 90%| | 2918/3250 [4:32:06<30:27,  5.50s/it]                                                      90%| | 2918/3250 [4:32:06<30:27,  5.50s/it] 90%| | 2919/3250 [4:32:11<30:06,  5.46s/it]                                                      90%| | 2919/3250 [4:32:11<30:06,  5.46s/it] 90%| | 2920/3250 [4:32:17<29:53,  5.44s/it]                                                      90%| | 2920/3250 [4:32:17<29:53,  5.44s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8494411110877991, 'eval_runtime': 1.6263, 'eval_samples_per_second': 7.378, 'eval_steps_per_second': 1.845, 'epoch': 0.9}
                                                      90%| | 2920/3250 [4:32:18<29:53,  5.44s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_4/tmp-checkpoint-2920
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2920
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2920
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2920
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2920
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2920/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2920/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2920/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2920/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.6639, 'learning_rate': 2.5103339954947626e-06, 'epoch': 0.9}
{'loss': 0.6503, 'learning_rate': 2.4952248198554073e-06, 'epoch': 0.9}
{'loss': 0.6581, 'learning_rate': 2.480160087376754e-06, 'epoch': 0.9}
{'loss': 0.6226, 'learning_rate': 2.465139812152639e-06, 'epoch': 0.9}
{'loss': 0.6974, 'learning_rate': 2.450164008235306e-06, 'epoch': 0.9}
 90%| | 2921/3250 [4:32:24<33:03,  6.03s/it]                                                      90%| | 2921/3250 [4:32:24<33:03,  6.03s/it] 90%| | 2922/3250 [4:32:29<31:53,  5.83s/it]                                                      90%| | 2922/3250 [4:32:29<31:53,  5.83s/it] 90%| | 2923/3250 [4:32:35<31:02,  5.70s/it]                                                      90%| | 2923/3250 [4:32:35<31:02,  5.70s/it] 90%| | 2924/3250 [4:32:40<30:25,  5.60s/it]                                                      90%| | 2924/3250 [4:32:40<30:25,  5.60s/it] 90%| | 2925/3250 [4:32:46<29:58,  5.53s/it]                                                      90%| | 2925/3250 [4:32:46<29:58,  5.53s/it] 90%|{'loss': 0.6883, 'learning_rate': 2.435232689635386e-06, 'epoch': 0.9}
{'loss': 0.647, 'learning_rate': 2.4203458703218997e-06, 'epoch': 0.9}
{'loss': 0.6366, 'learning_rate': 2.4055035642222224e-06, 'epoch': 0.9}
{'loss': 0.6434, 'learning_rate': 2.390705785222097e-06, 'epoch': 0.9}
{'loss': 0.6348, 'learning_rate': 2.3759525471656162e-06, 'epoch': 0.9}
 | 2926/3250 [4:32:51<29:36,  5.48s/it]                                                      90%| | 2926/3250 [4:32:51<29:36,  5.48s/it] 90%| | 2927/3250 [4:32:56<29:20,  5.45s/it]                                                      90%| | 2927/3250 [4:32:56<29:20,  5.45s/it] 90%| | 2928/3250 [4:33:02<29:09,  5.43s/it]                                                      90%| | 2928/3250 [4:33:02<29:09,  5.43s/it] 90%| | 2929/3250 [4:33:07<28:59,  5.42s/it]                                                      90%| | 2929/3250 [4:33:07<28:59,  5.42s/it] 90%| | 2930/3250 [4:33:12<28:50,  5.41s/it]                                                      90%| | 2930/3250 [4:33:12<28:50,  5.41s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8500329852104187, 'eval_runtime': 1.3819, 'eval_samples_per_second': 8.683, 'eval_steps_per_second': 2.171, 'epoch': 0.9}
                                                      90%| | 2930/3250 [4:33:14<28:50,  5.41s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_4/tmp-checkpoint-2930
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2930
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2930
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2930
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2930
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2930/pytorch_model.bin
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2930/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2930/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2930/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.649, 'learning_rate': 2.361243863855184e-06, 'epoch': 0.9}
{'loss': 0.6455, 'learning_rate': 2.3465797490515418e-06, 'epoch': 0.9}
{'loss': 0.6303, 'learning_rate': 2.3319602164737053e-06, 'epoch': 0.9}
{'loss': 0.6523, 'learning_rate': 2.3173852797990114e-06, 'epoch': 0.9}
{'loss': 0.6625, 'learning_rate': 2.3028549526630583e-06, 'epoch': 0.9}
 90%| | 2931/3250 [4:33:20<31:31,  5.93s/it]                                                      90%| | 2931/3250 [4:33:20<31:31,  5.93s/it] 90%| | 2932/3250 [4:33:25<30:33,  5.77s/it]                                                      90%| | 2932/3250 [4:33:25<30:33,  5.77s/it] 90%| | 2933/3250 [4:33:30<29:50,  5.65s/it]                                                      90%| | 2933/3250 [4:33:30<29:50,  5.65s/it] 90%| | 2934/3250 [4:33:36<29:18,  5.57s/it]                                                      90%| | 2934/3250 [4:33:36<29:18,  5.57s/it] 90%| | 2935/3250 [4:33:42<29:31,  5.62s/it]                                                      90%| | 2935/3250 [4:33:42<29:31,  5.62s/it] 90%|{'loss': 0.648, 'learning_rate': 2.288369248659722e-06, 'epoch': 0.9}
{'loss': 0.6526, 'learning_rate': 2.2739281813411116e-06, 'epoch': 0.9}
{'loss': 0.6623, 'learning_rate': 2.2595317642176038e-06, 'epoch': 0.9}
{'loss': 0.6664, 'learning_rate': 2.2451800107577805e-06, 'epoch': 0.9}
{'loss': 0.6187, 'learning_rate': 2.2308729343884395e-06, 'epoch': 0.9}
 | 2936/3250 [4:33:47<29:03,  5.55s/it]                                                      90%| | 2936/3250 [4:33:47<29:03,  5.55s/it] 90%| | 2937/3250 [4:33:52<28:40,  5.50s/it]                                                      90%| | 2937/3250 [4:33:52<28:40,  5.50s/it] 90%| | 2938/3250 [4:33:58<28:24,  5.46s/it]                                                      90%| | 2938/3250 [4:33:58<28:24,  5.46s/it] 90%| | 2939/3250 [4:34:03<28:12,  5.44s/it]                                                      90%| | 2939/3250 [4:34:03<28:12,  5.44s/it] 90%| | 2940/3250 [4:34:08<28:00,  5.42s/it]                                                      90%| | 2940/3250 [4:34:08<28:00,  5.42s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8488135933876038, 'eval_runtime': 1.3845, 'eval_samples_per_second': 8.667, 'eval_steps_per_second': 2.167, 'epoch': 0.9}
                                                      90%| | 2940/3250 [4:34:10<28:00,  5.42s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_4/tmp-checkpoint-2940
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2940
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2940
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2940
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2940
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2940/pytorch_model.bin
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2940/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2940/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2940/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.7019, 'learning_rate': 2.2166105484945853e-06, 'epoch': 0.9}
{'loss': 0.6526, 'learning_rate': 2.202392866419423e-06, 'epoch': 0.91}
{'loss': 0.6275, 'learning_rate': 2.188219901464317e-06, 'epoch': 0.91}
{'loss': 0.6343, 'learning_rate': 2.1740916668888113e-06, 'epoch': 0.91}
{'loss': 0.6221, 'learning_rate': 2.160008175910605e-06, 'epoch': 0.91}
 90%| | 2941/3250 [4:34:16<30:27,  5.91s/it]                                                      90%| | 2941/3250 [4:34:16<30:27,  5.91s/it] 91%| | 2942/3250 [4:34:21<29:35,  5.76s/it]                                                      91%| | 2942/3250 [4:34:21<29:35,  5.76s/it] 91%| | 2943/3250 [4:34:26<28:54,  5.65s/it]                                                      91%| | 2943/3250 [4:34:26<28:54,  5.65s/it] 91%| | 2944/3250 [4:34:32<28:24,  5.57s/it]                                                      91%| | 2944/3250 [4:34:32<28:24,  5.57s/it] 91%| | 2945/3250 [4:34:37<27:59,  5.51s/it]                                                      91%| | 2945/3250 [4:34:37<27:59,  5.51s/it] 91%|{'loss': 0.6768, 'learning_rate': 2.1459694417055034e-06, 'epoch': 0.91}
{'loss': 0.6266, 'learning_rate': 2.131975477407483e-06, 'epoch': 0.91}
{'loss': 0.6919, 'learning_rate': 2.1180262961086108e-06, 'epoch': 0.91}
{'loss': 1.1454, 'learning_rate': 2.1041219108590692e-06, 'epoch': 0.91}
{'loss': 0.6259, 'learning_rate': 2.0902623346671258e-06, 'epoch': 0.91}
 | 2946/3250 [4:34:42<27:43,  5.47s/it]                                                      91%| | 2946/3250 [4:34:42<27:43,  5.47s/it] 91%| | 2947/3250 [4:34:48<27:30,  5.45s/it]                                                      91%| | 2947/3250 [4:34:48<27:30,  5.45s/it] 91%| | 2948/3250 [4:34:53<27:19,  5.43s/it]                                                      91%| | 2948/3250 [4:34:53<27:19,  5.43s/it] 91%| | 2949/3250 [4:34:59<27:08,  5.41s/it]                                                      91%| | 2949/3250 [4:34:59<27:08,  5.41s/it] 91%| | 2950/3250 [4:35:04<27:00,  5.40s/it]                                                      91%| | 2950/3250 [4:35:04<27:00,  5.40s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.850075364112854, 'eval_runtime': 1.3781, 'eval_samples_per_second': 8.707, 'eval_steps_per_second': 2.177, 'epoch': 0.91}
                                                      91%| | 2950/3250 [4:35:05<27:00,  5.40s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_4/tmp-checkpoint-2950
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2950
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2950
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2950
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2950
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2950/pytorch_model.bin
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2950/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2950/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2950/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.6466, 'learning_rate': 2.076447580499119e-06, 'epoch': 0.91}
{'loss': 0.6819, 'learning_rate': 2.0626776612794607e-06, 'epoch': 0.91}
{'loss': 0.6676, 'learning_rate': 2.0489525898906294e-06, 'epoch': 0.91}
{'loss': 0.6279, 'learning_rate': 2.0352723791731366e-06, 'epoch': 0.91}
{'loss': 0.6396, 'learning_rate': 2.021637041925506e-06, 'epoch': 0.91}
 91%| | 2951/3250 [4:35:11<29:48,  5.98s/it]                                                      91%| | 2951/3250 [4:35:11<29:48,  5.98s/it] 91%| | 2952/3250 [4:35:17<28:47,  5.80s/it]                                                      91%| | 2952/3250 [4:35:17<28:47,  5.80s/it] 91%| | 2953/3250 [4:35:22<28:07,  5.68s/it]                                                      91%| | 2953/3250 [4:35:22<28:07,  5.68s/it] 91%| | 2954/3250 [4:35:27<27:35,  5.59s/it]                                                      91%| | 2954/3250 [4:35:27<27:35,  5.59s/it] 91%| | 2955/3250 [4:35:33<27:11,  5.53s/it]                                                      91%| | 2955/3250 [4:35:33<27:11,  5.53s/it] 91%|{'loss': 0.7287, 'learning_rate': 2.0080465909043113e-06, 'epoch': 0.91}
{'loss': 0.6485, 'learning_rate': 1.99450103882412e-06, 'epoch': 0.91}
{'loss': 0.656, 'learning_rate': 1.98100039835748e-06, 'epoch': 0.91}
{'loss': 0.6115, 'learning_rate': 1.967544682134942e-06, 'epoch': 0.91}
{'loss': 0.6603, 'learning_rate': 1.9541339027450256e-06, 'epoch': 0.91}
 | 2956/3250 [4:35:38<26:52,  5.48s/it]                                                      91%| | 2956/3250 [4:35:38<26:52,  5.48s/it] 91%| | 2957/3250 [4:35:44<26:36,  5.45s/it]                                                      91%| | 2957/3250 [4:35:44<26:36,  5.45s/it] 91%| | 2958/3250 [4:35:49<26:26,  5.43s/it]                                                      91%| | 2958/3250 [4:35:49<26:26,  5.43s/it] 91%| | 2959/3250 [4:35:54<26:14,  5.41s/it]                                                      91%| | 2959/3250 [4:35:54<26:14,  5.41s/it] 91%| | 2960/3250 [4:36:00<26:05,  5.40s/it]                                                      91%| | 2960/3250 [4:36:00<26:05,  5.40s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8488762378692627, 'eval_runtime': 1.6162, 'eval_samples_per_second': 7.425, 'eval_steps_per_second': 1.856, 'epoch': 0.91}
                                                      91%| | 2960/3250 [4:36:01<26:05,  5.40s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_4/tmp-checkpoint-2960
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2960I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2960

I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2960
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2960
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2960/pytorch_model.bin
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2960/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2960/pytorch_model.binthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2960/pytorch_model.bin

/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.666, 'learning_rate': 1.940768072734195e-06, 'epoch': 0.91}
{'loss': 0.6406, 'learning_rate': 1.92744720460688e-06, 'epoch': 0.91}
{'loss': 0.6344, 'learning_rate': 1.914171310825441e-06, 'epoch': 0.91}
{'loss': 0.6441, 'learning_rate': 1.9009404038101474e-06, 'epoch': 0.91}
{'loss': 0.664, 'learning_rate': 1.887754495939198e-06, 'epoch': 0.91}
 91%| | 2961/3250 [4:36:07<28:54,  6.00s/it]                                                      91%| | 2961/3250 [4:36:07<28:54,  6.00s/it] 91%| | 2962/3250 [4:36:12<27:53,  5.81s/it]                                                      91%| | 2962/3250 [4:36:12<27:53,  5.81s/it] 91%| | 2963/3250 [4:36:18<27:10,  5.68s/it]                                                      91%| | 2963/3250 [4:36:18<27:10,  5.68s/it] 91%| | 2964/3250 [4:36:23<26:38,  5.59s/it]                                                      91%| | 2964/3250 [4:36:23<26:38,  5.59s/it] 91%| | 2965/3250 [4:36:29<26:14,  5.52s/it]                                                      91%| | 2965/3250 [4:36:29<26:14,  5.52s/it] 91%|{'loss': 0.6748, 'learning_rate': 1.8746135995486858e-06, 'epoch': 0.91}
{'loss': 0.66, 'learning_rate': 1.8615177269326045e-06, 'epoch': 0.91}
{'loss': 0.6563, 'learning_rate': 1.848466890342815e-06, 'epoch': 0.91}
{'loss': 0.6692, 'learning_rate': 1.8354611019890332e-06, 'epoch': 0.91}
{'loss': 0.641, 'learning_rate': 1.8225003740388547e-06, 'epoch': 0.91}
| 2966/3250 [4:36:34<25:54,  5.47s/it]                                                      91%|| 2966/3250 [4:36:34<25:54,  5.47s/it] 91%|| 2967/3250 [4:36:39<25:39,  5.44s/it]                                                      91%|| 2967/3250 [4:36:39<25:39,  5.44s/it] 91%|| 2968/3250 [4:36:45<26:03,  5.54s/it]                                                      91%|| 2968/3250 [4:36:45<26:03,  5.54s/it] 91%|| 2969/3250 [4:36:50<25:44,  5.49s/it]                                                      91%|| 2969/3250 [4:36:50<25:44,  5.49s/it] 91%|| 2970/3250 [4:36:56<25:27,  5.46s/it]                                                      91%|| 2970/3250 [4:36:56<25:27,  5.46s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8496557474136353, 'eval_runtime': 1.3888, 'eval_samples_per_second': 8.641, 'eval_steps_per_second': 2.16, 'epoch': 0.91}
                                                      91%|| 2970/3250 [4:36:57<25:27,  5.46s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_4/tmp-checkpoint-2970
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2970
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2970
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2970
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2970
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2970/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2970/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2970/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2970/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.66, 'learning_rate': 1.8095847186177073e-06, 'epoch': 0.91}
{'loss': 0.655, 'learning_rate': 1.7967141478088422e-06, 'epoch': 0.91}
{'loss': 0.6322, 'learning_rate': 1.78388867365335e-06, 'epoch': 0.91}
{'loss': 0.6252, 'learning_rate': 1.7711083081501156e-06, 'epoch': 0.92}
{'loss': 0.6279, 'learning_rate': 1.7583730632558304e-06, 'epoch': 0.92}
 91%|| 2971/3250 [4:37:03<27:39,  5.95s/it]                                                      91%|| 2971/3250 [4:37:03<27:39,  5.95s/it] 91%|| 2972/3250 [4:37:08<26:45,  5.77s/it]                                                      91%|| 2972/3250 [4:37:08<26:45,  5.77s/it] 91%|| 2973/3250 [4:37:14<26:06,  5.66s/it]                                                      91%|| 2973/3250 [4:37:14<26:06,  5.66s/it] 92%|| 2974/3250 [4:37:19<25:37,  5.57s/it]                                                      92%|| 2974/3250 [4:37:19<25:37,  5.57s/it] 92%|| 2975/3250 [4:37:24<25:18,  5.52s/it]                                                      92%|| 2975/3250 [4:37:24<25:18,  5.52s/it] 92%|{'loss': 0.6754, 'learning_rate': 1.7456829508849748e-06, 'epoch': 0.92}
{'loss': 0.63, 'learning_rate': 1.733037982909791e-06, 'epoch': 0.92}
{'loss': 0.7019, 'learning_rate': 1.7204381711603045e-06, 'epoch': 0.92}
{'loss': 1.1589, 'learning_rate': 1.7078835274242922e-06, 'epoch': 0.92}
{'loss': 0.6019, 'learning_rate': 1.6953740634472581e-06, 'epoch': 0.92}
| 2976/3250 [4:37:30<25:01,  5.48s/it]                                                      92%|| 2976/3250 [4:37:30<25:01,  5.48s/it] 92%|| 2977/3250 [4:37:35<24:47,  5.45s/it]                                                      92%|| 2977/3250 [4:37:35<24:47,  5.45s/it] 92%|| 2978/3250 [4:37:41<24:37,  5.43s/it]                                                      92%|| 2978/3250 [4:37:41<24:37,  5.43s/it] 92%|| 2979/3250 [4:37:46<24:27,  5.42s/it]                                                      92%|| 2979/3250 [4:37:46<24:27,  5.42s/it] 92%|| 2980/3250 [4:37:51<24:19,  5.40s/it]                                                      92%|| 2980/3250 [4:37:51<24:19,  5.40s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.849554181098938, 'eval_runtime': 1.3802, 'eval_samples_per_second': 8.694, 'eval_steps_per_second': 2.174, 'epoch': 0.92}
                                                      92%|| 2980/3250 [4:37:53<24:19,  5.40s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_4/tmp-checkpoint-2980
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2980I AM HERE AND I AM SAVING THE MODEL

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2980
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2980
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2980
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2980/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2980/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2980/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2980/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.6288, 'learning_rate': 1.6829097909324632e-06, 'epoch': 0.92}
{'loss': 0.6635, 'learning_rate': 1.6704907215408573e-06, 'epoch': 0.92}
{'loss': 0.6695, 'learning_rate': 1.658116866891135e-06, 'epoch': 0.92}
{'loss': 0.6312, 'learning_rate': 1.6457882385596646e-06, 'epoch': 0.92}
{'loss': 0.6444, 'learning_rate': 1.6335048480805083e-06, 'epoch': 0.92}
 92%|| 2981/3250 [4:37:58<26:30,  5.91s/it]                                                      92%|| 2981/3250 [4:37:58<26:30,  5.91s/it] 92%|| 2982/3250 [4:38:04<25:41,  5.75s/it]                                                      92%|| 2982/3250 [4:38:04<25:41,  5.75s/it] 92%|| 2983/3250 [4:38:09<25:04,  5.63s/it]                                                      92%|| 2983/3250 [4:38:09<25:04,  5.63s/it] 92%|| 2984/3250 [4:38:15<24:58,  5.63s/it]                                                      92%|| 2984/3250 [4:38:15<24:58,  5.63s/it] 92%|| 2985/3250 [4:38:20<24:31,  5.55s/it]                                                      92%|| 2985/3250 [4:38:20<24:31,  5.55s/it] 92%|{'loss': 0.6921, 'learning_rate': 1.6212667069454291e-06, 'epoch': 0.92}
{'loss': 0.6894, 'learning_rate': 1.6090738266038186e-06, 'epoch': 0.92}
{'loss': 0.6407, 'learning_rate': 1.596926218462752e-06, 'epoch': 0.92}
{'loss': 0.6038, 'learning_rate': 1.584823893886933e-06, 'epoch': 0.92}
{'loss': 0.6653, 'learning_rate': 1.572766864198716e-06, 'epoch': 0.92}
| 2986/3250 [4:38:26<24:10,  5.49s/it]                                                      92%|| 2986/3250 [4:38:26<24:10,  5.49s/it] 92%|| 2987/3250 [4:38:31<23:54,  5.45s/it]                                                      92%|| 2987/3250 [4:38:31<23:54,  5.45s/it] 92%|| 2988/3250 [4:38:36<23:42,  5.43s/it]                                                      92%|| 2988/3250 [4:38:36<23:42,  5.43s/it] 92%|| 2989/3250 [4:38:42<23:31,  5.41s/it]                                                      92%|| 2989/3250 [4:38:42<23:31,  5.41s/it] 92%|| 2990/3250 [4:38:47<23:22,  5.39s/it]                                                      92%|| 2990/3250 [4:38:47<23:22,  5.39s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.850346028804779, 'eval_runtime': 1.3806, 'eval_samples_per_second': 8.692, 'eval_steps_per_second': 2.173, 'epoch': 0.92}
                                                      92%|| 2990/3250 [4:38:48<23:22,  5.39s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_4/tmp-checkpoint-2990
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2990
/projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2990
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2990
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2990
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2990/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2990/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2990/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-2990/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.6526, 'learning_rate': 1.5607551406780717e-06, 'epoch': 0.92}
{'loss': 0.6528, 'learning_rate': 1.548788734562584e-06, 'epoch': 0.92}
{'loss': 0.6375, 'learning_rate': 1.5368676570474471e-06, 'epoch': 0.92}
{'loss': 0.6476, 'learning_rate': 1.524991919285429e-06, 'epoch': 0.92}
{'loss': 0.6489, 'learning_rate': 1.5131615323869085e-06, 'epoch': 0.92}
 92%|| 2991/3250 [4:38:54<25:27,  5.90s/it]                                                      92%|| 2991/3250 [4:38:54<25:27,  5.90s/it] 92%|| 2992/3250 [4:38:59<24:39,  5.74s/it]                                                      92%|| 2992/3250 [4:38:59<24:39,  5.74s/it] 92%|| 2993/3250 [4:39:05<24:04,  5.62s/it]                                                      92%|| 2993/3250 [4:39:05<24:04,  5.62s/it] 92%|| 2994/3250 [4:39:10<23:37,  5.54s/it]                                                      92%|| 2994/3250 [4:39:10<23:37,  5.54s/it] 92%|| 2995/3250 [4:39:16<23:19,  5.49s/it]                                                      92%|| 2995/3250 [4:39:16<23:19,  5.49s/it] 92%|{'loss': 0.6833, 'learning_rate': 1.501376507419805e-06, 'epoch': 0.92}
{'loss': 0.6532, 'learning_rate': 1.4896368554096318e-06, 'epoch': 0.92}
{'loss': 0.6579, 'learning_rate': 1.4779425873394259e-06, 'epoch': 0.92}
{'loss': 0.6509, 'learning_rate': 1.4662937141497911e-06, 'epoch': 0.92}
{'loss': 0.671, 'learning_rate': 1.4546902467388268e-06, 'epoch': 0.92}
| 2996/3250 [4:39:21<23:03,  5.45s/it]                                                      92%|| 2996/3250 [4:39:21<23:03,  5.45s/it] 92%|| 2997/3250 [4:39:26<22:50,  5.42s/it]                                                      92%|| 2997/3250 [4:39:26<22:50,  5.42s/it] 92%|| 2998/3250 [4:39:32<22:40,  5.40s/it]                                                      92%|| 2998/3250 [4:39:32<22:40,  5.40s/it] 92%|| 2999/3250 [4:39:37<22:32,  5.39s/it]                                                      92%|| 2999/3250 [4:39:37<22:32,  5.39s/it] 92%|| 3000/3250 [4:39:43<23:11,  5.57s/it]                                                      92%|| 3000/3250 [4:39:43<23:11,  5.57s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8499269485473633, 'eval_runtime': 1.3745, 'eval_samples_per_second': 8.73, 'eval_steps_per_second': 2.183, 'epoch': 0.92}
                                                      92%|| 3000/3250 [4:39:44<23:11,  5.57s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_4/tmp-checkpoint-3000
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-3000
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-3000
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-3000
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-3000
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-3000/pytorch_model.bin
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-3000/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-3000/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-3000/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.6167, 'learning_rate': 1.443132195962188e-06, 'epoch': 0.92}
{'loss': 0.7095, 'learning_rate': 1.431619572633014e-06, 'epoch': 0.92}
{'loss': 0.636, 'learning_rate': 1.4201523875219724e-06, 'epoch': 0.92}
{'loss': 0.6444, 'learning_rate': 1.4087306513571985e-06, 'epoch': 0.92}
{'loss': 0.63, 'learning_rate': 1.3973543748243e-06, 'epoch': 0.92}
 92%|| 3001/3250 [4:39:50<25:03,  6.04s/it]                                                      92%|| 3001/3250 [4:39:50<25:03,  6.04s/it] 92%|| 3002/3250 [4:39:55<24:07,  5.83s/it]                                                      92%|| 3002/3250 [4:39:55<24:07,  5.83s/it] 92%|| 3003/3250 [4:40:01<23:26,  5.70s/it]                                                      92%|| 3003/3250 [4:40:01<23:26,  5.70s/it] 92%|| 3004/3250 [4:40:06<22:56,  5.59s/it]                                                      92%|| 3004/3250 [4:40:06<22:56,  5.59s/it] 92%|| 3005/3250 [4:40:12<22:33,  5.53s/it]                                                      92%|| 3005/3250 [4:40:12<22:33,  5.53s/it] 92%|{'loss': 0.6418, 'learning_rate': 1.3860235685663913e-06, 'epoch': 0.92}
{'loss': 0.6599, 'learning_rate': 1.3747382431840095e-06, 'epoch': 0.93}
{'loss': 0.6655, 'learning_rate': 1.3634984092351588e-06, 'epoch': 0.93}
{'loss': 0.6771, 'learning_rate': 1.3523040772352835e-06, 'epoch': 0.93}
{'loss': 1.1289, 'learning_rate': 1.341155257657256e-06, 'epoch': 0.93}
| 3006/3250 [4:40:17<22:16,  5.48s/it]                                                      92%|| 3006/3250 [4:40:17<22:16,  5.48s/it] 93%|| 3007/3250 [4:40:22<22:03,  5.45s/it]                                                      93%|| 3007/3250 [4:40:22<22:03,  5.45s/it] 93%|| 3008/3250 [4:40:28<21:53,  5.43s/it]                                                      93%|| 3008/3250 [4:40:28<21:53,  5.43s/it] 93%|| 3009/3250 [4:40:33<21:43,  5.41s/it]                                                      93%|| 3009/3250 [4:40:33<21:43,  5.41s/it] 93%|| 3010/3250 [4:40:38<21:33,  5.39s/it]                                                      93%|| 3010/3250 [4:40:38<21:33,  5.39s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8497377038002014, 'eval_runtime': 1.3721, 'eval_samples_per_second': 8.746, 'eval_steps_per_second': 2.186, 'epoch': 0.93}
                                                      93%|| 3010/3250 [4:40:40<21:33,  5.39s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_4/tmp-checkpoint-3010
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-3010
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-3010
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-3010
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-3010
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-3010/pytorch_model.bin
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-3010/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-3010/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-3010/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.635, 'learning_rate': 1.3300519609313832e-06, 'epoch': 0.93}
{'loss': 0.6608, 'learning_rate': 1.31899419744535e-06, 'epoch': 0.93}
{'loss': 0.6522, 'learning_rate': 1.3079819775442703e-06, 'epoch': 0.93}
{'loss': 0.6524, 'learning_rate': 1.297015311530647e-06, 'epoch': 0.93}
{'loss': 0.6141, 'learning_rate': 1.2860942096643569e-06, 'epoch': 0.93}
 93%|| 3011/3250 [4:40:45<23:29,  5.90s/it]                                                      93%|| 3011/3250 [4:40:45<23:29,  5.90s/it] 93%|| 3012/3250 [4:40:51<22:45,  5.74s/it]                                                      93%|| 3012/3250 [4:40:51<22:45,  5.74s/it] 93%|| 3013/3250 [4:40:56<22:13,  5.62s/it]                                                      93%|| 3013/3250 [4:40:56<22:13,  5.62s/it] 93%|| 3014/3250 [4:41:02<21:49,  5.55s/it]                                                      93%|| 3014/3250 [4:41:02<21:49,  5.55s/it] 93%|| 3015/3250 [4:41:07<21:30,  5.49s/it]                                                      93%|| 3015/3250 [4:41:07<21:30,  5.49s/it] 93%|{'loss': 0.6928, 'learning_rate': 1.2752186821626488e-06, 'epoch': 0.93}
{'loss': 0.6827, 'learning_rate': 1.2643887392001563e-06, 'epoch': 0.93}
{'loss': 0.6418, 'learning_rate': 1.2536043909088191e-06, 'epoch': 0.93}
{'loss': 0.6359, 'learning_rate': 1.2428656473779721e-06, 'epoch': 0.93}
{'loss': 0.6513, 'learning_rate': 1.232172518654251e-06, 'epoch': 0.93}
| 3016/3250 [4:41:12<21:17,  5.46s/it]                                                      93%|| 3016/3250 [4:41:12<21:17,  5.46s/it] 93%|| 3017/3250 [4:41:18<21:21,  5.50s/it]                                                      93%|| 3017/3250 [4:41:18<21:21,  5.50s/it] 93%|| 3018/3250 [4:41:23<21:06,  5.46s/it]                                                      93%|| 3018/3250 [4:41:23<21:06,  5.46s/it] 93%|| 3019/3250 [4:41:29<20:54,  5.43s/it]                                                      93%|| 3019/3250 [4:41:29<20:54,  5.43s/it] 93%|| 3020/3250 [4:41:34<20:45,  5.41s/it]                                                      93%|| 3020/3250 [4:41:34<20:45,  5.41s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.84982830286026, 'eval_runtime': 1.3846, 'eval_samples_per_second': 8.667, 'eval_steps_per_second': 2.167, 'epoch': 0.93}
                                                      93%|| 3020/3250 [4:41:35<20:45,  5.41s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_4/tmp-checkpoint-3020
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-3020I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-3020

I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-3020
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-3020
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-3020/pytorch_model.binthe pytorch model path is
 /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-3020/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-3020/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-3020/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.6313, 'learning_rate': 1.2215250147416313e-06, 'epoch': 0.93}
{'loss': 0.6497, 'learning_rate': 1.2109231456014058e-06, 'epoch': 0.93}
{'loss': 0.6526, 'learning_rate': 1.2003669211521628e-06, 'epoch': 0.93}
{'loss': 0.628, 'learning_rate': 1.189856351269797e-06, 'epoch': 0.93}
{'loss': 0.6551, 'learning_rate': 1.1793914457874756e-06, 'epoch': 0.93}
 93%|| 3021/3250 [4:41:41<22:40,  5.94s/it]                                                      93%|| 3021/3250 [4:41:41<22:40,  5.94s/it] 93%|| 3022/3250 [4:41:47<21:55,  5.77s/it]                                                      93%|| 3022/3250 [4:41:47<21:55,  5.77s/it] 93%|| 3023/3250 [4:41:52<21:21,  5.65s/it]                                                      93%|| 3023/3250 [4:41:52<21:21,  5.65s/it] 93%|| 3024/3250 [4:41:57<20:57,  5.56s/it]                                                      93%|| 3024/3250 [4:41:57<20:57,  5.56s/it] 93%|| 3025/3250 [4:42:03<20:38,  5.51s/it]                                                      93%|| 3025/3250 [4:42:03<20:38,  5.51s/it] 93%|{'loss': 0.6674, 'learning_rate': 1.1689722144956671e-06, 'epoch': 0.93}
{'loss': 0.6504, 'learning_rate': 1.1585986671420967e-06, 'epoch': 0.93}
{'loss': 0.6481, 'learning_rate': 1.148270813431751e-06, 'epoch': 0.93}
{'loss': 0.6597, 'learning_rate': 1.1379886630268677e-06, 'epoch': 0.93}
{'loss': 0.6699, 'learning_rate': 1.1277522255469296e-06, 'epoch': 0.93}
| 3026/3250 [4:42:08<20:23,  5.46s/it]                                                      93%|| 3026/3250 [4:42:08<20:23,  5.46s/it] 93%|| 3027/3250 [4:42:13<20:11,  5.43s/it]                                                      93%|| 3027/3250 [4:42:13<20:11,  5.43s/it] 93%|| 3028/3250 [4:42:19<20:01,  5.41s/it]                                                      93%|| 3028/3250 [4:42:19<20:01,  5.41s/it] 93%|| 3029/3250 [4:42:24<19:53,  5.40s/it]                                                      93%|| 3029/3250 [4:42:24<19:53,  5.40s/it] 93%|| 3030/3250 [4:42:29<19:45,  5.39s/it]                                                      93%|| 3030/3250 [4:42:29<19:45,  5.39s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8493719100952148, 'eval_runtime': 1.3698, 'eval_samples_per_second': 8.76, 'eval_steps_per_second': 2.19, 'epoch': 0.93}
                                                      93%|| 3030/3250 [4:42:31<19:45,  5.39s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_4/tmp-checkpoint-3030
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-3030
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-3030
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-3030
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-3030
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-3030/pytorch_model.bin
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-3030/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-3030/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-3030/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.6099, 'learning_rate': 1.1175615105686433e-06, 'epoch': 0.93}
{'loss': 0.7028, 'learning_rate': 1.107416527625954e-06, 'epoch': 0.93}
{'loss': 0.646, 'learning_rate': 1.0973172862100145e-06, 'epoch': 0.93}
{'loss': 0.6373, 'learning_rate': 1.0872637957691834e-06, 'epoch': 0.93}
{'loss': 0.626, 'learning_rate': 1.0772560657090202e-06, 'epoch': 0.93}
 93%|| 3031/3250 [4:42:37<21:32,  5.90s/it]                                                      93%|| 3031/3250 [4:42:37<21:32,  5.90s/it] 93%|| 3032/3250 [4:42:42<20:51,  5.74s/it]                                                      93%|| 3032/3250 [4:42:42<20:51,  5.74s/it] 93%|| 3033/3250 [4:42:48<21:02,  5.82s/it]                                                      93%|| 3033/3250 [4:42:48<21:02,  5.82s/it] 93%|| 3034/3250 [4:42:53<20:26,  5.68s/it]                                                      93%|| 3034/3250 [4:42:53<20:26,  5.68s/it] 93%|| 3035/3250 [4:42:59<20:00,  5.58s/it]                                                      93%|| 3035/3250 [4:42:59<20:00,  5.58s/it] 93%|{'loss': 0.6237, 'learning_rate': 1.0672941053922635e-06, 'epoch': 0.93}
{'loss': 0.6721, 'learning_rate': 1.0573779241388471e-06, 'epoch': 0.93}
{'loss': 0.6285, 'learning_rate': 1.0475075312258664e-06, 'epoch': 0.93}
{'loss': 0.6871, 'learning_rate': 1.037682935887585e-06, 'epoch': 0.94}
{'loss': 1.141, 'learning_rate': 1.0279041473154116e-06, 'epoch': 0.94}
| 3036/3250 [4:43:04<19:40,  5.52s/it]                                                      93%|| 3036/3250 [4:43:04<19:40,  5.52s/it] 93%|| 3037/3250 [4:43:09<19:25,  5.47s/it]                                                      93%|| 3037/3250 [4:43:09<19:25,  5.47s/it] 93%|| 3038/3250 [4:43:15<19:13,  5.44s/it]                                                      93%|| 3038/3250 [4:43:15<19:13,  5.44s/it] 94%|| 3039/3250 [4:43:20<19:04,  5.42s/it]                                                      94%|| 3039/3250 [4:43:20<19:04,  5.42s/it] 94%|| 3040/3250 [4:43:25<18:54,  5.40s/it]                                                      94%|| 3040/3250 [4:43:25<18:54,  5.40s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8502022624015808, 'eval_runtime': 1.3741, 'eval_samples_per_second': 8.733, 'eval_steps_per_second': 2.183, 'epoch': 0.94}
                                                      94%|| 3040/3250 [4:43:27<18:54,  5.40s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_4/tmp-checkpoint-3040
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-3040I AM HERE AND I AM SAVING THE MODEL

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-3040
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-3040
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-3040
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-3040/pytorch_model.bin
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-3040/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-3040/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-3040/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.6407, 'learning_rate': 1.0181711746579058e-06, 'epoch': 0.94}
{'loss': 0.6473, 'learning_rate': 1.008484027020773e-06, 'epoch': 0.94}
{'loss': 0.681, 'learning_rate': 9.988427134668298e-07, 'epoch': 0.94}
{'loss': 0.6593, 'learning_rate': 9.892472430160171e-07, 'epoch': 0.94}
{'loss': 0.6225, 'learning_rate': 9.796976246454037e-07, 'epoch': 0.94}
 94%|| 3041/3250 [4:43:33<20:35,  5.91s/it]                                                      94%|| 3041/3250 [4:43:33<20:35,  5.91s/it] 94%|| 3042/3250 [4:43:38<19:55,  5.75s/it]                                                      94%|| 3042/3250 [4:43:38<19:55,  5.75s/it] 94%|| 3043/3250 [4:43:43<19:25,  5.63s/it]                                                      94%|| 3043/3250 [4:43:43<19:25,  5.63s/it] 94%|| 3044/3250 [4:43:49<19:02,  5.55s/it]                                                      94%|| 3044/3250 [4:43:49<19:02,  5.55s/it] 94%|| 3045/3250 [4:43:54<18:46,  5.49s/it]                                                      94%|| 3045/3250 [4:43:54<18:46,  5.49s/it] 94%|{'loss': 0.629, 'learning_rate': 9.701938672891376e-07, 'epoch': 0.94}
{'loss': 0.7274, 'learning_rate': 9.607359798384785e-07, 'epoch': 0.94}
{'loss': 0.6475, 'learning_rate': 9.513239711417654e-07, 'epoch': 0.94}
{'loss': 0.6653, 'learning_rate': 9.419578500044157e-07, 'epoch': 0.94}
{'loss': 0.5966, 'learning_rate': 9.326376251889202e-07, 'epoch': 0.94}
| 3046/3250 [4:43:59<18:32,  5.45s/it]                                                      94%|| 3046/3250 [4:43:59<18:32,  5.45s/it] 94%|| 3047/3250 [4:44:05<18:21,  5.43s/it]                                                      94%|| 3047/3250 [4:44:05<18:21,  5.43s/it] 94%|| 3048/3250 [4:44:10<18:11,  5.40s/it]                                                      94%|| 3048/3250 [4:44:10<18:11,  5.40s/it] 94%|| 3049/3250 [4:44:15<18:03,  5.39s/it]                                                      94%|| 3049/3250 [4:44:15<18:03,  5.39s/it] 94%|| 3050/3250 [4:44:21<18:19,  5.50s/it]                                                      94%|| 3050/3250 [4:44:21<18:19,  5.50s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8491064310073853, 'eval_runtime': 1.3812, 'eval_samples_per_second': 8.688, 'eval_steps_per_second': 2.172, 'epoch': 0.94}
                                                      94%|| 3050/3250 [4:44:23<18:19,  5.50s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_4/tmp-checkpoint-3050
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-3050
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-3050
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-3050
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-3050
the pytorch model path isthe pytorch model path is the pytorch model path is/projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-3050/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-3050/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-3050/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-3050/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.6574, 'learning_rate': 9.233633054148205e-07, 'epoch': 0.94}
{'loss': 0.6696, 'learning_rate': 9.141348993587317e-07, 'epoch': 0.94}
{'loss': 0.6359, 'learning_rate': 9.049524156542977e-07, 'epoch': 0.94}
{'loss': 0.632, 'learning_rate': 8.958158628922019e-07, 'epoch': 0.94}
{'loss': 0.636, 'learning_rate': 8.867252496201572e-07, 'epoch': 0.94}
 94%|| 3051/3250 [4:44:28<19:49,  5.98s/it]                                                      94%|| 3051/3250 [4:44:28<19:49,  5.98s/it] 94%|| 3052/3250 [4:44:34<19:06,  5.79s/it]                                                      94%|| 3052/3250 [4:44:34<19:06,  5.79s/it] 94%|| 3053/3250 [4:44:39<18:36,  5.67s/it]                                                      94%|| 3053/3250 [4:44:39<18:36,  5.67s/it] 94%|| 3054/3250 [4:44:44<18:12,  5.57s/it]                                                      94%|| 3054/3250 [4:44:44<18:12,  5.57s/it] 94%|| 3055/3250 [4:44:50<17:53,  5.51s/it]                                                      94%|| 3055/3250 [4:44:50<17:53,  5.51s/it] 94%|{'loss': 0.6519, 'learning_rate': 8.776805843429103e-07, 'epoch': 0.94}
{'loss': 0.6752, 'learning_rate': 8.686818755221982e-07, 'epoch': 0.94}
{'loss': 0.6644, 'learning_rate': 8.597291315767808e-07, 'epoch': 0.94}
{'loss': 0.6574, 'learning_rate': 8.508223608824084e-07, 'epoch': 0.94}
{'loss': 0.6573, 'learning_rate': 8.419615717718377e-07, 'epoch': 0.94}
| 3056/3250 [4:44:55<17:39,  5.46s/it]                                                      94%|| 3056/3250 [4:44:55<17:39,  5.46s/it] 94%|| 3057/3250 [4:45:00<17:28,  5.43s/it]                                                      94%|| 3057/3250 [4:45:00<17:28,  5.43s/it] 94%|| 3058/3250 [4:45:06<17:18,  5.41s/it]                                                      94%|| 3058/3250 [4:45:06<17:18,  5.41s/it] 94%|| 3059/3250 [4:45:11<17:10,  5.39s/it]                                                      94%|| 3059/3250 [4:45:11<17:10,  5.39s/it] 94%|| 3060/3250 [4:45:17<17:03,  5.38s/it]                                                      94%|| 3060/3250 [4:45:17<17:03,  5.38s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8488298654556274, 'eval_runtime': 1.6075, 'eval_samples_per_second': 7.465, 'eval_steps_per_second': 1.866, 'epoch': 0.94}
                                                      94%|| 3060/3250 [4:45:18<17:03,  5.38s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_4/tmp-checkpoint-3060
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-3060
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-3060
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-3060
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-3060
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-3060/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-3060/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-3060/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-3060/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.6434, 'learning_rate': 8.331467725347708e-07, 'epoch': 0.94}
{'loss': 0.6641, 'learning_rate': 8.243779714179168e-07, 'epoch': 0.94}
{'loss': 0.6528, 'learning_rate': 8.156551766249409e-07, 'epoch': 0.94}
{'loss': 0.6315, 'learning_rate': 8.069783963164656e-07, 'epoch': 0.94}
{'loss': 0.6283, 'learning_rate': 7.983476386100641e-07, 'epoch': 0.94}
 94%|| 3061/3250 [4:45:24<18:46,  5.96s/it]                                                      94%|| 3061/3250 [4:45:24<18:46,  5.96s/it] 94%|| 3062/3250 [4:45:29<18:05,  5.78s/it]                                                      94%|| 3062/3250 [4:45:29<18:05,  5.78s/it] 94%|| 3063/3250 [4:45:35<17:36,  5.65s/it]                                                      94%|| 3063/3250 [4:45:35<17:36,  5.65s/it] 94%|| 3064/3250 [4:45:40<17:15,  5.57s/it]                                                      94%|| 3064/3250 [4:45:40<17:15,  5.57s/it] 94%|| 3065/3250 [4:45:45<16:58,  5.50s/it]                                                      94%|| 3065/3250 [4:45:45<16:58,  5.50s/it] 94%|{'loss': 0.6194, 'learning_rate': 7.897629115802551e-07, 'epoch': 0.94}
{'loss': 0.677, 'learning_rate': 7.812242232584865e-07, 'epoch': 0.94}
{'loss': 0.6387, 'learning_rate': 7.727315816331515e-07, 'epoch': 0.94}
{'loss': 0.7115, 'learning_rate': 7.642849946495445e-07, 'epoch': 0.94}
{'loss': 1.1563, 'learning_rate': 7.558844702098833e-07, 'epoch': 0.94}
| 3066/3250 [4:45:51<16:59,  5.54s/it]                                                      94%|| 3066/3250 [4:45:51<16:59,  5.54s/it] 94%|| 3067/3250 [4:45:56<16:44,  5.49s/it]                                                      94%|| 3067/3250 [4:45:56<16:44,  5.49s/it] 94%|| 3068/3250 [4:46:02<16:31,  5.45s/it]                                                      94%|| 3068/3250 [4:46:02<16:31,  5.45s/it] 94%|| 3069/3250 [4:46:07<16:21,  5.42s/it]                                                      94%|| 3069/3250 [4:46:07<16:21,  5.42s/it] 94%|| 3070/3250 [4:46:12<16:12,  5.40s/it]                                                      94%|| 3070/3250 [4:46:12<16:12,  5.40s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8494542837142944, 'eval_runtime': 1.3769, 'eval_samples_per_second': 8.715, 'eval_steps_per_second': 2.179, 'epoch': 0.94}
                                                      94%|| 3070/3250 [4:46:14<16:12,  5.40s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_4/tmp-checkpoint-3070
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-3070I AM HERE AND I AM SAVING THE MODEL

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-3070
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-3070
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-3070
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-3070/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-3070/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-3070/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-3070/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.6092, 'learning_rate': 7.475300161732978e-07, 'epoch': 0.94}
{'loss': 0.6388, 'learning_rate': 7.392216403558027e-07, 'epoch': 0.95}
{'loss': 0.6658, 'learning_rate': 7.309593505303136e-07, 'epoch': 0.95}
{'loss': 0.6763, 'learning_rate': 7.227431544266194e-07, 'epoch': 0.95}
{'loss': 0.6336, 'learning_rate': 7.145730597314049e-07, 'epoch': 0.95}
 94%|| 3071/3250 [4:46:19<17:36,  5.90s/it]                                                      94%|| 3071/3250 [4:46:19<17:36,  5.90s/it] 95%|| 3072/3250 [4:46:25<17:01,  5.74s/it]                                                      95%|| 3072/3250 [4:46:25<17:01,  5.74s/it] 95%|| 3073/3250 [4:46:30<16:35,  5.62s/it]                                                      95%|| 3073/3250 [4:46:30<16:35,  5.62s/it] 95%|| 3074/3250 [4:46:35<16:17,  5.55s/it]                                                      95%|| 3074/3250 [4:46:35<16:17,  5.55s/it] 95%|| 3075/3250 [4:46:41<16:02,  5.50s/it]                                                      95%|| 3075/3250 [4:46:41<16:02,  5.50s/it] 95%|{'loss': 0.6509, 'learning_rate': 7.064490740882057e-07, 'epoch': 0.95}
{'loss': 0.6998, 'learning_rate': 6.983712050974367e-07, 'epoch': 0.95}
{'loss': 0.6904, 'learning_rate': 6.903394603163582e-07, 'epoch': 0.95}
{'loss': 0.6424, 'learning_rate': 6.823538472590707e-07, 'epoch': 0.95}
{'loss': 0.6081, 'learning_rate': 6.744143733965369e-07, 'epoch': 0.95}
| 3076/3250 [4:46:46<15:49,  5.46s/it]                                                      95%|| 3076/3250 [4:46:46<15:49,  5.46s/it] 95%|| 3077/3250 [4:46:52<15:40,  5.43s/it]                                                      95%|| 3077/3250 [4:46:52<15:40,  5.43s/it] 95%|| 3078/3250 [4:46:57<15:31,  5.41s/it]                                                      95%|| 3078/3250 [4:46:57<15:31,  5.41s/it] 95%|| 3079/3250 [4:47:02<15:24,  5.40s/it]                                                      95%|| 3079/3250 [4:47:02<15:24,  5.40s/it] 95%|| 3080/3250 [4:47:08<15:16,  5.39s/it]                                                      95%|| 3080/3250 [4:47:08<15:16,  5.39s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8490986227989197, 'eval_runtime': 1.3711, 'eval_samples_per_second': 8.752, 'eval_steps_per_second': 2.188, 'epoch': 0.95}
                                                      95%|| 3080/3250 [4:47:09<15:16,  5.39s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_4/tmp-checkpoint-3080
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-3080
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-3080
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-3080
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-3080
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-3080/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-3080/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-3080/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-3080/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.6643, 'learning_rate': 6.66521046156543e-07, 'epoch': 0.95}
{'loss': 0.6393, 'learning_rate': 6.58673872923693e-07, 'epoch': 0.95}
{'loss': 0.6612, 'learning_rate': 6.508728610394255e-07, 'epoch': 0.95}
{'loss': 0.6545, 'learning_rate': 6.431180178019969e-07, 'epoch': 0.95}
{'loss': 0.6469, 'learning_rate': 6.354093504664538e-07, 'epoch': 0.95}
 95%|| 3081/3250 [4:47:15<16:39,  5.91s/it]                                                      95%|| 3081/3250 [4:47:15<16:39,  5.91s/it] 95%|| 3082/3250 [4:47:21<16:24,  5.86s/it]                                                      95%|| 3082/3250 [4:47:21<16:24,  5.86s/it] 95%|| 3083/3250 [4:47:26<15:53,  5.71s/it]                                                      95%|| 3083/3250 [4:47:26<15:53,  5.71s/it] 95%|| 3084/3250 [4:47:31<15:30,  5.60s/it]                                                      95%|| 3084/3250 [4:47:31<15:30,  5.60s/it] 95%|| 3085/3250 [4:47:37<15:12,  5.53s/it]                                                      95%|| 3085/3250 [4:47:37<15:12,  5.53s/it] 95%|{'loss': 0.6438, 'learning_rate': 6.277468662446495e-07, 'epoch': 0.95}
{'loss': 0.6817, 'learning_rate': 6.201305723052331e-07, 'epoch': 0.95}
{'loss': 0.6558, 'learning_rate': 6.125604757736436e-07, 'epoch': 0.95}
{'loss': 0.6683, 'learning_rate': 6.050365837320992e-07, 'epoch': 0.95}
{'loss': 0.6567, 'learning_rate': 5.97558903219575e-07, 'epoch': 0.95}
| 3086/3250 [4:47:42<14:58,  5.48s/it]                                                      95%|| 3086/3250 [4:47:42<14:58,  5.48s/it] 95%|| 3087/3250 [4:47:47<14:47,  5.45s/it]                                                      95%|| 3087/3250 [4:47:47<14:47,  5.45s/it] 95%|| 3088/3250 [4:47:53<14:38,  5.42s/it]                                                      95%|| 3088/3250 [4:47:53<14:38,  5.42s/it] 95%|| 3089/3250 [4:47:58<14:29,  5.40s/it]                                                      95%|| 3089/3250 [4:47:58<14:29,  5.40s/it] 95%|| 3090/3250 [4:48:03<14:22,  5.39s/it]                                                      95%|| 3090/3250 [4:48:03<14:22,  5.39s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8492251634597778, 'eval_runtime': 1.3785, 'eval_samples_per_second': 8.705, 'eval_steps_per_second': 2.176, 'epoch': 0.95}
                                                      95%|| 3090/3250 [4:48:05<14:22,  5.39s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_4/tmp-checkpoint-3090
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-3090
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-3090
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-3090
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-3090
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-3090/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-3090/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-3090/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-3090/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.6708, 'learning_rate': 5.901274412318359e-07, 'epoch': 0.95}
{'loss': 0.6262, 'learning_rate': 5.827422047213926e-07, 'epoch': 0.95}
{'loss': 0.7057, 'learning_rate': 5.754032005975129e-07, 'epoch': 0.95}
{'loss': 0.6398, 'learning_rate': 5.681104357262101e-07, 'epoch': 0.95}
{'loss': 0.6473, 'learning_rate': 5.608639169302543e-07, 'epoch': 0.95}
 95%|| 3091/3250 [4:48:11<15:39,  5.91s/it]                                                      95%|| 3091/3250 [4:48:11<15:39,  5.91s/it] 95%|| 3092/3250 [4:48:16<15:07,  5.74s/it]                                                      95%|| 3092/3250 [4:48:16<15:07,  5.74s/it] 95%|| 3093/3250 [4:48:21<14:43,  5.63s/it]                                                      95%|| 3093/3250 [4:48:21<14:43,  5.63s/it] 95%|| 3094/3250 [4:48:27<14:25,  5.55s/it]                                                      95%|| 3094/3250 [4:48:27<14:25,  5.55s/it] 95%|| 3095/3250 [4:48:32<14:11,  5.49s/it]                                                      95%|| 3095/3250 [4:48:32<14:11,  5.49s/it] 95%|{'loss': 0.6378, 'learning_rate': 5.536636509891225e-07, 'epoch': 0.95}
{'loss': 0.6392, 'learning_rate': 5.465096446390428e-07, 'epoch': 0.95}
{'loss': 0.6496, 'learning_rate': 5.394019045729448e-07, 'epoch': 0.95}
{'loss': 0.6712, 'learning_rate': 5.323404374404983e-07, 'epoch': 0.95}
{'loss': 0.6791, 'learning_rate': 5.253252498480576e-07, 'epoch': 0.95}
| 3096/3250 [4:48:37<14:00,  5.46s/it]                                                      95%|| 3096/3250 [4:48:37<14:00,  5.46s/it] 95%|| 3097/3250 [4:48:43<13:50,  5.43s/it]                                                      95%|| 3097/3250 [4:48:43<13:50,  5.43s/it] 95%|| 3098/3250 [4:48:48<13:42,  5.41s/it]                                                      95%|| 3098/3250 [4:48:48<13:42,  5.41s/it] 95%|| 3099/3250 [4:48:54<13:57,  5.55s/it]                                                      95%|| 3099/3250 [4:48:54<13:57,  5.55s/it] 95%|| 3100/3250 [4:48:59<13:43,  5.49s/it]                                                      95%|| 3100/3250 [4:48:59<13:43,  5.49s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8489029407501221, 'eval_runtime': 1.3728, 'eval_samples_per_second': 8.741, 'eval_steps_per_second': 2.185, 'epoch': 0.95}
                                                      95%|| 3100/3250 [4:49:01<13:43,  5.49s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_4/tmp-checkpoint-3100
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-3100
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-3100
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-3100
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-3100
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-3100/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-3100/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-3100/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-3100/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 1.1137, 'learning_rate': 5.183563483587006e-07, 'epoch': 0.95}
{'loss': 0.6337, 'learning_rate': 5.114337394921953e-07, 'epoch': 0.95}
{'loss': 0.6686, 'learning_rate': 5.045574297249833e-07, 'epoch': 0.95}
{'loss': 0.6643, 'learning_rate': 4.977274254902187e-07, 'epoch': 0.96}
{'loss': 0.6647, 'learning_rate': 4.909437331777179e-07, 'epoch': 0.96}
 95%|| 3101/3250 [4:49:06<14:48,  5.97s/it]                                                      95%|| 3101/3250 [4:49:06<14:48,  5.97s/it] 95%|| 3102/3250 [4:49:12<14:16,  5.79s/it]                                                      95%|| 3102/3250 [4:49:12<14:16,  5.79s/it] 95%|| 3103/3250 [4:49:17<13:51,  5.66s/it]                                                      95%|| 3103/3250 [4:49:17<13:51,  5.66s/it] 96%|| 3104/3250 [4:49:23<13:33,  5.58s/it]                                                      96%|| 3104/3250 [4:49:23<13:33,  5.58s/it] 96%|| 3105/3250 [4:49:28<13:18,  5.51s/it]                                                      96%|| 3105/3250 [4:49:28<13:18,  5.51s/it] 96%|{'loss': 0.632, 'learning_rate': 4.842063591339763e-07, 'epoch': 0.96}
{'loss': 0.6813, 'learning_rate': 4.77515309662152e-07, 'epoch': 0.96}
{'loss': 0.688, 'learning_rate': 4.7087059102206564e-07, 'epoch': 0.96}
{'loss': 0.638, 'learning_rate': 4.6427220943019436e-07, 'epoch': 0.96}
{'loss': 0.6344, 'learning_rate': 4.577201710596612e-07, 'epoch': 0.96}
| 3106/3250 [4:49:33<13:06,  5.46s/it]                                                      96%|| 3106/3250 [4:49:33<13:06,  5.46s/it] 96%|| 3107/3250 [4:49:39<12:56,  5.43s/it]                                                      96%|| 3107/3250 [4:49:39<12:56,  5.43s/it] 96%|| 3108/3250 [4:49:44<12:47,  5.41s/it]                                                      96%|| 3108/3250 [4:49:44<12:47,  5.41s/it] 96%|| 3109/3250 [4:49:49<12:39,  5.39s/it]                                                      96%|| 3109/3250 [4:49:49<12:39,  5.39s/it] 96%|| 3110/3250 [4:49:55<12:32,  5.38s/it]                                                      96%|| 3110/3250 [4:49:55<12:32,  5.38s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8485232591629028, 'eval_runtime': 1.3725, 'eval_samples_per_second': 8.743, 'eval_steps_per_second': 2.186, 'epoch': 0.96}
                                                      96%|| 3110/3250 [4:49:56<12:32,  5.38s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_4/tmp-checkpoint-3110
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-3110
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-3110
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-3110
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-3110
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-3110/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-3110/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-3110/pytorch_model.binthe pytorch model path is
 /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-3110/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.6489, 'learning_rate': 4.512144820402409e-07, 'epoch': 0.96}
{'loss': 0.6464, 'learning_rate': 4.447551484583312e-07, 'epoch': 0.96}
{'loss': 0.6379, 'learning_rate': 4.3834217635697616e-07, 'epoch': 0.96}
{'loss': 0.6503, 'learning_rate': 4.3197557173584315e-07, 'epoch': 0.96}
{'loss': 0.6334, 'learning_rate': 4.2565534055121204e-07, 'epoch': 0.96}
 96%|| 3111/3250 [4:50:02<13:39,  5.89s/it]                                                      96%|| 3111/3250 [4:50:02<13:39,  5.89s/it] 96%|| 3112/3250 [4:50:07<13:10,  5.73s/it]                                                      96%|| 3112/3250 [4:50:07<13:10,  5.73s/it] 96%|| 3113/3250 [4:50:12<12:49,  5.62s/it]                                                      96%|| 3113/3250 [4:50:12<12:49,  5.62s/it] 96%|| 3114/3250 [4:50:18<12:33,  5.54s/it]                                                      96%|| 3114/3250 [4:50:18<12:33,  5.54s/it] 96%|| 3115/3250 [4:50:24<12:35,  5.59s/it]                                                      96%|| 3115/3250 [4:50:24<12:35,  5.59s/it] 96%|{'loss': 0.6519, 'learning_rate': 4.193814887159919e-07, 'epoch': 0.96}
{'loss': 0.6684, 'learning_rate': 4.131540220996877e-07, 'epoch': 0.96}
{'loss': 0.6507, 'learning_rate': 4.069729465284167e-07, 'epoch': 0.96}
{'loss': 0.6428, 'learning_rate': 4.0083826778489206e-07, 'epoch': 0.96}
{'loss': 0.664, 'learning_rate': 3.947499916084285e-07, 'epoch': 0.96}
| 3116/3250 [4:50:29<12:19,  5.52s/it]                                                      96%|| 3116/3250 [4:50:29<12:19,  5.52s/it] 96%|| 3117/3250 [4:50:34<12:07,  5.47s/it]                                                      96%|| 3117/3250 [4:50:34<12:07,  5.47s/it] 96%|| 3118/3250 [4:50:40<11:57,  5.44s/it]                                                      96%|| 3118/3250 [4:50:40<11:57,  5.44s/it] 96%|| 3119/3250 [4:50:45<11:48,  5.41s/it]                                                      96%|| 3119/3250 [4:50:45<11:48,  5.41s/it] 96%|| 3120/3250 [4:50:50<11:41,  5.40s/it]                                                      96%|| 3120/3250 [4:50:50<11:41,  5.40s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8494842052459717, 'eval_runtime': 1.381, 'eval_samples_per_second': 8.689, 'eval_steps_per_second': 2.172, 'epoch': 0.96}
                                                      96%|| 3120/3250 [4:50:52<11:41,  5.40s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_4/tmp-checkpoint-3120
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-3120I AM HERE AND I AM SAVING THE MODEL

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-3120
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-3120
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-3120
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-3120/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-3120/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-3120/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-3120/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.6773, 'learning_rate': 3.887081236949086e-07, 'epoch': 0.96}
{'loss': 0.6103, 'learning_rate': 3.8271266969682194e-07, 'epoch': 0.96}
{'loss': 0.7134, 'learning_rate': 3.767636352232151e-07, 'epoch': 0.96}
{'loss': 0.6425, 'learning_rate': 3.7086102583972494e-07, 'epoch': 0.96}
{'loss': 0.635, 'learning_rate': 3.6500484706853964e-07, 'epoch': 0.96}
 96%|| 3121/3250 [4:50:57<12:42,  5.91s/it]                                                      96%|| 3121/3250 [4:50:57<12:42,  5.91s/it] 96%|| 3122/3250 [4:51:03<12:14,  5.74s/it]                                                      96%|| 3122/3250 [4:51:03<12:14,  5.74s/it] 96%|| 3123/3250 [4:51:08<11:54,  5.63s/it]                                                      96%|| 3123/3250 [4:51:08<11:54,  5.63s/it] 96%|| 3124/3250 [4:51:13<11:38,  5.54s/it]                                                      96%|| 3124/3250 [4:51:13<11:38,  5.54s/it] 96%|| 3125/3250 [4:51:19<11:27,  5.50s/it]                                                      96%|| 3125/3250 [4:51:19<11:27,  5.50s/it] 96%|{'loss': 0.6266, 'learning_rate': 3.591951043884212e-07, 'epoch': 0.96}
{'loss': 0.6274, 'learning_rate': 3.534318032346773e-07, 'epoch': 0.96}
{'loss': 0.6729, 'learning_rate': 3.4771494899917265e-07, 'epoch': 0.96}
{'loss': 0.6304, 'learning_rate': 3.420445470303235e-07, 'epoch': 0.96}
{'loss': 0.6862, 'learning_rate': 3.364206026330752e-07, 'epoch': 0.96}
| 3126/3250 [4:51:24<11:17,  5.46s/it]                                                      96%|| 3126/3250 [4:51:24<11:17,  5.46s/it] 96%|| 3127/3250 [4:51:30<11:07,  5.43s/it]                                                      96%|| 3127/3250 [4:51:30<11:07,  5.43s/it] 96%|| 3128/3250 [4:51:35<10:59,  5.41s/it]                                                      96%|| 3128/3250 [4:51:35<10:59,  5.41s/it] 96%|| 3129/3250 [4:51:40<10:52,  5.40s/it]                                                      96%|| 3129/3250 [4:51:40<10:52,  5.40s/it] 96%|| 3130/3250 [4:51:46<10:46,  5.38s/it]                                                      96%|| 3130/3250 [4:51:46<10:46,  5.38s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8496689200401306, 'eval_runtime': 1.3772, 'eval_samples_per_second': 8.713, 'eval_steps_per_second': 2.178, 'epoch': 0.96}
                                                      96%|| 3130/3250 [4:51:47<10:46,  5.38s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_4/tmp-checkpoint-3130
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-3130
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-3130
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-3130
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-3130
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-3130/pytorch_model.bin
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-3130/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-3130/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-3130/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 1.1403, 'learning_rate': 3.3084312106892446e-07, 'epoch': 0.96}
{'loss': 0.6471, 'learning_rate': 3.2531210755588624e-07, 'epoch': 0.96}
{'loss': 0.6384, 'learning_rate': 3.1982756726851584e-07, 'epoch': 0.96}
{'loss': 0.6756, 'learning_rate': 3.143895053378698e-07, 'epoch': 0.96}
{'loss': 0.6614, 'learning_rate': 3.0899792685155083e-07, 'epoch': 0.96}
 96%|| 3131/3250 [4:51:53<11:40,  5.89s/it]                                                      96%|| 3131/3250 [4:51:53<11:40,  5.89s/it] 96%|| 3132/3250 [4:51:59<11:33,  5.88s/it]                                                      96%|| 3132/3250 [4:51:59<11:33,  5.88s/it] 96%|| 3133/3250 [4:52:04<11:09,  5.73s/it]                                                      96%|| 3133/3250 [4:52:04<11:09,  5.73s/it] 96%|| 3134/3250 [4:52:09<10:51,  5.62s/it]                                                      96%|| 3134/3250 [4:52:09<10:51,  5.62s/it] 96%|| 3135/3250 [4:52:15<10:37,  5.54s/it]                                                      96%|| 3135/3250 [4:52:15<10:37,  5.54s/it] 96%|{'loss': 0.6257, 'learning_rate': 3.036528368536462e-07, 'epoch': 0.96}
{'loss': 0.6266, 'learning_rate': 2.9835424034476146e-07, 'epoch': 0.97}
{'loss': 0.7163, 'learning_rate': 2.9310214228202013e-07, 'epoch': 0.97}
{'loss': 0.6439, 'learning_rate': 2.8789654757901965e-07, 'epoch': 0.97}
{'loss': 0.6635, 'learning_rate': 2.8273746110585863e-07, 'epoch': 0.97}
| 3136/3250 [4:52:20<10:25,  5.49s/it]                                                      96%|| 3136/3250 [4:52:20<10:25,  5.49s/it] 97%|| 3137/3250 [4:52:25<10:15,  5.44s/it]                                                      97%|| 3137/3250 [4:52:25<10:15,  5.44s/it] 97%|| 3138/3250 [4:52:31<10:06,  5.41s/it]                                                      97%|| 3138/3250 [4:52:31<10:06,  5.41s/it] 97%|| 3139/3250 [4:52:36<09:58,  5.40s/it]                                                      97%|| 3139/3250 [4:52:36<09:58,  5.40s/it] 97%|| 3140/3250 [4:52:41<09:52,  5.39s/it]                                                      97%|| 3140/3250 [4:52:41<09:52,  5.39s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8489399552345276, 'eval_runtime': 1.3766, 'eval_samples_per_second': 8.717, 'eval_steps_per_second': 2.179, 'epoch': 0.97}
                                                      97%|| 3140/3250 [4:52:43<09:52,  5.39s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_4/tmp-checkpoint-3140
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-3140
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-3140
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-3140
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-3140
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-3140/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-3140/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-3140/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-3140/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.5951, 'learning_rate': 2.776248876891319e-07, 'epoch': 0.97}
{'loss': 0.6451, 'learning_rate': 2.725588321119188e-07, 'epoch': 0.97}
{'loss': 0.6752, 'learning_rate': 2.675392991137671e-07, 'epoch': 0.97}
{'loss': 0.6339, 'learning_rate': 2.625662933907147e-07, 'epoch': 0.97}
{'loss': 0.6282, 'learning_rate': 2.5763981959526786e-07, 'epoch': 0.97}
 97%|| 3141/3250 [4:52:49<10:43,  5.90s/it]                                                      97%|| 3141/3250 [4:52:49<10:43,  5.90s/it] 97%|| 3142/3250 [4:52:54<10:19,  5.74s/it]                                                      97%|| 3142/3250 [4:52:54<10:19,  5.74s/it] 97%|| 3143/3250 [4:52:59<10:02,  5.63s/it]                                                      97%|| 3143/3250 [4:52:59<10:02,  5.63s/it] 97%|| 3144/3250 [4:53:05<09:47,  5.54s/it]                                                      97%|| 3144/3250 [4:53:05<09:47,  5.54s/it] 97%|| 3145/3250 [4:53:10<09:36,  5.49s/it]                                                      97%|| 3145/3250 [4:53:10<09:36,  5.49s/it] 97%|{'loss': 0.6535, 'learning_rate': 2.527598823363786e-07, 'epoch': 0.97}
{'loss': 0.6602, 'learning_rate': 2.4792648617950056e-07, 'epoch': 0.97}
{'loss': 0.6821, 'learning_rate': 2.4313963564650546e-07, 'epoch': 0.97}
{'loss': 0.6613, 'learning_rate': 2.3839933521575543e-07, 'epoch': 0.97}
{'loss': 0.6561, 'learning_rate': 2.3370558932203635e-07, 'epoch': 0.97}
| 3146/3250 [4:53:15<09:26,  5.45s/it]                                                      97%|| 3146/3250 [4:53:15<09:26,  5.45s/it] 97%|| 3147/3250 [4:53:21<09:18,  5.42s/it]                                                      97%|| 3147/3250 [4:53:21<09:18,  5.42s/it] 97%|| 3148/3250 [4:53:26<09:22,  5.52s/it]                                                      97%|| 3148/3250 [4:53:26<09:22,  5.52s/it] 97%|| 3149/3250 [4:53:32<09:12,  5.47s/it]                                                      97%|| 3149/3250 [4:53:32<09:12,  5.47s/it] 97%|| 3150/3250 [4:53:37<09:04,  5.44s/it]                                                      97%|| 3150/3250 [4:53:37<09:04,  5.44s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8499306440353394, 'eval_runtime': 1.3812, 'eval_samples_per_second': 8.688, 'eval_steps_per_second': 2.172, 'epoch': 0.97}
                                                      97%|| 3150/3250 [4:53:39<09:04,  5.44s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_4/tmp-checkpoint-3150
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-3150
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-3150
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-3150
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-3150
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-3150/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-3150/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-3150/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-3150/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.6686, 'learning_rate': 2.290584023565856e-07, 'epoch': 0.97}
{'loss': 0.6481, 'learning_rate': 2.2445777866709205e-07, 'epoch': 0.97}
{'loss': 0.6664, 'learning_rate': 2.199037225576739e-07, 'epoch': 0.97}
{'loss': 0.6616, 'learning_rate': 2.153962382888841e-07, 'epoch': 0.97}
{'loss': 0.642, 'learning_rate': 2.1093533007770506e-07, 'epoch': 0.97}
 97%|| 3151/3250 [4:53:44<09:47,  5.93s/it]                                                      97%|| 3151/3250 [4:53:44<09:47,  5.93s/it] 97%|| 3152/3250 [4:53:50<09:24,  5.76s/it]                                                      97%|| 3152/3250 [4:53:50<09:24,  5.76s/it] 97%|| 3153/3250 [4:53:55<09:07,  5.65s/it]                                                      97%|| 3153/3250 [4:53:55<09:07,  5.65s/it] 97%|| 3154/3250 [4:54:00<08:53,  5.56s/it]                                                      97%|| 3154/3250 [4:54:00<08:53,  5.56s/it] 97%|| 3155/3250 [4:54:06<08:42,  5.50s/it]                                                      97%|| 3155/3250 [4:54:06<08:42,  5.50s/it] 97%|{'loss': 0.6264, 'learning_rate': 2.0652100209755388e-07, 'epoch': 0.97}
{'loss': 0.6241, 'learning_rate': 2.0215325847825485e-07, 'epoch': 0.97}
{'loss': 0.6786, 'learning_rate': 1.978321033060504e-07, 'epoch': 0.97}
{'loss': 0.6324, 'learning_rate': 1.935575406236123e-07, 'epoch': 0.97}
{'loss': 0.7065, 'learning_rate': 1.8932957443000832e-07, 'epoch': 0.97}
| 3156/3250 [4:54:11<08:33,  5.46s/it]                                                      97%|| 3156/3250 [4:54:11<08:33,  5.46s/it] 97%|| 3157/3250 [4:54:16<08:25,  5.44s/it]                                                      97%|| 3157/3250 [4:54:16<08:25,  5.44s/it] 97%|| 3158/3250 [4:54:22<08:18,  5.42s/it]                                                      97%|| 3158/3250 [4:54:22<08:18,  5.42s/it] 97%|| 3159/3250 [4:54:27<08:12,  5.41s/it]                                                      97%|| 3159/3250 [4:54:27<08:12,  5.41s/it] 97%|| 3160/3250 [4:54:33<08:05,  5.40s/it]                                                      97%|| 3160/3250 [4:54:33<08:05,  5.40s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.849251925945282, 'eval_runtime': 1.3703, 'eval_samples_per_second': 8.757, 'eval_steps_per_second': 2.189, 'epoch': 0.97}
                                                      97%|| 3160/3250 [4:54:34<08:05,  5.40s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_4/tmp-checkpoint-3160
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-3160
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-3160
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-3160
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-3160
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-3160/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-3160/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-3160/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-3160/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 1.1619, 'learning_rate': 1.851482086807299e-07, 'epoch': 0.97}
{'loss': 0.5991, 'learning_rate': 1.8101344728764234e-07, 'epoch': 0.97}
{'loss': 0.6405, 'learning_rate': 1.7692529411904578e-07, 'epoch': 0.97}
{'loss': 0.6645, 'learning_rate': 1.72883752999603e-07, 'epoch': 0.97}
{'loss': 0.6755, 'learning_rate': 1.688888277103895e-07, 'epoch': 0.97}
 97%|| 3161/3250 [4:54:40<08:44,  5.90s/it]                                                      97%|| 3161/3250 [4:54:40<08:44,  5.90s/it] 97%|| 3162/3250 [4:54:45<08:24,  5.73s/it]                                                      97%|| 3162/3250 [4:54:45<08:24,  5.73s/it] 97%|| 3163/3250 [4:54:50<08:08,  5.62s/it]                                                      97%|| 3163/3250 [4:54:50<08:08,  5.62s/it] 97%|| 3164/3250 [4:54:56<08:08,  5.68s/it]                                                      97%|| 3164/3250 [4:54:56<08:08,  5.68s/it] 97%|| 3165/3250 [4:55:02<07:54,  5.59s/it]                                                      97%|| 3165/3250 [4:55:02<07:54,  5.59s/it] 97%|{'loss': 0.6419, 'learning_rate': 1.6494052198886555e-07, 'epoch': 0.97}
{'loss': 0.642, 'learning_rate': 1.6103883952886534e-07, 'epoch': 0.97}
{'loss': 0.6959, 'learning_rate': 1.5718378398063005e-07, 'epoch': 0.97}
{'loss': 0.6814, 'learning_rate': 1.5337535895074695e-07, 'epoch': 0.98}
{'loss': 0.6474, 'learning_rate': 1.4961356800219927e-07, 'epoch': 0.98}
| 3166/3250 [4:55:07<07:43,  5.51s/it]                                                      97%|| 3166/3250 [4:55:07<07:43,  5.51s/it] 97%|| 3167/3250 [4:55:12<07:33,  5.47s/it]                                                      97%|| 3167/3250 [4:55:12<07:33,  5.47s/it] 97%|| 3168/3250 [4:55:18<07:25,  5.44s/it]                                                      97%|| 3168/3250 [4:55:18<07:25,  5.44s/it] 98%|| 3169/3250 [4:55:23<07:18,  5.41s/it]                                                      98%|| 3169/3250 [4:55:23<07:18,  5.41s/it] 98%|| 3170/3250 [4:55:28<07:11,  5.39s/it]                                                      98%|| 3170/3250 [4:55:28<07:11,  5.39s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8500510454177856, 'eval_runtime': 1.3707, 'eval_samples_per_second': 8.754, 'eval_steps_per_second': 2.189, 'epoch': 0.98}
                                                      98%|| 3170/3250 [4:55:30<07:11,  5.39s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_4/tmp-checkpoint-3170
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-3170
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-3170
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-3170
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-3170
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-3170/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-3170/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-3170/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-3170/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.6085, 'learning_rate': 1.4589841465433297e-07, 'epoch': 0.98}
{'loss': 0.6571, 'learning_rate': 1.4222990238287325e-07, 'epoch': 0.98}
{'loss': 0.6467, 'learning_rate': 1.3860803461989146e-07, 'epoch': 0.98}
{'loss': 0.6533, 'learning_rate': 1.3503281475383823e-07, 'epoch': 0.98}
{'loss': 0.6514, 'learning_rate': 1.3150424612951573e-07, 'epoch': 0.98}
 98%|| 3171/3250 [4:55:35<07:45,  5.89s/it]                                                      98%|| 3171/3250 [4:55:35<07:45,  5.89s/it] 98%|| 3172/3250 [4:55:41<07:28,  5.74s/it]                                                      98%|| 3172/3250 [4:55:41<07:28,  5.74s/it] 98%|| 3173/3250 [4:55:46<07:13,  5.62s/it]                                                      98%|| 3173/3250 [4:55:46<07:13,  5.62s/it] 98%|| 3174/3250 [4:55:51<07:01,  5.54s/it]                                                      98%|| 3174/3250 [4:55:51<07:01,  5.54s/it] 98%|| 3175/3250 [4:55:57<06:51,  5.49s/it]                                                      98%|| 3175/3250 [4:55:57<06:51,  5.49s/it] 98%|{'loss': 0.649, 'learning_rate': 1.280223320480778e-07, 'epoch': 0.98}
{'loss': 0.6465, 'learning_rate': 1.2458707576703533e-07, 'epoch': 0.98}
{'loss': 0.6765, 'learning_rate': 1.2119848050025085e-07, 'epoch': 0.98}
{'loss': 0.6561, 'learning_rate': 1.178565494179218e-07, 'epoch': 0.98}
{'loss': 0.6673, 'learning_rate': 1.1456128564660273e-07, 'epoch': 0.98}
| 3176/3250 [4:56:02<06:43,  5.45s/it]                                                      98%|| 3176/3250 [4:56:02<06:43,  5.45s/it] 98%|| 3177/3250 [4:56:08<06:36,  5.43s/it]                                                      98%|| 3177/3250 [4:56:08<06:36,  5.43s/it] 98%|| 3178/3250 [4:56:13<06:29,  5.42s/it]                                                      98%|| 3178/3250 [4:56:13<06:29,  5.42s/it] 98%|| 3179/3250 [4:56:18<06:23,  5.40s/it]                                                      98%|| 3179/3250 [4:56:18<06:23,  5.40s/it] 98%|| 3180/3250 [4:56:24<06:18,  5.40s/it]                                                      98%|| 3180/3250 [4:56:24<06:18,  5.40s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8495262861251831, 'eval_runtime': 1.3775, 'eval_samples_per_second': 8.711, 'eval_steps_per_second': 2.178, 'epoch': 0.98}
                                                      98%|| 3180/3250 [4:56:25<06:18,  5.40s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_4/tmp-checkpoint-3180
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-3180I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-3180

I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-3180
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-3180
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-3180/pytorch_model.bin
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-3180/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-3180/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-3180/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.6528, 'learning_rate': 1.1131269226918317e-07, 'epoch': 0.98}
{'loss': 0.662, 'learning_rate': 1.0811077232488753e-07, 'epoch': 0.98}
{'loss': 0.6178, 'learning_rate': 1.0495552880927517e-07, 'epoch': 0.98}
{'loss': 0.6978, 'learning_rate': 1.0184696467424037e-07, 'epoch': 0.98}
{'loss': 0.6248, 'learning_rate': 9.878508282800126e-08, 'epoch': 0.98}
 98%|| 3181/3250 [4:56:31<06:56,  6.03s/it]                                                      98%|| 3181/3250 [4:56:31<06:56,  6.03s/it] 98%|| 3182/3250 [4:56:37<06:36,  5.84s/it]                                                      98%|| 3182/3250 [4:56:37<06:36,  5.84s/it] 98%|| 3183/3250 [4:56:42<06:21,  5.70s/it]                                                      98%|| 3183/3250 [4:56:42<06:21,  5.70s/it] 98%|| 3184/3250 [4:56:47<06:09,  5.60s/it]                                                      98%|| 3184/3250 [4:56:47<06:09,  5.60s/it] 98%|| 3185/3250 [4:56:53<06:00,  5.54s/it]                                                      98%|| 3185/3250 [4:56:53<06:00,  5.54s/it] 98%|{'loss': 0.6406, 'learning_rate': 9.576988613511085e-08, 'epoch': 0.98}
{'loss': 0.6345, 'learning_rate': 9.280137741643491e-08, 'epoch': 0.98}
{'loss': 0.6484, 'learning_rate': 8.987955944917414e-08, 'epoch': 0.98}
{'loss': 0.642, 'learning_rate': 8.700443496683086e-08, 'epoch': 0.98}
{'loss': 0.6701, 'learning_rate': 8.417600665923675e-08, 'epoch': 0.98}
| 3186/3250 [4:56:58<05:51,  5.49s/it]                                                      98%|| 3186/3250 [4:56:58<05:51,  5.49s/it] 98%|| 3187/3250 [4:57:04<05:44,  5.46s/it]                                                      98%|| 3187/3250 [4:57:04<05:44,  5.46s/it] 98%|| 3188/3250 [4:57:09<05:36,  5.43s/it]                                                      98%|| 3188/3250 [4:57:09<05:36,  5.43s/it] 98%|| 3189/3250 [4:57:14<05:30,  5.42s/it]                                                      98%|| 3189/3250 [4:57:14<05:30,  5.42s/it] 98%|| 3190/3250 [4:57:20<05:24,  5.41s/it]                                                      98%|| 3190/3250 [4:57:20<05:24,  5.41s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8498905897140503, 'eval_runtime': 1.3845, 'eval_samples_per_second': 8.667, 'eval_steps_per_second': 2.167, 'epoch': 0.98}
                                                      98%|| 3190/3250 [4:57:21<05:24,  5.41s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_4/tmp-checkpoint-3190
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-3190
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-3190
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-3190
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-3190
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-3190/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-3190/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-3190/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-3190/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.6772, 'learning_rate': 8.139427717253622e-08, 'epoch': 0.98}
{'loss': 1.1248, 'learning_rate': 7.865924910916977e-08, 'epoch': 0.98}
{'loss': 0.6282, 'learning_rate': 7.597092502790725e-08, 'epoch': 0.98}
{'loss': 0.6616, 'learning_rate': 7.332930744380906e-08, 'epoch': 0.98}
{'loss': 0.6547, 'learning_rate': 7.073439882824273e-08, 'epoch': 0.98}
 98%|| 3191/3250 [4:57:27<05:49,  5.92s/it]                                                      98%|| 3191/3250 [4:57:27<05:49,  5.92s/it] 98%|| 3192/3250 [4:57:32<05:34,  5.76s/it]                                                      98%|| 3192/3250 [4:57:32<05:34,  5.76s/it] 98%|| 3193/3250 [4:57:38<05:21,  5.65s/it]                                                      98%|| 3193/3250 [4:57:38<05:21,  5.65s/it] 98%|| 3194/3250 [4:57:43<05:11,  5.57s/it]                                                      98%|| 3194/3250 [4:57:43<05:11,  5.57s/it] 98%|| 3195/3250 [4:57:48<05:03,  5.51s/it]                                                      98%|| 3195/3250 [4:57:48<05:03,  5.51s/it] 98%|{'loss': 0.662, 'learning_rate': 6.818620160887746e-08, 'epoch': 0.98}
{'loss': 0.6276, 'learning_rate': 6.568471816968957e-08, 'epoch': 0.98}
{'loss': 0.6875, 'learning_rate': 6.322995085094041e-08, 'epoch': 0.98}
{'loss': 0.6862, 'learning_rate': 6.08219019491929e-08, 'epoch': 0.98}
{'loss': 0.6414, 'learning_rate': 5.846057371730052e-08, 'epoch': 0.98}
| 3196/3250 [4:57:54<04:55,  5.48s/it]                                                      98%|| 3196/3250 [4:57:54<04:55,  5.48s/it] 98%|| 3197/3250 [4:58:00<04:56,  5.59s/it]                                                      98%|| 3197/3250 [4:58:00<04:56,  5.59s/it] 98%|| 3198/3250 [4:58:05<04:47,  5.53s/it]                                                      98%|| 3198/3250 [4:58:05<04:47,  5.53s/it] 98%|| 3199/3250 [4:58:10<04:39,  5.49s/it]                                                      98%|| 3199/3250 [4:58:10<04:39,  5.49s/it] 98%|| 3200/3250 [4:58:17<04:50,  5.82s/it]                                                      98%|| 3200/3250 [4:58:17<04:50,  5.82s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8496914505958557, 'eval_runtime': 1.7346, 'eval_samples_per_second': 6.918, 'eval_steps_per_second': 1.73, 'epoch': 0.98}
                                                      98%|| 3200/3250 [4:58:19<04:50,  5.82s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_4/tmp-checkpoint-3200
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-3200
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-3200
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-3200
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-3200
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-3200/pytorch_model.bin
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-3200/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-3200/pytorch_model.binthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-3200/pytorch_model.bin

/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.6316, 'learning_rate': 5.614596836440722e-08, 'epoch': 0.98}
{'loss': 0.6468, 'learning_rate': 5.3878088055947515e-08, 'epoch': 0.99}
{'loss': 0.6457, 'learning_rate': 5.165693491363532e-08, 'epoch': 0.99}
{'loss': 0.6484, 'learning_rate': 4.9482511015475074e-08, 'epoch': 0.99}
{'loss': 0.6467, 'learning_rate': 4.735481839575617e-08, 'epoch': 0.99}
 98%|| 3201/3250 [4:58:25<05:11,  6.35s/it]                                                      98%|| 3201/3250 [4:58:25<05:11,  6.35s/it] 99%|| 3202/3250 [4:58:30<04:50,  6.06s/it]                                                      99%|| 3202/3250 [4:58:30<04:50,  6.06s/it] 99%|| 3203/3250 [4:58:35<04:35,  5.86s/it]                                                      99%|| 3203/3250 [4:58:35<04:35,  5.86s/it] 99%|| 3204/3250 [4:58:41<04:22,  5.72s/it]                                                      99%|| 3204/3250 [4:58:41<04:22,  5.72s/it] 99%|| 3205/3250 [4:58:46<04:12,  5.62s/it]                                                      99%|| 3205/3250 [4:58:46<04:12,  5.62s/it] 99%|{'loss': 0.6452, 'learning_rate': 4.527385904504189e-08, 'epoch': 0.99}
{'loss': 0.6585, 'learning_rate': 4.323963491016936e-08, 'epoch': 0.99}
{'loss': 0.6729, 'learning_rate': 4.1252147894277336e-08, 'epoch': 0.99}
{'loss': 0.6477, 'learning_rate': 3.9311399856745145e-08, 'epoch': 0.99}
{'loss': 0.6632, 'learning_rate': 3.741739261324817e-08, 'epoch': 0.99}
| 3206/3250 [4:58:51<04:04,  5.55s/it]                                                      99%|| 3206/3250 [4:58:51<04:04,  5.55s/it] 99%|| 3207/3250 [4:58:57<03:56,  5.50s/it]                                                      99%|| 3207/3250 [4:58:57<03:56,  5.50s/it] 99%|| 3208/3250 [4:59:02<03:49,  5.47s/it]                                                      99%|| 3208/3250 [4:59:02<03:49,  5.47s/it] 99%|| 3209/3250 [4:59:08<03:43,  5.45s/it]                                                      99%|| 3209/3250 [4:59:08<03:43,  5.45s/it] 99%|| 3210/3250 [4:59:13<03:37,  5.43s/it]                                                      99%|| 3210/3250 [4:59:13<03:37,  5.43s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.84986412525177, 'eval_runtime': 1.3725, 'eval_samples_per_second': 8.743, 'eval_steps_per_second': 2.186, 'epoch': 0.99}
                                                      99%|| 3210/3250 [4:59:14<03:37,  5.43s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_4/tmp-checkpoint-3210
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-3210
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-3210
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-3210
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-3210
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-3210/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-3210/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-3210/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-3210/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.6675, 'learning_rate': 3.557012793573011e-08, 'epoch': 0.99}
{'loss': 0.676, 'learning_rate': 3.376960755240299e-08, 'epoch': 0.99}
{'loss': 0.6193, 'learning_rate': 3.2015833147741594e-08, 'epoch': 0.99}
{'loss': 0.7001, 'learning_rate': 3.030880636249456e-08, 'epoch': 0.99}
{'loss': 0.6492, 'learning_rate': 2.8648528793673302e-08, 'epoch': 0.99}
 99%|| 3211/3250 [4:59:20<03:52,  5.96s/it]                                                      99%|| 3211/3250 [4:59:20<03:52,  5.96s/it] 99%|| 3212/3250 [4:59:26<03:40,  5.79s/it]                                                      99%|| 3212/3250 [4:59:26<03:40,  5.79s/it] 99%|| 3213/3250 [4:59:31<03:30,  5.68s/it]                                                      99%|| 3213/3250 [4:59:31<03:30,  5.68s/it] 99%|| 3214/3250 [4:59:37<03:25,  5.72s/it]                                                      99%|| 3214/3250 [4:59:37<03:25,  5.72s/it] 99%|| 3215/3250 [4:59:42<03:16,  5.62s/it]                                                      99%|| 3215/3250 [4:59:42<03:16,  5.62s/it] 99%|{'loss': 0.6281, 'learning_rate': 2.7035001994552e-08, 'epoch': 0.99}
{'loss': 0.6262, 'learning_rate': 2.5468227474667595e-08, 'epoch': 0.99}
{'loss': 0.6301, 'learning_rate': 2.3948206699819786e-08, 'epoch': 0.99}
{'loss': 0.6774, 'learning_rate': 2.2474941092065492e-08, 'epoch': 0.99}
{'loss': 0.6264, 'learning_rate': 2.1048432029718845e-08, 'epoch': 0.99}
| 3216/3250 [4:59:48<03:08,  5.55s/it]                                                      99%|| 3216/3250 [4:59:48<03:08,  5.55s/it] 99%|| 3217/3250 [4:59:53<03:01,  5.51s/it]                                                      99%|| 3217/3250 [4:59:53<03:01,  5.51s/it] 99%|| 3218/3250 [4:59:58<02:55,  5.48s/it]                                                      99%|| 3218/3250 [4:59:58<02:55,  5.48s/it] 99%|| 3219/3250 [5:00:04<02:48,  5.45s/it]                                                      99%|| 3219/3250 [5:00:04<02:48,  5.45s/it] 99%|| 3220/3250 [5:00:09<02:43,  5.44s/it]                                                      99%|| 3220/3250 [5:00:09<02:43,  5.44s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8488080501556396, 'eval_runtime': 1.3786, 'eval_samples_per_second': 8.704, 'eval_steps_per_second': 2.176, 'epoch': 0.99}
                                                      99%|| 3220/3250 [5:00:11<02:43,  5.44s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_4/tmp-checkpoint-3220
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-3220I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-3220

I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-3220
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-3220
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-3220/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-3220/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-3220/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-3220/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.691, 'learning_rate': 1.9668680847356735e-08, 'epoch': 0.99}
{'loss': 1.1367, 'learning_rate': 1.8335688835802167e-08, 'epoch': 0.99}
{'loss': 0.6346, 'learning_rate': 1.7049457242140908e-08, 'epoch': 0.99}
{'loss': 0.6407, 'learning_rate': 1.580998726971039e-08, 'epoch': 0.99}
{'loss': 0.6713, 'learning_rate': 1.4617280078094152e-08, 'epoch': 0.99}
 99%|| 3221/3250 [5:00:16<02:52,  5.94s/it]                                                      99%|| 3221/3250 [5:00:16<02:52,  5.94s/it] 99%|| 3222/3250 [5:00:22<02:41,  5.78s/it]                                                      99%|| 3222/3250 [5:00:22<02:41,  5.78s/it] 99%|| 3223/3250 [5:00:27<02:33,  5.67s/it]                                                      99%|| 3223/3250 [5:00:27<02:33,  5.67s/it] 99%|| 3224/3250 [5:00:33<02:25,  5.59s/it]                                                      99%|| 3224/3250 [5:00:33<02:25,  5.59s/it] 99%|| 3225/3250 [5:00:38<02:18,  5.54s/it]                                                      99%|| 3225/3250 [5:00:38<02:18,  5.54s/it] 99%|{'loss': 0.6568, 'learning_rate': 1.3471336783132949e-08, 'epoch': 0.99}
{'loss': 0.6253, 'learning_rate': 1.2372158456919192e-08, 'epoch': 0.99}
{'loss': 0.6282, 'learning_rate': 1.1319746127785857e-08, 'epoch': 0.99}
{'loss': 0.7236, 'learning_rate': 1.0314100780317581e-08, 'epoch': 0.99}
{'loss': 0.6456, 'learning_rate': 9.355223355350662e-09, 'epoch': 0.99}
| 3226/3250 [5:00:43<02:11,  5.50s/it]                                                      99%|| 3226/3250 [5:00:43<02:11,  5.50s/it] 99%|| 3227/3250 [5:00:49<02:05,  5.47s/it]                                                      99%|| 3227/3250 [5:00:49<02:05,  5.47s/it] 99%|| 3228/3250 [5:00:54<01:59,  5.45s/it]                                                      99%|| 3228/3250 [5:00:54<01:59,  5.45s/it] 99%|| 3229/3250 [5:01:00<01:54,  5.44s/it]                                                      99%|| 3229/3250 [5:01:00<01:54,  5.44s/it] 99%|| 3230/3250 [5:01:06<01:52,  5.62s/it]                                                      99%|| 3230/3250 [5:01:06<01:52,  5.62s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8493950963020325, 'eval_runtime': 1.382, 'eval_samples_per_second': 8.683, 'eval_steps_per_second': 2.171, 'epoch': 0.99}
                                                      99%|| 3230/3250 [5:01:07<01:52,  5.62s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_4/tmp-checkpoint-3230
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-3230
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-3230
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-3230
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-3230
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-3230/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-3230/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-3230/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-3230/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.6639, 'learning_rate': 8.443114749967506e-09, 'epoch': 0.99}
{'loss': 0.6104, 'learning_rate': 7.577775817485533e-09, 'epoch': 0.99}
{'loss': 0.6521, 'learning_rate': 6.75920736747937e-09, 'epoch': 0.99}
{'loss': 0.6557, 'learning_rate': 5.987410165758656e-09, 'epoch': 1.0}
{'loss': 0.6339, 'learning_rate': 5.26238493438469e-09, 'epoch': 1.0}
 99%|| 3231/3250 [5:01:13<01:55,  6.09s/it]                                                      99%|| 3231/3250 [5:01:13<01:55,  6.09s/it] 99%|| 3232/3250 [5:01:18<01:45,  5.89s/it]                                                      99%|| 3232/3250 [5:01:18<01:45,  5.89s/it] 99%|| 3233/3250 [5:01:24<01:37,  5.75s/it]                                                      99%|| 3233/3250 [5:01:24<01:37,  5.75s/it]100%|| 3234/3250 [5:01:29<01:30,  5.64s/it]                                                     100%|| 3234/3250 [5:01:29<01:30,  5.64s/it]100%|| 3235/3250 [5:01:35<01:23,  5.57s/it]                                                     100%|| 3235/3250 [5:01:35<01:23,  5.57s/it]100%|{'loss': 0.626, 'learning_rate': 4.584132351642678e-09, 'epoch': 1.0}
{'loss': 0.6467, 'learning_rate': 3.9526530520916926e-09, 'epoch': 1.0}
{'loss': 0.6657, 'learning_rate': 3.3679476264980582e-09, 'epoch': 1.0}
{'loss': 0.6734, 'learning_rate': 2.830016621885312e-09, 'epoch': 1.0}
{'loss': 0.6626, 'learning_rate': 2.3388605415231024e-09, 'epoch': 1.0}
| 3236/3250 [5:01:40<01:17,  5.52s/it]                                                     100%|| 3236/3250 [5:01:40<01:17,  5.52s/it]100%|| 3237/3250 [5:01:45<01:11,  5.49s/it]                                                     100%|| 3237/3250 [5:01:45<01:11,  5.49s/it]100%|| 3238/3250 [5:01:51<01:05,  5.46s/it]                                                     100%|| 3238/3250 [5:01:51<01:05,  5.46s/it]100%|| 3239/3250 [5:01:56<00:59,  5.45s/it]                                                     100%|| 3239/3250 [5:01:56<00:59,  5.45s/it]100%|| 3240/3250 [5:02:02<00:54,  5.44s/it]                                                     100%|| 3240/3250 [5:02:02<00:54,  5.44s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8502486348152161, 'eval_runtime': 1.3744, 'eval_samples_per_second': 8.731, 'eval_steps_per_second': 2.183, 'epoch': 1.0}
                                                     100%|| 3240/3250 [5:02:03<00:54,  5.44s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_4/tmp-checkpoint-3240
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-3240
/projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-3240
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-3240
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-3240
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-3240/pytorch_model.bin
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-3240/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-3240/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-3240/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.6598, 'learning_rate': 1.894479844910535e-09, 'epoch': 1.0}
{'loss': 0.666, 'learning_rate': 1.4968749477872746e-09, 'epoch': 1.0}
{'loss': 0.6376, 'learning_rate': 1.1460462221279944e-09, 'epoch': 1.0}
{'loss': 0.6708, 'learning_rate': 8.419939961645807e-10, 'epoch': 1.0}
{'loss': 0.6605, 'learning_rate': 5.847185543417233e-10, 'epoch': 1.0}
100%|| 3241/3250 [5:02:09<00:53,  5.95s/it]                                                     100%|| 3241/3250 [5:02:09<00:53,  5.95s/it]100%|| 3242/3250 [5:02:14<00:46,  5.78s/it]                                                     100%|| 3242/3250 [5:02:14<00:46,  5.78s/it]100%|| 3243/3250 [5:02:20<00:39,  5.67s/it]                                                     100%|| 3243/3250 [5:02:20<00:39,  5.67s/it]100%|| 3244/3250 [5:02:25<00:33,  5.59s/it]                                                     100%|| 3244/3250 [5:02:25<00:33,  5.59s/it]100%|| 3245/3250 [5:02:30<00:27,  5.53s/it]                                                     100%|| 3245/3250 [5:02:30<00:27,  5.53s/it]100%|{'loss': 0.6496, 'learning_rate': 3.742201373557741e-10, 'epoch': 1.0}
{'loss': 0.626, 'learning_rate': 2.1049894213809317e-10, 'epoch': 1.0}
{'loss': 0.6248, 'learning_rate': 9.355512186615123e-11, 'epoch': 1.0}
{'loss': 0.6773, 'learning_rate': 2.338878593577398e-11, 'epoch': 1.0}
{'loss': 0.6226, 'learning_rate': 0.0, 'epoch': 1.0}
| 3246/3250 [5:02:36<00:22,  5.58s/it]                                                     100%|| 3246/3250 [5:02:36<00:22,  5.58s/it]100%|| 3247/3250 [5:02:41<00:16,  5.54s/it]                                                     100%|| 3247/3250 [5:02:41<00:16,  5.54s/it]100%|| 3248/3250 [5:02:47<00:11,  5.50s/it]                                                     100%|| 3248/3250 [5:02:47<00:11,  5.50s/it]100%|| 3249/3250 [5:02:52<00:05,  5.48s/it]                                                     100%|| 3249/3250 [5:02:52<00:05,  5.48s/it]100%|| 3250/3250 [5:02:58<00:00,  5.46s/it]                                                     100%|| 3250/3250 [5:02:58<00:00,  5.46s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8497648239135742, 'eval_runtime': 1.3836, 'eval_samples_per_second': 8.673, 'eval_steps_per_second': 2.168, 'epoch': 1.0}
                                                     100%|| 3250/3250 [5:02:59<00:00,  5.46s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_4/tmp-checkpoint-3250
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-3250
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-3250
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-3250
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-3250
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-3250/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-3250/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-3250/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_4/checkpoint-3250/pytorch_model.bin


Training completed. Do not forget to share your model on huggingface.co/models =)


{'train_runtime': 18201.2943, 'train_samples_per_second': 11.428, 'train_steps_per_second': 0.179, 'train_loss': 0.8064181493612436, 'epoch': 1.0}
Saving last checkpoint of the model
Saving last checkpoint of the model
Saving last checkpoint of the model
                                                     100%|| 3250/3250 [5:02:59<00:00,  5.46s/it]100%|| 3250/3250 [5:03:00<00:00,  5.59s/it]
Saving last checkpoint of the model
wandb: 
wandb: Run history:
wandb:                      eval/loss 
wandb:                   eval/runtime 
wandb:        eval/samples_per_second 
wandb:          eval/steps_per_second 
wandb:                    train/epoch 
wandb:              train/global_step 
wandb:            train/learning_rate 
wandb:                     train/loss 
wandb:               train/total_flos 
wandb:               train/train_loss 
wandb:            train/train_runtime 
wandb: train/train_samples_per_second 
wandb:   train/train_steps_per_second 
wandb: 
wandb: Run summary:
wandb:                      eval/loss 0.84976
wandb:                   eval/runtime 1.3836
wandb:        eval/samples_per_second 8.673
wandb:          eval/steps_per_second 2.168
wandb:                    train/epoch 1.0
wandb:              train/global_step 3250
wandb:            train/learning_rate 0.0
wandb:                     train/loss 0.6226
wandb:               train/total_flos 6.56191915032576e+17
wandb:               train/train_loss 0.80642
wandb:            train/train_runtime 18201.2943
wandb: train/train_samples_per_second 11.428
wandb:   train/train_steps_per_second 0.179
wandb: 
wandb:  View run FinalRuns-/projects/bbvz/bzd2/checkpoints_experiment_4 at: https://wandb.ai/complex_dnn/huggingface/runs/ithkj8wu
wandb:  View job at https://wandb.ai/complex_dnn/huggingface/jobs/QXJ0aWZhY3RDb2xsZWN0aW9uOjExOTYzMDY1Mg==/version_details/v9
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: /projects/bbvz/bzd2/wandb/run-20231213_172722-ithkj8wu/logs
/var/spool/slurmd/job2748380/slurm_script: line 116: --save_freq=50: command not found
