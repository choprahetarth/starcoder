Running "module reset". Resetting modules to system default. The following $MODULEPATH directories have been removed: None

Currently Loaded Modules:
  1) cue-login-env/1.0   8) libmd/1.0.4       15) openssl/3.1.3
  2) gcc/11.2.0          9) libbsd/0.11.7     16) util-linux-uuid/2.38.1
  3) ucx/1.11.2         10) expat/2.5.0       17) xz/5.2.4
  4) openmpi/4.1.2      11) gettext/0.19.8.1  18) python/3.11.6
  5) cuda/11.6.1        12) libffi/3.4.4      19) cudnn/8.9.0.131
  6) modtree/gpu        13) libxcrypt/4.4.35  20) anaconda3_gpu/23.9.0
  7) default            14) zlib-ng/2.1.3

 

job is starting on gpua032.delta.ncsa.illinois.edu
WARNING: A conda environment already exists at '/u/bzd2/.conda/envs/hetarth_py10'
Remove existing environment (y/[n])? 

CondaSystemExit: Exiting.

usage: conda [-h] [-v] [--no-plugins] [-V] COMMAND ...
conda: error: argument COMMAND: invalid choice: 'activate' (choose from 'clean', 'compare', 'config', 'create', 'info', 'init', 'install', 'list', 'notices', 'package', 'remove', 'uninstall', 'rename', 'run', 'search', 'update', 'upgrade', 'build', 'content-trust', 'convert', 'debug', 'develop', 'doctor', 'index', 'inspect', 'metapackage', 'render', 'repoquery', 'skeleton', 'env', 'verify', 'server', 'repo', 'token')
Starting script...
Output: Defaulting to user installation because normal site-packages is not writeable
Requirement already satisfied: torch in /sw/external/python/anaconda3/lib/python3.9/site-packages (2.2.0.dev20231003+cu121)
Requirement already satisfied: torchvision in /sw/external/python/anaconda3/lib/python3.9/site-packages (0.17.0.dev20231003+cu121)
Requirement already satisfied: torchaudio in /sw/external/python/anaconda3/lib/python3.9/site-packages (2.2.0.dev20231002)
Requirement already satisfied: filelock in /sw/external/python/anaconda3/lib/python3.9/site-packages (from torch) (3.9.0)
Requirement already satisfied: typing-extensions in /sw/external/python/anaconda3/lib/python3.9/site-packages (from torch) (4.7.1)
Requirement already satisfied: sympy in /sw/external/python/anaconda3/lib/python3.9/site-packages (from torch) (1.11.1)
Requirement already satisfied: networkx in /sw/external/python/anaconda3/lib/python3.9/site-packages (from torch) (3.1)
Requirement already satisfied: jinja2 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from torch) (3.1.2)
Requirement already satisfied: fsspec in /u/bzd2/.local/lib/python3.9/site-packages (from torch) (2023.6.0)
Requirement already satisfied: pytorch-triton==2.1.0+6e4932cda8 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from torch) (2.1.0+6e4932cda8)
Requirement already satisfied: numpy in /sw/external/python/anaconda3/lib/python3.9/site-packages (from torchvision) (1.24.3)
Requirement already satisfied: requests in /sw/external/python/anaconda3/lib/python3.9/site-packages (from torchvision) (2.31.0)
Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from torchvision) (9.4.0)
Requirement already satisfied: MarkupSafe>=2.0 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from jinja2->torch) (2.1.1)
Requirement already satisfied: charset-normalizer<4,>=2 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from requests->torchvision) (2.0.4)
Requirement already satisfied: idna<4,>=2.5 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from requests->torchvision) (3.4)
Requirement already satisfied: urllib3<3,>=1.21.1 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from requests->torchvision) (1.26.16)
Requirement already satisfied: certifi>=2017.4.17 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from requests->torchvision) (2023.7.22)
Requirement already satisfied: mpmath>=0.19 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from sympy->torch) (1.3.0)

Output: Defaulting to user installation because normal site-packages is not writeable
Requirement already satisfied: transformers in /u/bzd2/.local/lib/python3.9/site-packages (4.37.0.dev0)
Requirement already satisfied: filelock in /sw/external/python/anaconda3/lib/python3.9/site-packages (from transformers) (3.9.0)
Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /u/bzd2/.local/lib/python3.9/site-packages (from transformers) (0.19.4)
Requirement already satisfied: numpy>=1.17 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from transformers) (1.24.3)
Requirement already satisfied: packaging>=20.0 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from transformers) (23.1)
Requirement already satisfied: pyyaml>=5.1 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from transformers) (5.4.1)
Requirement already satisfied: regex!=2019.12.17 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from transformers) (2022.7.9)
Requirement already satisfied: requests in /sw/external/python/anaconda3/lib/python3.9/site-packages (from transformers) (2.31.0)
Requirement already satisfied: tokenizers<0.19,>=0.14 in /u/bzd2/.local/lib/python3.9/site-packages (from transformers) (0.15.0)
Requirement already satisfied: safetensors>=0.3.1 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from transformers) (0.3.2)
Requirement already satisfied: tqdm>=4.27 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from transformers) (4.65.0)
Requirement already satisfied: fsspec>=2023.5.0 in /u/bzd2/.local/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (2023.6.0)
Requirement already satisfied: typing-extensions>=3.7.4.3 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (4.7.1)
Requirement already satisfied: charset-normalizer<4,>=2 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from requests->transformers) (2.0.4)
Requirement already satisfied: idna<4,>=2.5 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from requests->transformers) (3.4)
Requirement already satisfied: urllib3<3,>=1.21.1 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from requests->transformers) (1.26.16)
Requirement already satisfied: certifi>=2017.4.17 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from requests->transformers) (2023.7.22)

Output: Defaulting to user installation because normal site-packages is not writeable
Collecting git+https://github.com/huggingface/peft.git
  Cloning https://github.com/huggingface/peft.git to /tmp/pip-req-build-de_8x34s
  Resolved https://github.com/huggingface/peft.git to commit 0f1e9091cc975eb5458cc163bf1843a34fb42b76
  Installing build dependencies: started
  Installing build dependencies: finished with status 'done'
  Getting requirements to build wheel: started
  Getting requirements to build wheel: finished with status 'done'
  Preparing metadata (pyproject.toml): started
  Preparing metadata (pyproject.toml): finished with status 'done'
Requirement already satisfied: numpy>=1.17 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from peft==0.7.2.dev0) (1.24.3)
Requirement already satisfied: packaging>=20.0 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from peft==0.7.2.dev0) (23.1)
Requirement already satisfied: psutil in /sw/external/python/anaconda3/lib/python3.9/site-packages (from peft==0.7.2.dev0) (5.9.0)
Requirement already satisfied: pyyaml in /sw/external/python/anaconda3/lib/python3.9/site-packages (from peft==0.7.2.dev0) (5.4.1)
Requirement already satisfied: torch>=1.13.0 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from peft==0.7.2.dev0) (2.2.0.dev20231003+cu121)
Requirement already satisfied: transformers in /u/bzd2/.local/lib/python3.9/site-packages (from peft==0.7.2.dev0) (4.37.0.dev0)
Requirement already satisfied: tqdm in /sw/external/python/anaconda3/lib/python3.9/site-packages (from peft==0.7.2.dev0) (4.65.0)
Requirement already satisfied: accelerate>=0.21.0 in /u/bzd2/.local/lib/python3.9/site-packages (from peft==0.7.2.dev0) (0.25.0)
Requirement already satisfied: safetensors in /sw/external/python/anaconda3/lib/python3.9/site-packages (from peft==0.7.2.dev0) (0.3.2)
Requirement already satisfied: huggingface-hub>=0.17.0 in /u/bzd2/.local/lib/python3.9/site-packages (from peft==0.7.2.dev0) (0.19.4)
Requirement already satisfied: filelock in /sw/external/python/anaconda3/lib/python3.9/site-packages (from huggingface-hub>=0.17.0->peft==0.7.2.dev0) (3.9.0)
Requirement already satisfied: fsspec>=2023.5.0 in /u/bzd2/.local/lib/python3.9/site-packages (from huggingface-hub>=0.17.0->peft==0.7.2.dev0) (2023.6.0)
Requirement already satisfied: requests in /sw/external/python/anaconda3/lib/python3.9/site-packages (from huggingface-hub>=0.17.0->peft==0.7.2.dev0) (2.31.0)
Requirement already satisfied: typing-extensions>=3.7.4.3 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from huggingface-hub>=0.17.0->peft==0.7.2.dev0) (4.7.1)
Requirement already satisfied: sympy in /sw/external/python/anaconda3/lib/python3.9/site-packages (from torch>=1.13.0->peft==0.7.2.dev0) (1.11.1)
Requirement already satisfied: networkx in /sw/external/python/anaconda3/lib/python3.9/site-packages (from torch>=1.13.0->peft==0.7.2.dev0) (3.1)
Requirement already satisfied: jinja2 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from torch>=1.13.0->peft==0.7.2.dev0) (3.1.2)
Requirement already satisfied: pytorch-triton==2.1.0+6e4932cda8 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from torch>=1.13.0->peft==0.7.2.dev0) (2.1.0+6e4932cda8)
Requirement already satisfied: regex!=2019.12.17 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from transformers->peft==0.7.2.dev0) (2022.7.9)
Requirement already satisfied: tokenizers<0.19,>=0.14 in /u/bzd2/.local/lib/python3.9/site-packages (from transformers->peft==0.7.2.dev0) (0.15.0)
Requirement already satisfied: MarkupSafe>=2.0 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from jinja2->torch>=1.13.0->peft==0.7.2.dev0) (2.1.1)
Requirement already satisfied: charset-normalizer<4,>=2 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from requests->huggingface-hub>=0.17.0->peft==0.7.2.dev0) (2.0.4)
Requirement already satisfied: idna<4,>=2.5 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from requests->huggingface-hub>=0.17.0->peft==0.7.2.dev0) (3.4)
Requirement already satisfied: urllib3<3,>=1.21.1 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from requests->huggingface-hub>=0.17.0->peft==0.7.2.dev0) (1.26.16)
Requirement already satisfied: certifi>=2017.4.17 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from requests->huggingface-hub>=0.17.0->peft==0.7.2.dev0) (2023.7.22)
Requirement already satisfied: mpmath>=0.19 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from sympy->torch>=1.13.0->peft==0.7.2.dev0) (1.3.0)

Output: Defaulting to user installation because normal site-packages is not writeable
Collecting git+https://github.com/huggingface/transformers
  Cloning https://github.com/huggingface/transformers to /tmp/pip-req-build-cvfg2sxo
  Resolved https://github.com/huggingface/transformers to commit 17506d1256c1780efc9e2a5898a828c10ad4ea69
  Installing build dependencies: started
  Installing build dependencies: finished with status 'done'
  Getting requirements to build wheel: started
  Getting requirements to build wheel: finished with status 'done'
  Preparing metadata (pyproject.toml): started
  Preparing metadata (pyproject.toml): finished with status 'done'
Requirement already satisfied: filelock in /sw/external/python/anaconda3/lib/python3.9/site-packages (from transformers==4.37.0.dev0) (3.9.0)
Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /u/bzd2/.local/lib/python3.9/site-packages (from transformers==4.37.0.dev0) (0.19.4)
Requirement already satisfied: numpy>=1.17 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from transformers==4.37.0.dev0) (1.24.3)
Requirement already satisfied: packaging>=20.0 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from transformers==4.37.0.dev0) (23.1)
Requirement already satisfied: pyyaml>=5.1 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from transformers==4.37.0.dev0) (5.4.1)
Requirement already satisfied: regex!=2019.12.17 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from transformers==4.37.0.dev0) (2022.7.9)
Requirement already satisfied: requests in /sw/external/python/anaconda3/lib/python3.9/site-packages (from transformers==4.37.0.dev0) (2.31.0)
Requirement already satisfied: tokenizers<0.19,>=0.14 in /u/bzd2/.local/lib/python3.9/site-packages (from transformers==4.37.0.dev0) (0.15.0)
Requirement already satisfied: safetensors>=0.3.1 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from transformers==4.37.0.dev0) (0.3.2)
Requirement already satisfied: tqdm>=4.27 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from transformers==4.37.0.dev0) (4.65.0)
Requirement already satisfied: fsspec>=2023.5.0 in /u/bzd2/.local/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers==4.37.0.dev0) (2023.6.0)
Requirement already satisfied: typing-extensions>=3.7.4.3 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers==4.37.0.dev0) (4.7.1)
Requirement already satisfied: charset-normalizer<4,>=2 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from requests->transformers==4.37.0.dev0) (2.0.4)
Requirement already satisfied: idna<4,>=2.5 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from requests->transformers==4.37.0.dev0) (3.4)
Requirement already satisfied: urllib3<3,>=1.21.1 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from requests->transformers==4.37.0.dev0) (1.26.16)
Requirement already satisfied: certifi>=2017.4.17 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from requests->transformers==4.37.0.dev0) (2023.7.22)

Output: Defaulting to user installation because normal site-packages is not writeable
Requirement already satisfied: datasets in /sw/external/python/anaconda3/lib/python3.9/site-packages (2.14.5)
Requirement already satisfied: numpy>=1.17 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from datasets) (1.24.3)
Requirement already satisfied: pyarrow>=8.0.0 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from datasets) (11.0.0)
Requirement already satisfied: dill<0.3.8,>=0.3.0 in /u/bzd2/.local/lib/python3.9/site-packages (from datasets) (0.3.7)
Requirement already satisfied: pandas in /sw/external/python/anaconda3/lib/python3.9/site-packages (from datasets) (2.0.3)
Requirement already satisfied: requests>=2.19.0 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from datasets) (2.31.0)
Requirement already satisfied: tqdm>=4.62.1 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from datasets) (4.65.0)
Requirement already satisfied: xxhash in /sw/external/python/anaconda3/lib/python3.9/site-packages (from datasets) (2.0.2)
Requirement already satisfied: multiprocess in /sw/external/python/anaconda3/lib/python3.9/site-packages (from datasets) (0.70.15)
Requirement already satisfied: fsspec[http]<2023.9.0,>=2023.1.0 in /u/bzd2/.local/lib/python3.9/site-packages (from datasets) (2023.6.0)
Requirement already satisfied: aiohttp in /sw/external/python/anaconda3/lib/python3.9/site-packages (from datasets) (3.8.5)
Requirement already satisfied: huggingface-hub<1.0.0,>=0.14.0 in /u/bzd2/.local/lib/python3.9/site-packages (from datasets) (0.19.4)
Requirement already satisfied: packaging in /sw/external/python/anaconda3/lib/python3.9/site-packages (from datasets) (23.1)
Requirement already satisfied: pyyaml>=5.1 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from datasets) (5.4.1)
Requirement already satisfied: attrs>=17.3.0 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from aiohttp->datasets) (23.1.0)
Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from aiohttp->datasets) (2.0.4)
Requirement already satisfied: multidict<7.0,>=4.5 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from aiohttp->datasets) (6.0.2)
Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from aiohttp->datasets) (4.0.2)
Requirement already satisfied: yarl<2.0,>=1.0 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from aiohttp->datasets) (1.8.1)
Requirement already satisfied: frozenlist>=1.1.1 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from aiohttp->datasets) (1.3.3)
Requirement already satisfied: aiosignal>=1.1.2 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from aiohttp->datasets) (1.2.0)
Requirement already satisfied: filelock in /sw/external/python/anaconda3/lib/python3.9/site-packages (from huggingface-hub<1.0.0,>=0.14.0->datasets) (3.9.0)
Requirement already satisfied: typing-extensions>=3.7.4.3 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from huggingface-hub<1.0.0,>=0.14.0->datasets) (4.7.1)
Requirement already satisfied: idna<4,>=2.5 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from requests>=2.19.0->datasets) (3.4)
Requirement already satisfied: urllib3<3,>=1.21.1 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from requests>=2.19.0->datasets) (1.26.16)
Requirement already satisfied: certifi>=2017.4.17 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from requests>=2.19.0->datasets) (2023.7.22)
Requirement already satisfied: python-dateutil>=2.8.2 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from pandas->datasets) (2.8.2)
Requirement already satisfied: pytz>=2020.1 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from pandas->datasets) (2023.3.post1)
Requirement already satisfied: tzdata>=2022.1 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from pandas->datasets) (2023.3)
Requirement already satisfied: six>=1.5 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)

Output: Defaulting to user installation because normal site-packages is not writeable
Requirement already satisfied: accelerate in /u/bzd2/.local/lib/python3.9/site-packages (0.25.0)
Requirement already satisfied: numpy>=1.17 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from accelerate) (1.24.3)
Requirement already satisfied: packaging>=20.0 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from accelerate) (23.1)
Requirement already satisfied: psutil in /sw/external/python/anaconda3/lib/python3.9/site-packages (from accelerate) (5.9.0)
Requirement already satisfied: pyyaml in /sw/external/python/anaconda3/lib/python3.9/site-packages (from accelerate) (5.4.1)
Requirement already satisfied: torch>=1.10.0 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from accelerate) (2.2.0.dev20231003+cu121)
Requirement already satisfied: huggingface-hub in /u/bzd2/.local/lib/python3.9/site-packages (from accelerate) (0.19.4)
Requirement already satisfied: safetensors>=0.3.1 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from accelerate) (0.3.2)
Requirement already satisfied: filelock in /sw/external/python/anaconda3/lib/python3.9/site-packages (from torch>=1.10.0->accelerate) (3.9.0)
Requirement already satisfied: typing-extensions in /sw/external/python/anaconda3/lib/python3.9/site-packages (from torch>=1.10.0->accelerate) (4.7.1)
Requirement already satisfied: sympy in /sw/external/python/anaconda3/lib/python3.9/site-packages (from torch>=1.10.0->accelerate) (1.11.1)
Requirement already satisfied: networkx in /sw/external/python/anaconda3/lib/python3.9/site-packages (from torch>=1.10.0->accelerate) (3.1)
Requirement already satisfied: jinja2 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from torch>=1.10.0->accelerate) (3.1.2)
Requirement already satisfied: fsspec in /u/bzd2/.local/lib/python3.9/site-packages (from torch>=1.10.0->accelerate) (2023.6.0)
Requirement already satisfied: pytorch-triton==2.1.0+6e4932cda8 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from torch>=1.10.0->accelerate) (2.1.0+6e4932cda8)
Requirement already satisfied: requests in /sw/external/python/anaconda3/lib/python3.9/site-packages (from huggingface-hub->accelerate) (2.31.0)
Requirement already satisfied: tqdm>=4.42.1 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from huggingface-hub->accelerate) (4.65.0)
Requirement already satisfied: MarkupSafe>=2.0 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.1)
Requirement already satisfied: charset-normalizer<4,>=2 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from requests->huggingface-hub->accelerate) (2.0.4)
Requirement already satisfied: idna<4,>=2.5 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from requests->huggingface-hub->accelerate) (3.4)
Requirement already satisfied: urllib3<3,>=1.21.1 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from requests->huggingface-hub->accelerate) (1.26.16)
Requirement already satisfied: certifi>=2017.4.17 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from requests->huggingface-hub->accelerate) (2023.7.22)
Requirement already satisfied: mpmath>=0.19 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)

Output: Defaulting to user installation because normal site-packages is not writeable
Requirement already satisfied: huggingface_hub in /u/bzd2/.local/lib/python3.9/site-packages (0.19.4)
Requirement already satisfied: filelock in /sw/external/python/anaconda3/lib/python3.9/site-packages (from huggingface_hub) (3.9.0)
Requirement already satisfied: fsspec>=2023.5.0 in /u/bzd2/.local/lib/python3.9/site-packages (from huggingface_hub) (2023.6.0)
Requirement already satisfied: requests in /sw/external/python/anaconda3/lib/python3.9/site-packages (from huggingface_hub) (2.31.0)
Requirement already satisfied: tqdm>=4.42.1 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from huggingface_hub) (4.65.0)
Requirement already satisfied: pyyaml>=5.1 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from huggingface_hub) (5.4.1)
Requirement already satisfied: typing-extensions>=3.7.4.3 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from huggingface_hub) (4.7.1)
Requirement already satisfied: packaging>=20.9 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from huggingface_hub) (23.1)
Requirement already satisfied: charset-normalizer<4,>=2 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from requests->huggingface_hub) (2.0.4)
Requirement already satisfied: idna<4,>=2.5 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from requests->huggingface_hub) (3.4)
Requirement already satisfied: urllib3<3,>=1.21.1 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from requests->huggingface_hub) (1.26.16)
Requirement already satisfied: certifi>=2017.4.17 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from requests->huggingface_hub) (2023.7.22)

Output: Defaulting to user installation because normal site-packages is not writeable
Requirement already satisfied: bitsandbytes in /u/bzd2/.local/lib/python3.9/site-packages (0.41.3.post2)

Output: Defaulting to user installation because normal site-packages is not writeable
Requirement already satisfied: wandb in /u/bzd2/.local/lib/python3.9/site-packages (0.16.1)
Requirement already satisfied: Click!=8.0.0,>=7.1 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from wandb) (8.0.4)
Requirement already satisfied: GitPython!=3.1.29,>=1.0.0 in /u/bzd2/.local/lib/python3.9/site-packages (from wandb) (3.1.40)
Requirement already satisfied: requests<3,>=2.0.0 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from wandb) (2.31.0)
Requirement already satisfied: psutil>=5.0.0 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from wandb) (5.9.0)
Requirement already satisfied: sentry-sdk>=1.0.0 in /u/bzd2/.local/lib/python3.9/site-packages (from wandb) (1.38.0)
Requirement already satisfied: docker-pycreds>=0.4.0 in /u/bzd2/.local/lib/python3.9/site-packages (from wandb) (0.4.0)
Requirement already satisfied: PyYAML in /sw/external/python/anaconda3/lib/python3.9/site-packages (from wandb) (5.4.1)
Requirement already satisfied: setproctitle in /sw/external/python/anaconda3/lib/python3.9/site-packages (from wandb) (1.3.2)
Requirement already satisfied: setuptools in /sw/external/python/anaconda3/lib/python3.9/site-packages (from wandb) (68.0.0)
Requirement already satisfied: appdirs>=1.4.3 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from wandb) (1.4.4)
Requirement already satisfied: typing-extensions in /sw/external/python/anaconda3/lib/python3.9/site-packages (from wandb) (4.7.1)
Requirement already satisfied: protobuf!=4.21.0,<5,>=3.15.0 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from wandb) (3.20.3)
Requirement already satisfied: six>=1.4.0 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)
Requirement already satisfied: gitdb<5,>=4.0.1 in /u/bzd2/.local/lib/python3.9/site-packages (from GitPython!=3.1.29,>=1.0.0->wandb) (4.0.11)
Requirement already satisfied: charset-normalizer<4,>=2 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from requests<3,>=2.0.0->wandb) (2.0.4)
Requirement already satisfied: idna<4,>=2.5 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from requests<3,>=2.0.0->wandb) (3.4)
Requirement already satisfied: urllib3<3,>=1.21.1 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from requests<3,>=2.0.0->wandb) (1.26.16)
Requirement already satisfied: certifi>=2017.4.17 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from requests<3,>=2.0.0->wandb) (2023.7.22)
Requirement already satisfied: smmap<6,>=3.0.1 in /u/bzd2/.local/lib/python3.9/site-packages (from gitdb<5,>=4.0.1->GitPython!=3.1.29,>=1.0.0->wandb) (5.0.1)

Output: Defaulting to user installation because normal site-packages is not writeable
Requirement already satisfied: scikit-learn in /sw/external/python/anaconda3/lib/python3.9/site-packages (1.3.0)
Requirement already satisfied: numpy>=1.17.3 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from scikit-learn) (1.24.3)
Requirement already satisfied: scipy>=1.5.0 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from scikit-learn) (1.8.1)
Requirement already satisfied: joblib>=1.1.1 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from scikit-learn) (1.2.0)
Requirement already satisfied: threadpoolctl>=2.0.0 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from scikit-learn) (2.2.0)

Output: Defaulting to user installation because normal site-packages is not writeable
Requirement already satisfied: code_bert_score in /u/bzd2/.local/lib/python3.9/site-packages (0.4.1)
Requirement already satisfied: torch>=1.0.0 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from code_bert_score) (2.2.0.dev20231003+cu121)
Requirement already satisfied: numpy in /sw/external/python/anaconda3/lib/python3.9/site-packages (from code_bert_score) (1.24.3)
Requirement already satisfied: pandas>=1.0.1 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from code_bert_score) (2.0.3)
Requirement already satisfied: requests in /sw/external/python/anaconda3/lib/python3.9/site-packages (from code_bert_score) (2.31.0)
Requirement already satisfied: tqdm>=4.31.1 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from code_bert_score) (4.65.0)
Requirement already satisfied: matplotlib in /sw/external/python/anaconda3/lib/python3.9/site-packages (from code_bert_score) (3.7.2)
Requirement already satisfied: transformers>=3.0.0 in /u/bzd2/.local/lib/python3.9/site-packages (from code_bert_score) (4.37.0.dev0)
Requirement already satisfied: python-dateutil>=2.8.2 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from pandas>=1.0.1->code_bert_score) (2.8.2)
Requirement already satisfied: pytz>=2020.1 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from pandas>=1.0.1->code_bert_score) (2023.3.post1)
Requirement already satisfied: tzdata>=2022.1 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from pandas>=1.0.1->code_bert_score) (2023.3)
Requirement already satisfied: filelock in /sw/external/python/anaconda3/lib/python3.9/site-packages (from torch>=1.0.0->code_bert_score) (3.9.0)
Requirement already satisfied: typing-extensions in /sw/external/python/anaconda3/lib/python3.9/site-packages (from torch>=1.0.0->code_bert_score) (4.7.1)
Requirement already satisfied: sympy in /sw/external/python/anaconda3/lib/python3.9/site-packages (from torch>=1.0.0->code_bert_score) (1.11.1)
Requirement already satisfied: networkx in /sw/external/python/anaconda3/lib/python3.9/site-packages (from torch>=1.0.0->code_bert_score) (3.1)
Requirement already satisfied: jinja2 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from torch>=1.0.0->code_bert_score) (3.1.2)
Requirement already satisfied: fsspec in /u/bzd2/.local/lib/python3.9/site-packages (from torch>=1.0.0->code_bert_score) (2023.6.0)
Requirement already satisfied: pytorch-triton==2.1.0+6e4932cda8 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from torch>=1.0.0->code_bert_score) (2.1.0+6e4932cda8)
Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /u/bzd2/.local/lib/python3.9/site-packages (from transformers>=3.0.0->code_bert_score) (0.19.4)
Requirement already satisfied: packaging>=20.0 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from transformers>=3.0.0->code_bert_score) (23.1)
Requirement already satisfied: pyyaml>=5.1 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from transformers>=3.0.0->code_bert_score) (5.4.1)
Requirement already satisfied: regex!=2019.12.17 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from transformers>=3.0.0->code_bert_score) (2022.7.9)
Requirement already satisfied: tokenizers<0.19,>=0.14 in /u/bzd2/.local/lib/python3.9/site-packages (from transformers>=3.0.0->code_bert_score) (0.15.0)
Requirement already satisfied: safetensors>=0.3.1 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from transformers>=3.0.0->code_bert_score) (0.3.2)
Requirement already satisfied: contourpy>=1.0.1 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from matplotlib->code_bert_score) (1.0.5)
Requirement already satisfied: cycler>=0.10 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from matplotlib->code_bert_score) (0.11.0)
Requirement already satisfied: fonttools>=4.22.0 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from matplotlib->code_bert_score) (4.25.0)
Requirement already satisfied: kiwisolver>=1.0.1 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from matplotlib->code_bert_score) (1.4.4)
Requirement already satisfied: pillow>=6.2.0 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from matplotlib->code_bert_score) (9.4.0)
Requirement already satisfied: pyparsing<3.1,>=2.3.1 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from matplotlib->code_bert_score) (3.0.9)
Requirement already satisfied: importlib-resources>=3.2.0 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from matplotlib->code_bert_score) (5.2.0)
Requirement already satisfied: charset-normalizer<4,>=2 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from requests->code_bert_score) (2.0.4)
Requirement already satisfied: idna<4,>=2.5 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from requests->code_bert_score) (3.4)
Requirement already satisfied: urllib3<3,>=1.21.1 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from requests->code_bert_score) (1.26.16)
Requirement already satisfied: certifi>=2017.4.17 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from requests->code_bert_score) (2023.7.22)
Requirement already satisfied: zipp>=3.1.0 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from importlib-resources>=3.2.0->matplotlib->code_bert_score) (3.11.0)
Requirement already satisfied: six>=1.5 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from python-dateutil>=2.8.2->pandas>=1.0.1->code_bert_score) (1.16.0)
Requirement already satisfied: MarkupSafe>=2.0 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from jinja2->torch>=1.0.0->code_bert_score) (2.1.1)
Requirement already satisfied: mpmath>=0.19 in /sw/external/python/anaconda3/lib/python3.9/site-packages (from sympy->torch>=1.0.0->code_bert_score) (1.3.0)

WARNING:__main__:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
2023-12-13 17:17:22.434879: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
2023-12-13 17:17:22.434880: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
2023-12-13 17:17:22.434890: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
2023-12-13 17:17:22.434888: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
2023-12-13 17:17:22.435395: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2023-12-13 17:17:22.435402: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2023-12-13 17:17:22.435402: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2023-12-13 17:17:22.435409: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2023-12-13 17:17:22.441450: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2023-12-13 17:17:22.441455: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2023-12-13 17:17:22.441458: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2023-12-13 17:17:22.441460: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2023-12-13 17:17:23.464774: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-12-13 17:17:23.464772: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-12-13 17:17:23.464789: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-12-13 17:17:23.464791: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
/u/bzd2/.local/lib/python3.9/site-packages/transformers/models/auto/tokenization_auto.py:691: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/transformers/models/auto/tokenization_auto.py:691: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/transformers/models/auto/tokenization_auto.py:691: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/transformers/models/auto/tokenization_auto.py:691: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.
  warnings.warn(
loading file vocab.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-1b/snapshots/182f0165fdf8da9c9935901eec65c94337f01c11/vocab.json
loading file merges.txt from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-1b/snapshots/182f0165fdf8da9c9935901eec65c94337f01c11/merges.txt
loading file tokenizer.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-1b/snapshots/182f0165fdf8da9c9935901eec65c94337f01c11/tokenizer.json
loading file added_tokens.json from cache at None
loading file vocab.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-1b/snapshots/182f0165fdf8da9c9935901eec65c94337f01c11/vocab.json
loading file special_tokens_map.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-1b/snapshots/182f0165fdf8da9c9935901eec65c94337f01c11/special_tokens_map.json
loading file tokenizer_config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-1b/snapshots/182f0165fdf8da9c9935901eec65c94337f01c11/tokenizer_config.json
loading file merges.txt from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-1b/snapshots/182f0165fdf8da9c9935901eec65c94337f01c11/merges.txt
loading file tokenizer.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-1b/snapshots/182f0165fdf8da9c9935901eec65c94337f01c11/tokenizer.json
loading file added_tokens.json from cache at None
loading file special_tokens_map.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-1b/snapshots/182f0165fdf8da9c9935901eec65c94337f01c11/special_tokens_map.json
loading file tokenizer_config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-1b/snapshots/182f0165fdf8da9c9935901eec65c94337f01c11/tokenizer_config.json
loading file vocab.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-1b/snapshots/182f0165fdf8da9c9935901eec65c94337f01c11/vocab.json
loading file merges.txt from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-1b/snapshots/182f0165fdf8da9c9935901eec65c94337f01c11/merges.txt
loading file tokenizer.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-1b/snapshots/182f0165fdf8da9c9935901eec65c94337f01c11/tokenizer.json
loading file added_tokens.json from cache at None
loading file special_tokens_map.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-1b/snapshots/182f0165fdf8da9c9935901eec65c94337f01c11/special_tokens_map.json
loading file tokenizer_config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-1b/snapshots/182f0165fdf8da9c9935901eec65c94337f01c11/tokenizer_config.json
loading file vocab.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-1b/snapshots/182f0165fdf8da9c9935901eec65c94337f01c11/vocab.json
loading file merges.txt from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-1b/snapshots/182f0165fdf8da9c9935901eec65c94337f01c11/merges.txt
loading file tokenizer.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-1b/snapshots/182f0165fdf8da9c9935901eec65c94337f01c11/tokenizer.json
loading file added_tokens.json from cache at None
loading file special_tokens_map.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-1b/snapshots/182f0165fdf8da9c9935901eec65c94337f01c11/special_tokens_map.json
loading file tokenizer_config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-1b/snapshots/182f0165fdf8da9c9935901eec65c94337f01c11/tokenizer_config.json
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/datasets/load.py:2089: FutureWarning: 'use_auth_token' was deprecated in favor of 'token' in version 2.14.0 and will be removed in 3.0.0.
You can remove this warning by passing 'token=True' instead.
  warnings.warn(
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/datasets/load.py:2089: FutureWarning: 'use_auth_token' was deprecated in favor of 'token' in version 2.14.0 and will be removed in 3.0.0.
You can remove this warning by passing 'token=True' instead.
  warnings.warn(
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/datasets/load.py:2089: FutureWarning: 'use_auth_token' was deprecated in favor of 'token' in version 2.14.0 and will be removed in 3.0.0.
You can remove this warning by passing 'token=True' instead.
  warnings.warn(
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/datasets/load.py:2089: FutureWarning: 'use_auth_token' was deprecated in favor of 'token' in version 2.14.0 and will be removed in 3.0.0.
You can remove this warning by passing 'token=True' instead.
  warnings.warn(
Loading the dataset in streaming mode
Loading the dataset in streaming mode
Loading the dataset in streaming mode
Loading the dataset in streaming mode
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<FOR DEBUGGING PURPOSES ONLY!>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
{'license': 'MIT', 'download_link': 'https://old-galaxy.ansible.com/austincloudguru/splunk_forwarder', 'input': 'name: Set logfile permissions', 'repo_name': 'splunk_forwarder', 'path': 'data/repos/austincloudguru/splunk_forwarder/tasks/main.yml', 'output': "ansible.posix.acl:\n  path: ''\n  entity: splunk\n  state: present\n  etype: user\n  permissions: r\n  recursive: true\nbecome: true\nwith_items: ''\n", 'download_count': '8962', 'org_name': 'austincloudguru'}
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<FOR DEBUGGING PURPOSES ONLY!>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
{'license': 'Apache-2.0-Short,Apache-2.0', 'download_link': 'https://old-galaxy.ansible.com/buluma/keepalived', 'input': 'name: Test if keepalived_vrrp_instances is set correctly', 'repo_name': 'keepalived', 'path': 'data/repos/buluma/keepalived/tasks/assert.yml', 'output': 'ansible.builtin.assert:\n  that:\n  - keepalived_vrrp_instances is defined\n  - keepalived_vrrp_instances is iterable\n  quiet: true\n', 'download_count': '1012', 'org_name': 'buluma'}
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<FOR DEBUGGING PURPOSES ONLY!>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
{'license': 'Apache-2.0-Short,Apache-2.0', 'download_link': 'https://old-galaxy.ansible.com/azavea/zookeeper', 'input': 'name: Pin Cloudera APT repositories', 'repo_name': 'zookeeper', 'path': 'data/repos/azavea/zookeeper/tasks/main.yml', 'output': 'template: src=cdh5.j2 dest=/etc/apt/preferences.d/cdh5\n', 'download_count': '3388', 'org_name': 'azavea'}<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<FOR DEBUGGING PURPOSES ONLY!>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>

<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<FOR DEBUGGING PURPOSES ONLY!>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
{'license': '', 'download_link': 'https://old-galaxy.ansible.com/panchal_yash/percona_xtradb_cluster_role', 'input': 'name: disable the mysql module on RHEL/CentOS 8', 'repo_name': 'percona_xtradb_cluster_role', 'path': 'data/repos/panchal_yash/percona_xtradb_cluster_role/tasks/rhel_80_repos.yml', 'output': 'command: /usr/bin/dnf module disable mysql -y\nwhen: ansible_distribution_major_version == "8"\n', 'download_count': '647', 'org_name': 'panchal_yash'}
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<FOR DEBUGGING PURPOSES ONLY!>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
{'org_name': 'austincloudguru', 'input': 'name: Set logfile permissions', 'license': 'MIT', 'output': "ansible.posix.acl:\n  path: ''\n  entity: splunk\n  state: present\n  etype: user\n  permissions: r\n  recursive: true\nbecome: true\nwith_items: ''\n", 'download_count': '8962', 'path': 'data/repos/austincloudguru/splunk_forwarder/tasks/main.yml', 'repo_name': 'splunk_forwarder', 'download_link': 'https://old-galaxy.ansible.com/austincloudguru/splunk_forwarder'}
{'license': 'Apache-2.0-Short,Apache-2.0', 'download_link': 'https://old-galaxy.ansible.com/robertdebock/logwatch', 'input': 'name: assert | Test logwatch_service', 'repo_name': 'logwatch', 'path': 'data/repos/robertdebock/logwatch/tasks/assert.yml', 'output': 'ansible.builtin.assert:\n  that:\n  - logwatch_service is defined\n  - logwatch_service is string\n  - logwatch_service is not none\n  quiet: true\n', 'download_count': '664', 'org_name': 'robertdebock'}
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<FOR DEBUGGING PURPOSES ONLY!>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>><<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<FOR DEBUGGING PURPOSES ONLY!>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>

{'license': '', 'download_link': 'https://old-galaxy.ansible.com/community/network', 'input': 'name: Test BGP - graceful-restart-helper', 'repo_name': 'network', 'path': 'data/repos/community/network/integration/targets/cnos_bgp/tasks/main.yml', 'output': "cnos_bgp: host= username=  password= deviceType= outputfile=./results/cnos_bgp__output.txt\n  asNum='' bgpArg1=''\nwith_items: '{''asNum'': 33, ''bgpArg1'': ''graceful-restart-helper''}'\n", 'download_count': '723055', 'org_name': 'community'}
{'org_name': 'buluma', 'input': 'name: Test if keepalived_vrrp_instances is set correctly', 'license': 'Apache-2.0-Short,Apache-2.0', 'output': 'ansible.builtin.assert:\n  that:\n  - keepalived_vrrp_instances is defined\n  - keepalived_vrrp_instances is iterable\n  quiet: true\n', 'download_count': '1012', 'path': 'data/repos/buluma/keepalived/tasks/assert.yml', 'repo_name': 'keepalived', 'download_link': 'https://old-galaxy.ansible.com/buluma/keepalived'}
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<FOR DEBUGGING PURPOSES ONLY!>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<FOR DEBUGGING PURPOSES ONLY!>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
{'license': '', 'download_link': 'https://old-galaxy.ansible.com/bertvv/tftp', 'input': 'name: "Ensure SELinux boolean \\u2018tftp_home_dir\\u2019 has the desired value"', 'repo_name': 'tftp', 'path': 'data/repos/bertvv/tftp/tasks/selinux.yml', 'output': "seboolean:\n  name: tftp_home_dir\n  state: 'false'\n  persistent: true\ntags: tftp\n", 'download_count': '9882', 'org_name': 'bertvv'}{'org_name': 'azavea', 'input': 'name: Pin Cloudera APT repositories', 'license': 'Apache-2.0-Short,Apache-2.0', 'output': 'template: src=cdh5.j2 dest=/etc/apt/preferences.d/cdh5\n', 'download_count': '3388', 'path': 'data/repos/azavea/zookeeper/tasks/main.yml', 'repo_name': 'zookeeper', 'download_link': 'https://old-galaxy.ansible.com/azavea/zookeeper'}

<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<FOR DEBUGGING PURPOSES ONLY!>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>><<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<FOR DEBUGGING PURPOSES ONLY!>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>

{'license': 'Apache-2.0-Short,Apache-2.0', 'download_link': 'https://old-galaxy.ansible.com/amritsingh/ec2_monitor', 'input': 'name: Install apt packages', 'repo_name': 'ec2_monitor', 'path': 'data/repos/amritsingh/ec2_monitor/tasks/prereq.yml', 'output': 'apt: name= state=present\nwith_items:\n- unzip\n- libwww-perl\n- libdatetime-perl\nwhen: ansible_os_family == "Debian"\nbecome: true\n', 'download_count': '8094', 'org_name': 'amritsingh'}{'org_name': 'panchal_yash', 'input': 'name: disable the mysql module on RHEL/CentOS 8', 'license': '', 'output': 'command: /usr/bin/dnf module disable mysql -y\nwhen: ansible_distribution_major_version == "8"\n', 'download_count': '647', 'path': 'data/repos/panchal_yash/percona_xtradb_cluster_role/tasks/rhel_80_repos.yml', 'repo_name': 'percona_xtradb_cluster_role', 'download_link': 'https://old-galaxy.ansible.com/panchal_yash/percona_xtradb_cluster_role'}

<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<FOR DEBUGGING PURPOSES ONLY!>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<FOR DEBUGGING PURPOSES ONLY!>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
{'license': 'MIT', 'download_link': 'https://old-galaxy.ansible.com/rhythmictech/ansible_role_efs_utils', 'input': 'name: run Debian build script', 'repo_name': 'ansible_role_efs_utils', 'path': 'data/repos/rhythmictech/ansible_role_efs_utils/tasks/setup-Debian.yml', 'output': 'shell: /usr/src/efs-utils/build-deb.sh\nargs:\n  chdir: /usr/src/efs-utils\n  creates: /usr/src/efs-utils/build/amazon-efs-utils*deb\ntags:\n- efs-utils\n', 'download_count': '1799', 'org_name': 'rhythmictech'}<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<FOR DEBUGGING PURPOSES ONLY!>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>{'org_name': 'robertdebock', 'input': 'name: assert | Test logwatch_service', 'license': 'Apache-2.0-Short,Apache-2.0', 'output': 'ansible.builtin.assert:\n  that:\n  - logwatch_service is defined\n  - logwatch_service is string\n  - logwatch_service is not none\n  quiet: true\n', 'download_count': '664', 'path': 'data/repos/robertdebock/logwatch/tasks/assert.yml', 'repo_name': 'logwatch', 'download_link': 'https://old-galaxy.ansible.com/robertdebock/logwatch'}


<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<FOR DEBUGGING PURPOSES ONLY!>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>><<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<FOR DEBUGGING PURPOSES ONLY!>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>

{'license': 'Apache-2.0-Short,Apache-2.0', 'download_link': 'https://old-galaxy.ansible.com/buluma/centos_base', 'input': 'name: Vim alias in .bashrc', 'repo_name': 'centos_base', 'path': 'data/repos/buluma/centos_base/tasks/main.yml', 'output': 'ansible.builtin.lineinfile:\n  dest: /root/.bashrc\n  line: alias vi=vim\n  state: present\nwhen: centos_base_basic_vim_tweaks | bool\n', 'download_count': '12437', 'org_name': 'buluma'}{'license': 'MIT', 'download_count': '8962', 'repo_name': 'splunk_forwarder', 'output': "ansible.posix.acl:\n  path: ''\n  entity: splunk\n  state: present\n  etype: user\n  permissions: r\n  recursive: true\nbecome: true\nwith_items: ''\n", 'download_link': 'https://old-galaxy.ansible.com/austincloudguru/splunk_forwarder', 'path': 'data/repos/austincloudguru/splunk_forwarder/tasks/main.yml', 'input': 'name: Set logfile permissions', 'org_name': 'austincloudguru'}{'org_name': 'community', 'input': 'name: Test BGP - graceful-restart-helper', 'license': '', 'output': "cnos_bgp: host= username=  password= deviceType= outputfile=./results/cnos_bgp__output.txt\n  asNum='' bgpArg1=''\nwith_items: '{''asNum'': 33, ''bgpArg1'': ''graceful-restart-helper''}'\n", 'download_count': '723055', 'path': 'data/repos/community/network/integration/targets/cnos_bgp/tasks/main.yml', 'repo_name': 'network', 'download_link': 'https://old-galaxy.ansible.com/community/network'}


<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<FOR DEBUGGING PURPOSES ONLY!>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<FOR DEBUGGING PURPOSES ONLY!>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
{'org_name': 'bertvv', 'input': 'name: "Ensure SELinux boolean \\u2018tftp_home_dir\\u2019 has the desired value"', 'license': '', 'output': "seboolean:\n  name: tftp_home_dir\n  state: 'false'\n  persistent: true\ntags: tftp\n", 'download_count': '9882', 'path': 'data/repos/bertvv/tftp/tasks/selinux.yml', 'repo_name': 'tftp', 'download_link': 'https://old-galaxy.ansible.com/bertvv/tftp'}
{'license': 'Apache-2.0-Short,Apache-2.0', 'download_count': '1012', 'repo_name': 'keepalived', 'output': 'ansible.builtin.assert:\n  that:\n  - keepalived_vrrp_instances is defined\n  - keepalived_vrrp_instances is iterable\n  quiet: true\n', 'download_link': 'https://old-galaxy.ansible.com/buluma/keepalived', 'path': 'data/repos/buluma/keepalived/tasks/assert.yml', 'input': 'name: Test if keepalived_vrrp_instances is set correctly', 'org_name': 'buluma'}
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<FOR DEBUGGING PURPOSES ONLY!>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<FOR DEBUGGING PURPOSES ONLY!>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
{'org_name': 'amritsingh', 'input': 'name: Install apt packages', 'license': 'Apache-2.0-Short,Apache-2.0', 'output': 'apt: name= state=present\nwith_items:\n- unzip\n- libwww-perl\n- libdatetime-perl\nwhen: ansible_os_family == "Debian"\nbecome: true\n', 'download_count': '8094', 'path': 'data/repos/amritsingh/ec2_monitor/tasks/prereq.yml', 'repo_name': 'ec2_monitor', 'download_link': 'https://old-galaxy.ansible.com/amritsingh/ec2_monitor'}{'license': 'Apache-2.0-Short,Apache-2.0', 'download_count': '3388', 'repo_name': 'zookeeper', 'output': 'template: src=cdh5.j2 dest=/etc/apt/preferences.d/cdh5\n', 'download_link': 'https://old-galaxy.ansible.com/azavea/zookeeper', 'path': 'data/repos/azavea/zookeeper/tasks/main.yml', 'input': 'name: Pin Cloudera APT repositories', 'org_name': 'azavea'}

<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<FOR DEBUGGING PURPOSES ONLY!>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<FOR DEBUGGING PURPOSES ONLY!>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
{'org_name': 'rhythmictech', 'input': 'name: run Debian build script', 'license': 'MIT', 'output': 'shell: /usr/src/efs-utils/build-deb.sh\nargs:\n  chdir: /usr/src/efs-utils\n  creates: /usr/src/efs-utils/build/amazon-efs-utils*deb\ntags:\n- efs-utils\n', 'download_count': '1799', 'path': 'data/repos/rhythmictech/ansible_role_efs_utils/tasks/setup-Debian.yml', 'repo_name': 'ansible_role_efs_utils', 'download_link': 'https://old-galaxy.ansible.com/rhythmictech/ansible_role_efs_utils'}
{'license': '', 'download_count': '647', 'repo_name': 'percona_xtradb_cluster_role', 'output': 'command: /usr/bin/dnf module disable mysql -y\nwhen: ansible_distribution_major_version == "8"\n', 'download_link': 'https://old-galaxy.ansible.com/panchal_yash/percona_xtradb_cluster_role', 'path': 'data/repos/panchal_yash/percona_xtradb_cluster_role/tasks/rhel_80_repos.yml', 'input': 'name: disable the mysql module on RHEL/CentOS 8', 'org_name': 'panchal_yash'}
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<FOR DEBUGGING PURPOSES ONLY!>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<FOR DEBUGGING PURPOSES ONLY!>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
{'org_name': 'buluma', 'input': 'name: Vim alias in .bashrc', 'license': 'Apache-2.0-Short,Apache-2.0', 'output': 'ansible.builtin.lineinfile:\n  dest: /root/.bashrc\n  line: alias vi=vim\n  state: present\nwhen: centos_base_basic_vim_tweaks | bool\n', 'download_count': '12437', 'path': 'data/repos/buluma/centos_base/tasks/main.yml', 'repo_name': 'centos_base', 'download_link': 'https://old-galaxy.ansible.com/buluma/centos_base'}
{'license': 'Apache-2.0-Short,Apache-2.0', 'download_count': '664', 'repo_name': 'logwatch', 'output': 'ansible.builtin.assert:\n  that:\n  - logwatch_service is defined\n  - logwatch_service is string\n  - logwatch_service is not none\n  quiet: true\n', 'download_link': 'https://old-galaxy.ansible.com/robertdebock/logwatch', 'path': 'data/repos/robertdebock/logwatch/tasks/assert.yml', 'input': 'name: assert | Test logwatch_service', 'org_name': 'robertdebock'}<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<FOR DEBUGGING PURPOSES ONLY!>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>

<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<FOR DEBUGGING PURPOSES ONLY!>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
{'license': '', 'download_count': '723055', 'repo_name': 'network', 'output': "cnos_bgp: host= username=  password= deviceType= outputfile=./results/cnos_bgp__output.txt\n  asNum='' bgpArg1=''\nwith_items: '{''asNum'': 33, ''bgpArg1'': ''graceful-restart-helper''}'\n", 'download_link': 'https://old-galaxy.ansible.com/community/network', 'path': 'data/repos/community/network/integration/targets/cnos_bgp/tasks/main.yml', 'input': 'name: Test BGP - graceful-restart-helper', 'org_name': 'community'}
{'input': 'name: Set logfile permissions', 'path': 'data/repos/austincloudguru/splunk_forwarder/tasks/main.yml', 'org_name': 'austincloudguru', 'repo_name': 'splunk_forwarder', 'output': "ansible.posix.acl:\n  path: ''\n  entity: splunk\n  state: present\n  etype: user\n  permissions: r\n  recursive: true\nbecome: true\nwith_items: ''\n", 'license': 'MIT', 'download_link': 'https://old-galaxy.ansible.com/austincloudguru/splunk_forwarder', 'download_count': '8962'}<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<FOR DEBUGGING PURPOSES ONLY!>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>

<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<FOR DEBUGGING PURPOSES ONLY!>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>{'license': '', 'download_count': '9882', 'repo_name': 'tftp', 'output': "seboolean:\n  name: tftp_home_dir\n  state: 'false'\n  persistent: true\ntags: tftp\n", 'download_link': 'https://old-galaxy.ansible.com/bertvv/tftp', 'path': 'data/repos/bertvv/tftp/tasks/selinux.yml', 'input': 'name: "Ensure SELinux boolean \\u2018tftp_home_dir\\u2019 has the desired value"', 'org_name': 'bertvv'}

<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<FOR DEBUGGING PURPOSES ONLY!>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
{'input': 'name: Test if keepalived_vrrp_instances is set correctly', 'path': 'data/repos/buluma/keepalived/tasks/assert.yml', 'org_name': 'buluma', 'repo_name': 'keepalived', 'output': 'ansible.builtin.assert:\n  that:\n  - keepalived_vrrp_instances is defined\n  - keepalived_vrrp_instances is iterable\n  quiet: true\n', 'license': 'Apache-2.0-Short,Apache-2.0', 'download_link': 'https://old-galaxy.ansible.com/buluma/keepalived', 'download_count': '1012'}
{'license': 'Apache-2.0-Short,Apache-2.0', 'download_count': '8094', 'repo_name': 'ec2_monitor', 'output': 'apt: name= state=present\nwith_items:\n- unzip\n- libwww-perl\n- libdatetime-perl\nwhen: ansible_os_family == "Debian"\nbecome: true\n', 'download_link': 'https://old-galaxy.ansible.com/amritsingh/ec2_monitor', 'path': 'data/repos/amritsingh/ec2_monitor/tasks/prereq.yml', 'input': 'name: Install apt packages', 'org_name': 'amritsingh'}<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<FOR DEBUGGING PURPOSES ONLY!>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>

<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<FOR DEBUGGING PURPOSES ONLY!>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
{'input': 'name: Pin Cloudera APT repositories', 'path': 'data/repos/azavea/zookeeper/tasks/main.yml', 'org_name': 'azavea', 'repo_name': 'zookeeper', 'output': 'template: src=cdh5.j2 dest=/etc/apt/preferences.d/cdh5\n', 'license': 'Apache-2.0-Short,Apache-2.0', 'download_link': 'https://old-galaxy.ansible.com/azavea/zookeeper', 'download_count': '3388'}
{'license': 'MIT', 'download_count': '1799', 'repo_name': 'ansible_role_efs_utils', 'output': 'shell: /usr/src/efs-utils/build-deb.sh\nargs:\n  chdir: /usr/src/efs-utils\n  creates: /usr/src/efs-utils/build/amazon-efs-utils*deb\ntags:\n- efs-utils\n', 'download_link': 'https://old-galaxy.ansible.com/rhythmictech/ansible_role_efs_utils', 'path': 'data/repos/rhythmictech/ansible_role_efs_utils/tasks/setup-Debian.yml', 'input': 'name: run Debian build script', 'org_name': 'rhythmictech'}<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<FOR DEBUGGING PURPOSES ONLY!>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>

<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<FOR DEBUGGING PURPOSES ONLY!>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
{'input': 'name: disable the mysql module on RHEL/CentOS 8', 'path': 'data/repos/panchal_yash/percona_xtradb_cluster_role/tasks/rhel_80_repos.yml', 'org_name': 'panchal_yash', 'repo_name': 'percona_xtradb_cluster_role', 'output': 'command: /usr/bin/dnf module disable mysql -y\nwhen: ansible_distribution_major_version == "8"\n', 'license': '', 'download_link': 'https://old-galaxy.ansible.com/panchal_yash/percona_xtradb_cluster_role', 'download_count': '647'}
{'license': 'Apache-2.0-Short,Apache-2.0', 'download_count': '12437', 'repo_name': 'centos_base', 'output': 'ansible.builtin.lineinfile:\n  dest: /root/.bashrc\n  line: alias vi=vim\n  state: present\nwhen: centos_base_basic_vim_tweaks | bool\n', 'download_link': 'https://old-galaxy.ansible.com/buluma/centos_base', 'path': 'data/repos/buluma/centos_base/tasks/main.yml', 'input': 'name: Vim alias in .bashrc', 'org_name': 'buluma'}<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<FOR DEBUGGING PURPOSES ONLY!>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>><<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<FOR DEBUGGING PURPOSES ONLY!>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>


{'input': 'name: assert | Test logwatch_service', 'path': 'data/repos/robertdebock/logwatch/tasks/assert.yml', 'org_name': 'robertdebock', 'repo_name': 'logwatch', 'output': 'ansible.builtin.assert:\n  that:\n  - logwatch_service is defined\n  - logwatch_service is string\n  - logwatch_service is not none\n  quiet: true\n', 'license': 'Apache-2.0-Short,Apache-2.0', 'download_link': 'https://old-galaxy.ansible.com/robertdebock/logwatch', 'download_count': '664'}
{'license': 'MIT', 'download_link': 'https://old-galaxy.ansible.com/igor_nikiforov/etcd', 'input': 'name: Create etcd systemd service', 'repo_name': 'etcd', 'path': 'data/repos/igor_nikiforov/etcd/tasks/main.yml', 'output': 'template:\n  src: etcd.service.j2\n  dest: /etc/systemd/system/etcd.service\n  owner: root\n  group: root\n  mode: 420\n', 'download_count': '1748631', 'org_name': 'igor_nikiforov'}
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<FOR DEBUGGING PURPOSES ONLY!>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
{'input': 'name: Test BGP - graceful-restart-helper', 'path': 'data/repos/community/network/integration/targets/cnos_bgp/tasks/main.yml', 'org_name': 'community', 'repo_name': 'network', 'output': "cnos_bgp: host= username=  password= deviceType= outputfile=./results/cnos_bgp__output.txt\n  asNum='' bgpArg1=''\nwith_items: '{''asNum'': 33, ''bgpArg1'': ''graceful-restart-helper''}'\n", 'license': '', 'download_link': 'https://old-galaxy.ansible.com/community/network', 'download_count': '723055'}
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<FOR DEBUGGING PURPOSES ONLY!>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
{'input': 'name: "Ensure SELinux boolean \\u2018tftp_home_dir\\u2019 has the desired value"', 'path': 'data/repos/bertvv/tftp/tasks/selinux.yml', 'org_name': 'bertvv', 'repo_name': 'tftp', 'output': "seboolean:\n  name: tftp_home_dir\n  state: 'false'\n  persistent: true\ntags: tftp\n", 'license': '', 'download_link': 'https://old-galaxy.ansible.com/bertvv/tftp', 'download_count': '9882'}
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<FOR DEBUGGING PURPOSES ONLY!>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
{'input': 'name: Install apt packages', 'path': 'data/repos/amritsingh/ec2_monitor/tasks/prereq.yml', 'org_name': 'amritsingh', 'repo_name': 'ec2_monitor', 'output': 'apt: name= state=present\nwith_items:\n- unzip\n- libwww-perl\n- libdatetime-perl\nwhen: ansible_os_family == "Debian"\nbecome: true\n', 'license': 'Apache-2.0-Short,Apache-2.0', 'download_link': 'https://old-galaxy.ansible.com/amritsingh/ec2_monitor', 'download_count': '8094'}
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<FOR DEBUGGING PURPOSES ONLY!>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>><<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<FOR DEBUGGING PURPOSES ONLY!>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>

{'input': 'name: run Debian build script', 'path': 'data/repos/rhythmictech/ansible_role_efs_utils/tasks/setup-Debian.yml', 'org_name': 'rhythmictech', 'repo_name': 'ansible_role_efs_utils', 'output': 'shell: /usr/src/efs-utils/build-deb.sh\nargs:\n  chdir: /usr/src/efs-utils\n  creates: /usr/src/efs-utils/build/amazon-efs-utils*deb\ntags:\n- efs-utils\n', 'license': 'MIT', 'download_link': 'https://old-galaxy.ansible.com/rhythmictech/ansible_role_efs_utils', 'download_count': '1799'}{'org_name': 'igor_nikiforov', 'input': 'name: Create etcd systemd service', 'license': 'MIT', 'output': 'template:\n  src: etcd.service.j2\n  dest: /etc/systemd/system/etcd.service\n  owner: root\n  group: root\n  mode: 420\n', 'download_count': '1748631', 'path': 'data/repos/igor_nikiforov/etcd/tasks/main.yml', 'repo_name': 'etcd', 'download_link': 'https://old-galaxy.ansible.com/igor_nikiforov/etcd'}

<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<FOR DEBUGGING PURPOSES ONLY!>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
{'input': 'name: Vim alias in .bashrc', 'path': 'data/repos/buluma/centos_base/tasks/main.yml', 'org_name': 'buluma', 'repo_name': 'centos_base', 'output': 'ansible.builtin.lineinfile:\n  dest: /root/.bashrc\n  line: alias vi=vim\n  state: present\nwhen: centos_base_basic_vim_tweaks | bool\n', 'license': 'Apache-2.0-Short,Apache-2.0', 'download_link': 'https://old-galaxy.ansible.com/buluma/centos_base', 'download_count': '12437'}
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<FOR DEBUGGING PURPOSES ONLY!>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
{'license': 'MIT', 'download_count': '1748631', 'repo_name': 'etcd', 'output': 'template:\n  src: etcd.service.j2\n  dest: /etc/systemd/system/etcd.service\n  owner: root\n  group: root\n  mode: 420\n', 'download_link': 'https://old-galaxy.ansible.com/igor_nikiforov/etcd', 'path': 'data/repos/igor_nikiforov/etcd/tasks/main.yml', 'input': 'name: Create etcd systemd service', 'org_name': 'igor_nikiforov'}
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<FOR DEBUGGING PURPOSES ONLY!>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
{'input': 'name: Create etcd systemd service', 'path': 'data/repos/igor_nikiforov/etcd/tasks/main.yml', 'org_name': 'igor_nikiforov', 'repo_name': 'etcd', 'output': 'template:\n  src: etcd.service.j2\n  dest: /etc/systemd/system/etcd.service\n  owner: root\n  group: root\n  mode: 420\n', 'license': 'MIT', 'download_link': 'https://old-galaxy.ansible.com/igor_nikiforov/etcd', 'download_count': '1748631'}
  0%|          | 0/400 [00:00<?, ?it/s]  0%|          | 0/400 [00:00<?, ?it/s]  0%|          | 0/400 [00:00<?, ?it/s]  0%|          | 0/400 [00:00<?, ?it/s]  0%|          | 1/400 [00:00<03:14,  2.05it/s]  0%|          | 1/400 [00:00<03:14,  2.05it/s]  0%|          | 1/400 [00:00<03:14,  2.05it/s]  0%|          | 1/400 [00:00<03:14,  2.05it/s]100%|| 400/400 [00:00<00:00, 723.23it/s]
100%|| 400/400 [00:00<00:00, 722.29it/s]
100%|| 400/400 [00:00<00:00, 721.58it/s]
100%|| 400/400 [00:00<00:00, 720.93it/s]
The character to token ratio of the dataset is: 3.14
Loading the model
The character to token ratio of the dataset is: 3.14
Loading the model
The character to token ratio of the dataset is: 3.14
Loading the model
The character to token ratio of the dataset is: 3.14
Loading the model
Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
/u/bzd2/.local/lib/python3.9/site-packages/transformers/models/auto/auto_factory.py:472: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.
  warnings.warn(
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-1b/snapshots/182f0165fdf8da9c9935901eec65c94337f01c11/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "bigcode/starcoderbase-1b",
  "activation_function": "gelu_pytorch_tanh",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 2048,
  "n_head": 16,
  "n_inner": 8192,
  "n_layer": 24,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.37.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

/u/bzd2/.local/lib/python3.9/site-packages/transformers/models/auto/auto_factory.py:472: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/transformers/models/auto/auto_factory.py:472: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/transformers/models/auto/auto_factory.py:472: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.
  warnings.warn(
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-1b/snapshots/182f0165fdf8da9c9935901eec65c94337f01c11/config.json
loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-1b/snapshots/182f0165fdf8da9c9935901eec65c94337f01c11/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "bigcode/starcoderbase-1b",
  "activation_function": "gelu_pytorch_tanh",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 2048,
  "n_head": 16,
  "n_inner": 8192,
  "n_layer": 24,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.37.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

Model config GPTBigCodeConfig {
  "_name_or_path": "bigcode/starcoderbase-1b",
  "activation_function": "gelu_pytorch_tanh",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 2048,
  "n_head": 16,
  "n_inner": 8192,
  "n_layer": 24,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.37.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

loading configuration file config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-1b/snapshots/182f0165fdf8da9c9935901eec65c94337f01c11/config.json
Model config GPTBigCodeConfig {
  "_name_or_path": "bigcode/starcoderbase-1b",
  "activation_function": "gelu_pytorch_tanh",
  "architectures": [
    "GPTBigCodeForCausalLM"
  ],
  "attention_softmax_in_fp32": true,
  "attn_pdrop": 0.1,
  "bos_token_id": 0,
  "embd_pdrop": 0.1,
  "eos_token_id": 0,
  "inference_runner": 0,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "max_batch_size": null,
  "max_sequence_length": null,
  "model_type": "gpt_bigcode",
  "multi_query": true,
  "n_embd": 2048,
  "n_head": 16,
  "n_inner": 8192,
  "n_layer": 24,
  "n_positions": 8192,
  "pad_key_length": true,
  "pre_allocate_kv_cache": false,
  "resid_pdrop": 0.1,
  "scale_attention_softmax_in_fp32": true,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "torch_dtype": "float32",
  "transformers_version": "4.37.0.dev0",
  "use_cache": true,
  "validate_runner_input": true,
  "vocab_size": 49152
}

Overriding torch_dtype=None with `torch_dtype=torch.float16` due to requirements of `bitsandbytes` to enable model loading in 8-bit or 4-bit. Pass your own torch_dtype to specify the dtype of the remaining non-linear layers or pass torch_dtype=torch.float16 to remove this warning.
Overriding torch_dtype=None with `torch_dtype=torch.float16` due to requirements of `bitsandbytes` to enable model loading in 8-bit or 4-bit. Pass your own torch_dtype to specify the dtype of the remaining non-linear layers or pass torch_dtype=torch.float16 to remove this warning.
Overriding torch_dtype=None with `torch_dtype=torch.float16` due to requirements of `bitsandbytes` to enable model loading in 8-bit or 4-bit. Pass your own torch_dtype to specify the dtype of the remaining non-linear layers or pass torch_dtype=torch.float16 to remove this warning.
Overriding torch_dtype=None with `torch_dtype=torch.float16` due to requirements of `bitsandbytes` to enable model loading in 8-bit or 4-bit. Pass your own torch_dtype to specify the dtype of the remaining non-linear layers or pass torch_dtype=torch.float16 to remove this warning.
loading weights file model.safetensors from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-1b/snapshots/182f0165fdf8da9c9935901eec65c94337f01c11/model.safetensors
loading weights file model.safetensors from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-1b/snapshots/182f0165fdf8da9c9935901eec65c94337f01c11/model.safetensors
loading weights file model.safetensors from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-1b/snapshots/182f0165fdf8da9c9935901eec65c94337f01c11/model.safetensors
loading weights file model.safetensors from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-1b/snapshots/182f0165fdf8da9c9935901eec65c94337f01c11/model.safetensors
Instantiating GPTBigCodeForCausalLM model under default dtype torch.float16.
Instantiating GPTBigCodeForCausalLM model under default dtype torch.float16.
Instantiating GPTBigCodeForCausalLM model under default dtype torch.float16.
Instantiating GPTBigCodeForCausalLM model under default dtype torch.float16.
Generate config GenerationConfig {
  "bos_token_id": 0,
  "eos_token_id": 0
}

Generate config GenerationConfig {
  "bos_token_id": 0,
  "eos_token_id": 0
}

Generate config GenerationConfig {
  "bos_token_id": 0,
  "eos_token_id": 0
}

Generate config GenerationConfig {
  "bos_token_id": 0,
  "eos_token_id": 0
}

huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Detected 8-bit loading: activating 8-bit loading for this model
Detected 8-bit loading: activating 8-bit loading for this model
Detected 8-bit loading: activating 8-bit loading for this model
Detected 8-bit loading: activating 8-bit loading for this model
All model checkpoint weights were used when initializing GPTBigCodeForCausalLM.

All the weights of GPTBigCodeForCausalLM were initialized from the model checkpoint at bigcode/starcoderbase-1b.
If your task is similar to the task the model of the checkpoint was trained on, you can already use GPTBigCodeForCausalLM for predictions without further training.
loading configuration file generation_config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-1b/snapshots/182f0165fdf8da9c9935901eec65c94337f01c11/generation_config.json
Generate config GenerationConfig {
  "bos_token_id": 0,
  "eos_token_id": 0
}

/u/bzd2/.local/lib/python3.9/site-packages/peft/utils/other.py:141: FutureWarning: prepare_model_for_int8_training is deprecated and will be removed in a future version. Use prepare_model_for_kbit_training instead.
  warnings.warn(
All model checkpoint weights were used when initializing GPTBigCodeForCausalLM.

All the weights of GPTBigCodeForCausalLM were initialized from the model checkpoint at bigcode/starcoderbase-1b.
If your task is similar to the task the model of the checkpoint was trained on, you can already use GPTBigCodeForCausalLM for predictions without further training.
All model checkpoint weights were used when initializing GPTBigCodeForCausalLM.

All the weights of GPTBigCodeForCausalLM were initialized from the model checkpoint at bigcode/starcoderbase-1b.
If your task is similar to the task the model of the checkpoint was trained on, you can already use GPTBigCodeForCausalLM for predictions without further training.
All model checkpoint weights were used when initializing GPTBigCodeForCausalLM.

All the weights of GPTBigCodeForCausalLM were initialized from the model checkpoint at bigcode/starcoderbase-1b.
If your task is similar to the task the model of the checkpoint was trained on, you can already use GPTBigCodeForCausalLM for predictions without further training.
loading configuration file generation_config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-1b/snapshots/182f0165fdf8da9c9935901eec65c94337f01c11/generation_config.json
Generate config GenerationConfig {
  "bos_token_id": 0,
  "eos_token_id": 0
}

loading configuration file generation_config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-1b/snapshots/182f0165fdf8da9c9935901eec65c94337f01c11/generation_config.json
Generate config GenerationConfig {
  "bos_token_id": 0,
  "eos_token_id": 0
}

trainable params: 7176192 || all params: 1144383488 || trainable%: 0.6270793029827428
Starting main loop
PyTorch: setting up devices
Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
/u/bzd2/.local/lib/python3.9/site-packages/peft/utils/other.py:141: FutureWarning: prepare_model_for_int8_training is deprecated and will be removed in a future version. Use prepare_model_for_kbit_training instead.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/peft/utils/other.py:141: FutureWarning: prepare_model_for_int8_training is deprecated and will be removed in a future version. Use prepare_model_for_kbit_training instead.
  warnings.warn(
trainable params: 7176192 || all params: 1144383488 || trainable%: 0.6270793029827428
Starting main loop
PyTorch: setting up devices
trainable params: 7176192 || all params: 1144383488 || trainable%: 0.6270793029827428
Starting main loop
PyTorch: setting up devices
loading configuration file generation_config.json from cache at /projects/bbvz/bzd2/hub/models--bigcode--starcoderbase-1b/snapshots/182f0165fdf8da9c9935901eec65c94337f01c11/generation_config.json
Generate config GenerationConfig {
  "bos_token_id": 0,
  "eos_token_id": 0
}

/u/bzd2/.local/lib/python3.9/site-packages/peft/utils/other.py:141: FutureWarning: prepare_model_for_int8_training is deprecated and will be removed in a future version. Use prepare_model_for_kbit_training instead.
  warnings.warn(
trainable params: 7176192 || all params: 1144383488 || trainable%: 0.6270793029827428
Starting main loop
PyTorch: setting up devices
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Training...
Training...
max_steps is given, it will override any value given in num_train_epochs
Using auto half precision backend
Training...
Training...
Currently training with a batch size of: 1
***** Running training *****
  Num examples = 416,000
  Num Epochs = 9,223,372,036,854,775,807
  Instantaneous batch size per device = 1
  Total train batch size (w. parallel, distributed & accumulation) = 64
  Gradient Accumulation steps = 16
  Total optimization steps = 6,500
  Number of trainable parameters = 7,176,192
Automatic Weights & Biases logging enabled, to disable set os.environ["WANDB_DISABLED"] = "true"
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
wandb: Currently logged in as: hetarthvader (complex_dnn). Use `wandb login --relogin` to force relogin
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
wandb: Tracking run with wandb version 0.16.1
wandb: Run data is saved locally in /projects/bbvz/bzd2/wandb/run-20231213_171835-yjccl13z
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run FinalRuns-/projects/bbvz/bzd2/checkpoints_experiment_6
wandb:  View project at https://wandb.ai/complex_dnn/huggingface
wandb:  View run at https://wandb.ai/complex_dnn/huggingface/runs/yjccl13z
  0%|          | 0/6500 [00:00<?, ?it/s]/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 2.0924, 'learning_rate': 5e-05, 'epoch': 0.0}
{'loss': 2.1296, 'learning_rate': 0.0001, 'epoch': 0.0}
{'loss': 2.0875, 'learning_rate': 9.999999415640199e-05, 'epoch': 0.0}
{'loss': 2.037, 'learning_rate': 9.999997662560938e-05, 'epoch': 0.0}
{'loss': 1.9552, 'learning_rate': 9.999994740762622e-05, 'epoch': 0.0}
{'loss': 2.1583, 'learning_rate': 9.999990650245936e-05, 'epoch': 0.0}
  0%|          | 1/6500 [00:14<26:51:58, 14.88s/it]                                                     0%|          | 1/6500 [00:14<26:51:58, 14.88s/it]  0%|          | 2/6500 [00:21<18:12:46, 10.09s/it]                                                     0%|          | 2/6500 [00:21<18:12:46, 10.09s/it]  0%|          | 3/6500 [00:28<15:20:58,  8.51s/it]                                                     0%|          | 3/6500 [00:28<15:20:58,  8.51s/it]  0%|          | 4/6500 [00:34<14:00:19,  7.76s/it]                                                     0%|          | 4/6500 [00:34<14:00:19,  7.76s/it]  0%|          | 5/6500 [00:41<13:15:19,  7.35s/it]                                                     0%|          | 5/6500 [00:41<13:15:19,  7.35s/it]  0%|          | 6/6500 [00:48<12:48:23,  7.10s/it]                                                     0%|          | 6/6500 [00:48<12:48:23,  7.10s/it]  0%|          | 7/6500 [00:54<12:30:55,  6.94s/it]                             {'loss': 1.9057, 'learning_rate': 9.999985391011837e-05, 'epoch': 0.0}
{'loss': 1.8069, 'learning_rate': 9.999978963061551e-05, 'epoch': 0.0}
{'loss': 1.7982, 'learning_rate': 9.999971366396584e-05, 'epoch': 0.0}
{'loss': 1.7299, 'learning_rate': 9.99996260101871e-05, 'epoch': 0.0}
                        0%|          | 7/6500 [00:54<12:30:55,  6.94s/it]  0%|          | 8/6500 [01:01<12:19:25,  6.83s/it]                                                     0%|          | 8/6500 [01:01<12:19:25,  6.83s/it]  0%|          | 9/6500 [01:07<12:11:17,  6.76s/it]                                                     0%|          | 9/6500 [01:07<12:11:17,  6.76s/it]  0%|          | 10/6500 [01:14<12:06:16,  6.71s/it]                                                      0%|          | 10/6500 [01:14<12:06:16,  6.71s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 1.6685303449630737, 'eval_runtime': 1.5854, 'eval_samples_per_second': 7.569, 'eval_steps_per_second': 1.892, 'epoch': 0.0}
                                                      0%|          | 10/6500 [01:16<12:06:16,  6.71s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-10
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in   /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-10
/projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-10
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-10
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-10
the pytorch model path is the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-10/pytorch_model.bin
/projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-10/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-10/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-10/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 1.7077, 'learning_rate': 9.99995266692998e-05, 'epoch': 0.0}
{'loss': 1.7033, 'learning_rate': 9.999941564132713e-05, 'epoch': 0.0}
{'loss': 1.7113, 'learning_rate': 9.999929292629505e-05, 'epoch': 0.0}
{'loss': 1.6424, 'learning_rate': 9.999915852423225e-05, 'epoch': 0.0}
{'loss': 1.6343, 'learning_rate': 9.999901243517017e-05, 'epoch': 0.0}
{'loss': 1.5449, 'learning_rate': 9.99988546591429e-05, 'epoch': 0.0}
  0%|          | 11/6500 [01:23<13:16:23,  7.36s/it]                                                      0%|          | 11/6500 [01:23<13:16:23,  7.36s/it]  0%|          | 12/6500 [01:29<12:51:05,  7.13s/it]                                                      0%|          | 12/6500 [01:29<12:51:05,  7.13s/it]  0%|          | 13/6500 [01:36<12:33:38,  6.97s/it]                                                      0%|          | 13/6500 [01:36<12:33:38,  6.97s/it]  0%|          | 14/6500 [01:43<12:21:24,  6.86s/it]                                                      0%|          | 14/6500 [01:43<12:21:24,  6.86s/it]  0%|          | 15/6500 [01:49<12:13:02,  6.78s/it]                                                      0%|          | 15/6500 [01:49<12:13:02,  6.78s/it]  0%|          | 16/6500 [01:56<12:06:44,  6.72s/it]                                                      0%|          | 16/6500 [01:56<12:06:44,  6.72s/it]  0%|          | 17/6500 [02:03<12:33:11,  6.97s/it]          {'loss': 1.5469, 'learning_rate': 9.999868519618736e-05, 'epoch': 0.0}
{'loss': 1.5184, 'learning_rate': 9.999850404634316e-05, 'epoch': 0.0}
{'loss': 1.4783, 'learning_rate': 9.999831120965261e-05, 'epoch': 0.0}
{'loss': 1.4786, 'learning_rate': 9.999810668616086e-05, 'epoch': 0.0}
                                            0%|          | 17/6500 [02:03<12:33:11,  6.97s/it]  0%|          | 18/6500 [02:10<12:20:45,  6.86s/it]                                                      0%|          | 18/6500 [02:10<12:20:45,  6.86s/it]  0%|          | 19/6500 [02:17<12:12:27,  6.78s/it]                                                      0%|          | 19/6500 [02:17<12:12:27,  6.78s/it]  0%|          | 20/6500 [02:23<12:06:23,  6.73s/it]                                                      0%|          | 20/6500 [02:23<12:06:23,  6.73s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 1.4264671802520752, 'eval_runtime': 1.4984, 'eval_samples_per_second': 8.008, 'eval_steps_per_second': 2.002, 'epoch': 0.0}
                                                      0%|          | 20/6500 [02:25<12:06:23,  6.73s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-20
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-20I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-20

I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-20
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-20
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-20/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-20/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-20/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-20/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 1.7276, 'learning_rate': 9.999789047591562e-05, 'epoch': 0.0}
{'loss': 1.4856, 'learning_rate': 9.999766257896749e-05, 'epoch': 0.0}
{'loss': 1.4238, 'learning_rate': 9.999742299536971e-05, 'epoch': 0.0}
{'loss': 1.4458, 'learning_rate': 9.999717172517832e-05, 'epoch': 0.0}
{'loss': 1.4282, 'learning_rate': 9.999690876845202e-05, 'epoch': 0.0}
{'loss': 1.3926, 'learning_rate': 9.999663412525226e-05, 'epoch': 0.0}
  0%|          | 21/6500 [02:32<13:01:23,  7.24s/it]                                                      0%|          | 21/6500 [02:32<13:01:23,  7.24s/it]  0%|          | 22/6500 [02:38<12:40:27,  7.04s/it]                                                      0%|          | 22/6500 [02:38<12:40:27,  7.04s/it]  0%|          | 23/6500 [02:45<12:26:09,  6.91s/it]                                                      0%|          | 23/6500 [02:45<12:26:09,  6.91s/it]  0%|          | 24/6500 [02:51<12:15:39,  6.82s/it]                                                      0%|          | 24/6500 [02:51<12:15:39,  6.82s/it]  0%|          | 25/6500 [02:58<12:08:20,  6.75s/it]                                                      0%|          | 25/6500 [02:58<12:08:20,  6.75s/it]  0%|          | 26/6500 [03:05<12:03:25,  6.70s/it]                                                      0%|          | 26/6500 [03:05<12:03:25,  6.70s/it]  0%|          | 27/6500 [03:11<11:59:50,  6.67s/it]          {'loss': 1.4565, 'learning_rate': 9.999634779564329e-05, 'epoch': 0.0}
{'loss': 1.4339, 'learning_rate': 9.999604977969197e-05, 'epoch': 0.0}
{'loss': 1.4109, 'learning_rate': 9.999574007746801e-05, 'epoch': 0.0}
{'loss': 1.418, 'learning_rate': 9.99954186890438e-05, 'epoch': 0.0}
                                            0%|          | 27/6500 [03:11<11:59:50,  6.67s/it]  0%|          | 28/6500 [03:18<11:57:20,  6.65s/it]                                                      0%|          | 28/6500 [03:18<11:57:20,  6.65s/it]  0%|          | 29/6500 [03:24<11:55:37,  6.64s/it]                                                      0%|          | 29/6500 [03:24<11:55:37,  6.64s/it]  0%|          | 30/6500 [03:31<11:54:19,  6.62s/it]                                                      0%|          | 30/6500 [03:31<11:54:19,  6.62s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 1.3570195436477661, 'eval_runtime': 1.5032, 'eval_samples_per_second': 7.983, 'eval_steps_per_second': 1.996, 'epoch': 0.0}
                                                      0%|          | 30/6500 [03:32<11:54:19,  6.62s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-30
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  I AM HERE AND I AM SAVING THE MODEL
/projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-30the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-30

I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-30
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-30
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-30/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-30/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-30/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-30/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 1.3621, 'learning_rate': 9.999508561449444e-05, 'epoch': 0.0}
{'loss': 1.3703, 'learning_rate': 9.999474085389778e-05, 'epoch': 0.0}
{'loss': 1.3692, 'learning_rate': 9.999438440733445e-05, 'epoch': 0.01}
{'loss': 1.394, 'learning_rate': 9.999401627488769e-05, 'epoch': 0.01}
{'loss': 1.3783, 'learning_rate': 9.999363645664363e-05, 'epoch': 0.01}
{'loss': 1.6134, 'learning_rate': 9.9993244952691e-05, 'epoch': 0.01}
  0%|          | 31/6500 [03:39<12:51:40,  7.16s/it]                                                      0%|          | 31/6500 [03:39<12:51:40,  7.16s/it]  0%|          | 32/6500 [03:46<12:33:36,  6.99s/it]                                                      0%|          | 32/6500 [03:46<12:33:36,  6.99s/it]  1%|          | 33/6500 [03:53<12:49:58,  7.14s/it]                                                      1%|          | 33/6500 [03:53<12:49:58,  7.14s/it]  1%|          | 34/6500 [04:00<12:32:40,  6.98s/it]                                                      1%|          | 34/6500 [04:00<12:32:40,  6.98s/it]  1%|          | 35/6500 [04:07<12:19:59,  6.87s/it]                                                      1%|          | 35/6500 [04:07<12:19:59,  6.87s/it]  1%|          | 36/6500 [04:13<12:11:10,  6.79s/it]                                                      1%|          | 36/6500 [04:13<12:11:10,  6.79s/it]  1%|          | 37/6500 [04:20<12:05:02,  6.73s/it]          {'loss': 1.3788, 'learning_rate': 9.999284176312134e-05, 'epoch': 0.01}
{'loss': 1.348, 'learning_rate': 9.999242688802886e-05, 'epoch': 0.01}
{'loss': 1.338, 'learning_rate': 9.999200032751056e-05, 'epoch': 0.01}
{'loss': 1.3463, 'learning_rate': 9.999156208166614e-05, 'epoch': 0.01}
                                            1%|          | 37/6500 [04:20<12:05:02,  6.73s/it]  1%|          | 38/6500 [04:27<12:00:43,  6.69s/it]                                                      1%|          | 38/6500 [04:27<12:00:43,  6.69s/it]  1%|          | 39/6500 [04:33<11:57:40,  6.66s/it]                                                      1%|          | 39/6500 [04:33<11:57:40,  6.66s/it]  1%|          | 40/6500 [04:40<11:56:01,  6.65s/it]                                                      1%|          | 40/6500 [04:40<11:56:01,  6.65s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 1.3118410110473633, 'eval_runtime': 1.5067, 'eval_samples_per_second': 7.965, 'eval_steps_per_second': 1.991, 'epoch': 0.01}
                                                      1%|          | 40/6500 [04:41<11:56:01,  6.65s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-40
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-40
/projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-40
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-40
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-40
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-40/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-40/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-40/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-40/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 1.331, 'learning_rate': 9.999111215059804e-05, 'epoch': 0.01}
{'loss': 1.3667, 'learning_rate': 9.999065053441144e-05, 'epoch': 0.01}
{'loss': 1.3533, 'learning_rate': 9.99901772332142e-05, 'epoch': 0.01}
{'loss': 1.3525, 'learning_rate': 9.998969224711698e-05, 'epoch': 0.01}
{'loss': 1.326, 'learning_rate': 9.998919557623315e-05, 'epoch': 0.01}
{'loss': 1.3176, 'learning_rate': 9.99886872206788e-05, 'epoch': 0.01}
  1%|          | 41/6500 [04:48<12:55:36,  7.20s/it]                                                      1%|          | 41/6500 [04:48<12:55:36,  7.20s/it]  1%|          | 42/6500 [04:55<12:35:59,  7.02s/it]                                                      1%|          | 42/6500 [04:55<12:35:59,  7.02s/it]  1%|          | 43/6500 [05:01<12:21:52,  6.89s/it]                                                      1%|          | 43/6500 [05:01<12:21:52,  6.89s/it]  1%|          | 44/6500 [05:08<12:11:50,  6.80s/it]                                                      1%|          | 44/6500 [05:08<12:11:50,  6.80s/it]  1%|          | 45/6500 [05:15<12:04:52,  6.74s/it]                                                      1%|          | 45/6500 [05:15<12:04:52,  6.74s/it]  1%|          | 46/6500 [05:21<12:00:04,  6.69s/it]                                                      1%|          | 46/6500 [05:21<12:00:04,  6.69s/it]  1%|          | 47/6500 [05:28<11:56:06,  6.66s/it]          {'loss': 1.3033, 'learning_rate': 9.998816718057274e-05, 'epoch': 0.01}
{'loss': 1.2911, 'learning_rate': 9.998763545603654e-05, 'epoch': 0.01}
{'loss': 1.3344, 'learning_rate': 9.998709204719447e-05, 'epoch': 0.01}
{'loss': 1.3124, 'learning_rate': 9.998653695417356e-05, 'epoch': 0.01}
                                            1%|          | 47/6500 [05:28<11:56:06,  6.66s/it]  1%|          | 48/6500 [05:34<11:53:42,  6.64s/it]                                                      1%|          | 48/6500 [05:34<11:53:42,  6.64s/it]  1%|          | 49/6500 [05:42<12:20:52,  6.89s/it]                                                      1%|          | 49/6500 [05:42<12:20:52,  6.89s/it]  1%|          | 50/6500 [05:48<12:11:11,  6.80s/it]                                                      1%|          | 50/6500 [05:48<12:11:11,  6.80s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 1.2748281955718994, 'eval_runtime': 1.4924, 'eval_samples_per_second': 8.041, 'eval_steps_per_second': 2.01, 'epoch': 0.01}
                                                      1%|          | 50/6500 [05:50<12:11:11,  6.80s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-50
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  I AM HERE AND I AM SAVING THE MODEL
/projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-50the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-50

I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-50
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-50
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-50/pytorch_model.bin
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-50/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-50/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-50/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 1.5563, 'learning_rate': 9.998597017710356e-05, 'epoch': 0.01}
{'loss': 1.296, 'learning_rate': 9.998539171611697e-05, 'epoch': 0.01}
{'loss': 1.2909, 'learning_rate': 9.998480157134897e-05, 'epoch': 0.01}
{'loss': 1.2757, 'learning_rate': 9.998419974293752e-05, 'epoch': 0.01}
{'loss': 1.2806, 'learning_rate': 9.998358623102327e-05, 'epoch': 0.01}
{'loss': 1.2669, 'learning_rate': 9.998296103574967e-05, 'epoch': 0.01}
  1%|          | 51/6500 [05:57<13:02:52,  7.28s/it]                                                      1%|          | 51/6500 [05:57<13:02:52,  7.28s/it]  1%|          | 52/6500 [06:03<12:40:25,  7.08s/it]                                                      1%|          | 52/6500 [06:03<12:40:25,  7.08s/it]  1%|          | 53/6500 [06:10<12:24:47,  6.93s/it]                                                      1%|          | 53/6500 [06:10<12:24:47,  6.93s/it]  1%|          | 54/6500 [06:17<12:13:27,  6.83s/it]                                                      1%|          | 54/6500 [06:17<12:13:27,  6.83s/it]  1%|          | 55/6500 [06:23<12:05:33,  6.75s/it]                                                      1%|          | 55/6500 [06:23<12:05:33,  6.75s/it]  1%|          | 56/6500 [06:30<12:00:02,  6.70s/it]                                                      1%|          | 56/6500 [06:30<12:00:02,  6.70s/it]  1%|          | 57/6500 [06:36<11:55:52,  6.67s/it]          {'loss': 1.3299, 'learning_rate': 9.99823241572628e-05, 'epoch': 0.01}
{'loss': 1.2817, 'learning_rate': 9.998167559571158e-05, 'epoch': 0.01}
{'loss': 1.2906, 'learning_rate': 9.998101535124758e-05, 'epoch': 0.01}
{'loss': 1.2662, 'learning_rate': 9.998034342402513e-05, 'epoch': 0.01}
                                            1%|          | 57/6500 [06:36<11:55:52,  6.67s/it]  1%|          | 58/6500 [06:43<11:53:33,  6.65s/it]                                                      1%|          | 58/6500 [06:43<11:53:33,  6.65s/it]  1%|          | 59/6500 [06:50<11:51:36,  6.63s/it]                                                      1%|          | 59/6500 [06:50<11:51:36,  6.63s/it]  1%|          | 60/6500 [06:56<11:50:31,  6.62s/it]                                                      1%|          | 60/6500 [06:56<11:50:31,  6.62s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 1.2423219680786133, 'eval_runtime': 1.4956, 'eval_samples_per_second': 8.023, 'eval_steps_per_second': 2.006, 'epoch': 0.01}
                                                      1%|          | 60/6500 [06:58<11:50:31,  6.62s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-60
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-60
 /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-60
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-60
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-60
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-60/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-60/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-60/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-60/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 1.2724, 'learning_rate': 9.997965981420128e-05, 'epoch': 0.01}
{'loss': 1.2649, 'learning_rate': 9.997896452193584e-05, 'epoch': 0.01}
{'loss': 1.2478, 'learning_rate': 9.997825754739132e-05, 'epoch': 0.01}
{'loss': 1.29, 'learning_rate': 9.997753889073296e-05, 'epoch': 0.01}
{'loss': 1.2614, 'learning_rate': 9.997680855212877e-05, 'epoch': 0.01}
{'loss': 1.5075, 'learning_rate': 9.997606653174942e-05, 'epoch': 0.01}
  1%|          | 61/6500 [07:05<12:50:31,  7.18s/it]                                                      1%|          | 61/6500 [07:05<12:50:31,  7.18s/it]  1%|          | 62/6500 [07:11<12:31:16,  7.00s/it]                                                      1%|          | 62/6500 [07:11<12:31:16,  7.00s/it]  1%|          | 63/6500 [07:18<12:18:18,  6.88s/it]                                                      1%|          | 63/6500 [07:18<12:18:18,  6.88s/it]  1%|          | 64/6500 [07:24<12:08:53,  6.80s/it]                                                      1%|          | 64/6500 [07:24<12:08:53,  6.80s/it]  1%|          | 65/6500 [07:32<12:30:58,  7.00s/it]                                                      1%|          | 65/6500 [07:32<12:30:58,  7.00s/it]  1%|          | 66/6500 [07:38<12:17:49,  6.88s/it]                                                      1%|          | 66/6500 [07:38<12:17:49,  6.88s/it]  1%|          | 67/6500 [07:45<12:08:50,  6.80s/it]          {'loss': 1.2338, 'learning_rate': 9.99753128297684e-05, 'epoch': 0.01}
{'loss': 1.2696, 'learning_rate': 9.997454744636186e-05, 'epoch': 0.01}
{'loss': 1.2224, 'learning_rate': 9.99737703817087e-05, 'epoch': 0.01}
{'loss': 1.2299, 'learning_rate': 9.997298163599056e-05, 'epoch': 0.01}
                                            1%|          | 67/6500 [07:45<12:08:50,  6.80s/it]  1%|          | 68/6500 [07:52<12:02:29,  6.74s/it]                                                      1%|          | 68/6500 [07:52<12:02:29,  6.74s/it]  1%|          | 69/6500 [07:58<11:57:53,  6.70s/it]                                                      1%|          | 69/6500 [07:58<11:57:53,  6.70s/it]  1%|          | 70/6500 [08:05<11:54:54,  6.67s/it]                                                      1%|          | 70/6500 [08:05<11:54:54,  6.67s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 1.217464804649353, 'eval_runtime': 1.4944, 'eval_samples_per_second': 8.03, 'eval_steps_per_second': 2.008, 'epoch': 0.01}
                                                      1%|          | 70/6500 [08:06<11:54:54,  6.67s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-70
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-70
/projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-70
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-70
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-70
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-70/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-70/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-70/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-70/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 1.2389, 'learning_rate': 9.99721812093918e-05, 'epoch': 0.01}
{'loss': 1.286, 'learning_rate': 9.99713691020995e-05, 'epoch': 0.01}
{'loss': 1.2542, 'learning_rate': 9.997054531430353e-05, 'epoch': 0.01}
{'loss': 1.2535, 'learning_rate': 9.996970984619641e-05, 'epoch': 0.01}
{'loss': 1.2044, 'learning_rate': 9.996886269797344e-05, 'epoch': 0.01}
{'loss': 1.25, 'learning_rate': 9.996800386983263e-05, 'epoch': 0.01}
  1%|          | 71/6500 [08:13<12:50:12,  7.19s/it]                                                      1%|          | 71/6500 [08:13<12:50:12,  7.19s/it]  1%|          | 72/6500 [08:20<12:31:12,  7.01s/it]                                                      1%|          | 72/6500 [08:20<12:31:12,  7.01s/it]  1%|          | 73/6500 [08:26<12:17:52,  6.89s/it]                                                      1%|          | 73/6500 [08:26<12:17:52,  6.89s/it]  1%|          | 74/6500 [08:33<12:08:47,  6.80s/it]                                                      1%|          | 74/6500 [08:33<12:08:47,  6.80s/it]  1%|          | 75/6500 [08:40<12:02:08,  6.74s/it]                                                      1%|          | 75/6500 [08:40<12:02:08,  6.74s/it]  1%|          | 76/6500 [08:46<11:57:04,  6.70s/it]                                                      1%|          | 76/6500 [08:46<11:57:04,  6.70s/it]  1%|          | 77/6500 [08:53<11:53:49,  6.67s/it]          {'loss': 1.2241, 'learning_rate': 9.996713336197472e-05, 'epoch': 0.01}
{'loss': 1.2179, 'learning_rate': 9.996625117460318e-05, 'epoch': 0.01}
{'loss': 1.2155, 'learning_rate': 9.996535730792425e-05, 'epoch': 0.01}
{'loss': 1.2551, 'learning_rate': 9.996445176214684e-05, 'epoch': 0.01}
                                            1%|          | 77/6500 [08:53<11:53:49,  6.67s/it]  1%|          | 78/6500 [08:59<11:51:17,  6.65s/it]                                                      1%|          | 78/6500 [08:59<11:51:17,  6.65s/it]  1%|          | 79/6500 [09:06<11:49:48,  6.63s/it]                                                      1%|          | 79/6500 [09:06<11:49:48,  6.63s/it]  1%|          | 80/6500 [09:13<11:48:43,  6.62s/it]                                                      1%|          | 80/6500 [09:13<11:48:43,  6.62s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 1.1950886249542236, 'eval_runtime': 1.4995, 'eval_samples_per_second': 8.003, 'eval_steps_per_second': 2.001, 'epoch': 0.01}
                                                      1%|          | 80/6500 [09:14<11:48:43,  6.62s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-80
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-80
 /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-80
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-80
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-80
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-80/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-80/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-80/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-80/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 1.468, 'learning_rate': 9.996353453748261e-05, 'epoch': 0.01}
{'loss': 1.2006, 'learning_rate': 9.996260563414597e-05, 'epoch': 0.01}
{'loss': 1.2336, 'learning_rate': 9.996166505235404e-05, 'epoch': 0.01}
{'loss': 1.1855, 'learning_rate': 9.996071279232667e-05, 'epoch': 0.01}
{'loss': 1.1854, 'learning_rate': 9.995974885428647e-05, 'epoch': 0.01}
{'loss': 1.2184, 'learning_rate': 9.995877323845872e-05, 'epoch': 0.01}
  1%|          | 81/6500 [09:22<13:06:08,  7.35s/it]                                                      1%|          | 81/6500 [09:22<13:06:08,  7.35s/it]  1%|         | 82/6500 [09:28<12:42:06,  7.12s/it]                                                      1%|         | 82/6500 [09:28<12:42:06,  7.12s/it]  1%|         | 83/6500 [09:35<12:25:03,  6.97s/it]                                                      1%|         | 83/6500 [09:35<12:25:03,  6.97s/it]  1%|         | 84/6500 [09:42<12:13:28,  6.86s/it]                                                      1%|         | 84/6500 [09:42<12:13:28,  6.86s/it]  1%|         | 85/6500 [09:48<12:05:46,  6.79s/it]                                                      1%|         | 85/6500 [09:48<12:05:46,  6.79s/it]  1%|         | 86/6500 [09:55<11:59:39,  6.73s/it]                                                      1%|         | 86/6500 [09:55<11:59:39,  6.73s/it]  1%|         | 87/6500 [10:01<11:55:14,{'loss': 1.2216, 'learning_rate': 9.995778594507148e-05, 'epoch': 0.01}
{'loss': 1.2386, 'learning_rate': 9.995678697435552e-05, 'epoch': 0.01}
{'loss': 1.2223, 'learning_rate': 9.995577632654436e-05, 'epoch': 0.01}
{'loss': 1.1683, 'learning_rate': 9.99547540018742e-05, 'epoch': 0.01}
  6.69s/it]                                                      1%|         | 87/6500 [10:01<11:55:14,  6.69s/it]  1%|         | 88/6500 [10:08<11:53:33,  6.68s/it]                                                      1%|         | 88/6500 [10:08<11:53:33,  6.68s/it]  1%|         | 89/6500 [10:15<11:50:50,  6.65s/it]                                                      1%|         | 89/6500 [10:15<11:50:50,  6.65s/it]  1%|         | 90/6500 [10:21<11:49:05,  6.64s/it]                                                      1%|         | 90/6500 [10:21<11:49:05,  6.64s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 1.1764265298843384, 'eval_runtime': 1.7724, 'eval_samples_per_second': 6.771, 'eval_steps_per_second': 1.693, 'epoch': 0.01}
                                                      1%|         | 90/6500 [10:23<11:49:05,  6.64s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-90
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-90
/projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-90
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-90
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-90
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-90/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-90/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-90/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-90/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 1.2085, 'learning_rate': 9.995372000058404e-05, 'epoch': 0.01}
{'loss': 1.1891, 'learning_rate': 9.995267432291555e-05, 'epoch': 0.01}
{'loss': 1.198, 'learning_rate': 9.995161696911315e-05, 'epoch': 0.01}
{'loss': 1.1916, 'learning_rate': 9.9950547939424e-05, 'epoch': 0.01}
{'loss': 1.4516, 'learning_rate': 9.994946723409797e-05, 'epoch': 0.01}
{'loss': 1.189, 'learning_rate': 9.994837485338766e-05, 'epoch': 0.01}
  1%|         | 91/6500 [10:30<12:52:19,  7.23s/it]                                                      1%|         | 91/6500 [10:30<12:52:19,  7.23s/it]  1%|         | 92/6500 [10:36<12:31:42,  7.04s/it]                                                      1%|         | 92/6500 [10:36<12:31:42,  7.04s/it]  1%|         | 93/6500 [10:43<12:17:37,  6.91s/it]                                                      1%|         | 93/6500 [10:43<12:17:37,  6.91s/it]  1%|         | 94/6500 [10:50<12:07:27,  6.81s/it]                                                      1%|         | 94/6500 [10:50<12:07:27,  6.81s/it]  1%|         | 95/6500 [10:56<12:00:02,  6.75s/it]                                                      1%|         | 95/6500 [10:56<12:00:02,  6.75s/it]  1%|         | 96/6500 [11:03<11:55:01,  6.70s/it]                                                      1%|         | 96/6500 [11:03<11:55:01,  6.70s/it]  1%|         | 97/6500 [11:10<12:11{'loss': 1.1743, 'learning_rate': 9.994727079754844e-05, 'epoch': 0.01}
{'loss': 1.1896, 'learning_rate': 9.994615506683834e-05, 'epoch': 0.02}
{'loss': 1.1695, 'learning_rate': 9.994502766151818e-05, 'epoch': 0.02}
{'loss': 1.1522, 'learning_rate': 9.994388858185147e-05, 'epoch': 0.02}
:46,  6.86s/it]                                                      1%|         | 97/6500 [11:10<12:11:46,  6.86s/it]  2%|         | 98/6500 [11:17<12:04:16,  6.79s/it]                                                      2%|         | 98/6500 [11:17<12:04:16,  6.79s/it]  2%|         | 99/6500 [11:23<11:57:47,  6.73s/it]                                                      2%|         | 99/6500 [11:23<11:57:47,  6.73s/it]  2%|         | 100/6500 [11:30<11:53:32,  6.69s/it]                                                       2%|         | 100/6500 [11:30<11:53:32,  6.69s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 1.1610369682312012, 'eval_runtime': 1.5077, 'eval_samples_per_second': 7.959, 'eval_steps_per_second': 1.99, 'epoch': 0.02}
                                                       2%|         | 100/6500 [11:31<11:53:32,  6.69s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-100
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-100 /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-100

I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-100
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-100
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-100/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-100/pytorch_model.binthe pytorch model path is
 /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-100/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-100/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 1.2062, 'learning_rate': 9.994273782810447e-05, 'epoch': 0.02}
{'loss': 1.1942, 'learning_rate': 9.994157540054616e-05, 'epoch': 0.02}
{'loss': 1.1936, 'learning_rate': 9.994040129944824e-05, 'epoch': 0.02}
{'loss': 1.1908, 'learning_rate': 9.993921552508518e-05, 'epoch': 0.02}
{'loss': 1.1456, 'learning_rate': 9.993801807773411e-05, 'epoch': 0.02}
{'loss': 1.1761, 'learning_rate': 9.993680895767495e-05, 'epoch': 0.02}
  2%|         | 101/6500 [11:38<12:47:03,  7.19s/it]                                                       2%|         | 101/6500 [11:38<12:47:03,  7.19s/it]  2%|         | 102/6500 [11:45<12:27:27,  7.01s/it]                                                       2%|         | 102/6500 [11:45<12:27:27,  7.01s/it]  2%|         | 103/6500 [11:51<12:13:36,  6.88s/it]                                                       2%|         | 103/6500 [11:51<12:13:36,  6.88s/it]  2%|         | 104/6500 [11:58<12:04:11,  6.79s/it]                                                       2%|         | 104/6500 [11:58<12:04:11,  6.79s/it]  2%|         | 105/6500 [12:05<11:58:00,  6.74s/it]                                                       2%|         | 105/6500 [12:05<11:58:00,  6.74s/it]  2%|         | 106/6500 [12:11<11:53:09,  6.69s/it]                                                       2%|         | 106/6500 [12:11<11:53:09,  6.69s/it]  2%|         | 10{'loss': 1.1599, 'learning_rate': 9.993558816519031e-05, 'epoch': 0.02}
{'loss': 1.1891, 'learning_rate': 9.993435570056556e-05, 'epoch': 0.02}
{'loss': 1.1771, 'learning_rate': 9.993311156408876e-05, 'epoch': 0.02}
{'loss': 1.408, 'learning_rate': 9.993185575605073e-05, 'epoch': 0.02}
7/6500 [12:18<11:49:48,  6.66s/it]                                                       2%|         | 107/6500 [12:18<11:49:48,  6.66s/it]  2%|         | 108/6500 [12:24<11:47:32,  6.64s/it]                                                       2%|         | 108/6500 [12:24<11:47:32,  6.64s/it]  2%|         | 109/6500 [12:31<11:45:26,  6.62s/it]                                                       2%|         | 109/6500 [12:31<11:45:26,  6.62s/it]  2%|         | 110/6500 [12:37<11:43:57,  6.61s/it]                                                       2%|         | 110/6500 [12:37<11:43:57,  6.61s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 1.1466809511184692, 'eval_runtime': 1.4949, 'eval_samples_per_second': 8.027, 'eval_steps_per_second': 2.007, 'epoch': 0.02}
                                                       2%|         | 110/6500 [12:39<11:43:57,  6.61s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-110
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-110I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-110

I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-110
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-110
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-110/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-110/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-110/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-110/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 1.1757, 'learning_rate': 9.993058827674501e-05, 'epoch': 0.02}
{'loss': 1.1293, 'learning_rate': 9.992930912646787e-05, 'epoch': 0.02}
{'loss': 1.1664, 'learning_rate': 9.99280183055183e-05, 'epoch': 0.02}
{'loss': 1.1477, 'learning_rate': 9.9926715814198e-05, 'epoch': 0.02}
{'loss': 1.1434, 'learning_rate': 9.992540165281145e-05, 'epoch': 0.02}
{'loss': 1.1855, 'learning_rate': 9.992407582166581e-05, 'epoch': 0.02}
  2%|         | 111/6500 [12:46<12:38:47,  7.13s/it]                                                       2%|         | 111/6500 [12:46<12:38:47,  7.13s/it]  2%|         | 112/6500 [12:52<12:21:27,  6.96s/it]                                                       2%|         | 112/6500 [12:52<12:21:27,  6.96s/it]  2%|         | 113/6500 [12:59<12:08:57,  6.85s/it]                                                       2%|         | 113/6500 [12:59<12:08:57,  6.85s/it]  2%|         | 114/6500 [13:06<12:29:25,  7.04s/it]                                                       2%|         | 114/6500 [13:06<12:29:25,  7.04s/it]  2%|         | 115/6500 [13:13<12:14:53,  6.91s/it]                                                       2%|         | 115/6500 [13:13<12:14:53,  6.91s/it]  2%|         | 116/6500 [13:20<12:04:26,  6.81s/it]                                                       2%|         | 116/6500 [13:20<12:04:26,  6.81s/it]  2%|         | 11{'loss': 1.1731, 'learning_rate': 9.9922738321071e-05, 'epoch': 0.02}
{'loss': 1.1768, 'learning_rate': 9.992138915133965e-05, 'epoch': 0.02}
{'loss': 1.1465, 'learning_rate': 9.992002831278708e-05, 'epoch': 0.02}
{'loss': 1.1506, 'learning_rate': 9.991865580573143e-05, 'epoch': 0.02}
7/6500 [13:26<11:57:30,  6.74s/it]                                                       2%|         | 117/6500 [13:26<11:57:30,  6.74s/it]  2%|         | 118/6500 [13:33<11:52:37,  6.70s/it]                                                       2%|         | 118/6500 [13:33<11:52:37,  6.70s/it]  2%|         | 119/6500 [13:39<11:49:19,  6.67s/it]                                                       2%|         | 119/6500 [13:39<11:49:19,  6.67s/it]  2%|         | 120/6500 [13:46<11:46:29,  6.64s/it]                                                       2%|         | 120/6500 [13:46<11:46:29,  6.64s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 1.1351091861724854, 'eval_runtime': 1.499, 'eval_samples_per_second': 8.005, 'eval_steps_per_second': 2.001, 'epoch': 0.02}
                                                       2%|         | 120/6500 [13:48<11:46:29,  6.64s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-120
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-120
/projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-120
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-120
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-120
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-120/pytorch_model.bin
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-120/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-120/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-120/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 1.1371, 'learning_rate': 9.99172716304935e-05, 'epoch': 0.02}
{'loss': 1.1305, 'learning_rate': 9.991587578739684e-05, 'epoch': 0.02}
{'loss': 1.1788, 'learning_rate': 9.991446827676767e-05, 'epoch': 0.02}
{'loss': 1.1505, 'learning_rate': 9.991304909893506e-05, 'epoch': 0.02}
{'loss': 1.3929, 'learning_rate': 9.991161825423067e-05, 'epoch': 0.02}
{'loss': 1.1421, 'learning_rate': 9.9910175742989e-05, 'epoch': 0.02}
  2%|         | 121/6500 [13:54<12:42:59,  7.18s/it]                                                       2%|         | 121/6500 [13:54<12:42:59,  7.18s/it]  2%|         | 122/6500 [14:01<12:24:07,  7.00s/it]                                                       2%|         | 122/6500 [14:01<12:24:07,  7.00s/it]  2%|         | 123/6500 [14:08<12:11:47,  6.89s/it]                                                       2%|         | 123/6500 [14:08<12:11:47,  6.89s/it]  2%|         | 124/6500 [14:14<12:02:14,  6.80s/it]                                                       2%|         | 124/6500 [14:14<12:02:14,  6.80s/it]  2%|         | 125/6500 [14:21<11:55:14,  6.73s/it]                                                       2%|         | 125/6500 [14:21<11:55:14,  6.73s/it]  2%|         | 126/6500 [14:27<11:50:33,  6.69s/it]                                                       2%|         | 126/6500 [14:27<11:50:33,  6.69s/it]  2%|         | 12{'loss': 1.129, 'learning_rate': 9.990872156554721e-05, 'epoch': 0.02}
{'loss': 1.1305, 'learning_rate': 9.990725572224521e-05, 'epoch': 0.02}
{'loss': 1.1152, 'learning_rate': 9.990577821342561e-05, 'epoch': 0.02}
{'loss': 1.117, 'learning_rate': 9.99042890394338e-05, 'epoch': 0.02}
7/6500 [14:34<11:47:08,  6.66s/it]                                                       2%|         | 127/6500 [14:34<11:47:08,  6.66s/it]  2%|         | 128/6500 [14:41<11:44:47,  6.64s/it]                                                       2%|         | 128/6500 [14:41<11:44:47,  6.64s/it]  2%|         | 129/6500 [14:47<11:43:07,  6.62s/it]                                                       2%|         | 129/6500 [14:47<11:43:07,  6.62s/it]  2%|         | 130/6500 [14:55<12:10:25,  6.88s/it]                                                       2%|         | 130/6500 [14:55<12:10:25,  6.88s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 1.1238071918487549, 'eval_runtime': 1.4973, 'eval_samples_per_second': 8.014, 'eval_steps_per_second': 2.004, 'epoch': 0.02}
                                                       2%|         | 130/6500 [14:56<12:10:25,  6.88s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-130
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-130
/projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-130
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-130
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-130
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-130/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-130/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-130/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-130/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 1.161, 'learning_rate': 9.990278820061783e-05, 'epoch': 0.02}
{'loss': 1.1474, 'learning_rate': 9.990127569732855e-05, 'epoch': 0.02}
{'loss': 1.1606, 'learning_rate': 9.989975152991947e-05, 'epoch': 0.02}
{'loss': 1.1164, 'learning_rate': 9.989821569874687e-05, 'epoch': 0.02}
{'loss': 1.1301, 'learning_rate': 9.989666820416974e-05, 'epoch': 0.02}
{'loss': 1.1281, 'learning_rate': 9.989510904654979e-05, 'epoch': 0.02}
  2%|         | 131/6500 [15:03<13:00:06,  7.35s/it]                                                       2%|         | 131/6500 [15:03<13:00:06,  7.35s/it]  2%|         | 132/6500 [15:10<12:35:34,  7.12s/it]                                                       2%|         | 132/6500 [15:10<12:35:34,  7.12s/it]  2%|         | 133/6500 [15:16<12:18:59,  6.96s/it]                                                       2%|         | 133/6500 [15:16<12:18:59,  6.96s/it]  2%|         | 134/6500 [15:23<12:06:50,  6.85s/it]                                                       2%|         | 134/6500 [15:23<12:06:50,  6.85s/it]  2%|         | 135/6500 [15:29<11:57:57,  6.77s/it]                                                       2%|         | 135/6500 [15:29<11:57:57,  6.77s/it]  2%|         | 136/6500 [15:36<11:52:01,  6.71s/it]                                                       2%|         | 136/6500 [15:36<11:52:01,  6.71s/it]  2%|         | 13{'loss': 1.1223, 'learning_rate': 9.989353822625146e-05, 'epoch': 0.02}
{'loss': 1.1469, 'learning_rate': 9.989195574364194e-05, 'epoch': 0.02}
{'loss': 1.1275, 'learning_rate': 9.98903615990911e-05, 'epoch': 0.02}
{'loss': 1.3777, 'learning_rate': 9.988875579297159e-05, 'epoch': 0.02}
7/6500 [15:43<11:47:58,  6.68s/it]                                                       2%|         | 137/6500 [15:43<11:47:58,  6.68s/it]  2%|         | 138/6500 [15:49<11:45:07,  6.65s/it]                                                       2%|         | 138/6500 [15:49<11:45:07,  6.65s/it]  2%|         | 139/6500 [15:56<11:42:35,  6.63s/it]                                                       2%|         | 139/6500 [15:56<11:42:35,  6.63s/it]  2%|         | 140/6500 [16:02<11:40:54,  6.61s/it]                                                       2%|         | 140/6500 [16:02<11:40:54,  6.61s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 1.1155017614364624, 'eval_runtime': 1.5031, 'eval_samples_per_second': 7.983, 'eval_steps_per_second': 1.996, 'epoch': 0.02}
                                                       2%|         | 140/6500 [16:04<11:40:54,  6.61s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-140
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-140
 /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-140
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-140
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-140
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-140/pytorch_model.bin
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-140/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-140/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-140/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 1.0941, 'learning_rate': 9.988713832565874e-05, 'epoch': 0.02}
{'loss': 1.1379, 'learning_rate': 9.988550919753061e-05, 'epoch': 0.02}
{'loss': 1.099, 'learning_rate': 9.988386840896803e-05, 'epoch': 0.02}
{'loss': 1.0988, 'learning_rate': 9.98822159603545e-05, 'epoch': 0.02}
{'loss': 1.0992, 'learning_rate': 9.988055185207628e-05, 'epoch': 0.02}
{'loss': 1.158, 'learning_rate': 9.987887608452235e-05, 'epoch': 0.02}
  2%|         | 141/6500 [16:11<12:35:57,  7.13s/it]                                                       2%|         | 141/6500 [16:11<12:35:57,  7.13s/it]  2%|         | 142/6500 [16:17<12:18:17,  6.97s/it]                                                       2%|         | 142/6500 [16:17<12:18:17,  6.97s/it]  2%|         | 143/6500 [16:24<12:05:59,  6.85s/it]                                                       2%|         | 143/6500 [16:24<12:05:59,  6.85s/it]  2%|         | 144/6500 [16:30<11:57:10,  6.77s/it]                                                       2%|         | 144/6500 [16:30<11:57:10,  6.77s/it]  2%|         | 145/6500 [16:37<11:51:09,  6.71s/it]                                                       2%|         | 145/6500 [16:37<11:51:09,  6.71s/it]  2%|         | 146/6500 [16:44<12:15:33,  6.95s/it]                                                       2%|         | 146/6500 [16:45<12:15:33,  6.95s/it]  2%|         | 14{'loss': 1.1288, 'learning_rate': 9.98771886580844e-05, 'epoch': 0.02}
{'loss': 1.1345, 'learning_rate': 9.987548957315685e-05, 'epoch': 0.02}
{'loss': 1.0878, 'learning_rate': 9.987377883013687e-05, 'epoch': 0.02}
{'loss': 1.1298, 'learning_rate': 9.987205642942432e-05, 'epoch': 0.02}
7/6500 [16:51<12:04:08,  6.84s/it]                                                       2%|         | 147/6500 [16:51<12:04:08,  6.84s/it]  2%|         | 148/6500 [16:58<11:56:12,  6.77s/it]                                                       2%|         | 148/6500 [16:58<11:56:12,  6.77s/it]  2%|         | 149/6500 [17:04<11:50:33,  6.71s/it]                                                       2%|         | 149/6500 [17:04<11:50:33,  6.71s/it]  2%|         | 150/6500 [17:11<11:46:36,  6.68s/it]                                                       2%|         | 150/6500 [17:11<11:46:36,  6.68s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 1.1070917844772339, 'eval_runtime': 1.5032, 'eval_samples_per_second': 7.983, 'eval_steps_per_second': 1.996, 'epoch': 0.02}
                                                       2%|         | 150/6500 [17:12<11:46:36,  6.68s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-150
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-150
 /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-150
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-150
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-150
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-150/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-150/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-150/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-150/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 1.0997, 'learning_rate': 9.98703223714218e-05, 'epoch': 0.02}
{'loss': 1.1097, 'learning_rate': 9.986857665653466e-05, 'epoch': 0.02}
{'loss': 1.1144, 'learning_rate': 9.986681928517092e-05, 'epoch': 0.02}
{'loss': 1.1238, 'learning_rate': 9.986505025774138e-05, 'epoch': 0.02}
{'loss': 1.3567, 'learning_rate': 9.986326957465951e-05, 'epoch': 0.02}
{'loss': 1.0832, 'learning_rate': 9.986147723634156e-05, 'epoch': 0.02}
  2%|         | 151/6500 [17:19<12:39:08,  7.17s/it]                                                       2%|         | 151/6500 [17:19<12:39:08,  7.17s/it]  2%|         | 152/6500 [17:26<12:20:07,  7.00s/it]                                                       2%|         | 152/6500 [17:26<12:20:07,  7.00s/it]  2%|         | 153/6500 [17:32<12:07:03,  6.87s/it]                                                       2%|         | 153/6500 [17:32<12:07:03,  6.87s/it]  2%|         | 154/6500 [17:39<11:57:41,  6.79s/it]                                                       2%|         | 154/6500 [17:39<11:57:41,  6.79s/it]  2%|         | 155/6500 [17:46<11:51:23,  6.73s/it]                                                       2%|         | 155/6500 [17:46<11:51:23,  6.73s/it]  2%|         | 156/6500 [17:52<11:46:25,  6.68s/it]                                                       2%|         | 156/6500 [17:52<11:46:25,  6.68s/it]  2%|         | 15{'loss': 1.1157, 'learning_rate': 9.985967324320646e-05, 'epoch': 0.02}
{'loss': 1.0757, 'learning_rate': 9.985785759567591e-05, 'epoch': 0.02}
{'loss': 1.0782, 'learning_rate': 9.985603029417427e-05, 'epoch': 0.02}
{'loss': 1.1112, 'learning_rate': 9.985419133912869e-05, 'epoch': 0.02}
7/6500 [17:59<11:43:00,  6.65s/it]                                                       2%|         | 157/6500 [17:59<11:43:00,  6.65s/it]  2%|         | 158/6500 [18:05<11:40:20,  6.63s/it]                                                       2%|         | 158/6500 [18:05<11:40:20,  6.63s/it]  2%|         | 159/6500 [18:12<11:39:23,  6.62s/it]                                                       2%|         | 159/6500 [18:12<11:39:23,  6.62s/it]  2%|         | 160/6500 [18:18<11:38:20,  6.61s/it]                                                       2%|         | 160/6500 [18:18<11:38:20,  6.61s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 1.0975732803344727, 'eval_runtime': 1.4911, 'eval_samples_per_second': 8.048, 'eval_steps_per_second': 2.012, 'epoch': 0.02}
                                                       2%|         | 160/6500 [18:20<11:38:20,  6.61s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-160
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-160I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-160

I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-160
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-160
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-160/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-160/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-160/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-160/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 1.0935, 'learning_rate': 9.9852340730969e-05, 'epoch': 0.02}
{'loss': 1.1237, 'learning_rate': 9.985047847012776e-05, 'epoch': 0.02}
{'loss': 1.1299, 'learning_rate': 9.984860455704028e-05, 'epoch': 0.03}
{'loss': 1.0566, 'learning_rate': 9.984671899214457e-05, 'epoch': 0.03}
{'loss': 1.1239, 'learning_rate': 9.984482177588138e-05, 'epoch': 0.03}
{'loss': 1.0902, 'learning_rate': 9.984291290869415e-05, 'epoch': 0.03}
  2%|         | 161/6500 [18:27<12:32:54,  7.13s/it]                                                       2%|         | 161/6500 [18:27<12:32:54,  7.13s/it]  2%|         | 162/6500 [18:34<12:47:47,  7.27s/it]                                                       2%|         | 162/6500 [18:34<12:47:47,  7.27s/it]  3%|         | 163/6500 [18:41<12:26:19,  7.07s/it]                                                       3%|         | 163/6500 [18:41<12:26:19,  7.07s/it]  3%|         | 164/6500 [18:48<12:11:07,  6.92s/it]                                                       3%|         | 164/6500 [18:48<12:11:07,  6.92s/it]  3%|         | 165/6500 [18:54<12:00:14,  6.82s/it]                                                       3%|         | 165/6500 [18:54<12:00:14,  6.82s/it]  3%|         | 166/6500 [19:01<11:52:40,  6.75s/it]                                                       3%|         | 166/6500 [19:01<11:52:40,  6.75s/it]  3%|         | 16{'loss': 1.1042, 'learning_rate': 9.984099239102909e-05, 'epoch': 0.03}
{'loss': 1.0815, 'learning_rate': 9.983906022333507e-05, 'epoch': 0.03}
{'loss': 1.3612, 'learning_rate': 9.983711640606377e-05, 'epoch': 0.03}
{'loss': 1.0973, 'learning_rate': 9.983516093966952e-05, 'epoch': 0.03}
7/6500 [19:07<11:48:03,  6.71s/it]                                                       3%|         | 167/6500 [19:07<11:48:03,  6.71s/it]  3%|         | 168/6500 [19:14<11:44:04,  6.67s/it]                                                       3%|         | 168/6500 [19:14<11:44:04,  6.67s/it]  3%|         | 169/6500 [19:21<11:40:57,  6.64s/it]                                                       3%|         | 169/6500 [19:21<11:40:57,  6.64s/it]  3%|         | 170/6500 [19:27<11:39:01,  6.63s/it]                                                       3%|         | 170/6500 [19:27<11:39:01,  6.63s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 1.0908052921295166, 'eval_runtime': 1.492, 'eval_samples_per_second': 8.043, 'eval_steps_per_second': 2.011, 'epoch': 0.03}
                                                       3%|         | 170/6500 [19:29<11:39:01,  6.63s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-170
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-170
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-170
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-170
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-170
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-170/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-170/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-170/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-170/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 1.0649, 'learning_rate': 9.98331938246094e-05, 'epoch': 0.03}
{'loss': 1.1008, 'learning_rate': 9.983121506134322e-05, 'epoch': 0.03}
{'loss': 1.0638, 'learning_rate': 9.98292246503335e-05, 'epoch': 0.03}
{'loss': 1.0687, 'learning_rate': 9.982722259204548e-05, 'epoch': 0.03}
{'loss': 1.1019, 'learning_rate': 9.982520888694713e-05, 'epoch': 0.03}
{'loss': 1.1097, 'learning_rate': 9.982318353550915e-05, 'epoch': 0.03}
  3%|         | 171/6500 [19:35<12:32:50,  7.14s/it]                                                       3%|         | 171/6500 [19:35<12:32:50,  7.14s/it]  3%|         | 172/6500 [19:42<12:15:14,  6.97s/it]                                                       3%|         | 172/6500 [19:42<12:15:14,  6.97s/it]  3%|         | 173/6500 [19:49<12:03:11,  6.86s/it]                                                       3%|         | 173/6500 [19:49<12:03:11,  6.86s/it]  3%|         | 174/6500 [19:55<11:54:31,  6.78s/it]                                                       3%|         | 174/6500 [19:55<11:54:31,  6.78s/it]  3%|         | 175/6500 [20:02<11:48:31,  6.72s/it]                                                       3%|         | 175/6500 [20:02<11:48:31,  6.72s/it]  3%|         | 176/6500 [20:08<11:44:32,  6.68s/it]                                                       3%|         | 176/6500 [20:08<11:44:32,  6.68s/it]  3%|         | 17{'loss': 1.1081, 'learning_rate': 9.982114653820494e-05, 'epoch': 0.03}
{'loss': 1.0985, 'learning_rate': 9.981909789551065e-05, 'epoch': 0.03}
{'loss': 1.0514, 'learning_rate': 9.981703760790515e-05, 'epoch': 0.03}
{'loss': 1.0997, 'learning_rate': 9.981496567586997e-05, 'epoch': 0.03}
7/6500 [20:15<11:41:35,  6.66s/it]                                                       3%|         | 177/6500 [20:15<11:41:35,  6.66s/it]  3%|         | 178/6500 [20:22<11:59:35,  6.83s/it]                                                       3%|         | 178/6500 [20:22<11:59:35,  6.83s/it]  3%|         | 179/6500 [20:29<11:51:54,  6.76s/it]                                                       3%|         | 179/6500 [20:29<11:51:54,  6.76s/it]  3%|         | 180/6500 [20:35<11:46:33,  6.71s/it]                                                       3%|         | 180/6500 [20:35<11:46:33,  6.71s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 1.0824166536331177, 'eval_runtime': 1.5111, 'eval_samples_per_second': 7.941, 'eval_steps_per_second': 1.985, 'epoch': 0.03}
                                                       3%|         | 180/6500 [20:37<11:46:33,  6.71s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-180
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-180
/projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-180
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-180
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-180
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-180/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-180/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-180/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-180/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 1.069, 'learning_rate': 9.981288209988946e-05, 'epoch': 0.03}
{'loss': 1.1095, 'learning_rate': 9.981078688045062e-05, 'epoch': 0.03}
{'loss': 1.0793, 'learning_rate': 9.980868001804322e-05, 'epoch': 0.03}
{'loss': 1.334, 'learning_rate': 9.980656151315969e-05, 'epoch': 0.03}
{'loss': 1.0834, 'learning_rate': 9.980443136629525e-05, 'epoch': 0.03}
{'loss': 1.0458, 'learning_rate': 9.980228957794777e-05, 'epoch': 0.03}
  3%|         | 181/6500 [20:44<12:37:50,  7.20s/it]                                                       3%|         | 181/6500 [20:44<12:37:50,  7.20s/it]  3%|         | 182/6500 [20:50<12:19:00,  7.02s/it]                                                       3%|         | 182/6500 [20:50<12:19:00,  7.02s/it]  3%|         | 183/6500 [20:57<12:05:29,  6.89s/it]                                                       3%|         | 183/6500 [20:57<12:05:29,  6.89s/it]  3%|         | 184/6500 [21:04<11:56:05,  6.80s/it]                                                       3%|         | 184/6500 [21:04<11:56:05,  6.80s/it]  3%|         | 185/6500 [21:10<11:49:19,  6.74s/it]                                                       3%|         | 185/6500 [21:10<11:49:19,  6.74s/it]  3%|         | 186/6500 [21:17<11:44:49,  6.70s/it]                                                       3%|         | 186/6500 [21:17<11:44:49,  6.70s/it]  3%|         | 18{'loss': 1.0884, 'learning_rate': 9.980013614861792e-05, 'epoch': 0.03}
{'loss': 1.0556, 'learning_rate': 9.979797107880903e-05, 'epoch': 0.03}
{'loss': 1.0559, 'learning_rate': 9.979579436902717e-05, 'epoch': 0.03}
{'loss': 1.1039, 'learning_rate': 9.979360601978116e-05, 'epoch': 0.03}
7/6500 [21:23<11:41:16,  6.67s/it]                                                       3%|         | 187/6500 [21:23<11:41:16,  6.67s/it]  3%|         | 188/6500 [21:30<11:38:38,  6.64s/it]                                                       3%|         | 188/6500 [21:30<11:38:38,  6.64s/it]  3%|         | 189/6500 [21:36<11:37:03,  6.63s/it]                                                       3%|         | 189/6500 [21:36<11:37:03,  6.63s/it]  3%|         | 190/6500 [21:43<11:36:02,  6.62s/it]                                                       3%|         | 190/6500 [21:43<11:36:02,  6.62s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 1.0756245851516724, 'eval_runtime': 1.5008, 'eval_samples_per_second': 7.996, 'eval_steps_per_second': 1.999, 'epoch': 0.03}
                                                       3%|         | 190/6500 [21:45<11:36:02,  6.62s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-190
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-190
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-190
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-190
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-190
the pytorch model path isthe pytorch model path is  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-190/pytorch_model.bin/projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-190/pytorch_model.bin

the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-190/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-190/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 1.0979, 'learning_rate': 9.979140603158248e-05, 'epoch': 0.03}
{'loss': 1.0816, 'learning_rate': 9.978919440494539e-05, 'epoch': 0.03}
{'loss': 1.0771, 'learning_rate': 9.978697114038681e-05, 'epoch': 0.03}
{'loss': 1.0603, 'learning_rate': 9.978473623842644e-05, 'epoch': 0.03}
{'loss': 1.0596, 'learning_rate': 9.978248969958668e-05, 'epoch': 0.03}
{'loss': 1.0586, 'learning_rate': 9.978023152439263e-05, 'epoch': 0.03}
  3%|         | 191/6500 [21:51<12:31:18,  7.15s/it]                                                       3%|         | 191/6500 [21:51<12:31:18,  7.15s/it]  3%|         | 192/6500 [21:58<12:13:39,  6.98s/it]                                                       3%|         | 192/6500 [21:58<12:13:39,  6.98s/it]  3%|         | 193/6500 [22:05<12:01:33,  6.86s/it]                                                       3%|         | 193/6500 [22:05<12:01:33,  6.86s/it]  3%|         | 194/6500 [22:12<12:19:32,  7.04s/it]                                                       3%|         | 194/6500 [22:12<12:19:32,  7.04s/it]  3%|         | 195/6500 [22:19<12:06:14,  6.91s/it]                                                       3%|         | 195/6500 [22:19<12:06:14,  6.91s/it]  3%|         | 196/6500 [22:25<11:55:39,  6.81s/it]                                                       3%|         | 196/6500 [22:25<11:55:39,  6.81s/it]  3%|         | 19{'loss': 1.1008, 'learning_rate': 9.977796171337212e-05, 'epoch': 0.03}
{'loss': 1.0698, 'learning_rate': 9.977568026705574e-05, 'epoch': 0.03}
{'loss': 1.3188, 'learning_rate': 9.977338718597672e-05, 'epoch': 0.03}
{'loss': 1.0698, 'learning_rate': 9.977108247067108e-05, 'epoch': 0.03}
7/6500 [22:32<11:48:48,  6.75s/it]                                                       3%|         | 197/6500 [22:32<11:48:48,  6.75s/it]  3%|         | 198/6500 [22:38<11:44:05,  6.70s/it]                                                       3%|         | 198/6500 [22:38<11:44:05,  6.70s/it]  3%|         | 199/6500 [22:45<11:40:49,  6.67s/it]                                                       3%|         | 199/6500 [22:45<11:40:49,  6.67s/it]  3%|         | 200/6500 [22:52<11:38:32,  6.65s/it]                                                       3%|         | 200/6500 [22:52<11:38:32,  6.65s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 1.0706660747528076, 'eval_runtime': 1.4918, 'eval_samples_per_second': 8.044, 'eval_steps_per_second': 2.011, 'epoch': 0.03}
                                                       3%|         | 200/6500 [22:53<11:38:32,  6.65s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-200
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  I AM HERE AND I AM SAVING THE MODEL
/projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-200the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-200

I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-200
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-200
the pytorch model path isthe pytorch model path is  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-200/pytorch_model.bin/projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-200/pytorch_model.bin

the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-200/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-200/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 1.0426, 'learning_rate': 9.976876612167752e-05, 'epoch': 0.03}
{'loss': 1.0522, 'learning_rate': 9.976643813953747e-05, 'epoch': 0.03}
{'loss': 1.0436, 'learning_rate': 9.976409852479511e-05, 'epoch': 0.03}
{'loss': 1.0415, 'learning_rate': 9.976174727799728e-05, 'epoch': 0.03}
{'loss': 1.0833, 'learning_rate': 9.975938439969357e-05, 'epoch': 0.03}
{'loss': 1.071, 'learning_rate': 9.975700989043633e-05, 'epoch': 0.03}
  3%|         | 201/6500 [23:00<12:33:30,  7.18s/it]                                                       3%|         | 201/6500 [23:00<12:33:30,  7.18s/it]  3%|         | 202/6500 [23:07<12:14:42,  7.00s/it]                                                       3%|         | 202/6500 [23:07<12:14:42,  7.00s/it]  3%|         | 203/6500 [23:13<12:02:12,  6.88s/it]                                                       3%|         | 203/6500 [23:13<12:02:12,  6.88s/it]  3%|         | 204/6500 [23:20<11:53:13,  6.80s/it]                                                       3%|         | 204/6500 [23:20<11:53:13,  6.80s/it]  3%|         | 205/6500 [23:26<11:47:17,  6.74s/it]                                                       3%|         | 205/6500 [23:26<11:47:17,  6.74s/it]  3%|         | 206/6500 [23:33<11:43:22,  6.71s/it]                                                       3%|         | 206/6500 [23:33<11:43:22,  6.71s/it]  3%|         | 20{'loss': 1.0773, 'learning_rate': 9.975462375078053e-05, 'epoch': 0.03}
{'loss': 1.0492, 'learning_rate': 9.975222598128394e-05, 'epoch': 0.03}
{'loss': 1.0621, 'learning_rate': 9.974981658250704e-05, 'epoch': 0.03}
{'loss': 1.0649, 'learning_rate': 9.974739555501298e-05, 'epoch': 0.03}
7/6500 [23:40<11:39:39,  6.67s/it]                                                       3%|         | 207/6500 [23:40<11:39:39,  6.67s/it]  3%|         | 208/6500 [23:46<11:37:34,  6.65s/it]                                                       3%|         | 208/6500 [23:46<11:37:34,  6.65s/it]  3%|         | 209/6500 [23:53<11:35:40,  6.63s/it]                                                       3%|         | 209/6500 [23:53<11:35:40,  6.63s/it]  3%|         | 210/6500 [23:59<11:33:58,  6.62s/it]                                                       3%|         | 210/6500 [23:59<11:33:58,  6.62s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 1.0629836320877075, 'eval_runtime': 1.4991, 'eval_samples_per_second': 8.005, 'eval_steps_per_second': 2.001, 'epoch': 0.03}
                                                       3%|         | 210/6500 [24:01<11:33:58,  6.62s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-210
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-210I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-210

I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-210
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-210
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-210/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-210/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-210/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-210/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 1.0532, 'learning_rate': 9.974496289936769e-05, 'epoch': 0.03}
{'loss': 1.0823, 'learning_rate': 9.974251861613977e-05, 'epoch': 0.03}
{'loss': 1.0587, 'learning_rate': 9.974006270590058e-05, 'epoch': 0.03}
{'loss': 1.3181, 'learning_rate': 9.973759516922414e-05, 'epoch': 0.03}
{'loss': 1.0408, 'learning_rate': 9.973511600668724e-05, 'epoch': 0.03}
{'loss': 1.0413, 'learning_rate': 9.973262521886937e-05, 'epoch': 0.03}
  3%|         | 211/6500 [24:09<12:55:03,  7.39s/it]                                                       3%|         | 211/6500 [24:09<12:55:03,  7.39s/it]  3%|         | 212/6500 [24:15<12:30:04,  7.16s/it]                                                       3%|         | 212/6500 [24:15<12:30:04,  7.16s/it]  3%|         | 213/6500 [24:22<12:12:10,  6.99s/it]                                                       3%|         | 213/6500 [24:22<12:12:10,  6.99s/it]  3%|         | 214/6500 [24:28<11:59:42,  6.87s/it]                                                       3%|         | 214/6500 [24:28<11:59:42,  6.87s/it]  3%|         | 215/6500 [24:35<11:50:41,  6.78s/it]                                                       3%|         | 215/6500 [24:35<11:50:41,  6.78s/it]  3%|         | 216/6500 [24:42<11:44:51,  6.73s/it]                                                       3%|         | 216/6500 [24:42<11:44:51,  6.73s/it]  3%|         | 21{'loss': 1.0318, 'learning_rate': 9.973012280635273e-05, 'epoch': 0.03}
{'loss': 1.0308, 'learning_rate': 9.972760876972226e-05, 'epoch': 0.03}
{'loss': 1.0306, 'learning_rate': 9.972508310956557e-05, 'epoch': 0.03}
{'loss': 1.0932, 'learning_rate': 9.972254582647305e-05, 'epoch': 0.03}
7/6500 [24:48<11:40:24,  6.69s/it]                                                       3%|         | 217/6500 [24:48<11:40:24,  6.69s/it]  3%|         | 218/6500 [24:55<11:37:20,  6.66s/it]                                                       3%|         | 218/6500 [24:55<11:37:20,  6.66s/it]  3%|         | 219/6500 [25:01<11:35:14,  6.64s/it]                                                       3%|         | 219/6500 [25:01<11:35:14,  6.64s/it]  3%|         | 220/6500 [25:08<11:33:35,  6.63s/it]                                                       3%|         | 220/6500 [25:08<11:33:35,  6.63s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 1.0583254098892212, 'eval_runtime': 1.4929, 'eval_samples_per_second': 8.038, 'eval_steps_per_second': 2.01, 'epoch': 0.03}
                                                       3%|         | 220/6500 [25:10<11:33:35,  6.63s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-220
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-220
 /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-220
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-220
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-220
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-220/pytorch_model.bin
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-220/pytorch_model.bin 
/projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-220/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-220/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 1.0682, 'learning_rate': 9.971999692103777e-05, 'epoch': 0.03}
{'loss': 1.07, 'learning_rate': 9.971743639385551e-05, 'epoch': 0.03}
{'loss': 1.0213, 'learning_rate': 9.971486424552477e-05, 'epoch': 0.03}
{'loss': 1.0613, 'learning_rate': 9.971228047664677e-05, 'epoch': 0.03}
{'loss': 1.0408, 'learning_rate': 9.970968508782549e-05, 'epoch': 0.03}
{'loss': 1.0437, 'learning_rate': 9.970707807966755e-05, 'epoch': 0.03}
  3%|         | 221/6500 [25:16<12:26:58,  7.14s/it]                                                       3%|         | 221/6500 [25:16<12:26:58,  7.14s/it]  3%|         | 222/6500 [25:23<12:09:30,  6.97s/it]                                                       3%|         | 222/6500 [25:23<12:09:30,  6.97s/it]  3%|         | 223/6500 [25:30<11:58:02,  6.86s/it]                                                       3%|         | 223/6500 [25:30<11:58:02,  6.86s/it]  3%|         | 224/6500 [25:36<11:49:27,  6.78s/it]                                                       3%|         | 224/6500 [25:36<11:49:27,  6.78s/it]  3%|         | 225/6500 [25:43<11:43:11,  6.72s/it]                                                       3%|         | 225/6500 [25:43<11:43:11,  6.72s/it]  3%|         | 226/6500 [25:49<11:38:30,  6.68s/it]                                                       3%|         | 226/6500 [25:49<11:38:30,  6.68s/it]  3%|         | 22{'loss': 1.0521, 'learning_rate': 9.970445945278233e-05, 'epoch': 0.03}
{'loss': 1.0478, 'learning_rate': 9.970182920778193e-05, 'epoch': 0.04}
{'loss': 1.3004, 'learning_rate': 9.969918734528114e-05, 'epoch': 0.04}
{'loss': 1.0087, 'learning_rate': 9.969653386589748e-05, 'epoch': 0.04}
7/6500 [25:57<12:04:07,  6.93s/it]                                                       3%|         | 227/6500 [25:57<12:04:07,  6.93s/it]  4%|         | 228/6500 [26:03<11:53:27,  6.83s/it]                                                       4%|         | 228/6500 [26:03<11:53:27,  6.83s/it]  4%|         | 229/6500 [26:10<11:46:24,  6.76s/it]                                                       4%|         | 229/6500 [26:10<11:46:24,  6.76s/it]  4%|         | 230/6500 [26:17<11:40:51,  6.71s/it]                                                       4%|         | 230/6500 [26:17<11:40:51,  6.71s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 1.0536280870437622, 'eval_runtime': 1.4963, 'eval_samples_per_second': 8.02, 'eval_steps_per_second': 2.005, 'epoch': 0.04}
                                                       4%|         | 230/6500 [26:18<11:40:51,  6.71s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-230
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-230I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-230

I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-230
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-230
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-230/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-230/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-230/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-230/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 1.0483, 'learning_rate': 9.96938687702512e-05, 'epoch': 0.04}
{'loss': 1.0131, 'learning_rate': 9.969119205896523e-05, 'epoch': 0.04}
{'loss': 1.0119, 'learning_rate': 9.968850373266522e-05, 'epoch': 0.04}
{'loss': 1.0353, 'learning_rate': 9.968580379197961e-05, 'epoch': 0.04}
{'loss': 1.0487, 'learning_rate': 9.968309223753944e-05, 'epoch': 0.04}
{'loss': 1.0723, 'learning_rate': 9.968036906997855e-05, 'epoch': 0.04}
  4%|         | 231/6500 [26:25<12:35:02,  7.23s/it]                                                       4%|         | 231/6500 [26:25<12:35:02,  7.23s/it]  4%|         | 232/6500 [26:32<12:14:47,  7.03s/it]                                                       4%|         | 232/6500 [26:32<12:14:47,  7.03s/it]  4%|         | 233/6500 [26:38<12:00:54,  6.90s/it]                                                       4%|         | 233/6500 [26:38<12:00:54,  6.90s/it]  4%|         | 234/6500 [26:45<11:51:30,  6.81s/it]                                                       4%|         | 234/6500 [26:45<11:51:30,  6.81s/it]  4%|         | 235/6500 [26:51<11:44:49,  6.75s/it]                                                       4%|         | 235/6500 [26:51<11:44:49,  6.75s/it]  4%|         | 236/6500 [26:58<11:40:04,  6.71s/it]                                                       4%|         | 236/6500 [26:58<11:40:04,  6.71s/it]  4%|         | 23{'loss': 1.0542, 'learning_rate': 9.967763428993344e-05, 'epoch': 0.04}
{'loss': 1.0073, 'learning_rate': 9.967488789804337e-05, 'epoch': 0.04}
{'loss': 1.0441, 'learning_rate': 9.967212989495028e-05, 'epoch': 0.04}
{'loss': 1.0372, 'learning_rate': 9.966936028129882e-05, 'epoch': 0.04}
7/6500 [27:05<11:36:33,  6.67s/it]                                                       4%|         | 237/6500 [27:05<11:36:33,  6.67s/it]  4%|         | 238/6500 [27:11<11:34:16,  6.65s/it]                                                       4%|         | 238/6500 [27:11<11:34:16,  6.65s/it]  4%|         | 239/6500 [27:18<11:32:26,  6.64s/it]                                                       4%|         | 239/6500 [27:18<11:32:26,  6.64s/it]  4%|         | 240/6500 [27:24<11:30:45,  6.62s/it]                                                       4%|         | 240/6500 [27:24<11:30:45,  6.62s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 1.0467650890350342, 'eval_runtime': 1.5005, 'eval_samples_per_second': 7.998, 'eval_steps_per_second': 1.999, 'epoch': 0.04}
                                                       4%|         | 240/6500 [27:26<11:30:45,  6.62s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-240
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-240I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-240

I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-240
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-240
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-240/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-240/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-240/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-240/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 1.056, 'learning_rate': 9.966657905773642e-05, 'epoch': 0.04}
{'loss': 1.0113, 'learning_rate': 9.966378622491312e-05, 'epoch': 0.04}
{'loss': 1.3001, 'learning_rate': 9.966098178348176e-05, 'epoch': 0.04}
{'loss': 1.0512, 'learning_rate': 9.965816573409785e-05, 'epoch': 0.04}
{'loss': 0.9923, 'learning_rate': 9.965533807741964e-05, 'epoch': 0.04}
{'loss': 1.0472, 'learning_rate': 9.965249881410805e-05, 'epoch': 0.04}
  4%|         | 241/6500 [27:33<12:25:33,  7.15s/it]                                                       4%|         | 241/6500 [27:33<12:25:33,  7.15s/it]  4%|         | 242/6500 [27:39<12:08:33,  6.99s/it]                                                       4%|         | 242/6500 [27:39<12:08:33,  6.99s/it]  4%|         | 243/6500 [27:47<12:14:58,  7.05s/it]                                                       4%|         | 243/6500 [27:47<12:14:58,  7.05s/it]  4%|         | 244/6500 [27:53<12:00:32,  6.91s/it]                                                       4%|         | 244/6500 [27:53<12:00:32,  6.91s/it]  4%|         | 245/6500 [28:00<11:50:22,  6.81s/it]                                                       4%|         | 245/6500 [28:00<11:50:22,  6.81s/it]  4%|         | 246/6500 [28:06<11:43:04,  6.75s/it]                                                       4%|         | 246/6500 [28:06<11:43:04,  6.75s/it]  4%|         | 24{'loss': 0.989, 'learning_rate': 9.964964794482675e-05, 'epoch': 0.04}
{'loss': 1.0016, 'learning_rate': 9.964678547024213e-05, 'epoch': 0.04}
{'loss': 1.0288, 'learning_rate': 9.964391139102325e-05, 'epoch': 0.04}
{'loss': 1.0355, 'learning_rate': 9.964102570784193e-05, 'epoch': 0.04}
7/6500 [28:13<11:38:16,  6.70s/it]                                                       4%|         | 247/6500 [28:13<11:38:16,  6.70s/it]  4%|         | 248/6500 [28:20<11:34:36,  6.67s/it]                                                       4%|         | 248/6500 [28:20<11:34:36,  6.67s/it]  4%|         | 249/6500 [28:26<11:32:12,  6.64s/it]                                                       4%|         | 249/6500 [28:26<11:32:12,  6.64s/it]  4%|         | 250/6500 [28:33<11:30:40,  6.63s/it]                                                       4%|         | 250/6500 [28:33<11:30:40,  6.63s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 1.0426052808761597, 'eval_runtime': 1.7614, 'eval_samples_per_second': 6.813, 'eval_steps_per_second': 1.703, 'epoch': 0.04}
                                                       4%|         | 250/6500 [28:34<11:30:40,  6.63s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-250
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-250 /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-250

I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-250
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-250
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-250/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-250/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-250/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-250/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 1.0577, 'learning_rate': 9.963812842137267e-05, 'epoch': 0.04}
{'loss': 1.0334, 'learning_rate': 9.963521953229268e-05, 'epoch': 0.04}
{'loss': 0.9865, 'learning_rate': 9.963229904128196e-05, 'epoch': 0.04}
{'loss': 1.0456, 'learning_rate': 9.962936694902307e-05, 'epoch': 0.04}
{'loss': 1.0167, 'learning_rate': 9.96264232562014e-05, 'epoch': 0.04}
{'loss': 1.0412, 'learning_rate': 9.962346796350504e-05, 'epoch': 0.04}
  4%|         | 251/6500 [28:41<12:34:07,  7.24s/it]                                                       4%|         | 251/6500 [28:41<12:34:07,  7.24s/it]  4%|         | 252/6500 [28:48<12:13:49,  7.05s/it]                                                       4%|         | 252/6500 [28:48<12:13:49,  7.05s/it]  4%|         | 253/6500 [28:55<11:59:42,  6.91s/it]                                                       4%|         | 253/6500 [28:55<11:59:42,  6.91s/it]  4%|         | 254/6500 [29:01<11:50:03,  6.82s/it]                                                       4%|         | 254/6500 [29:01<11:50:03,  6.82s/it]  4%|         | 255/6500 [29:08<11:42:21,  6.75s/it]                                                       4%|         | 255/6500 [29:08<11:42:21,  6.75s/it]  4%|         | 256/6500 [29:14<11:37:14,  6.70s/it]                                                       4%|         | 256/6500 [29:14<11:37:14,  6.70s/it]  4%|         | 25{'loss': 1.0207, 'learning_rate': 9.962050107162477e-05, 'epoch': 0.04}
{'loss': 1.2739, 'learning_rate': 9.961752258125406e-05, 'epoch': 0.04}
{'loss': 1.0301, 'learning_rate': 9.961453249308914e-05, 'epoch': 0.04}
{'loss': 0.9851, 'learning_rate': 9.96115308078289e-05, 'epoch': 0.04}
7/6500 [29:21<11:33:51,  6.67s/it]                                                       4%|         | 257/6500 [29:21<11:33:51,  6.67s/it]  4%|         | 258/6500 [29:28<11:31:06,  6.64s/it]                                                       4%|         | 258/6500 [29:28<11:31:06,  6.64s/it]  4%|         | 259/6500 [29:35<11:48:15,  6.81s/it]                                                       4%|         | 259/6500 [29:35<11:48:15,  6.81s/it]  4%|         | 260/6500 [29:41<11:41:29,  6.75s/it]                                                       4%|         | 260/6500 [29:41<11:41:29,  6.75s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 1.0367591381072998, 'eval_runtime': 1.5016, 'eval_samples_per_second': 7.991, 'eval_steps_per_second': 1.998, 'epoch': 0.04}
                                                       4%|         | 260/6500 [29:43<11:41:29,  6.75s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-260
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-260
/projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-260
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-260
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-260
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-260/pytorch_model.bin
the pytorch model path isthe pytorch model path is  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-260/pytorch_model.bin/projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-260/pytorch_model.bin

the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-260/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 1.0343, 'learning_rate': 9.960851752617498e-05, 'epoch': 0.04}
{'loss': 0.9907, 'learning_rate': 9.96054926488317e-05, 'epoch': 0.04}
{'loss': 0.9952, 'learning_rate': 9.960245617650613e-05, 'epoch': 0.04}
{'loss': 1.0435, 'learning_rate': 9.959940810990802e-05, 'epoch': 0.04}
{'loss': 1.0371, 'learning_rate': 9.959634844974983e-05, 'epoch': 0.04}
{'loss': 1.0334, 'learning_rate': 9.959327719674674e-05, 'epoch': 0.04}
  4%|         | 261/6500 [29:50<12:31:58,  7.23s/it]                                                       4%|         | 261/6500 [29:50<12:31:58,  7.23s/it]  4%|         | 262/6500 [29:56<12:11:44,  7.04s/it]                                                       4%|         | 262/6500 [29:56<12:11:44,  7.04s/it]  4%|         | 263/6500 [30:03<11:57:43,  6.90s/it]                                                       4%|         | 263/6500 [30:03<11:57:43,  6.90s/it]  4%|         | 264/6500 [30:09<11:48:14,  6.81s/it]                                                       4%|         | 264/6500 [30:09<11:48:14,  6.81s/it]  4%|         | 265/6500 [30:16<11:41:15,  6.75s/it]                                                       4%|         | 265/6500 [30:16<11:41:15,  6.75s/it]  4%|         | 266/6500 [30:23<11:36:31,  6.70s/it]                                                       4%|         | 266/6500 [30:23<11:36:31,  6.70s/it]  4%|         | 26{'loss': 1.028, 'learning_rate': 9.959019435161664e-05, 'epoch': 0.04}
{'loss': 0.9952, 'learning_rate': 9.958709991508012e-05, 'epoch': 0.04}
{'loss': 1.0104, 'learning_rate': 9.958399388786049e-05, 'epoch': 0.04}
{'loss': 0.9973, 'learning_rate': 9.958087627068376e-05, 'epoch': 0.04}
7/6500 [30:29<11:33:38,  6.68s/it]                                                       4%|         | 267/6500 [30:29<11:33:38,  6.68s/it]  4%|         | 268/6500 [30:36<11:31:14,  6.66s/it]                                                       4%|         | 268/6500 [30:36<11:31:14,  6.66s/it]  4%|         | 269/6500 [30:42<11:29:21,  6.64s/it]                                                       4%|         | 269/6500 [30:42<11:29:21,  6.64s/it]  4%|         | 270/6500 [30:49<11:27:29,  6.62s/it]                                                       4%|         | 270/6500 [30:49<11:27:29,  6.62s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 1.0312691926956177, 'eval_runtime': 1.4914, 'eval_samples_per_second': 8.046, 'eval_steps_per_second': 2.011, 'epoch': 0.04}
                                                       4%|         | 270/6500 [30:51<11:27:29,  6.62s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-270
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in   /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-270
/projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-270
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-270
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-270
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-270/pytorch_model.bin
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-270/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-270/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-270/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 1.0429, 'learning_rate': 9.957774706427867e-05, 'epoch': 0.04}
{'loss': 1.0233, 'learning_rate': 9.957460626937664e-05, 'epoch': 0.04}
{'loss': 1.2539, 'learning_rate': 9.957145388671181e-05, 'epoch': 0.04}
{'loss': 1.0122, 'learning_rate': 9.956828991702103e-05, 'epoch': 0.04}
{'loss': 0.9932, 'learning_rate': 9.956511436104385e-05, 'epoch': 0.04}
{'loss': 0.995, 'learning_rate': 9.956192721952257e-05, 'epoch': 0.04}
  4%|         | 271/6500 [30:57<12:21:23,  7.14s/it]                                                       4%|         | 271/6500 [30:57<12:21:23,  7.14s/it]  4%|         | 272/6500 [31:04<12:04:23,  6.98s/it]                                                       4%|         | 272/6500 [31:04<12:04:23,  6.98s/it]  4%|         | 273/6500 [31:11<11:52:15,  6.86s/it]                                                       4%|         | 273/6500 [31:11<11:52:15,  6.86s/it]  4%|         | 274/6500 [31:17<11:44:02,  6.78s/it]                                                       4%|         | 274/6500 [31:17<11:44:02,  6.78s/it]  4%|         | 275/6500 [31:25<12:04:46,  6.99s/it]                                                       4%|         | 275/6500 [31:25<12:04:46,  6.99s/it]  4%|         | 276/6500 [31:31<11:52:02,  6.86s/it]                                                       4%|         | 276/6500 [31:31<11:52:02,  6.86s/it]  4%|         | 27{'loss': 0.9781, 'learning_rate': 9.955872849320213e-05, 'epoch': 0.04}
{'loss': 0.987, 'learning_rate': 9.955551818283024e-05, 'epoch': 0.04}
{'loss': 1.0194, 'learning_rate': 9.955229628915727e-05, 'epoch': 0.04}
{'loss': 1.0191, 'learning_rate': 9.954906281293634e-05, 'epoch': 0.04}
7/6500 [31:38<11:43:22,  6.78s/it]                                                       4%|         | 277/6500 [31:38<11:43:22,  6.78s/it]  4%|         | 278/6500 [31:44<11:37:25,  6.73s/it]                                                       4%|         | 278/6500 [31:44<11:37:25,  6.73s/it]  4%|         | 279/6500 [31:51<11:33:00,  6.68s/it]                                                       4%|         | 279/6500 [31:51<11:33:00,  6.68s/it]  4%|         | 280/6500 [31:58<11:29:48,  6.65s/it]                                                       4%|         | 280/6500 [31:58<11:29:48,  6.65s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 1.0280967950820923, 'eval_runtime': 1.4925, 'eval_samples_per_second': 8.04, 'eval_steps_per_second': 2.01, 'epoch': 0.04}
                                                       4%|         | 280/6500 [31:59<11:29:48,  6.65s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-280
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-280I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-280

I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-280
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-280
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-280/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-280/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-280/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-280/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 1.0238, 'learning_rate': 9.954581775492322e-05, 'epoch': 0.04}
{'loss': 0.9983, 'learning_rate': 9.954256111587645e-05, 'epoch': 0.04}
{'loss': 1.0007, 'learning_rate': 9.953929289655724e-05, 'epoch': 0.04}
{'loss': 1.008, 'learning_rate': 9.953601309772953e-05, 'epoch': 0.04}
{'loss': 0.9867, 'learning_rate': 9.953272172015992e-05, 'epoch': 0.04}
{'loss': 1.0372, 'learning_rate': 9.952941876461779e-05, 'epoch': 0.04}
  4%|         | 281/6500 [32:06<12:22:46,  7.17s/it]                                                       4%|         | 281/6500 [32:06<12:22:46,  7.17s/it]  4%|         | 282/6500 [32:13<12:04:37,  6.99s/it]                                                       4%|         | 282/6500 [32:13<12:04:37,  6.99s/it]  4%|         | 283/6500 [32:19<11:51:47,  6.87s/it]                                                       4%|         | 283/6500 [32:19<11:51:47,  6.87s/it]  4%|         | 284/6500 [32:26<11:43:11,  6.79s/it]                                                       4%|         | 284/6500 [32:26<11:43:11,  6.79s/it]  4%|         | 285/6500 [32:32<11:36:37,  6.73s/it]                                                       4%|         | 285/6500 [32:32<11:36:37,  6.73s/it]  4%|         | 286/6500 [32:39<11:31:58,  6.68s/it]                                                       4%|         | 286/6500 [32:39<11:31:58,  6.68s/it]  4%|         | 28{'loss': 1.0068, 'learning_rate': 9.952610423187516e-05, 'epoch': 0.04}
{'loss': 1.2585, 'learning_rate': 9.952277812270681e-05, 'epoch': 0.04}
{'loss': 0.9945, 'learning_rate': 9.951944043789016e-05, 'epoch': 0.04}
{'loss': 0.9869, 'learning_rate': 9.951609117820538e-05, 'epoch': 0.04}
7/6500 [32:45<11:28:54,  6.65s/it]                                                       4%|         | 287/6500 [32:45<11:28:54,  6.65s/it]  4%|         | 288/6500 [32:52<11:26:54,  6.63s/it]                                                       4%|         | 288/6500 [32:52<11:26:54,  6.63s/it]  4%|         | 289/6500 [32:59<11:24:56,  6.62s/it]                                                       4%|         | 289/6500 [32:59<11:24:56,  6.62s/it]  4%|         | 290/6500 [33:05<11:24:02,  6.61s/it]                                                       4%|         | 290/6500 [33:05<11:24:02,  6.61s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 1.023958444595337, 'eval_runtime': 1.4906, 'eval_samples_per_second': 8.051, 'eval_steps_per_second': 2.013, 'epoch': 0.04}
                                                       4%|         | 290/6500 [33:07<11:24:02,  6.61s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-290
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  I AM HERE AND I AM SAVING THE MODEL
/projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-290the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-290

I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-290
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-290
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-290/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-290/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-290/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-290/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.9832, 'learning_rate': 9.951273034443537e-05, 'epoch': 0.04}
{'loss': 0.9751, 'learning_rate': 9.950935793736567e-05, 'epoch': 0.04}
{'loss': 0.9751, 'learning_rate': 9.950597395778458e-05, 'epoch': 0.05}
{'loss': 1.0183, 'learning_rate': 9.950257840648307e-05, 'epoch': 0.05}
{'loss': 1.0131, 'learning_rate': 9.949917128425485e-05, 'epoch': 0.05}
{'loss': 1.0158, 'learning_rate': 9.94957525918963e-05, 'epoch': 0.05}
  4%|         | 291/6500 [33:14<12:44:04,  7.38s/it]                                                       4%|         | 291/6500 [33:14<12:44:04,  7.38s/it]  4%|         | 292/6500 [33:21<12:19:30,  7.15s/it]                                                       4%|         | 292/6500 [33:21<12:19:30,  7.15s/it]  5%|         | 293/6500 [33:28<12:01:31,  6.97s/it]                                                       5%|         | 293/6500 [33:28<12:01:31,  6.97s/it]  5%|         | 294/6500 [33:34<11:48:53,  6.85s/it]                                                       5%|         | 294/6500 [33:34<11:48:53,  6.85s/it]  5%|         | 295/6500 [33:41<11:40:17,  6.77s/it]                                                       5%|         | 295/6500 [33:41<11:40:17,  6.77s/it]  5%|         | 296/6500 [33:47<11:34:04,  6.71s/it]                                                       5%|         | 296/6500 [33:47<11:34:04,  6.71s/it]  5%|         | 29{'loss': 0.9864, 'learning_rate': 9.949232233020653e-05, 'epoch': 0.05}
{'loss': 0.9965, 'learning_rate': 9.948888049998731e-05, 'epoch': 0.05}
{'loss': 0.9945, 'learning_rate': 9.948542710204319e-05, 'epoch': 0.05}
{'loss': 0.9819, 'learning_rate': 9.948196213718135e-05, 'epoch': 0.05}
7/6500 [33:54<11:29:48,  6.67s/it]                                                       5%|         | 297/6500 [33:54<11:29:48,  6.67s/it]  5%|         | 298/6500 [34:00<11:26:41,  6.64s/it]                                                       5%|         | 298/6500 [34:00<11:26:41,  6.64s/it]  5%|         | 299/6500 [34:07<11:24:54,  6.63s/it]                                                       5%|         | 299/6500 [34:07<11:24:54,  6.63s/it]  5%|         | 300/6500 [34:14<11:23:28,  6.61s/it]                                                       5%|         | 300/6500 [34:14<11:23:28,  6.61s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 1.0178793668746948, 'eval_runtime': 1.4942, 'eval_samples_per_second': 8.031, 'eval_steps_per_second': 2.008, 'epoch': 0.05}
                                                       5%|         | 300/6500 [34:15<11:23:28,  6.61s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-300
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-300
/projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-300
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-300
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-300
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-300/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-300/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-300/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-300/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 1.0114, 'learning_rate': 9.947848560621172e-05, 'epoch': 0.05}
{'loss': 0.9843, 'learning_rate': 9.94749975099469e-05, 'epoch': 0.05}
{'loss': 1.2573, 'learning_rate': 9.947149784920225e-05, 'epoch': 0.05}
{'loss': 0.9604, 'learning_rate': 9.946798662479577e-05, 'epoch': 0.05}
{'loss': 0.9923, 'learning_rate': 9.946446383754817e-05, 'epoch': 0.05}
{'loss': 0.9754, 'learning_rate': 9.946092948828289e-05, 'epoch': 0.05}
  5%|         | 301/6500 [34:22<12:16:44,  7.13s/it]                                                       5%|         | 301/6500 [34:22<12:16:44,  7.13s/it]  5%|         | 302/6500 [34:29<11:59:14,  6.96s/it]                                                       5%|         | 302/6500 [34:29<11:59:14,  6.96s/it]  5%|         | 303/6500 [34:35<11:47:27,  6.85s/it]                                                       5%|         | 303/6500 [34:35<11:47:27,  6.85s/it]  5%|         | 304/6500 [34:42<11:38:51,  6.77s/it]                                                       5%|         | 304/6500 [34:42<11:38:51,  6.77s/it]  5%|         | 305/6500 [34:48<11:32:48,  6.71s/it]                                                       5%|         | 305/6500 [34:48<11:32:48,  6.71s/it]  5%|         | 306/6500 [34:55<11:28:36,  6.67s/it]                                                       5%|         | 306/6500 [34:55<11:28:36,  6.67s/it]  5%|         | 30{'loss': 0.9637, 'learning_rate': 9.94573835778261e-05, 'epoch': 0.05}
{'loss': 0.9638, 'learning_rate': 9.945382610700657e-05, 'epoch': 0.05}
{'loss': 1.0155, 'learning_rate': 9.94502570766559e-05, 'epoch': 0.05}
{'loss': 1.0117, 'learning_rate': 9.944667648760828e-05, 'epoch': 0.05}
7/6500 [35:02<11:51:59,  6.90s/it]                                                       5%|         | 307/6500 [35:02<11:51:59,  6.90s/it]  5%|         | 308/6500 [35:09<11:42:00,  6.80s/it]                                                       5%|         | 308/6500 [35:09<11:42:00,  6.80s/it]  5%|         | 309/6500 [35:15<11:35:08,  6.74s/it]                                                       5%|         | 309/6500 [35:15<11:35:08,  6.74s/it]  5%|         | 310/6500 [35:22<11:30:28,  6.69s/it]                                                       5%|         | 310/6500 [35:22<11:30:28,  6.69s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 1.0141223669052124, 'eval_runtime': 1.492, 'eval_samples_per_second': 8.043, 'eval_steps_per_second': 2.011, 'epoch': 0.05}
                                                       5%|         | 310/6500 [35:24<11:30:28,  6.69s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-310
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-310
/projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-310
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-310
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-310
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-310/pytorch_model.bin
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-310/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-310/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-310/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.9993, 'learning_rate': 9.944308434070069e-05, 'epoch': 0.05}
{'loss': 0.9566, 'learning_rate': 9.943948063677274e-05, 'epoch': 0.05}
{'loss': 0.9977, 'learning_rate': 9.94358653766668e-05, 'epoch': 0.05}
{'loss': 0.9812, 'learning_rate': 9.943223856122788e-05, 'epoch': 0.05}
{'loss': 0.9931, 'learning_rate': 9.942860019130377e-05, 'epoch': 0.05}
{'loss': 0.9762, 'learning_rate': 9.942495026774489e-05, 'epoch': 0.05}
  5%|         | 311/6500 [35:30<12:22:57,  7.20s/it]                                                       5%|         | 311/6500 [35:30<12:22:57,  7.20s/it]  5%|         | 312/6500 [35:37<12:04:00,  7.02s/it]                                                       5%|         | 312/6500 [35:37<12:04:00,  7.02s/it]  5%|         | 313/6500 [35:44<11:49:56,  6.88s/it]                                                       5%|         | 313/6500 [35:44<11:49:56,  6.88s/it]  5%|         | 314/6500 [35:50<11:40:12,  6.79s/it]                                                       5%|         | 314/6500 [35:50<11:40:12,  6.79s/it]  5%|         | 315/6500 [35:57<11:33:07,  6.72s/it]                                                       5%|         | 315/6500 [35:57<11:33:07,  6.72s/it]  5%|         | 316/6500 [36:03<11:28:25,  6.68s/it]                                                       5%|         | 316/6500 [36:03<11:28:25,  6.68s/it]  5%|         | 31{'loss': 0.9929, 'learning_rate': 9.94212887914044e-05, 'epoch': 0.05}
{'loss': 1.2393, 'learning_rate': 9.941761576313812e-05, 'epoch': 0.05}
{'loss': 0.9552, 'learning_rate': 9.941393118380466e-05, 'epoch': 0.05}
{'loss': 0.9919, 'learning_rate': 9.94102350542652e-05, 'epoch': 0.05}
7/6500 [36:10<11:25:00,  6.65s/it]                                                       5%|         | 317/6500 [36:10<11:25:00,  6.65s/it]  5%|         | 318/6500 [36:16<11:23:14,  6.63s/it]                                                       5%|         | 318/6500 [36:16<11:23:14,  6.63s/it]  5%|         | 319/6500 [36:23<11:21:17,  6.61s/it]                                                       5%|         | 319/6500 [36:23<11:21:17,  6.61s/it]  5%|         | 320/6500 [36:30<11:19:39,  6.60s/it]                                                       5%|         | 320/6500 [36:30<11:19:39,  6.60s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 1.0101954936981201, 'eval_runtime': 1.4862, 'eval_samples_per_second': 8.074, 'eval_steps_per_second': 2.019, 'epoch': 0.05}
                                                       5%|         | 320/6500 [36:31<11:19:39,  6.60s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-320
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-320
/projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-320
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-320
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-320
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-320/pytorch_model.bin
 the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-320/pytorch_model.bin
/projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-320/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-320/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.9452, 'learning_rate': 9.940652737538372e-05, 'epoch': 0.05}
{'loss': 0.9437, 'learning_rate': 9.940280814802685e-05, 'epoch': 0.05}
{'loss': 0.9825, 'learning_rate': 9.939907737306397e-05, 'epoch': 0.05}
{'loss': 0.9909, 'learning_rate': 9.939533505136708e-05, 'epoch': 0.05}
{'loss': 1.0082, 'learning_rate': 9.939158118381098e-05, 'epoch': 0.05}
{'loss': 0.9897, 'learning_rate': 9.938781577127306e-05, 'epoch': 0.05}
  5%|         | 321/6500 [36:38<12:13:42,  7.12s/it]                                                       5%|         | 321/6500 [36:38<12:13:42,  7.12s/it]  5%|         | 322/6500 [36:45<11:57:08,  6.96s/it]                                                       5%|         | 322/6500 [36:45<11:57:08,  6.96s/it]  5%|         | 323/6500 [36:51<11:45:21,  6.85s/it]                                                       5%|         | 323/6500 [36:51<11:45:21,  6.85s/it]  5%|         | 324/6500 [36:58<11:55:37,  6.95s/it]                                                       5%|         | 324/6500 [36:58<11:55:37,  6.95s/it]  5%|         | 325/6500 [37:05<11:44:26,  6.84s/it]                                                       5%|         | 325/6500 [37:05<11:44:26,  6.84s/it]  5%|         | 326/6500 [37:12<11:36:06,  6.76s/it]                                                       5%|         | 326/6500 [37:12<11:36:06,  6.76s/it]  5%|         | 32{'loss': 0.9404, 'learning_rate': 9.93840388146335e-05, 'epoch': 0.05}
{'loss': 0.9844, 'learning_rate': 9.938025031477512e-05, 'epoch': 0.05}
{'loss': 0.9626, 'learning_rate': 9.937645027258347e-05, 'epoch': 0.05}
{'loss': 1.0011, 'learning_rate': 9.937263868894678e-05, 'epoch': 0.05}
7/6500 [37:18<11:30:38,  6.71s/it]                                                       5%|         | 327/6500 [37:18<11:30:38,  6.71s/it]  5%|         | 328/6500 [37:25<11:26:29,  6.67s/it]                                                       5%|         | 328/6500 [37:25<11:26:29,  6.67s/it]  5%|         | 329/6500 [37:31<11:23:46,  6.65s/it]                                                       5%|         | 329/6500 [37:31<11:23:46,  6.65s/it]  5%|         | 330/6500 [37:38<11:21:40,  6.63s/it]                                                       5%|         | 330/6500 [37:38<11:21:40,  6.63s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 1.0056570768356323, 'eval_runtime': 1.7349, 'eval_samples_per_second': 6.917, 'eval_steps_per_second': 1.729, 'epoch': 0.05}
                                                       5%|         | 330/6500 [37:40<11:21:40,  6.63s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-330
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-330
 /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-330
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-330
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-330
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-330/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-330/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-330/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-330/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.9656, 'learning_rate': 9.936881556475599e-05, 'epoch': 0.05}
{'loss': 1.2339, 'learning_rate': 9.936498090090474e-05, 'epoch': 0.05}
{'loss': 0.9808, 'learning_rate': 9.936113469828933e-05, 'epoch': 0.05}
{'loss': 0.9351, 'learning_rate': 9.935727695780881e-05, 'epoch': 0.05}
{'loss': 0.9813, 'learning_rate': 9.93534076803649e-05, 'epoch': 0.05}
{'loss': 0.9338, 'learning_rate': 9.934952686686201e-05, 'epoch': 0.05}
  5%|         | 331/6500 [37:46<12:22:21,  7.22s/it]                                                       5%|         | 331/6500 [37:46<12:22:21,  7.22s/it]  5%|         | 332/6500 [37:53<12:02:08,  7.02s/it]                                                       5%|         | 332/6500 [37:53<12:02:08,  7.02s/it]  5%|         | 333/6500 [38:00<11:48:54,  6.90s/it]                                                       5%|         | 333/6500 [38:00<11:48:54,  6.90s/it]  5%|         | 334/6500 [38:06<11:39:13,  6.80s/it]                                                       5%|         | 334/6500 [38:06<11:39:13,  6.80s/it]  5%|         | 335/6500 [38:13<11:32:26,  6.74s/it]                                                       5%|         | 335/6500 [38:13<11:32:26,  6.74s/it]  5%|         | 336/6500 [38:19<11:27:37,  6.69s/it]                                                       5%|         | 336/6500 [38:19<11:27:37,  6.69s/it]  5%|         | 33{'loss': 0.9419, 'learning_rate': 9.934563451820728e-05, 'epoch': 0.05}
{'loss': 0.9756, 'learning_rate': 9.93417306353105e-05, 'epoch': 0.05}
{'loss': 0.9749, 'learning_rate': 9.933781521908419e-05, 'epoch': 0.05}
{'loss': 0.991, 'learning_rate': 9.933388827044355e-05, 'epoch': 0.05}
7/6500 [38:26<11:24:16,  6.66s/it]                                                       5%|         | 337/6500 [38:26<11:24:16,  6.66s/it]  5%|         | 338/6500 [38:33<11:21:56,  6.64s/it]                                                       5%|         | 338/6500 [38:33<11:21:56,  6.64s/it]  5%|         | 339/6500 [38:39<11:20:34,  6.63s/it]                                                       5%|         | 339/6500 [38:39<11:20:34,  6.63s/it]  5%|         | 340/6500 [38:46<11:41:29,  6.83s/it]                                                       5%|         | 340/6500 [38:46<11:41:29,  6.83s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 1.0018641948699951, 'eval_runtime': 1.5101, 'eval_samples_per_second': 7.947, 'eval_steps_per_second': 1.987, 'epoch': 0.05}
                                                       5%|         | 340/6500 [38:48<11:41:29,  6.83s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-340
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  I AM HERE AND I AM SAVING THE MODEL
/projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-340the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-340

I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-340
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-340
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-340/pytorch_model.binthe pytorch model path is
 /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-340/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-340/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-340/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.9713, 'learning_rate': 9.932994979030647e-05, 'epoch': 0.05}
{'loss': 0.9464, 'learning_rate': 9.932599977959356e-05, 'epoch': 0.05}
{'loss': 0.97, 'learning_rate': 9.932203823922812e-05, 'epoch': 0.05}
{'loss': 0.95, 'learning_rate': 9.931806517013612e-05, 'epoch': 0.05}
{'loss': 0.9986, 'learning_rate': 9.931408057324625e-05, 'epoch': 0.05}
{'loss': 0.9561, 'learning_rate': 9.931008444948988e-05, 'epoch': 0.05}
  5%|         | 341/6500 [38:55<12:29:22,  7.30s/it]                                                       5%|         | 341/6500 [38:55<12:29:22,  7.30s/it]  5%|         | 342/6500 [39:01<12:06:55,  7.08s/it]                                                       5%|         | 342/6500 [39:01<12:06:55,  7.08s/it]  5%|         | 343/6500 [39:08<11:51:19,  6.93s/it]                                                       5%|         | 343/6500 [39:08<11:51:19,  6.93s/it]  5%|         | 344/6500 [39:15<11:40:27,  6.83s/it]                                                       5%|         | 344/6500 [39:15<11:40:27,  6.83s/it]  5%|         | 345/6500 [39:21<11:32:52,  6.75s/it]                                                       5%|         | 345/6500 [39:21<11:32:52,  6.75s/it]  5%|         | 346/6500 [39:28<11:27:26,  6.70s/it]                                                       5%|         | 346/6500 [39:28<11:27:26,  6.70s/it]  5%|         | 34{'loss': 1.219, 'learning_rate': 9.930607679980107e-05, 'epoch': 0.05}
{'loss': 0.9735, 'learning_rate': 9.93020576251166e-05, 'epoch': 0.05}
{'loss': 0.918, 'learning_rate': 9.929802692637593e-05, 'epoch': 0.05}
{'loss': 0.9777, 'learning_rate': 9.929398470452118e-05, 'epoch': 0.05}
7/6500 [39:34<11:23:14,  6.66s/it]                                                       5%|         | 347/6500 [39:34<11:23:14,  6.66s/it]  5%|         | 348/6500 [39:41<11:20:49,  6.64s/it]                                                       5%|         | 348/6500 [39:41<11:20:49,  6.64s/it]  5%|         | 349/6500 [39:48<11:19:29,  6.63s/it]                                                       5%|         | 349/6500 [39:48<11:19:29,  6.63s/it]  5%|         | 350/6500 [39:54<11:17:52,  6.61s/it]                                                       5%|         | 350/6500 [39:54<11:17:52,  6.61s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.9978129267692566, 'eval_runtime': 1.7323, 'eval_samples_per_second': 6.927, 'eval_steps_per_second': 1.732, 'epoch': 0.05}
                                                       5%|         | 350/6500 [39:56<11:17:52,  6.61s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-350
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in   /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-350
/projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-350
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-350
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-350
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-350/pytorch_model.bin
the pytorch model path is the pytorch model path is/projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-350/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-350/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-350/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.9294, 'learning_rate': 9.928993096049724e-05, 'epoch': 0.05}
{'loss': 0.9368, 'learning_rate': 9.928586569525162e-05, 'epoch': 0.05}
{'loss': 0.9801, 'learning_rate': 9.928178890973455e-05, 'epoch': 0.05}
{'loss': 0.9787, 'learning_rate': 9.927770060489897e-05, 'epoch': 0.05}
{'loss': 0.9719, 'learning_rate': 9.927360078170048e-05, 'epoch': 0.05}
{'loss': 0.9609, 'learning_rate': 9.92694894410974e-05, 'epoch': 0.05}
  5%|         | 351/6500 [40:03<12:18:21,  7.20s/it]                                                       5%|         | 351/6500 [40:03<12:18:21,  7.20s/it]  5%|         | 352/6500 [40:09<11:59:21,  7.02s/it]                                                       5%|         | 352/6500 [40:09<11:59:21,  7.02s/it]  5%|         | 353/6500 [40:16<11:46:10,  6.89s/it]                                                       5%|         | 353/6500 [40:16<11:46:10,  6.89s/it]  5%|         | 354/6500 [40:22<11:37:03,  6.80s/it]                                                       5%|         | 354/6500 [40:22<11:37:03,  6.80s/it]  5%|         | 355/6500 [40:29<11:30:21,  6.74s/it]                                                       5%|         | 355/6500 [40:29<11:30:21,  6.74s/it]  5%|         | 356/6500 [40:36<11:44:07,  6.88s/it]                                                       5%|         | 356/6500 [40:36<11:44:07,  6.88s/it]  5%|         | 35{'loss': 0.9485, 'learning_rate': 9.926536658405072e-05, 'epoch': 0.05}
{'loss': 0.9671, 'learning_rate': 9.926123221152415e-05, 'epoch': 0.06}
{'loss': 0.9538, 'learning_rate': 9.925708632448405e-05, 'epoch': 0.06}
{'loss': 0.9915, 'learning_rate': 9.925292892389953e-05, 'epoch': 0.06}
7/6500 [40:43<11:35:14,  6.79s/it]                                                       5%|         | 357/6500 [40:43<11:35:14,  6.79s/it]  6%|         | 358/6500 [40:49<11:28:49,  6.73s/it]                                                       6%|         | 358/6500 [40:49<11:28:49,  6.73s/it]  6%|         | 359/6500 [40:56<11:24:20,  6.69s/it]                                                       6%|         | 359/6500 [40:56<11:24:20,  6.69s/it]  6%|         | 360/6500 [41:03<11:21:39,  6.66s/it]                                                       6%|         | 360/6500 [41:03<11:21:39,  6.66s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.9942463040351868, 'eval_runtime': 1.4977, 'eval_samples_per_second': 8.012, 'eval_steps_per_second': 2.003, 'epoch': 0.06}
                                                       6%|         | 360/6500 [41:04<11:21:39,  6.66s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-360
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-360
 /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-360
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-360
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-360
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-360/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-360/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-360/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-360/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.953, 'learning_rate': 9.924876001074231e-05, 'epoch': 0.06}
{'loss': 1.2054, 'learning_rate': 9.92445795859869e-05, 'epoch': 0.06}
{'loss': 0.9587, 'learning_rate': 9.924038765061042e-05, 'epoch': 0.06}
{'loss': 0.9421, 'learning_rate': 9.923618420559268e-05, 'epoch': 0.06}
{'loss': 0.9424, 'learning_rate': 9.923196925191629e-05, 'epoch': 0.06}
{'loss': 0.9326, 'learning_rate': 9.922774279056639e-05, 'epoch': 0.06}
  6%|         | 361/6500 [41:11<12:16:08,  7.19s/it]                                                       6%|         | 361/6500 [41:11<12:16:08,  7.19s/it]  6%|         | 362/6500 [41:18<11:57:14,  7.01s/it]                                                       6%|         | 362/6500 [41:18<11:57:14,  7.01s/it]  6%|         | 363/6500 [41:24<11:43:49,  6.88s/it]                                                       6%|         | 363/6500 [41:24<11:43:49,  6.88s/it]  6%|         | 364/6500 [41:31<11:34:24,  6.79s/it]                                                       6%|         | 364/6500 [41:31<11:34:24,  6.79s/it]  6%|         | 365/6500 [41:37<11:28:20,  6.73s/it]                                                       6%|         | 365/6500 [41:37<11:28:20,  6.73s/it]  6%|         | 366/6500 [41:44<11:24:01,  6.69s/it]                                                       6%|         | 366/6500 [41:44<11:24:01,  6.69s/it]  6%|         | 36{'loss': 0.9357, 'learning_rate': 9.922350482253093e-05, 'epoch': 0.06}
{'loss': 0.9679, 'learning_rate': 9.921925534880051e-05, 'epoch': 0.06}
{'loss': 0.9723, 'learning_rate': 9.921499437036841e-05, 'epoch': 0.06}
{'loss': 0.9774, 'learning_rate': 9.92107218882306e-05, 'epoch': 0.06}
7/6500 [41:51<11:20:30,  6.66s/it]                                                       6%|         | 367/6500 [41:51<11:20:30,  6.66s/it]  6%|         | 368/6500 [41:57<11:17:55,  6.63s/it]                                                       6%|         | 368/6500 [41:57<11:17:55,  6.63s/it]  6%|         | 369/6500 [42:04<11:16:27,  6.62s/it]                                                       6%|         | 369/6500 [42:04<11:16:27,  6.62s/it]  6%|         | 370/6500 [42:10<11:15:05,  6.61s/it]                                                       6%|         | 370/6500 [42:10<11:15:05,  6.61s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.9902186989784241, 'eval_runtime': 1.4839, 'eval_samples_per_second': 8.087, 'eval_steps_per_second': 2.022, 'epoch': 0.06}
                                                       6%|         | 370/6500 [42:12<11:15:05,  6.61s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-370
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  I AM HERE AND I AM SAVING THE MODEL
/projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-370the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-370

I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-370
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-370
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-370/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-370/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-370/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-370/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.9357, 'learning_rate': 9.920643790338575e-05, 'epoch': 0.06}
{'loss': 0.9554, 'learning_rate': 9.920214241683523e-05, 'epoch': 0.06}
{'loss': 0.9476, 'learning_rate': 9.919783542958308e-05, 'epoch': 0.06}
{'loss': 0.9448, 'learning_rate': 9.9193516942636e-05, 'epoch': 0.06}
{'loss': 0.9653, 'learning_rate': 9.918918695700348e-05, 'epoch': 0.06}
{'loss': 0.9474, 'learning_rate': 9.918484547369755e-05, 'epoch': 0.06}
  6%|         | 371/6500 [42:19<12:09:55,  7.15s/it]                                                       6%|         | 371/6500 [42:19<12:09:55,  7.15s/it]  6%|         | 372/6500 [42:26<12:18:51,  7.23s/it]                                                       6%|         | 372/6500 [42:26<12:18:51,  7.23s/it]  6%|         | 373/6500 [42:33<11:58:49,  7.04s/it]                                                       6%|         | 373/6500 [42:33<11:58:49,  7.04s/it]  6%|         | 374/6500 [42:39<11:45:19,  6.91s/it]                                                       6%|         | 374/6500 [42:39<11:45:19,  6.91s/it]  6%|         | 375/6500 [42:46<11:35:17,  6.81s/it]                                                       6%|         | 375/6500 [42:46<11:35:17,  6.81s/it]  6%|         | 376/6500 [42:53<11:28:26,  6.74s/it]                                                       6%|         | 376/6500 [42:53<11:28:26,  6.74s/it]  6%|         | 37{'loss': 1.219, 'learning_rate': 9.918049249373305e-05, 'epoch': 0.06}
{'loss': 0.9284, 'learning_rate': 9.917612801812744e-05, 'epoch': 0.06}
{'loss': 0.949, 'learning_rate': 9.917175204790093e-05, 'epoch': 0.06}
{'loss': 0.923, 'learning_rate': 9.916736458407632e-05, 'epoch': 0.06}
7/6500 [42:59<11:22:53,  6.69s/it]                                                       6%|         | 377/6500 [42:59<11:22:53,  6.69s/it]  6%|         | 378/6500 [43:06<11:19:24,  6.66s/it]                                                       6%|         | 378/6500 [43:06<11:19:24,  6.66s/it]  6%|         | 379/6500 [43:12<11:16:50,  6.63s/it]                                                       6%|         | 379/6500 [43:12<11:16:50,  6.63s/it]  6%|         | 380/6500 [43:19<11:15:19,  6.62s/it]                                                       6%|         | 380/6500 [43:19<11:15:19,  6.62s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.9890955090522766, 'eval_runtime': 1.4867, 'eval_samples_per_second': 8.071, 'eval_steps_per_second': 2.018, 'epoch': 0.06}
                                                       6%|         | 380/6500 [43:20<11:15:19,  6.62s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-380
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in I AM HERE AND I AM SAVING THE MODEL 
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-380
/projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-380
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-380
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-380
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-380/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-380/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-380/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-380/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.9132, 'learning_rate': 9.91629656276792e-05, 'epoch': 0.06}
{'loss': 0.9175, 'learning_rate': 9.915855517973776e-05, 'epoch': 0.06}
{'loss': 0.9667, 'learning_rate': 9.915413324128295e-05, 'epoch': 0.06}
{'loss': 0.9563, 'learning_rate': 9.914969981334834e-05, 'epoch': 0.06}
{'loss': 0.957, 'learning_rate': 9.914525489697026e-05, 'epoch': 0.06}
{'loss': 0.9065, 'learning_rate': 9.914079849318764e-05, 'epoch': 0.06}
  6%|         | 381/6500 [43:27<12:07:38,  7.13s/it]                                                       6%|         | 381/6500 [43:27<12:07:38,  7.13s/it]  6%|         | 382/6500 [43:34<11:50:40,  6.97s/it]                                                       6%|         | 382/6500 [43:34<11:50:40,  6.97s/it]  6%|         | 383/6500 [43:40<11:38:46,  6.85s/it]                                                       6%|         | 383/6500 [43:40<11:38:46,  6.85s/it]  6%|         | 384/6500 [43:47<11:30:44,  6.78s/it]                                                       6%|         | 384/6500 [43:47<11:30:44,  6.78s/it]  6%|         | 385/6500 [43:54<11:24:49,  6.72s/it]                                                       6%|         | 385/6500 [43:54<11:24:49,  6.72s/it]  6%|         | 386/6500 [44:00<11:20:35,  6.68s/it]                                                       6%|         | 386/6500 [44:00<11:20:35,  6.68s/it]  6%|         | 38{'loss': 0.9628, 'learning_rate': 9.913633060304214e-05, 'epoch': 0.06}
{'loss': 0.9499, 'learning_rate': 9.913185122757814e-05, 'epoch': 0.06}
{'loss': 0.9434, 'learning_rate': 9.912736036784264e-05, 'epoch': 0.06}
{'loss': 0.9495, 'learning_rate': 9.912285802488534e-05, 'epoch': 0.06}
7/6500 [44:07<11:17:18,  6.65s/it]                                                       6%|         | 387/6500 [44:07<11:17:18,  6.65s/it]  6%|         | 388/6500 [44:14<11:42:19,  6.89s/it]                                                       6%|         | 388/6500 [44:14<11:42:19,  6.89s/it]  6%|         | 389/6500 [44:21<11:32:40,  6.80s/it]                                                       6%|         | 389/6500 [44:21<11:32:40,  6.80s/it]  6%|         | 390/6500 [44:27<11:26:02,  6.74s/it]                                                       6%|         | 390/6500 [44:27<11:26:02,  6.74s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.9835798740386963, 'eval_runtime': 1.4868, 'eval_samples_per_second': 8.071, 'eval_steps_per_second': 2.018, 'epoch': 0.06}
                                                       6%|         | 390/6500 [44:29<11:26:02,  6.74s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-390
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-390
/projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-390
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-390
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-390
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-390/pytorch_model.bin
the pytorch model path is the pytorch model path is/projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-390/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-390/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-390/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.9432, 'learning_rate': 9.911834419975866e-05, 'epoch': 0.06}
{'loss': 1.2051, 'learning_rate': 9.911381889351765e-05, 'epoch': 0.06}
{'loss': 0.9122, 'learning_rate': 9.91092821072201e-05, 'epoch': 0.06}
{'loss': 0.9519, 'learning_rate': 9.910473384192647e-05, 'epoch': 0.06}
{'loss': 0.9122, 'learning_rate': 9.910017409869984e-05, 'epoch': 0.06}
{'loss': 0.9121, 'learning_rate': 9.909560287860606e-05, 'epoch': 0.06}
  6%|         | 391/6500 [44:36<12:16:53,  7.24s/it]                                                       6%|         | 391/6500 [44:36<12:16:53,  7.24s/it]  6%|         | 392/6500 [44:42<11:56:40,  7.04s/it]                                                       6%|         | 392/6500 [44:42<11:56:40,  7.04s/it]  6%|         | 393/6500 [44:49<11:42:37,  6.90s/it]                                                       6%|         | 393/6500 [44:49<11:42:37,  6.90s/it]  6%|         | 394/6500 [44:55<11:32:49,  6.81s/it]                                                       6%|         | 394/6500 [44:55<11:32:49,  6.81s/it]  6%|         | 395/6500 [45:02<11:26:01,  6.74s/it]                                                       6%|         | 395/6500 [45:02<11:26:01,  6.74s/it]  6%|         | 396/6500 [45:09<11:21:20,  6.70s/it]                                                       6%|         | 396/6500 [45:09<11:21:20,  6.70s/it]  6%|         | 39{'loss': 0.932, 'learning_rate': 9.90910201827136e-05, 'epoch': 0.06}
{'loss': 0.9458, 'learning_rate': 9.908642601209366e-05, 'epoch': 0.06}
{'loss': 0.9681, 'learning_rate': 9.908182036782009e-05, 'epoch': 0.06}
{'loss': 0.9635, 'learning_rate': 9.907720325096943e-05, 'epoch': 0.06}
7/6500 [45:15<11:17:58,  6.67s/it]                                                       6%|         | 397/6500 [45:15<11:17:58,  6.67s/it]  6%|         | 398/6500 [45:22<11:15:28,  6.64s/it]                                                       6%|         | 398/6500 [45:22<11:15:28,  6.64s/it]  6%|         | 399/6500 [45:28<11:14:06,  6.63s/it]                                                       6%|         | 399/6500 [45:28<11:14:06,  6.63s/it]  6%|         | 400/6500 [45:35<11:12:32,  6.62s/it]                                                       6%|         | 400/6500 [45:35<11:12:32,  6.62s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.9806891083717346, 'eval_runtime': 1.4871, 'eval_samples_per_second': 8.07, 'eval_steps_per_second': 2.017, 'epoch': 0.06}
                                                       6%|         | 400/6500 [45:37<11:12:32,  6.62s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-400
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-400
 /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-400
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-400
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-400
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-400/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-400/pytorch_model.bin
the pytorch model path is the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-400/pytorch_model.bin
/projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-400/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.9007, 'learning_rate': 9.90725746626209e-05, 'epoch': 0.06}
{'loss': 0.9481, 'learning_rate': 9.906793460385642e-05, 'epoch': 0.06}
{'loss': 0.9288, 'learning_rate': 9.906328307576056e-05, 'epoch': 0.06}
{'loss': 0.9594, 'learning_rate': 9.905862007942058e-05, 'epoch': 0.06}
{'loss': 0.9131, 'learning_rate': 9.905394561592645e-05, 'epoch': 0.06}
{'loss': 1.2, 'learning_rate': 9.904925968637078e-05, 'epoch': 0.06}
  6%|         | 401/6500 [45:43<12:06:39,  7.15s/it]                                                       6%|         | 401/6500 [45:43<12:06:39,  7.15s/it]  6%|         | 402/6500 [45:50<11:49:03,  6.98s/it]                                                       6%|         | 402/6500 [45:50<11:49:03,  6.98s/it]  6%|         | 403/6500 [45:57<11:36:42,  6.86s/it]                                                       6%|         | 403/6500 [45:57<11:36:42,  6.86s/it]  6%|         | 404/6500 [46:04<11:54:51,  7.04s/it]                                                       6%|         | 404/6500 [46:04<11:54:51,  7.04s/it]  6%|         | 405/6500 [46:11<11:41:13,  6.90s/it]                                                       6%|         | 405/6500 [46:11<11:41:13,  6.90s/it]  6%|         | 406/6500 [46:17<11:31:07,  6.80s/it]                                                       6%|         | 406/6500 [46:17<11:31:07,  6.80s/it]  6%|         | 40{'loss': 0.9423, 'learning_rate': 9.904456229184887e-05, 'epoch': 0.06}
{'loss': 0.8988, 'learning_rate': 9.903985343345873e-05, 'epoch': 0.06}
{'loss': 0.9416, 'learning_rate': 9.903513311230104e-05, 'epoch': 0.06}
{'loss': 0.891, 'learning_rate': 9.90304013294791e-05, 'epoch': 0.06}
7/6500 [46:24<11:24:14,  6.74s/it]                                                       6%|         | 407/6500 [46:24<11:24:14,  6.74s/it]  6%|         | 408/6500 [46:30<11:19:20,  6.69s/it]                                                       6%|         | 408/6500 [46:30<11:19:20,  6.69s/it]  6%|         | 409/6500 [46:37<11:15:41,  6.66s/it]                                                       6%|         | 409/6500 [46:37<11:15:41,  6.66s/it]  6%|         | 410/6500 [46:44<11:13:15,  6.63s/it]                                                       6%|         | 410/6500 [46:44<11:13:15,  6.63s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.9770160913467407, 'eval_runtime': 1.4887, 'eval_samples_per_second': 8.061, 'eval_steps_per_second': 2.015, 'epoch': 0.06}
                                                       6%|         | 410/6500 [46:45<11:13:15,  6.63s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-410
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-410
/projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-410
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-410
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-410
the pytorch model path isthe pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-410/pytorch_model.bin 
/projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-410/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-410/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-410/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.9043, 'learning_rate': 9.902565808609896e-05, 'epoch': 0.06}
{'loss': 0.9334, 'learning_rate': 9.902090338326932e-05, 'epoch': 0.06}
{'loss': 0.9277, 'learning_rate': 9.901613722210158e-05, 'epoch': 0.06}
{'loss': 0.9579, 'learning_rate': 9.901135960370977e-05, 'epoch': 0.06}
{'loss': 0.9333, 'learning_rate': 9.900657052921066e-05, 'epoch': 0.06}
{'loss': 0.8916, 'learning_rate': 9.900176999972366e-05, 'epoch': 0.06}
  6%|         | 411/6500 [46:52<12:05:11,  7.15s/it]                                                       6%|         | 411/6500 [46:52<12:05:11,  7.15s/it]  6%|         | 412/6500 [46:58<11:47:51,  6.98s/it]                                                       6%|         | 412/6500 [46:58<11:47:51,  6.98s/it]  6%|         | 413/6500 [47:05<11:35:57,  6.86s/it]                                                       6%|         | 413/6500 [47:05<11:35:57,  6.86s/it]  6%|         | 414/6500 [47:12<11:27:38,  6.78s/it]                                                       6%|         | 414/6500 [47:12<11:27:38,  6.78s/it]  6%|         | 415/6500 [47:18<11:21:47,  6.72s/it]                                                       6%|         | 415/6500 [47:18<11:21:47,  6.72s/it]  6%|         | 416/6500 [47:25<11:17:24,  6.68s/it]                                                       6%|         | 416/6500 [47:25<11:17:24,  6.68s/it]  6%|         | 41{'loss': 0.9399, 'learning_rate': 9.899695801637085e-05, 'epoch': 0.06}
{'loss': 0.9214, 'learning_rate': 9.899213458027701e-05, 'epoch': 0.06}
{'loss': 0.9521, 'learning_rate': 9.898729969256958e-05, 'epoch': 0.06}
{'loss': 0.9173, 'learning_rate': 9.89824533543787e-05, 'epoch': 0.06}
7/6500 [47:31<11:14:19,  6.65s/it]                                                       6%|         | 417/6500 [47:31<11:14:19,  6.65s/it]  6%|         | 418/6500 [47:38<11:12:20,  6.63s/it]                                                       6%|         | 418/6500 [47:38<11:12:20,  6.63s/it]  6%|         | 419/6500 [47:45<11:10:46,  6.62s/it]                                                       6%|         | 419/6500 [47:45<11:10:46,  6.62s/it]  6%|         | 420/6500 [47:51<11:10:04,  6.61s/it]                                                       6%|         | 420/6500 [47:51<11:10:04,  6.61s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.9746017456054688, 'eval_runtime': 1.4887, 'eval_samples_per_second': 8.061, 'eval_steps_per_second': 2.015, 'epoch': 0.06}
                                                       6%|         | 420/6500 [47:53<11:10:04,  6.61s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-420
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-420
/projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-420
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-420
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-420
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-420/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-420/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-420/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-420/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 1.1805, 'learning_rate': 9.897759556683716e-05, 'epoch': 0.06}
{'loss': 0.9374, 'learning_rate': 9.897272633108046e-05, 'epoch': 0.06}
{'loss': 0.8886, 'learning_rate': 9.896784564824673e-05, 'epoch': 0.07}
{'loss': 0.9329, 'learning_rate': 9.896295351947681e-05, 'epoch': 0.07}
{'loss': 0.884, 'learning_rate': 9.895804994591421e-05, 'epoch': 0.07}
{'loss': 0.8951, 'learning_rate': 9.89531349287051e-05, 'epoch': 0.07}
  6%|         | 421/6500 [48:00<12:20:29,  7.31s/it]                                                       6%|         | 421/6500 [48:00<12:20:29,  7.31s/it]  6%|         | 422/6500 [48:07<11:58:49,  7.10s/it]                                                       6%|         | 422/6500 [48:07<11:58:49,  7.10s/it]  7%|         | 423/6500 [48:13<11:43:18,  6.94s/it]                                                       7%|         | 423/6500 [48:13<11:43:18,  6.94s/it]  7%|         | 424/6500 [48:20<11:32:03,  6.83s/it]                                                       7%|         | 424/6500 [48:20<11:32:03,  6.83s/it]  7%|         | 425/6500 [48:26<11:24:20,  6.76s/it]                                                       7%|         | 425/6500 [48:26<11:24:20,  6.76s/it]  7%|         | 426/6500 [48:33<11:18:53,  6.71s/it]                                                       7%|         | 426/6500 [48:33<11:18:53,  6.71s/it]  7%|         | 42{'loss': 0.9374, 'learning_rate': 9.894820846899835e-05, 'epoch': 0.07}
{'loss': 0.9273, 'learning_rate': 9.894327056794547e-05, 'epoch': 0.07}
{'loss': 0.9277, 'learning_rate': 9.893832122670068e-05, 'epoch': 0.07}
{'loss': 0.9199, 'learning_rate': 9.893336044642085e-05, 'epoch': 0.07}
7/6500 [48:40<11:14:35,  6.66s/it]                                                       7%|         | 427/6500 [48:40<11:14:35,  6.66s/it]  7%|         | 428/6500 [48:46<11:11:57,  6.64s/it]                                                       7%|         | 428/6500 [48:46<11:11:57,  6.64s/it]  7%|         | 429/6500 [48:53<11:10:01,  6.62s/it]                                                       7%|         | 429/6500 [48:53<11:10:01,  6.62s/it]  7%|         | 430/6500 [48:59<11:08:56,  6.61s/it]                                                       7%|         | 430/6500 [48:59<11:08:56,  6.61s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.9715170860290527, 'eval_runtime': 1.7501, 'eval_samples_per_second': 6.857, 'eval_steps_per_second': 1.714, 'epoch': 0.07}
                                                       7%|         | 430/6500 [49:01<11:08:56,  6.61s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-430
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-430
 /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-430
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-430
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-430
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-430/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-430/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-430/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-430/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.9042, 'learning_rate': 9.892838822826553e-05, 'epoch': 0.07}
{'loss': 0.9183, 'learning_rate': 9.892340457339695e-05, 'epoch': 0.07}
{'loss': 0.9129, 'learning_rate': 9.891840948298003e-05, 'epoch': 0.07}
{'loss': 0.9467, 'learning_rate': 9.89134029581823e-05, 'epoch': 0.07}
{'loss': 0.9138, 'learning_rate': 9.890838500017403e-05, 'epoch': 0.07}
{'loss': 1.1607, 'learning_rate': 9.890335561012815e-05, 'epoch': 0.07}
  7%|         | 431/6500 [49:08<12:10:25,  7.22s/it]                                                       7%|         | 431/6500 [49:08<12:10:25,  7.22s/it]  7%|         | 432/6500 [49:15<11:51:24,  7.03s/it]                                                       7%|         | 432/6500 [49:15<11:51:24,  7.03s/it]  7%|         | 433/6500 [49:21<11:37:35,  6.90s/it]                                                       7%|         | 433/6500 [49:21<11:37:35,  6.90s/it]  7%|         | 434/6500 [49:28<11:27:59,  6.81s/it]                                                       7%|         | 434/6500 [49:28<11:27:59,  6.81s/it]  7%|         | 435/6500 [49:34<11:21:15,  6.74s/it]                                                       7%|         | 435/6500 [49:34<11:21:15,  6.74s/it]  7%|         | 436/6500 [49:41<11:16:08,  6.69s/it]                                                       7%|         | 436/6500 [49:41<11:16:08,  6.69s/it]  7%|         | 43{'loss': 0.925, 'learning_rate': 9.889831478922023e-05, 'epoch': 0.07}
{'loss': 0.8987, 'learning_rate': 9.889326253862852e-05, 'epoch': 0.07}
{'loss': 0.9019, 'learning_rate': 9.888819885953398e-05, 'epoch': 0.07}
{'loss': 0.8951, 'learning_rate': 9.888312375312019e-05, 'epoch': 0.07}
7/6500 [49:48<11:30:58,  6.84s/it]                                                       7%|         | 437/6500 [49:48<11:30:58,  6.84s/it]  7%|         | 438/6500 [49:55<11:22:53,  6.76s/it]                                                       7%|         | 438/6500 [49:55<11:22:53,  6.76s/it]  7%|         | 439/6500 [50:01<11:17:47,  6.71s/it]                                                       7%|         | 439/6500 [50:01<11:17:47,  6.71s/it]  7%|         | 440/6500 [50:08<11:13:49,  6.67s/it]                                                       7%|         | 440/6500 [50:08<11:13:49,  6.67s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.9678573608398438, 'eval_runtime': 1.5121, 'eval_samples_per_second': 7.936, 'eval_steps_per_second': 1.984, 'epoch': 0.07}
                                                       7%|         | 440/6500 [50:09<11:13:49,  6.67s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-440
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-440
/projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-440
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-440
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-440
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-440/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-440/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-440/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-440/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.8959, 'learning_rate': 9.887803722057344e-05, 'epoch': 0.07}
{'loss': 0.9275, 'learning_rate': 9.887293926308267e-05, 'epoch': 0.07}
{'loss': 0.9308, 'learning_rate': 9.886782988183952e-05, 'epoch': 0.07}
{'loss': 0.9262, 'learning_rate': 9.886270907803823e-05, 'epoch': 0.07}
{'loss': 0.9082, 'learning_rate': 9.88575768528758e-05, 'epoch': 0.07}
{'loss': 0.9151, 'learning_rate': 9.885243320755184e-05, 'epoch': 0.07}
  7%|         | 441/6500 [50:16<12:06:25,  7.19s/it]                                                       7%|         | 441/6500 [50:16<12:06:25,  7.19s/it]  7%|         | 442/6500 [50:23<11:47:21,  7.01s/it]                                                       7%|         | 442/6500 [50:23<11:47:21,  7.01s/it]  7%|         | 443/6500 [50:29<11:34:17,  6.88s/it]                                                       7%|         | 443/6500 [50:29<11:34:17,  6.88s/it]  7%|         | 444/6500 [50:36<11:25:20,  6.79s/it]                                                       7%|         | 444/6500 [50:36<11:25:20,  6.79s/it]  7%|         | 445/6500 [50:43<11:18:51,  6.73s/it]                                                       7%|         | 445/6500 [50:43<11:18:51,  6.73s/it]  7%|         | 446/6500 [50:49<11:14:30,  6.68s/it]                                                       7%|         | 446/6500 [50:49<11:14:30,  6.68s/it]  7%|         | 44{'loss': 0.9043, 'learning_rate': 9.884727814326864e-05, 'epoch': 0.07}
{'loss': 0.912, 'learning_rate': 9.884211166123116e-05, 'epoch': 0.07}
{'loss': 0.9222, 'learning_rate': 9.883693376264707e-05, 'epoch': 0.07}
{'loss': 0.9131, 'learning_rate': 9.883174444872663e-05, 'epoch': 0.07}
7/6500 [50:56<11:11:01,  6.65s/it]                                                       7%|         | 447/6500 [50:56<11:11:01,  6.65s/it]  7%|         | 448/6500 [51:02<11:08:54,  6.63s/it]                                                       7%|         | 448/6500 [51:02<11:08:54,  6.63s/it]  7%|         | 449/6500 [51:09<11:07:23,  6.62s/it]                                                       7%|         | 449/6500 [51:09<11:07:23,  6.62s/it]  7%|         | 450/6500 [51:15<11:06:10,  6.61s/it]                                                       7%|         | 450/6500 [51:15<11:06:10,  6.61s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.9650096297264099, 'eval_runtime': 1.4871, 'eval_samples_per_second': 8.069, 'eval_steps_per_second': 2.017, 'epoch': 0.07}
                                                       7%|         | 450/6500 [51:17<11:06:10,  6.61s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-450
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-450 /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-450

I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-450
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-450
the pytorch model path isthe pytorch model path isthe pytorch model path is  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-450/pytorch_model.bin/projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-450/pytorch_model.bin

 /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-450/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-450/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 1.168, 'learning_rate': 9.882654372068284e-05, 'epoch': 0.07}
{'loss': 0.8959, 'learning_rate': 9.88213315797313e-05, 'epoch': 0.07}
{'loss': 0.9041, 'learning_rate': 9.881610802709036e-05, 'epoch': 0.07}
{'loss': 0.885, 'learning_rate': 9.881087306398097e-05, 'epoch': 0.07}
{'loss': 0.878, 'learning_rate': 9.880562669162677e-05, 'epoch': 0.07}
{'loss': 0.8784, 'learning_rate': 9.880036891125409e-05, 'epoch': 0.07}
  7%|         | 451/6500 [51:24<11:59:04,  7.13s/it]                                                       7%|         | 451/6500 [51:24<11:59:04,  7.13s/it]  7%|         | 452/6500 [51:30<11:42:13,  6.97s/it]                                                       7%|         | 452/6500 [51:30<11:42:13,  6.97s/it]  7%|         | 453/6500 [51:38<11:56:34,  7.11s/it]                                                       7%|         | 453/6500 [51:38<11:56:34,  7.11s/it]  7%|         | 454/6500 [51:44<11:40:35,  6.95s/it]                                                       7%|         | 454/6500 [51:44<11:40:35,  6.95s/it]  7%|         | 455/6500 [51:51<11:28:56,  6.84s/it]                                                       7%|         | 455/6500 [51:51<11:28:56,  6.84s/it]  7%|         | 456/6500 [51:58<11:20:43,  6.76s/it]                                                       7%|         | 456/6500 [51:58<11:20:43,  6.76s/it]  7%|         | 45{'loss': 0.9223, 'learning_rate': 9.879509972409188e-05, 'epoch': 0.07}
{'loss': 0.9209, 'learning_rate': 9.878981913137179e-05, 'epoch': 0.07}
{'loss': 0.9251, 'learning_rate': 9.878452713432813e-05, 'epoch': 0.07}
{'loss': 0.8758, 'learning_rate': 9.877922373419786e-05, 'epoch': 0.07}
7/6500 [52:04<11:14:45,  6.70s/it]                                                       7%|         | 457/6500 [52:04<11:14:45,  6.70s/it]  7%|         | 458/6500 [52:11<11:11:13,  6.67s/it]                                                       7%|         | 458/6500 [52:11<11:11:13,  6.67s/it]  7%|         | 459/6500 [52:17<11:08:26,  6.64s/it]                                                       7%|         | 459/6500 [52:17<11:08:26,  6.64s/it]  7%|         | 460/6500 [52:24<11:06:24,  6.62s/it]                                                       7%|         | 460/6500 [52:24<11:06:24,  6.62s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.9627028107643127, 'eval_runtime': 1.4887, 'eval_samples_per_second': 8.061, 'eval_steps_per_second': 2.015, 'epoch': 0.07}
                                                       7%|         | 460/6500 [52:25<11:06:24,  6.62s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-460
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-460
/projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-460
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-460
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-460
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-460/pytorch_model.bin
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-460/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-460/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-460/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.9152, 'learning_rate': 9.877390893222061e-05, 'epoch': 0.07}
{'loss': 0.9047, 'learning_rate': 9.876858272963871e-05, 'epoch': 0.07}
{'loss': 0.9045, 'learning_rate': 9.876324512769713e-05, 'epoch': 0.07}
{'loss': 0.9124, 'learning_rate': 9.875789612764346e-05, 'epoch': 0.07}
{'loss': 0.9005, 'learning_rate': 9.875253573072804e-05, 'epoch': 0.07}
{'loss': 1.165, 'learning_rate': 9.874716393820383e-05, 'epoch': 0.07}
  7%|         | 461/6500 [52:32<11:57:39,  7.13s/it]                                                       7%|         | 461/6500 [52:32<11:57:39,  7.13s/it]  7%|         | 462/6500 [52:39<11:40:59,  6.97s/it]                                                       7%|         | 462/6500 [52:39<11:40:59,  6.97s/it]  7%|         | 463/6500 [52:45<11:28:59,  6.85s/it]                                                       7%|         | 463/6500 [52:45<11:28:59,  6.85s/it]  7%|         | 464/6500 [52:52<11:20:30,  6.76s/it]                                                       7%|         | 464/6500 [52:52<11:20:30,  6.76s/it]  7%|         | 465/6500 [52:59<11:14:46,  6.71s/it]                                                       7%|         | 465/6500 [52:59<11:14:46,  6.71s/it]  7%|         | 466/6500 [53:05<11:10:36,  6.67s/it]                                                       7%|         | 466/6500 [53:05<11:10:36,  6.67s/it]  7%|         | 46{'loss': 0.8763, 'learning_rate': 9.87417807513264e-05, 'epoch': 0.07}
{'loss': 0.9195, 'learning_rate': 9.87363861713541e-05, 'epoch': 0.07}
{'loss': 0.8681, 'learning_rate': 9.873098019954786e-05, 'epoch': 0.07}
{'loss': 0.8774, 'learning_rate': 9.872556283717125e-05, 'epoch': 0.07}
7/6500 [53:12<11:07:31,  6.64s/it]                                                       7%|         | 467/6500 [53:12<11:07:31,  6.64s/it]  7%|         | 468/6500 [53:18<11:05:16,  6.62s/it]                                                       7%|         | 468/6500 [53:18<11:05:16,  6.62s/it]  7%|         | 469/6500 [53:26<11:30:16,  6.87s/it]                                                       7%|         | 469/6500 [53:26<11:30:16,  6.87s/it]  7%|         | 470/6500 [53:32<11:21:44,  6.78s/it]                                                       7%|         | 470/6500 [53:32<11:21:44,  6.78s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.9595010280609131, 'eval_runtime': 1.4874, 'eval_samples_per_second': 8.068, 'eval_steps_per_second': 2.017, 'epoch': 0.07}
                                                       7%|         | 470/6500 [53:34<11:21:44,  6.78s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-470
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-470I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-470

I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-470
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-470
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-470/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-470/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-470/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-470/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.8805, 'learning_rate': 9.872013408549061e-05, 'epoch': 0.07}
{'loss': 0.9077, 'learning_rate': 9.871469394577484e-05, 'epoch': 0.07}
{'loss': 0.9272, 'learning_rate': 9.870924241929558e-05, 'epoch': 0.07}
{'loss': 0.9064, 'learning_rate': 9.870377950732703e-05, 'epoch': 0.07}
{'loss': 0.8778, 'learning_rate': 9.869830521114616e-05, 'epoch': 0.07}
{'loss': 0.9076, 'learning_rate': 9.869281953203254e-05, 'epoch': 0.07}
  7%|         | 471/6500 [53:41<12:08:56,  7.25s/it]                                                       7%|         | 471/6500 [53:41<12:08:56,  7.25s/it]  7%|         | 472/6500 [53:47<11:48:20,  7.05s/it]                                                       7%|         | 472/6500 [53:47<11:48:20,  7.05s/it]  7%|         | 473/6500 [53:54<11:33:56,  6.91s/it]                                                       7%|         | 473/6500 [53:54<11:33:56,  6.91s/it]  7%|         | 474/6500 [54:00<11:23:48,  6.81s/it]                                                       7%|         | 474/6500 [54:00<11:23:48,  6.81s/it]  7%|         | 475/6500 [54:07<11:16:41,  6.74s/it]                                                       7%|         | 475/6500 [54:07<11:16:41,  6.74s/it]  7%|         | 476/6500 [54:13<11:11:40,  6.69s/it]                                                       7%|         | 476/6500 [54:13<11:11:40,  6.69s/it]  7%|         | 47{'loss': 0.8856, 'learning_rate': 9.86873224712684e-05, 'epoch': 0.07}
{'loss': 0.9235, 'learning_rate': 9.868181403013865e-05, 'epoch': 0.07}
{'loss': 0.8711, 'learning_rate': 9.867629420993086e-05, 'epoch': 0.07}
{'loss': 1.0426, 'learning_rate': 9.867076301193528e-05, 'epoch': 0.07}
7/6500 [54:20<11:08:06,  6.66s/it]                                                       7%|         | 477/6500 [54:20<11:08:06,  6.66s/it]  7%|         | 478/6500 [54:27<11:05:30,  6.63s/it]                                                       7%|         | 478/6500 [54:27<11:05:30,  6.63s/it]  7%|         | 479/6500 [54:33<11:03:58,  6.62s/it]                                                       7%|         | 479/6500 [54:33<11:03:58,  6.62s/it]  7%|         | 480/6500 [54:40<11:02:20,  6.60s/it]                                                       7%|         | 480/6500 [54:40<11:02:20,  6.60s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.9561816453933716, 'eval_runtime': 1.4898, 'eval_samples_per_second': 8.055, 'eval_steps_per_second': 2.014, 'epoch': 0.07}
                                                       7%|         | 480/6500 [54:41<11:02:20,  6.60s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-480
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-480
 /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-480
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-480
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-480
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-480/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-480/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-480/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-480/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 1.0218, 'learning_rate': 9.866522043744475e-05, 'epoch': 0.07}
{'loss': 0.8578, 'learning_rate': 9.865966648775483e-05, 'epoch': 0.07}
{'loss': 0.916, 'learning_rate': 9.865410116416374e-05, 'epoch': 0.07}
{'loss': 0.8574, 'learning_rate': 9.86485244679723e-05, 'epoch': 0.07}
{'loss': 0.8641, 'learning_rate': 9.864293640048407e-05, 'epoch': 0.07}
{'loss': 0.8835, 'learning_rate': 9.863733696300521e-05, 'epoch': 0.07}
  7%|         | 481/6500 [54:48<11:54:20,  7.12s/it]                                                       7%|         | 481/6500 [54:48<11:54:20,  7.12s/it]  7%|         | 482/6500 [54:55<11:37:47,  6.96s/it]                                                       7%|         | 482/6500 [54:55<11:37:47,  6.96s/it]  7%|         | 483/6500 [55:01<11:26:16,  6.84s/it]                                                       7%|         | 483/6500 [55:01<11:26:16,  6.84s/it]  7%|         | 484/6500 [55:08<11:18:13,  6.76s/it]                                                       7%|         | 484/6500 [55:08<11:18:13,  6.76s/it]  7%|         | 485/6500 [55:15<11:37:23,  6.96s/it]                                                       7%|         | 485/6500 [55:15<11:37:23,  6.96s/it]  7%|         | 486/6500 [55:22<11:25:58,  6.84s/it]                                                       7%|         | 486/6500 [55:22<11:25:58,  6.84s/it]  7%|         | 48{'loss': 0.8927, 'learning_rate': 9.863172615684455e-05, 'epoch': 0.07}
{'loss': 0.93, 'learning_rate': 9.86261039833136e-05, 'epoch': 0.08}
{'loss': 0.9008, 'learning_rate': 9.862047044372648e-05, 'epoch': 0.08}
{'loss': 0.8542, 'learning_rate': 9.861482553940003e-05, 'epoch': 0.08}
7/6500 [55:28<11:17:59,  6.77s/it]                                                       7%|         | 487/6500 [55:28<11:17:59,  6.77s/it]  8%|         | 488/6500 [55:35<11:12:01,  6.71s/it]                                                       8%|         | 488/6500 [55:35<11:12:01,  6.71s/it]  8%|         | 489/6500 [55:42<11:08:14,  6.67s/it]                                                       8%|         | 489/6500 [55:42<11:08:14,  6.67s/it]  8%|         | 490/6500 [55:48<11:05:26,  6.64s/it]                                                       8%|         | 490/6500 [55:48<11:05:26,  6.64s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.9543749690055847, 'eval_runtime': 1.4839, 'eval_samples_per_second': 8.087, 'eval_steps_per_second': 2.022, 'epoch': 0.08}
                                                       8%|         | 490/6500 [55:50<11:05:26,  6.64s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-490
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-490
/projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-490
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-490
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-490
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-490/pytorch_model.binthe pytorch model path isthe pytorch model path is
 /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-490/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-490/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-490/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.8963, 'learning_rate': 9.860916927165366e-05, 'epoch': 0.08}
{'loss': 0.878, 'learning_rate': 9.860350164180954e-05, 'epoch': 0.08}
{'loss': 0.9244, 'learning_rate': 9.859782265119244e-05, 'epoch': 0.08}
{'loss': 0.8739, 'learning_rate': 9.859213230112976e-05, 'epoch': 0.08}
{'loss': 1.1463, 'learning_rate': 9.85864305929516e-05, 'epoch': 0.08}
{'loss': 0.8982, 'learning_rate': 9.85807175279907e-05, 'epoch': 0.08}
  8%|         | 491/6500 [55:56<11:56:20,  7.15s/it]                                                       8%|         | 491/6500 [55:57<11:56:20,  7.15s/it]  8%|         | 492/6500 [56:03<11:38:47,  6.98s/it]                                                       8%|         | 492/6500 [56:03<11:38:47,  6.98s/it]  8%|         | 493/6500 [56:10<11:26:24,  6.86s/it]                                                       8%|         | 493/6500 [56:10<11:26:24,  6.86s/it]  8%|         | 494/6500 [56:16<11:17:40,  6.77s/it]                                                       8%|         | 494/6500 [56:16<11:17:40,  6.77s/it]  8%|         | 495/6500 [56:23<11:11:19,  6.71s/it]                                                       8%|         | 495/6500 [56:23<11:11:19,  6.71s/it]  8%|         | 496/6500 [56:29<11:07:14,  6.67s/it]                                                       8%|         | 496/6500 [56:29<11:07:14,  6.67s/it]  8%|         | 49{'loss': 0.8495, 'learning_rate': 9.857499310758245e-05, 'epoch': 0.08}
{'loss': 0.8956, 'learning_rate': 9.85692573330649e-05, 'epoch': 0.08}
{'loss': 0.8489, 'learning_rate': 9.856351020577876e-05, 'epoch': 0.08}
{'loss': 0.8561, 'learning_rate': 9.855775172706738e-05, 'epoch': 0.08}
7/6500 [56:36<11:04:04,  6.64s/it]                                                       8%|         | 497/6500 [56:36<11:04:04,  6.64s/it]  8%|         | 498/6500 [56:42<11:01:53,  6.62s/it]                                                       8%|         | 498/6500 [56:42<11:01:53,  6.62s/it]  8%|         | 499/6500 [56:49<11:00:44,  6.61s/it]                                                       8%|         | 499/6500 [56:49<11:00:44,  6.61s/it]  8%|         | 500/6500 [56:56<10:59:29,  6.59s/it]                                                       8%|         | 500/6500 [56:56<10:59:29,  6.59s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.9513653516769409, 'eval_runtime': 1.4825, 'eval_samples_per_second': 8.094, 'eval_steps_per_second': 2.024, 'epoch': 0.08}
                                                       8%|         | 500/6500 [56:57<10:59:29,  6.59s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-500
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-500
/projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-500
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-500
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-500
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-500/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-500/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-500/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-500/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.8987, 'learning_rate': 9.855198189827677e-05, 'epoch': 0.08}
{'loss': 0.8997, 'learning_rate': 9.85462007207556e-05, 'epoch': 0.08}
{'loss': 0.8905, 'learning_rate': 9.854040819585517e-05, 'epoch': 0.08}
{'loss': 0.89, 'learning_rate': 9.853460432492944e-05, 'epoch': 0.08}
{'loss': 0.8628, 'learning_rate': 9.852878910933507e-05, 'epoch': 0.08}
{'loss': 0.8752, 'learning_rate': 9.852296255043129e-05, 'epoch': 0.08}
  8%|         | 501/6500 [57:05<12:08:56,  7.29s/it]                                                       8%|         | 501/6500 [57:05<12:08:56,  7.29s/it]  8%|         | 502/6500 [57:11<11:47:03,  7.07s/it]                                                       8%|         | 502/6500 [57:11<11:47:03,  7.07s/it]  8%|         | 503/6500 [57:18<11:32:06,  6.92s/it]                                                       8%|         | 503/6500 [57:18<11:32:06,  6.92s/it]  8%|         | 504/6500 [57:24<11:21:34,  6.82s/it]                                                       8%|         | 504/6500 [57:24<11:21:34,  6.82s/it]  8%|         | 505/6500 [57:31<11:14:02,  6.75s/it]                                                       8%|         | 505/6500 [57:31<11:14:02,  6.75s/it]  8%|         | 506/6500 [57:37<11:08:33,  6.69s/it]                                                       8%|         | 506/6500 [57:37<11:08:33,  6.69s/it]  8%|         | 50{'loss': 0.866, 'learning_rate': 9.851712464958005e-05, 'epoch': 0.08}
{'loss': 0.9158, 'learning_rate': 9.85112754081459e-05, 'epoch': 0.08}
{'loss': 0.8853, 'learning_rate': 9.850541482749608e-05, 'epoch': 0.08}
{'loss': 1.1236, 'learning_rate': 9.849954290900046e-05, 'epoch': 0.08}
7/6500 [57:44<11:04:35,  6.65s/it]                                                       8%|         | 507/6500 [57:44<11:04:35,  6.65s/it]  8%|         | 508/6500 [57:51<11:02:00,  6.63s/it]                                                       8%|         | 508/6500 [57:51<11:02:00,  6.63s/it]  8%|         | 509/6500 [57:57<11:00:11,  6.61s/it]                                                       8%|         | 509/6500 [57:57<11:00:11,  6.61s/it]  8%|         | 510/6500 [58:04<10:58:38,  6.60s/it]                                                       8%|         | 510/6500 [58:04<10:58:38,  6.60s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.9491676092147827, 'eval_runtime': 1.4942, 'eval_samples_per_second': 8.031, 'eval_steps_per_second': 2.008, 'epoch': 0.08}
                                                       8%|         | 510/6500 [58:05<10:58:38,  6.60s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-510
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  I AM HERE AND I AM SAVING THE MODEL
/projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-510the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-510

I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-510
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-510
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-510/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-510/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-510/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-510/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.8879, 'learning_rate': 9.849365965403157e-05, 'epoch': 0.08}
{'loss': 0.8541, 'learning_rate': 9.848776506396458e-05, 'epoch': 0.08}
{'loss': 0.8708, 'learning_rate': 9.848185914017733e-05, 'epoch': 0.08}
{'loss': 0.8409, 'learning_rate': 9.847594188405027e-05, 'epoch': 0.08}
{'loss': 0.855, 'learning_rate': 9.847001329696653e-05, 'epoch': 0.08}
{'loss': 0.8846, 'learning_rate': 9.846407338031189e-05, 'epoch': 0.08}
  8%|         | 511/6500 [58:12<11:51:04,  7.12s/it]                                                       8%|         | 511/6500 [58:12<11:51:04,  7.12s/it]  8%|         | 512/6500 [58:19<11:34:01,  6.95s/it]                                                       8%|         | 512/6500 [58:19<11:34:01,  6.95s/it]  8%|         | 513/6500 [58:25<11:22:14,  6.84s/it]                                                       8%|         | 513/6500 [58:25<11:22:14,  6.84s/it]  8%|         | 514/6500 [58:32<11:14:28,  6.76s/it]                                                       8%|         | 514/6500 [58:32<11:14:28,  6.76s/it]  8%|         | 515/6500 [58:38<11:08:39,  6.70s/it]                                                       8%|         | 515/6500 [58:38<11:08:39,  6.70s/it]  8%|         | 516/6500 [58:45<11:04:26,  6.66s/it]                                                       8%|         | 516/6500 [58:45<11:04:26,  6.66s/it]  8%|         | 51{'loss': 0.8836, 'learning_rate': 9.845812213547475e-05, 'epoch': 0.08}
{'loss': 0.8961, 'learning_rate': 9.84521595638462e-05, 'epoch': 0.08}
{'loss': 0.8664, 'learning_rate': 9.844618566681996e-05, 'epoch': 0.08}
{'loss': 0.8697, 'learning_rate': 9.844020044579237e-05, 'epoch': 0.08}
7/6500 [58:51<11:01:50,  6.64s/it]                                                       8%|         | 517/6500 [58:51<11:01:50,  6.64s/it]  8%|         | 518/6500 [58:59<11:28:53,  6.91s/it]                                                       8%|         | 518/6500 [58:59<11:28:53,  6.91s/it]  8%|         | 519/6500 [59:06<11:18:37,  6.81s/it]                                                       8%|         | 519/6500 [59:06<11:18:37,  6.81s/it]  8%|         | 520/6500 [59:12<11:11:09,  6.73s/it]                                                       8%|         | 520/6500 [59:12<11:11:09,  6.73s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.9460111856460571, 'eval_runtime': 1.4844, 'eval_samples_per_second': 8.084, 'eval_steps_per_second': 2.021, 'epoch': 0.08}
                                                       8%|         | 520/6500 [59:14<11:11:09,  6.73s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-520
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-520
 /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-520
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-520
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-520
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-520/pytorch_model.bin
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-520/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-520/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-520/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.8793, 'learning_rate': 9.843420390216242e-05, 'epoch': 0.08}
{'loss': 0.8552, 'learning_rate': 9.842819603733182e-05, 'epoch': 0.08}
{'loss': 0.9051, 'learning_rate': 9.842217685270484e-05, 'epoch': 0.08}
{'loss': 0.872, 'learning_rate': 9.841614634968843e-05, 'epoch': 0.08}
{'loss': 1.1351, 'learning_rate': 9.84101045296922e-05, 'epoch': 0.08}
{'loss': 0.8698, 'learning_rate': 9.840405139412836e-05, 'epoch': 0.08}
  8%|         | 521/6500 [59:20<11:57:58,  7.20s/it]                                                       8%|         | 521/6500 [59:20<11:57:58,  7.20s/it]  8%|         | 522/6500 [59:27<11:39:04,  7.02s/it]                                                       8%|         | 522/6500 [59:27<11:39:04,  7.02s/it]  8%|         | 523/6500 [59:34<11:25:36,  6.88s/it]                                                       8%|         | 523/6500 [59:34<11:25:36,  6.88s/it]  8%|         | 524/6500 [59:40<11:16:25,  6.79s/it]                                                       8%|         | 524/6500 [59:40<11:16:25,  6.79s/it]  8%|         | 525/6500 [59:47<11:09:35,  6.72s/it]                                                       8%|         | 525/6500 [59:47<11:09:35,  6.72s/it]  8%|         | 526/6500 [59:53<11:04:55,  6.68s/it]                                                       8%|         | 526/6500 [59:53<11:04:55,  6.68s/it]  8%|         | 52{'loss': 0.8705, 'learning_rate': 9.83979869444118e-05, 'epoch': 0.08}
{'loss': 0.8503, 'learning_rate': 9.839191118196007e-05, 'epoch': 0.08}
{'loss': 0.8426, 'learning_rate': 9.838582410819332e-05, 'epoch': 0.08}
{'loss': 0.8487, 'learning_rate': 9.83797257245344e-05, 'epoch': 0.08}
7/6500 [1:00:00<11:01:35,  6.65s/it]                                                         8%|         | 527/6500 [1:00:00<11:01:35,  6.65s/it]  8%|         | 528/6500 [1:00:06<10:59:17,  6.62s/it]                                                         8%|         | 528/6500 [1:00:06<10:59:17,  6.62s/it]  8%|         | 529/6500 [1:00:13<10:57:54,  6.61s/it]                                                         8%|         | 529/6500 [1:00:13<10:57:54,  6.61s/it]  8%|         | 530/6500 [1:00:20<10:56:36,  6.60s/it]                                                         8%|         | 530/6500 [1:00:20<10:56:36,  6.60s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.9432976841926575, 'eval_runtime': 1.4811, 'eval_samples_per_second': 8.102, 'eval_steps_per_second': 2.026, 'epoch': 0.08}
                                                         8%|         | 530/6500 [1:00:21<10:56:36,  6.60s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-530
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  I AM HERE AND I AM SAVING THE MODEL
/projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-530the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-530

I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-530
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-530
the pytorch model path isthe pytorch model path is  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-530/pytorch_model.bin/projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-530/pytorch_model.bin

the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-530/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-530/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.8744, 'learning_rate': 9.837361603240872e-05, 'epoch': 0.08}
{'loss': 0.886, 'learning_rate': 9.836749503324442e-05, 'epoch': 0.08}
{'loss': 0.8868, 'learning_rate': 9.836136272847223e-05, 'epoch': 0.08}
{'loss': 0.8596, 'learning_rate': 9.835521911952555e-05, 'epoch': 0.08}
{'loss': 0.8757, 'learning_rate': 9.83490642078404e-05, 'epoch': 0.08}
{'loss': 0.8664, 'learning_rate': 9.834289799485545e-05, 'epoch': 0.08}
  8%|         | 531/6500 [1:00:28<11:48:46,  7.12s/it]                                                         8%|         | 531/6500 [1:00:28<11:48:46,  7.12s/it]  8%|         | 532/6500 [1:00:35<11:32:07,  6.96s/it]                                                         8%|         | 532/6500 [1:00:35<11:32:07,  6.96s/it]  8%|         | 533/6500 [1:00:41<11:20:10,  6.84s/it]                                                         8%|         | 533/6500 [1:00:41<11:20:10,  6.84s/it]  8%|         | 534/6500 [1:00:48<11:36:48,  7.01s/it]                                                         8%|         | 534/6500 [1:00:48<11:36:48,  7.01s/it]  8%|         | 535/6500 [1:00:55<11:23:38,  6.88s/it]                                                         8%|         | 535/6500 [1:00:55<11:23:38,  6.88s/it]  8%|         | 536/6500 [1:01:02<11:14:30,  6.79s/it]                                                         8%|         | 536/6500 [1:01:02<11:14:{'loss': 0.8675, 'learning_rate': 9.833672048201204e-05, 'epoch': 0.08}
{'loss': 0.8858, 'learning_rate': 9.83305316707541e-05, 'epoch': 0.08}
{'loss': 0.8588, 'learning_rate': 9.832433156252822e-05, 'epoch': 0.08}
{'loss': 1.1323, 'learning_rate': 9.831812015878368e-05, 'epoch': 0.08}
30,  6.79s/it]  8%|         | 537/6500 [1:01:08<11:07:53,  6.72s/it]                                                         8%|         | 537/6500 [1:01:08<11:07:53,  6.72s/it]  8%|         | 538/6500 [1:01:15<11:03:14,  6.67s/it]                                                         8%|         | 538/6500 [1:01:15<11:03:14,  6.67s/it]  8%|         | 539/6500 [1:01:21<11:00:05,  6.64s/it]                                                         8%|         | 539/6500 [1:01:21<11:00:05,  6.64s/it]  8%|         | 540/6500 [1:01:28<10:57:49,  6.62s/it]                                                         8%|         | 540/6500 [1:01:28<10:57:49,  6.62s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.9428647756576538, 'eval_runtime': 1.4852, 'eval_samples_per_second': 8.08, 'eval_steps_per_second': 2.02, 'epoch': 0.08}
                                                         8%|         | 540/6500 [1:01:29<10:57:49,  6.62s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-540
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-540I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in 
 /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-540
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-540
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-540
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-540/pytorch_model.bin /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-540/pytorch_model.bin

the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-540/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-540/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.8385, 'learning_rate': 9.831189746097232e-05, 'epoch': 0.08}
{'loss': 0.8783, 'learning_rate': 9.830566347054868e-05, 'epoch': 0.08}
{'loss': 0.8526, 'learning_rate': 9.82994181889699e-05, 'epoch': 0.08}
{'loss': 0.8411, 'learning_rate': 9.829316161769578e-05, 'epoch': 0.08}
{'loss': 0.836, 'learning_rate': 9.828689375818877e-05, 'epoch': 0.08}
{'loss': 0.8827, 'learning_rate': 9.828061461191392e-05, 'epoch': 0.08}
  8%|         | 541/6500 [1:01:36<11:50:04,  7.15s/it]                                                         8%|         | 541/6500 [1:01:36<11:50:04,  7.15s/it]  8%|         | 542/6500 [1:01:43<11:32:42,  6.98s/it]                                                         8%|         | 542/6500 [1:01:43<11:32:42,  6.98s/it]  8%|         | 543/6500 [1:01:49<11:20:47,  6.86s/it]                                                         8%|         | 543/6500 [1:01:49<11:20:47,  6.86s/it]  8%|         | 544/6500 [1:01:56<11:12:09,  6.77s/it]                                                         8%|         | 544/6500 [1:01:56<11:12:09,  6.77s/it]  8%|         | 545/6500 [1:02:03<11:05:56,  6.71s/it]                                                         8%|         | 545/6500 [1:02:03<11:05:56,  6.71s/it]  8%|         | 546/6500 [1:02:09<11:01:50,  6.67s/it]                                                         8%|         | 546/6500 [1:02:09<11:01:{'loss': 0.8852, 'learning_rate': 9.827432418033897e-05, 'epoch': 0.08}
{'loss': 0.8704, 'learning_rate': 9.826802246493425e-05, 'epoch': 0.08}
{'loss': 0.8395, 'learning_rate': 9.826170946717274e-05, 'epoch': 0.08}
{'loss': 0.88, 'learning_rate': 9.825538518853009e-05, 'epoch': 0.08}
50,  6.67s/it]  8%|         | 547/6500 [1:02:16<10:58:44,  6.64s/it]                                                         8%|         | 547/6500 [1:02:16<10:58:44,  6.64s/it]  8%|         | 548/6500 [1:02:22<10:56:32,  6.62s/it]                                                         8%|         | 548/6500 [1:02:22<10:56:32,  6.62s/it]  8%|         | 549/6500 [1:02:29<10:55:25,  6.61s/it]                                                         8%|         | 549/6500 [1:02:29<10:55:25,  6.61s/it]  8%|         | 550/6500 [1:02:36<11:19:54,  6.86s/it]                                                         8%|         | 550/6500 [1:02:36<11:19:54,  6.86s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.9387409090995789, 'eval_runtime': 1.4791, 'eval_samples_per_second': 8.113, 'eval_steps_per_second': 2.028, 'epoch': 0.08}
                                                         8%|         | 550/6500 [1:02:38<11:19:54,  6.86s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-550
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-550I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-550

I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-550
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-550
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-550/pytorch_model.bin
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-550/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-550/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-550/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.8514, 'learning_rate': 9.824904963048455e-05, 'epoch': 0.08}
{'loss': 0.8677, 'learning_rate': 9.824270279451701e-05, 'epoch': 0.08}
{'loss': 0.8667, 'learning_rate': 9.823634468211103e-05, 'epoch': 0.09}
{'loss': 0.8695, 'learning_rate': 9.822997529475275e-05, 'epoch': 0.09}
{'loss': 1.1224, 'learning_rate': 9.822359463393099e-05, 'epoch': 0.09}
{'loss': 0.8366, 'learning_rate': 9.821720270113718e-05, 'epoch': 0.09}
  8%|         | 551/6500 [1:02:45<12:04:02,  7.30s/it]                                                         8%|         | 551/6500 [1:02:45<12:04:02,  7.30s/it]  8%|         | 552/6500 [1:02:51<11:42:07,  7.08s/it]                                                         8%|         | 552/6500 [1:02:51<11:42:07,  7.08s/it]  9%|         | 553/6500 [1:02:58<11:27:01,  6.93s/it]                                                         9%|         | 553/6500 [1:02:58<11:27:01,  6.93s/it]  9%|         | 554/6500 [1:03:04<11:16:34,  6.83s/it]                                                         9%|         | 554/6500 [1:03:04<11:16:34,  6.83s/it]  9%|         | 555/6500 [1:03:11<11:08:42,  6.75s/it]                                                         9%|         | 555/6500 [1:03:11<11:08:42,  6.75s/it]  9%|         | 556/6500 [1:03:18<11:03:30,  6.70s/it]                                                         9%|         | 556/6500 [1:03:18<11:03:{'loss': 0.873, 'learning_rate': 9.821079949786541e-05, 'epoch': 0.09}
{'loss': 0.8236, 'learning_rate': 9.820438502561238e-05, 'epoch': 0.09}
{'loss': 0.8304, 'learning_rate': 9.819795928587745e-05, 'epoch': 0.09}
{'loss': 0.8529, 'learning_rate': 9.819152228016257e-05, 'epoch': 0.09}
30,  6.70s/it]  9%|         | 557/6500 [1:03:24<10:59:16,  6.66s/it]                                                         9%|         | 557/6500 [1:03:24<10:59:16,  6.66s/it]  9%|         | 558/6500 [1:03:31<10:56:40,  6.63s/it]                                                         9%|         | 558/6500 [1:03:31<10:56:40,  6.63s/it]  9%|         | 559/6500 [1:03:37<10:55:06,  6.62s/it]                                                         9%|         | 559/6500 [1:03:37<10:55:06,  6.62s/it]  9%|         | 560/6500 [1:03:44<10:53:49,  6.60s/it]                                                         9%|         | 560/6500 [1:03:44<10:53:49,  6.60s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.9365458488464355, 'eval_runtime': 1.5144, 'eval_samples_per_second': 7.924, 'eval_steps_per_second': 1.981, 'epoch': 0.09}
                                                         9%|         | 560/6500 [1:03:45<10:53:49,  6.60s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-560
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-560 /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-560

I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-560
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-560
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-560/pytorch_model.bin
the pytorch model path isthe pytorch model path is  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-560/pytorch_model.bin/projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-560/pytorch_model.bin

the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-560/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.8429, 'learning_rate': 9.81850740099724e-05, 'epoch': 0.09}
{'loss': 0.8795, 'learning_rate': 9.817861447681411e-05, 'epoch': 0.09}
{'loss': 0.879, 'learning_rate': 9.817214368219763e-05, 'epoch': 0.09}
{'loss': 0.8206, 'learning_rate': 9.816566162763546e-05, 'epoch': 0.09}
{'loss': 0.8711, 'learning_rate': 9.815916831464273e-05, 'epoch': 0.09}
{'loss': 0.8502, 'learning_rate': 9.815266374473721e-05, 'epoch': 0.09}
  9%|         | 561/6500 [1:03:52<11:46:11,  7.13s/it]                                                         9%|         | 561/6500 [1:03:52<11:46:11,  7.13s/it]  9%|         | 562/6500 [1:03:59<11:29:35,  6.97s/it]                                                         9%|         | 562/6500 [1:03:59<11:29:35,  6.97s/it]  9%|         | 563/6500 [1:04:05<11:17:30,  6.85s/it]                                                         9%|         | 563/6500 [1:04:05<11:17:30,  6.85s/it]  9%|         | 564/6500 [1:04:12<11:09:23,  6.77s/it]                                                         9%|         | 564/6500 [1:04:12<11:09:23,  6.77s/it]  9%|         | 565/6500 [1:04:18<11:03:10,  6.70s/it]                                                         9%|         | 565/6500 [1:04:18<11:03:10,  6.70s/it]  9%|         | 566/6500 [1:04:26<11:31:23,  6.99s/it]                                                         9%|         | 566/6500 [1:04:26<11:31:{'loss': 0.8962, 'learning_rate': 9.814614791943933e-05, 'epoch': 0.09}
{'loss': 0.8337, 'learning_rate': 9.813962084027211e-05, 'epoch': 0.09}
{'loss': 1.1216, 'learning_rate': 9.813308250876121e-05, 'epoch': 0.09}
{'loss': 0.865, 'learning_rate': 9.812653292643492e-05, 'epoch': 0.09}
23,  6.99s/it]  9%|         | 567/6500 [1:04:33<11:18:35,  6.86s/it]                                                         9%|         | 567/6500 [1:04:33<11:18:35,  6.86s/it]  9%|         | 568/6500 [1:04:39<11:10:15,  6.78s/it]                                                         9%|         | 568/6500 [1:04:39<11:10:15,  6.78s/it]  9%|         | 569/6500 [1:04:46<11:08:04,  6.76s/it]                                                         9%|         | 569/6500 [1:04:46<11:08:04,  6.76s/it]  9%|         | 570/6500 [1:04:53<11:02:56,  6.71s/it]                                                         9%|         | 570/6500 [1:04:53<11:02:56,  6.71s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.9363829493522644, 'eval_runtime': 1.4784, 'eval_samples_per_second': 8.117, 'eval_steps_per_second': 2.029, 'epoch': 0.09}
                                                         9%|         | 570/6500 [1:04:54<11:02:56,  6.71s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-570
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-570
/projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-570
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-570
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-570
the pytorch model path isthe pytorch model path isthe pytorch model path is  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-570/pytorch_model.bin/projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-570/pytorch_model.bin

 /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-570/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-570/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.8182, 'learning_rate': 9.811997209482418e-05, 'epoch': 0.09}
{'loss': 0.8643, 'learning_rate': 9.811340001546251e-05, 'epoch': 0.09}
{'loss': 0.8144, 'learning_rate': 9.810681668988615e-05, 'epoch': 0.09}
{'loss': 0.8355, 'learning_rate': 9.810022211963388e-05, 'epoch': 0.09}
{'loss': 0.8601, 'learning_rate': 9.809361630624714e-05, 'epoch': 0.09}
{'loss': 0.8631, 'learning_rate': 9.808699925127001e-05, 'epoch': 0.09}
  9%|         | 571/6500 [1:05:01<11:55:00,  7.24s/it]                                                         9%|         | 571/6500 [1:05:01<11:55:00,  7.24s/it]  9%|         | 572/6500 [1:05:10<12:51:56,  7.81s/it]                                                         9%|         | 572/6500 [1:05:10<12:51:56,  7.81s/it]  9%|         | 573/6500 [1:05:17<12:17:51,  7.47s/it]                                                         9%|         | 573/6500 [1:05:17<12:17:51,  7.47s/it]  9%|         | 574/6500 [1:05:23<11:51:17,  7.20s/it]                                                         9%|         | 574/6500 [1:05:23<11:51:17,  7.20s/it]  9%|         | 575/6500 [1:05:30<11:32:54,  7.02s/it]                                                         9%|         | 575/6500 [1:05:30<11:32:54,  7.02s/it]  9%|         | 576/6500 [1:05:37<11:19:53,  6.89s/it]                                                         9%|         | 576/6500 [1:05:37<11:19:{'loss': 0.8656, 'learning_rate': 9.808037095624917e-05, 'epoch': 0.09}
{'loss': 0.8643, 'learning_rate': 9.807373142273395e-05, 'epoch': 0.09}
{'loss': 0.8168, 'learning_rate': 9.80670806522763e-05, 'epoch': 0.09}
{'loss': 0.8612, 'learning_rate': 9.80604186464308e-05, 'epoch': 0.09}
53,  6.89s/it]  9%|         | 577/6500 [1:05:43<11:10:53,  6.80s/it]                                                         9%|         | 577/6500 [1:05:43<11:10:53,  6.80s/it]  9%|         | 578/6500 [1:05:50<11:04:17,  6.73s/it]                                                         9%|         | 578/6500 [1:05:50<11:04:17,  6.73s/it]  9%|         | 579/6500 [1:05:56<10:59:43,  6.69s/it]                                                         9%|         | 579/6500 [1:05:56<10:59:43,  6.69s/it]  9%|         | 580/6500 [1:06:03<10:56:46,  6.66s/it]                                                         9%|         | 580/6500 [1:06:03<10:56:46,  6.66s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.9314566254615784, 'eval_runtime': 1.5584, 'eval_samples_per_second': 7.7, 'eval_steps_per_second': 1.925, 'epoch': 0.09}
                                                         9%|         | 580/6500 [1:06:05<10:56:46,  6.66s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-580
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-580
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-580
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-580
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-580
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-580/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-580/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-580/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-580/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.8334, 'learning_rate': 9.805374540675468e-05, 'epoch': 0.09}
{'loss': 0.8875, 'learning_rate': 9.804706093480771e-05, 'epoch': 0.09}
{'loss': 0.8363, 'learning_rate': 9.804036523215239e-05, 'epoch': 0.09}
{'loss': 1.1093, 'learning_rate': 9.803365830035379e-05, 'epoch': 0.09}
{'loss': 0.8558, 'learning_rate': 9.80269401409796e-05, 'epoch': 0.09}
{'loss': 0.8055, 'learning_rate': 9.802021075560017e-05, 'epoch': 0.09}
  9%|         | 581/6500 [1:06:11<11:50:56,  7.21s/it]                                                         9%|         | 581/6500 [1:06:11<11:50:56,  7.21s/it]  9%|         | 582/6500 [1:06:19<12:04:29,  7.35s/it]                                                         9%|         | 582/6500 [1:06:19<12:04:29,  7.35s/it]  9%|         | 583/6500 [1:06:26<11:41:55,  7.12s/it]                                                         9%|         | 583/6500 [1:06:26<11:41:55,  7.12s/it]  9%|         | 584/6500 [1:06:32<11:25:51,  6.96s/it]                                                         9%|         | 584/6500 [1:06:32<11:25:51,  6.96s/it]  9%|         | 585/6500 [1:06:39<11:14:16,  6.84s/it]                                                         9%|         | 585/6500 [1:06:39<11:14:16,  6.84s/it]  9%|         | 586/6500 [1:06:45<11:06:38,  6.76s/it]                                                         9%|         | 586/6500 [1:06:45<11:06:{'loss': 0.8604, 'learning_rate': 9.801347014578846e-05, 'epoch': 0.09}
{'loss': 0.8063, 'learning_rate': 9.800671831312e-05, 'epoch': 0.09}
{'loss': 0.8235, 'learning_rate': 9.799995525917304e-05, 'epoch': 0.09}
{'loss': 0.8557, 'learning_rate': 9.799318098552837e-05, 'epoch': 0.09}
38,  6.76s/it]  9%|         | 587/6500 [1:06:52<11:01:16,  6.71s/it]                                                         9%|         | 587/6500 [1:06:52<11:01:16,  6.71s/it]  9%|         | 588/6500 [1:06:59<10:57:40,  6.67s/it]                                                         9%|         | 588/6500 [1:06:59<10:57:40,  6.67s/it]  9%|         | 589/6500 [1:07:05<10:54:29,  6.64s/it]                                                         9%|         | 589/6500 [1:07:05<10:54:29,  6.64s/it]  9%|         | 590/6500 [1:07:12<10:52:48,  6.63s/it]                                                         9%|         | 590/6500 [1:07:12<10:52:48,  6.63s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.9284977912902832, 'eval_runtime': 1.4898, 'eval_samples_per_second': 8.055, 'eval_steps_per_second': 2.014, 'epoch': 0.09}
                                                         9%|         | 590/6500 [1:07:13<10:52:48,  6.63s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-590
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  I AM HERE AND I AM SAVING THE MODEL/projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-590
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-590

I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-590
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-590
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-590/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-590/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-590/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-590/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.8514, 'learning_rate': 9.798639549376945e-05, 'epoch': 0.09}
{'loss': 0.8535, 'learning_rate': 9.797959878548236e-05, 'epoch': 0.09}
{'loss': 0.8418, 'learning_rate': 9.797279086225576e-05, 'epoch': 0.09}
{'loss': 0.827, 'learning_rate': 9.796597172568099e-05, 'epoch': 0.09}
{'loss': 0.8465, 'learning_rate': 9.795914137735194e-05, 'epoch': 0.09}
{'loss': 0.8328, 'learning_rate': 9.795229981886521e-05, 'epoch': 0.09}
  9%|         | 591/6500 [1:07:20<11:44:40,  7.16s/it]                                                         9%|         | 591/6500 [1:07:20<11:44:40,  7.16s/it]  9%|         | 592/6500 [1:07:27<11:27:32,  6.98s/it]                                                         9%|         | 592/6500 [1:07:27<11:27:32,  6.98s/it]  9%|         | 593/6500 [1:07:33<11:15:46,  6.86s/it]                                                         9%|         | 593/6500 [1:07:33<11:15:46,  6.86s/it]  9%|         | 594/6500 [1:07:40<11:06:57,  6.78s/it]                                                         9%|         | 594/6500 [1:07:40<11:06:57,  6.78s/it]  9%|         | 595/6500 [1:07:46<11:00:48,  6.71s/it]                                                         9%|         | 595/6500 [1:07:46<11:00:48,  6.71s/it]  9%|         | 596/6500 [1:07:53<10:56:44,  6.67s/it]                                                         9%|         | 596/6500 [1:07:53<10:56:{'loss': 0.8738, 'learning_rate': 9.794544705181995e-05, 'epoch': 0.09}
{'loss': 0.8392, 'learning_rate': 9.793858307781796e-05, 'epoch': 0.09}
{'loss': 1.0987, 'learning_rate': 9.793170789846364e-05, 'epoch': 0.09}
{'loss': 0.8452, 'learning_rate': 9.792482151536402e-05, 'epoch': 0.09}
44,  6.67s/it]  9%|         | 597/6500 [1:08:00<10:53:42,  6.64s/it]                                                         9%|         | 597/6500 [1:08:00<10:53:42,  6.64s/it]  9%|         | 598/6500 [1:08:07<11:09:22,  6.80s/it]                                                         9%|         | 598/6500 [1:08:07<11:09:22,  6.80s/it]  9%|         | 599/6500 [1:08:13<11:02:45,  6.74s/it]                                                         9%|         | 599/6500 [1:08:13<11:02:45,  6.74s/it]  9%|         | 600/6500 [1:08:20<10:57:53,  6.69s/it]                                                         9%|         | 600/6500 [1:08:20<10:57:53,  6.69s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.9296708106994629, 'eval_runtime': 1.5167, 'eval_samples_per_second': 7.912, 'eval_steps_per_second': 1.978, 'epoch': 0.09}
                                                         9%|         | 600/6500 [1:08:21<10:57:53,  6.69s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-600
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-600I AM HERE AND I AM SAVING THE MODEL

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-600
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-600
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-600
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-600/pytorch_model.bin
the pytorch model path is the pytorch model path is/projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-600/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-600/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-600/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.8176, 'learning_rate': 9.791792393012877e-05, 'epoch': 0.09}
{'loss': 0.8188, 'learning_rate': 9.791101514437014e-05, 'epoch': 0.09}
{'loss': 0.8068, 'learning_rate': 9.790409515970302e-05, 'epoch': 0.09}
{'loss': 0.814, 'learning_rate': 9.789716397774493e-05, 'epoch': 0.09}
{'loss': 0.8378, 'learning_rate': 9.789022160011597e-05, 'epoch': 0.09}
{'loss': 0.8525, 'learning_rate': 9.78832680284389e-05, 'epoch': 0.09}
  9%|         | 601/6500 [1:08:28<11:48:55,  7.21s/it]                                                         9%|         | 601/6500 [1:08:28<11:48:55,  7.21s/it]  9%|         | 602/6500 [1:08:35<11:30:13,  7.02s/it]                                                         9%|         | 602/6500 [1:08:35<11:30:13,  7.02s/it]  9%|         | 603/6500 [1:08:42<11:17:10,  6.89s/it]                                                         9%|         | 603/6500 [1:08:42<11:17:10,  6.89s/it]  9%|         | 604/6500 [1:08:48<11:07:55,  6.80s/it]                                                         9%|         | 604/6500 [1:08:48<11:07:55,  6.80s/it]  9%|         | 605/6500 [1:08:55<11:00:58,  6.73s/it]                                                         9%|         | 605/6500 [1:08:55<11:00:58,  6.73s/it]  9%|         | 606/6500 [1:09:01<10:56:10,  6.68s/it]                                                         9%|         | 606/6500 [1:09:01<10:56:{'loss': 0.8568, 'learning_rate': 9.787630326433905e-05, 'epoch': 0.09}
{'loss': 0.8264, 'learning_rate': 9.786932730944441e-05, 'epoch': 0.09}
{'loss': 0.8328, 'learning_rate': 9.786234016538557e-05, 'epoch': 0.09}
{'loss': 0.8348, 'learning_rate': 9.785534183379572e-05, 'epoch': 0.09}
10,  6.68s/it]  9%|         | 607/6500 [1:09:08<10:53:24,  6.65s/it]                                                         9%|         | 607/6500 [1:09:08<10:53:24,  6.65s/it]  9%|         | 608/6500 [1:09:14<10:50:48,  6.63s/it]                                                         9%|         | 608/6500 [1:09:14<10:50:48,  6.63s/it]  9%|         | 609/6500 [1:09:21<10:49:09,  6.61s/it]                                                         9%|         | 609/6500 [1:09:21<10:49:09,  6.61s/it]  9%|         | 610/6500 [1:09:28<10:47:59,  6.60s/it]                                                         9%|         | 610/6500 [1:09:28<10:47:59,  6.60s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.9259690046310425, 'eval_runtime': 1.4889, 'eval_samples_per_second': 8.06, 'eval_steps_per_second': 2.015, 'epoch': 0.09}
                                                         9%|         | 610/6500 [1:09:29<10:47:59,  6.60s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-610
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in   /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-610
/projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-610
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-610
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-610
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-610/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-610/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-610/pytorch_model.binthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-610/pytorch_model.bin

/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.8376, 'learning_rate': 9.784833231631068e-05, 'epoch': 0.09}
{'loss': 0.8503, 'learning_rate': 9.784131161456888e-05, 'epoch': 0.09}
{'loss': 0.8263, 'learning_rate': 9.783427973021136e-05, 'epoch': 0.09}
{'loss': 1.1067, 'learning_rate': 9.782723666488181e-05, 'epoch': 0.09}
{'loss': 0.8193, 'learning_rate': 9.782018242022648e-05, 'epoch': 0.09}
{'loss': 0.8264, 'learning_rate': 9.781311699789426e-05, 'epoch': 0.09}
  9%|         | 611/6500 [1:09:36<11:40:41,  7.14s/it]                                                         9%|         | 611/6500 [1:09:36<11:40:41,  7.14s/it]  9%|         | 612/6500 [1:09:43<11:23:44,  6.97s/it]                                                         9%|         | 612/6500 [1:09:43<11:23:44,  6.97s/it]  9%|         | 613/6500 [1:09:49<11:11:52,  6.85s/it]                                                         9%|         | 613/6500 [1:09:49<11:11:52,  6.85s/it]  9%|         | 614/6500 [1:09:56<11:03:31,  6.76s/it]                                                         9%|         | 614/6500 [1:09:56<11:03:31,  6.76s/it]  9%|         | 615/6500 [1:10:03<11:22:43,  6.96s/it]                                                         9%|         | 615/6500 [1:10:03<11:22:43,  6.96s/it]  9%|         | 616/6500 [1:10:10<11:10:58,  6.84s/it]                                                         9%|         | 616/6500 [1:10:10<11:10:{'loss': 0.8186, 'learning_rate': 9.780604039953665e-05, 'epoch': 0.09}
{'loss': 0.8059, 'learning_rate': 9.779895262680775e-05, 'epoch': 0.1}
{'loss': 0.8105, 'learning_rate': 9.77918536813643e-05, 'epoch': 0.1}
{'loss': 0.8529, 'learning_rate': 9.778474356486564e-05, 'epoch': 0.1}
58,  6.84s/it]  9%|         | 617/6500 [1:10:16<11:02:43,  6.76s/it]                                                         9%|         | 617/6500 [1:10:16<11:02:43,  6.76s/it] 10%|         | 618/6500 [1:10:23<10:57:05,  6.70s/it]                                                        10%|         | 618/6500 [1:10:23<10:57:05,  6.70s/it] 10%|         | 619/6500 [1:10:29<10:52:58,  6.66s/it]                                                        10%|         | 619/6500 [1:10:29<10:52:58,  6.66s/it] 10%|         | 620/6500 [1:10:36<10:50:14,  6.64s/it]                                                        10%|         | 620/6500 [1:10:36<10:50:14,  6.64s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.9237824082374573, 'eval_runtime': 1.4849, 'eval_samples_per_second': 8.081, 'eval_steps_per_second': 2.02, 'epoch': 0.1}
                                                        10%|         | 620/6500 [1:10:37<10:50:14,  6.64s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-620
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-620I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-620

I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-620
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-620
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-620/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-620/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-620/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-620/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.8506, 'learning_rate': 9.777762227897371e-05, 'epoch': 0.1}
{'loss': 0.8414, 'learning_rate': 9.777048982535306e-05, 'epoch': 0.1}
{'loss': 0.8071, 'learning_rate': 9.776334620567085e-05, 'epoch': 0.1}
{'loss': 0.8462, 'learning_rate': 9.77561914215969e-05, 'epoch': 0.1}
{'loss': 0.8218, 'learning_rate': 9.774902547480353e-05, 'epoch': 0.1}
{'loss': 0.8329, 'learning_rate': 9.77418483669658e-05, 'epoch': 0.1}
 10%|         | 621/6500 [1:10:44<11:41:46,  7.16s/it]                                                        10%|         | 621/6500 [1:10:44<11:41:46,  7.16s/it] 10%|         | 622/6500 [1:10:51<11:24:19,  6.99s/it]                                                        10%|         | 622/6500 [1:10:51<11:24:19,  6.99s/it] 10%|         | 623/6500 [1:10:57<11:11:50,  6.86s/it]                                                        10%|         | 623/6500 [1:10:57<11:11:50,  6.86s/it] 10%|         | 624/6500 [1:11:04<11:02:54,  6.77s/it]                                                        10%|         | 624/6500 [1:11:04<11:02:54,  6.77s/it] 10%|         | 625/6500 [1:11:11<10:56:41,  6.71s/it]                                                        10%|         | 625/6500 [1:11:11<10:56:41,  6.71s/it] 10%|         | 626/6500 [1:11:17<10:52:23,  6.66s/it]                                                        10%|         | 626/6500 [1:11:17<10:52:{'loss': 0.8359, 'learning_rate': 9.773466009976129e-05, 'epoch': 0.1}
{'loss': 0.8252, 'learning_rate': 9.77274606748702e-05, 'epoch': 0.1}
{'loss': 1.0959, 'learning_rate': 9.772025009397537e-05, 'epoch': 0.1}
{'loss': 0.7998, 'learning_rate': 9.771302835876224e-05, 'epoch': 0.1}
23,  6.66s/it] 10%|         | 627/6500 [1:11:24<10:49:36,  6.64s/it]                                                        10%|         | 627/6500 [1:11:24<10:49:36,  6.64s/it] 10%|         | 628/6500 [1:11:30<10:47:50,  6.62s/it]                                                        10%|         | 628/6500 [1:11:30<10:47:50,  6.62s/it] 10%|         | 629/6500 [1:11:37<10:46:22,  6.61s/it]                                                        10%|         | 629/6500 [1:11:37<10:46:22,  6.61s/it] 10%|         | 630/6500 [1:11:43<10:45:28,  6.60s/it]                                                        10%|         | 630/6500 [1:11:43<10:45:28,  6.60s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.9236454963684082, 'eval_runtime': 1.4903, 'eval_samples_per_second': 8.052, 'eval_steps_per_second': 2.013, 'epoch': 0.1}
                                                        10%|         | 630/6500 [1:11:45<10:45:28,  6.60s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-630
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-630I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-630

I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-630
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-630
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-630/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-630/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-630/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-630/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.8488, 'learning_rate': 9.77057954709188e-05, 'epoch': 0.1}
{'loss': 0.7875, 'learning_rate': 9.769855143213575e-05, 'epoch': 0.1}
{'loss': 0.7954, 'learning_rate': 9.769129624410631e-05, 'epoch': 0.1}
{'loss': 0.8163, 'learning_rate': 9.768402990852635e-05, 'epoch': 0.1}
{'loss': 0.8137, 'learning_rate': 9.76767524270943e-05, 'epoch': 0.1}
{'loss': 0.8523, 'learning_rate': 9.766946380151125e-05, 'epoch': 0.1}
 10%|         | 631/6500 [1:11:53<12:01:06,  7.37s/it]                                                        10%|         | 631/6500 [1:11:53<12:01:06,  7.37s/it] 10%|         | 632/6500 [1:11:59<11:37:47,  7.13s/it]                                                        10%|         | 632/6500 [1:11:59<11:37:47,  7.13s/it] 10%|         | 633/6500 [1:12:06<11:21:12,  6.97s/it]                                                        10%|         | 633/6500 [1:12:06<11:21:12,  6.97s/it] 10%|         | 634/6500 [1:12:12<11:09:24,  6.85s/it]                                                        10%|         | 634/6500 [1:12:12<11:09:24,  6.85s/it] 10%|         | 635/6500 [1:12:19<11:01:42,  6.77s/it]                                                        10%|         | 635/6500 [1:12:19<11:01:42,  6.77s/it] 10%|         | 636/6500 [1:12:26<10:55:52,  6.71s/it]                                                        10%|         | 636/6500 [1:12:26<10:55:{'loss': 0.8502, 'learning_rate': 9.766216403348089e-05, 'epoch': 0.1}
{'loss': 0.7986, 'learning_rate': 9.765485312470946e-05, 'epoch': 0.1}
{'loss': 0.8266, 'learning_rate': 9.764753107690588e-05, 'epoch': 0.1}
{'loss': 0.8224, 'learning_rate': 9.76401978917816e-05, 'epoch': 0.1}
52,  6.71s/it] 10%|         | 637/6500 [1:12:32<10:51:58,  6.67s/it]                                                        10%|         | 637/6500 [1:12:32<10:51:58,  6.67s/it] 10%|         | 638/6500 [1:12:39<10:48:38,  6.64s/it]                                                        10%|         | 638/6500 [1:12:39<10:48:38,  6.64s/it] 10%|         | 639/6500 [1:12:45<10:46:53,  6.62s/it]                                                        10%|         | 639/6500 [1:12:45<10:46:53,  6.62s/it] 10%|         | 640/6500 [1:12:52<10:45:43,  6.61s/it]                                                        10%|         | 640/6500 [1:12:52<10:45:43,  6.61s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.9202255606651306, 'eval_runtime': 1.4909, 'eval_samples_per_second': 8.049, 'eval_steps_per_second': 2.012, 'epoch': 0.1}
                                                        10%|         | 640/6500 [1:12:53<10:45:43,  6.61s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-640
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-640
 /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-640
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-640
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-640
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-640/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-640/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-640/pytorch_model.binthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-640/pytorch_model.bin

/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.8594, 'learning_rate': 9.763285357105072e-05, 'epoch': 0.1}
{'loss': 0.7977, 'learning_rate': 9.762549811642991e-05, 'epoch': 0.1}
{'loss': 1.0794, 'learning_rate': 9.761813152963853e-05, 'epoch': 0.1}
{'loss': 0.8416, 'learning_rate': 9.761075381239839e-05, 'epoch': 0.1}
{'loss': 0.7877, 'learning_rate': 9.760336496643403e-05, 'epoch': 0.1}
{'loss': 0.8393, 'learning_rate': 9.759596499347254e-05, 'epoch': 0.1}
 10%|         | 641/6500 [1:13:00<11:36:15,  7.13s/it]                                                        10%|         | 641/6500 [1:13:00<11:36:15,  7.13s/it] 10%|         | 642/6500 [1:13:07<11:19:35,  6.96s/it]                                                        10%|         | 642/6500 [1:13:07<11:19:35,  6.96s/it] 10%|         | 643/6500 [1:13:13<11:07:53,  6.84s/it]                                                        10%|         | 643/6500 [1:13:13<11:07:53,  6.84s/it] 10%|         | 644/6500 [1:13:20<10:59:34,  6.76s/it]                                                        10%|         | 644/6500 [1:13:20<10:59:34,  6.76s/it] 10%|         | 645/6500 [1:13:26<10:53:45,  6.70s/it]                                                        10%|         | 645/6500 [1:13:26<10:53:45,  6.70s/it] 10%|         | 646/6500 [1:13:33<10:49:36,  6.66s/it]                                                        10%|         | 646/6500 [1:13:33<10:49:{'loss': 0.7875, 'learning_rate': 9.758855389524364e-05, 'epoch': 0.1}
{'loss': 0.7998, 'learning_rate': 9.75811316734796e-05, 'epoch': 0.1}
{'loss': 0.8158, 'learning_rate': 9.757369832991532e-05, 'epoch': 0.1}
{'loss': 0.8152, 'learning_rate': 9.756625386628832e-05, 'epoch': 0.1}
36,  6.66s/it] 10%|         | 647/6500 [1:13:40<11:13:17,  6.90s/it]                                                        10%|         | 647/6500 [1:13:40<11:13:17,  6.90s/it] 10%|         | 648/6500 [1:13:47<11:03:08,  6.80s/it]                                                        10%|         | 648/6500 [1:13:47<11:03:08,  6.80s/it] 10%|         | 649/6500 [1:13:54<10:56:25,  6.73s/it]                                                        10%|         | 649/6500 [1:13:54<10:56:25,  6.73s/it] 10%|         | 650/6500 [1:14:00<10:51:38,  6.68s/it]                                                        10%|         | 650/6500 [1:14:00<10:51:38,  6.68s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.9171623587608337, 'eval_runtime': 1.4783, 'eval_samples_per_second': 8.117, 'eval_steps_per_second': 2.029, 'epoch': 0.1}
                                                        10%|         | 650/6500 [1:14:02<10:51:38,  6.68s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-650
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-650
/projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-650
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-650
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-650
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-650/pytorch_model.bin
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-650/pytorch_model.bin
 the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-650/pytorch_model.bin
/projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-650/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.8468, 'learning_rate': 9.755879828433869e-05, 'epoch': 0.1}
{'loss': 0.8291, 'learning_rate': 9.755133158580912e-05, 'epoch': 0.1}
{'loss': 0.7906, 'learning_rate': 9.75438537724449e-05, 'epoch': 0.1}
{'loss': 0.8233, 'learning_rate': 9.753636484599393e-05, 'epoch': 0.1}
{'loss': 0.807, 'learning_rate': 9.752886480820671e-05, 'epoch': 0.1}
{'loss': 0.8512, 'learning_rate': 9.752135366083632e-05, 'epoch': 0.1}
 10%|         | 651/6500 [1:14:09<11:44:03,  7.22s/it]                                                        10%|         | 651/6500 [1:14:09<11:44:03,  7.22s/it] 10%|         | 652/6500 [1:14:15<11:24:46,  7.03s/it]                                                        10%|         | 652/6500 [1:14:15<11:24:46,  7.03s/it] 10%|         | 653/6500 [1:14:22<11:11:08,  6.89s/it]                                                        10%|         | 653/6500 [1:14:22<11:11:08,  6.89s/it] 10%|         | 654/6500 [1:14:28<11:01:38,  6.79s/it]                                                        10%|         | 654/6500 [1:14:28<11:01:38,  6.79s/it] 10%|         | 655/6500 [1:14:35<10:54:42,  6.72s/it]                                                        10%|         | 655/6500 [1:14:35<10:54:42,  6.72s/it] 10%|         | 656/6500 [1:14:41<10:49:49,  6.67s/it]                                                        10%|         | 656/6500 [1:14:41<10:49:{'loss': 0.8052, 'learning_rate': 9.751383140563845e-05, 'epoch': 0.1}
{'loss': 1.0737, 'learning_rate': 9.750629804437137e-05, 'epoch': 0.1}
{'loss': 0.8289, 'learning_rate': 9.749875357879597e-05, 'epoch': 0.1}
{'loss': 0.775, 'learning_rate': 9.749119801067572e-05, 'epoch': 0.1}
49,  6.67s/it] 10%|         | 657/6500 [1:14:48<10:46:34,  6.64s/it]                                                        10%|         | 657/6500 [1:14:48<10:46:34,  6.64s/it] 10%|         | 658/6500 [1:14:55<10:44:08,  6.62s/it]                                                        10%|         | 658/6500 [1:14:55<10:44:08,  6.62s/it] 10%|         | 659/6500 [1:15:01<10:42:25,  6.60s/it]                                                        10%|         | 659/6500 [1:15:01<10:42:25,  6.60s/it] 10%|         | 660/6500 [1:15:08<10:40:54,  6.58s/it]                                                        10%|         | 660/6500 [1:15:08<10:40:54,  6.58s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.916434645652771, 'eval_runtime': 1.4816, 'eval_samples_per_second': 8.099, 'eval_steps_per_second': 2.025, 'epoch': 0.1}
                                                        10%|         | 660/6500 [1:15:09<10:40:54,  6.58s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-660
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in I AM HERE AND I AM SAVING THE MODEL 
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-660
/projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-660
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-660
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-660
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-660/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-660/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-660/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-660/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.8253, 'learning_rate': 9.74836313417767e-05, 'epoch': 0.1}
{'loss': 0.7845, 'learning_rate': 9.747605357386754e-05, 'epoch': 0.1}
{'loss': 0.7937, 'learning_rate': 9.746846470871951e-05, 'epoch': 0.1}
{'loss': 0.8249, 'learning_rate': 9.746086474810649e-05, 'epoch': 0.1}
{'loss': 0.8271, 'learning_rate': 9.745325369380489e-05, 'epoch': 0.1}
{'loss': 0.8233, 'learning_rate': 9.744563154759375e-05, 'epoch': 0.1}
 10%|         | 661/6500 [1:15:16<11:31:40,  7.11s/it]                                                        10%|         | 661/6500 [1:15:16<11:31:40,  7.11s/it] 10%|         | 662/6500 [1:15:23<11:15:58,  6.95s/it]                                                        10%|         | 662/6500 [1:15:23<11:15:58,  6.95s/it] 10%|         | 663/6500 [1:15:30<11:29:39,  7.09s/it]                                                        10%|         | 663/6500 [1:15:30<11:29:39,  7.09s/it] 10%|         | 664/6500 [1:15:37<11:14:28,  6.93s/it]                                                        10%|         | 664/6500 [1:15:37<11:14:28,  6.93s/it] 10%|         | 665/6500 [1:15:43<11:03:59,  6.83s/it]                                                        10%|         | 665/6500 [1:15:43<11:03:59,  6.83s/it] 10%|         | 666/6500 [1:15:50<10:56:31,  6.75s/it]                                                        10%|         | 666/6500 [1:15:50<10:56:{'loss': 0.8158, 'learning_rate': 9.743799831125472e-05, 'epoch': 0.1}
{'loss': 0.8026, 'learning_rate': 9.743035398657201e-05, 'epoch': 0.1}
{'loss': 0.803, 'learning_rate': 9.742269857533244e-05, 'epoch': 0.1}
{'loss': 0.7987, 'learning_rate': 9.74150320793254e-05, 'epoch': 0.1}
31,  6.75s/it] 10%|         | 667/6500 [1:15:56<10:51:15,  6.70s/it]                                                        10%|         | 667/6500 [1:15:56<10:51:15,  6.70s/it] 10%|         | 668/6500 [1:16:03<10:47:11,  6.66s/it]                                                        10%|         | 668/6500 [1:16:03<10:47:11,  6.66s/it] 10%|         | 669/6500 [1:16:09<10:44:16,  6.63s/it]                                                        10%|         | 669/6500 [1:16:09<10:44:16,  6.63s/it] 10%|         | 670/6500 [1:16:16<10:42:10,  6.61s/it]                                                        10%|         | 670/6500 [1:16:16<10:42:10,  6.61s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.9127311110496521, 'eval_runtime': 1.4821, 'eval_samples_per_second': 8.097, 'eval_steps_per_second': 2.024, 'epoch': 0.1}
                                                        10%|         | 670/6500 [1:16:17<10:42:10,  6.61s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-670
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  I AM HERE AND I AM SAVING THE MODEL
/projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-670the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-670

I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-670
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-670
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-670/pytorch_model.bin
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-670/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-670/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-670/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.8461, 'learning_rate': 9.740735450034292e-05, 'epoch': 0.1}
{'loss': 0.8141, 'learning_rate': 9.739966584017956e-05, 'epoch': 0.1}
{'loss': 1.0632, 'learning_rate': 9.739196610063251e-05, 'epoch': 0.1}
{'loss': 0.811, 'learning_rate': 9.738425528350152e-05, 'epoch': 0.1}
{'loss': 0.8001, 'learning_rate': 9.737653339058896e-05, 'epoch': 0.1}
{'loss': 0.7883, 'learning_rate': 9.736880042369978e-05, 'epoch': 0.1}
 10%|         | 671/6500 [1:16:24<11:33:47,  7.14s/it]                                                        10%|         | 671/6500 [1:16:24<11:33:47,  7.14s/it] 10%|         | 672/6500 [1:16:31<11:16:59,  6.97s/it]                                                        10%|         | 672/6500 [1:16:31<11:16:59,  6.97s/it] 10%|         | 673/6500 [1:16:38<11:05:14,  6.85s/it]                                                        10%|         | 673/6500 [1:16:38<11:05:14,  6.85s/it] 10%|         | 674/6500 [1:16:44<10:56:37,  6.76s/it]                                                        10%|         | 674/6500 [1:16:44<10:56:37,  6.76s/it] 10%|         | 675/6500 [1:16:51<10:50:36,  6.70s/it]                                                        10%|         | 675/6500 [1:16:51<10:50:36,  6.70s/it] 10%|         | 676/6500 [1:16:57<10:46:13,  6.66s/it]                                                        10%|         | 676/6500 [1:16:57<10:46:{'loss': 0.7742, 'learning_rate': 9.736105638464151e-05, 'epoch': 0.1}
{'loss': 0.7883, 'learning_rate': 9.735330127522425e-05, 'epoch': 0.1}
{'loss': 0.8171, 'learning_rate': 9.734553509726074e-05, 'epoch': 0.1}
{'loss': 0.8236, 'learning_rate': 9.733775785256629e-05, 'epoch': 0.1}
13,  6.66s/it] 10%|         | 677/6500 [1:17:04<10:43:34,  6.63s/it]                                                        10%|         | 677/6500 [1:17:04<10:43:34,  6.63s/it] 10%|         | 678/6500 [1:17:10<10:41:37,  6.61s/it]                                                        10%|         | 678/6500 [1:17:10<10:41:37,  6.61s/it] 10%|         | 679/6500 [1:17:17<10:56:59,  6.77s/it]                                                        10%|         | 679/6500 [1:17:17<10:56:59,  6.77s/it] 10%|         | 680/6500 [1:17:24<10:50:59,  6.71s/it]                                                        10%|         | 680/6500 [1:17:24<10:50:59,  6.71s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.9112572073936462, 'eval_runtime': 1.516, 'eval_samples_per_second': 7.916, 'eval_steps_per_second': 1.979, 'epoch': 0.1}
                                                        10%|         | 680/6500 [1:17:26<10:50:59,  6.71s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-680
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-680
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-680
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-680
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-680
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-680/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-680/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-680/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-680/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.8198, 'learning_rate': 9.732996954295874e-05, 'epoch': 0.1}
{'loss': 0.801, 'learning_rate': 9.732217017025858e-05, 'epoch': 0.1}
{'loss': 0.8019, 'learning_rate': 9.731435973628886e-05, 'epoch': 0.11}
{'loss': 0.8032, 'learning_rate': 9.730653824287523e-05, 'epoch': 0.11}
{'loss': 0.7917, 'learning_rate': 9.729870569184593e-05, 'epoch': 0.11}
{'loss': 0.8279, 'learning_rate': 9.729086208503174e-05, 'epoch': 0.11}
 10%|         | 681/6500 [1:17:32<11:39:23,  7.21s/it]                                                        10%|         | 681/6500 [1:17:32<11:39:23,  7.21s/it] 10%|         | 682/6500 [1:17:39<11:20:28,  7.02s/it]                                                        10%|         | 682/6500 [1:17:39<11:20:28,  7.02s/it] 11%|         | 683/6500 [1:17:46<11:06:51,  6.88s/it]                                                        11%|         | 683/6500 [1:17:46<11:06:51,  6.88s/it] 11%|         | 684/6500 [1:17:52<10:57:30,  6.78s/it]                                                        11%|         | 684/6500 [1:17:52<10:57:30,  6.78s/it] 11%|         | 685/6500 [1:17:59<10:50:50,  6.72s/it]                                                        11%|         | 685/6500 [1:17:59<10:50:50,  6.72s/it] 11%|         | 686/6500 [1:18:05<10:46:16,  6.67s/it]                                                        11%|         | 686/6500 [1:18:05<10:46:{'loss': 0.8067, 'learning_rate': 9.728300742426609e-05, 'epoch': 0.11}
{'loss': 1.068, 'learning_rate': 9.727514171138492e-05, 'epoch': 0.11}
{'loss': 0.7956, 'learning_rate': 9.726726494822681e-05, 'epoch': 0.11}
{'loss': 0.7933, 'learning_rate': 9.725937713663292e-05, 'epoch': 0.11}
16,  6.67s/it] 11%|         | 687/6500 [1:18:12<10:43:11,  6.64s/it]                                                        11%|         | 687/6500 [1:18:12<10:43:11,  6.64s/it] 11%|         | 688/6500 [1:18:18<10:40:57,  6.62s/it]                                                        11%|         | 688/6500 [1:18:18<10:40:57,  6.62s/it] 11%|         | 689/6500 [1:18:25<10:39:11,  6.60s/it]                                                        11%|         | 689/6500 [1:18:25<10:39:11,  6.60s/it] 11%|         | 690/6500 [1:18:31<10:37:44,  6.59s/it]                                                        11%|         | 690/6500 [1:18:31<10:37:44,  6.59s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.9112944602966309, 'eval_runtime': 1.4803, 'eval_samples_per_second': 8.107, 'eval_steps_per_second': 2.027, 'epoch': 0.11}
                                                        11%|         | 690/6500 [1:18:33<10:37:44,  6.59s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-690
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in I AM HERE AND I AM SAVING THE MODEL
 the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-690
/projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-690
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-690
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-690
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-690/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-690/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-690/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-690/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.7859, 'learning_rate': 9.725147827844697e-05, 'epoch': 0.11}
{'loss': 0.7715, 'learning_rate': 9.724356837551525e-05, 'epoch': 0.11}
{'loss': 0.7774, 'learning_rate': 9.723564742968667e-05, 'epoch': 0.11}
{'loss': 0.8054, 'learning_rate': 9.722771544281271e-05, 'epoch': 0.11}
{'loss': 0.8146, 'learning_rate': 9.721977241674742e-05, 'epoch': 0.11}
{'loss': 0.812, 'learning_rate': 9.721181835334741e-05, 'epoch': 0.11}
 11%|         | 691/6500 [1:18:40<11:28:44,  7.11s/it]                                                        11%|         | 691/6500 [1:18:40<11:28:44,  7.11s/it] 11%|         | 692/6500 [1:18:46<11:12:36,  6.95s/it]                                                        11%|         | 692/6500 [1:18:46<11:12:36,  6.95s/it] 11%|         | 693/6500 [1:18:53<11:01:08,  6.83s/it]                                                        11%|         | 693/6500 [1:18:53<11:01:08,  6.83s/it] 11%|         | 694/6500 [1:18:59<10:53:10,  6.75s/it]                                                        11%|         | 694/6500 [1:18:59<10:53:10,  6.75s/it] 11%|         | 695/6500 [1:19:07<11:15:56,  6.99s/it]                                                        11%|         | 695/6500 [1:19:07<11:15:56,  6.99s/it] 11%|         | 696/6500 [1:19:14<11:03:33,  6.86s/it]                                                        11%|         | 696/6500 [1:19:14<11:03:{'loss': 0.7913, 'learning_rate': 9.720385325447192e-05, 'epoch': 0.11}
{'loss': 0.8012, 'learning_rate': 9.719587712198275e-05, 'epoch': 0.11}
{'loss': 0.8002, 'learning_rate': 9.718788995774423e-05, 'epoch': 0.11}
{'loss': 0.7967, 'learning_rate': 9.717989176362337e-05, 'epoch': 0.11}
33,  6.86s/it] 11%|         | 697/6500 [1:19:20<10:54:37,  6.77s/it]                                                        11%|         | 697/6500 [1:19:20<10:54:37,  6.77s/it] 11%|         | 698/6500 [1:19:27<10:48:04,  6.70s/it]                                                        11%|         | 698/6500 [1:19:27<10:48:04,  6.70s/it] 11%|         | 699/6500 [1:19:33<10:43:43,  6.66s/it]                                                        11%|         | 699/6500 [1:19:33<10:43:43,  6.66s/it] 11%|         | 700/6500 [1:19:40<10:40:53,  6.63s/it]                                                        11%|         | 700/6500 [1:19:40<10:40:53,  6.63s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.9064748287200928, 'eval_runtime': 1.48, 'eval_samples_per_second': 8.108, 'eval_steps_per_second': 2.027, 'epoch': 0.11}
                                                        11%|         | 700/6500 [1:19:41<10:40:53,  6.63s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-700
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in I AM HERE AND I AM SAVING THE MODEL
 the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-700
/projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-700
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-700
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-700
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-700/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-700/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-700/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-700/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.8164, 'learning_rate': 9.717188254148966e-05, 'epoch': 0.11}
{'loss': 0.7936, 'learning_rate': 9.71638622932152e-05, 'epoch': 0.11}
{'loss': 1.0713, 'learning_rate': 9.715583102067469e-05, 'epoch': 0.11}
{'loss': 0.7738, 'learning_rate': 9.714778872574541e-05, 'epoch': 0.11}
{'loss': 0.8198, 'learning_rate': 9.713973541030716e-05, 'epoch': 0.11}
{'loss': 0.769, 'learning_rate': 9.713167107624239e-05, 'epoch': 0.11}
 11%|         | 701/6500 [1:19:48<11:31:34,  7.16s/it]                                                        11%|         | 701/6500 [1:19:48<11:31:34,  7.16s/it] 11%|         | 702/6500 [1:19:55<11:14:16,  6.98s/it]                                                        11%|         | 702/6500 [1:19:55<11:14:16,  6.98s/it] 11%|         | 703/6500 [1:20:01<11:02:01,  6.85s/it]                                                        11%|         | 703/6500 [1:20:01<11:02:01,  6.85s/it] 11%|         | 704/6500 [1:20:08<10:53:19,  6.76s/it]                                                        11%|         | 704/6500 [1:20:08<10:53:19,  6.76s/it] 11%|         | 705/6500 [1:20:14<10:47:06,  6.70s/it]                                                        11%|         | 705/6500 [1:20:14<10:47:06,  6.70s/it] 11%|         | 706/6500 [1:20:21<10:42:52,  6.66s/it]                                                        11%|         | 706/6500 [1:20:21<10:42:{'loss': 0.7708, 'learning_rate': 9.712359572543606e-05, 'epoch': 0.11}
{'loss': 0.7816, 'learning_rate': 9.711550935977576e-05, 'epoch': 0.11}
{'loss': 0.8006, 'learning_rate': 9.71074119811516e-05, 'epoch': 0.11}
{'loss': 0.8188, 'learning_rate': 9.709930359145631e-05, 'epoch': 0.11}
52,  6.66s/it] 11%|         | 707/6500 [1:20:28<10:40:04,  6.63s/it]                                                        11%|         | 707/6500 [1:20:28<10:40:04,  6.63s/it] 11%|         | 708/6500 [1:20:34<10:38:14,  6.61s/it]                                                        11%|         | 708/6500 [1:20:34<10:38:14,  6.61s/it] 11%|         | 709/6500 [1:20:41<10:36:25,  6.59s/it]                                                        11%|         | 709/6500 [1:20:41<10:36:25,  6.59s/it] 11%|         | 710/6500 [1:20:47<10:35:23,  6.58s/it]                                                        11%|         | 710/6500 [1:20:47<10:35:23,  6.58s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.9067463874816895, 'eval_runtime': 1.4781, 'eval_samples_per_second': 8.119, 'eval_steps_per_second': 2.03, 'epoch': 0.11}
                                                        11%|         | 710/6500 [1:20:49<10:35:23,  6.58s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-710
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  I AM HERE AND I AM SAVING THE MODEL
/projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-710the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-710

I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-710
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-710
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-710/pytorch_model.bin
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-710/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-710/pytorch_model.binthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-710/pytorch_model.bin

/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.8062, 'learning_rate': 9.709118419258518e-05, 'epoch': 0.11}
{'loss': 0.771, 'learning_rate': 9.708305378643604e-05, 'epoch': 0.11}
{'loss': 0.8068, 'learning_rate': 9.707491237490937e-05, 'epoch': 0.11}
{'loss': 0.7923, 'learning_rate': 9.706675995990815e-05, 'epoch': 0.11}
{'loss': 0.8029, 'learning_rate': 9.705859654333797e-05, 'epoch': 0.11}
{'loss': 0.7923, 'learning_rate': 9.705042212710695e-05, 'epoch': 0.11}
 11%|         | 711/6500 [1:20:56<11:26:53,  7.12s/it]                                                        11%|         | 711/6500 [1:20:56<11:26:53,  7.12s/it] 11%|         | 712/6500 [1:21:03<11:36:33,  7.22s/it]                                                        11%|         | 712/6500 [1:21:03<11:36:33,  7.22s/it] 11%|         | 713/6500 [1:21:10<11:18:22,  7.03s/it]                                                        11%|         | 713/6500 [1:21:10<11:18:22,  7.03s/it] 11%|         | 714/6500 [1:21:16<11:05:30,  6.90s/it]                                                        11%|         | 714/6500 [1:21:16<11:05:30,  6.90s/it] 11%|         | 715/6500 [1:21:23<10:56:28,  6.81s/it]                                                        11%|         | 715/6500 [1:21:23<10:56:28,  6.81s/it] 11%|         | 716/6500 [1:21:29<10:50:12,  6.74s/it]                                                        11%|         | 716/6500 [1:21:29<10:50:{'loss': 0.8167, 'learning_rate': 9.704223671312584e-05, 'epoch': 0.11}
{'loss': 1.0523, 'learning_rate': 9.703404030330791e-05, 'epoch': 0.11}
{'loss': 0.7653, 'learning_rate': 9.702583289956903e-05, 'epoch': 0.11}
{'loss': 0.8133, 'learning_rate': 9.701761450382765e-05, 'epoch': 0.11}
12,  6.74s/it] 11%|         | 717/6500 [1:21:36<10:45:30,  6.70s/it]                                                        11%|         | 717/6500 [1:21:36<10:45:30,  6.70s/it] 11%|         | 718/6500 [1:21:43<10:41:47,  6.66s/it]                                                        11%|         | 718/6500 [1:21:43<10:41:47,  6.66s/it] 11%|         | 719/6500 [1:21:49<10:39:19,  6.64s/it]                                                        11%|         | 719/6500 [1:21:49<10:39:19,  6.64s/it] 11%|         | 720/6500 [1:21:56<10:37:55,  6.62s/it]                                                        11%|         | 720/6500 [1:21:56<10:37:55,  6.62s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.9056454300880432, 'eval_runtime': 1.4773, 'eval_samples_per_second': 8.123, 'eval_steps_per_second': 2.031, 'epoch': 0.11}
                                                        11%|         | 720/6500 [1:21:57<10:37:55,  6.62s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-720
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-720
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-720
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-720
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-720
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-720/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-720/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-720/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-720/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.7609, 'learning_rate': 9.700938511800474e-05, 'epoch': 0.11}
{'loss': 0.7693, 'learning_rate': 9.700114474402387e-05, 'epoch': 0.11}
{'loss': 0.7857, 'learning_rate': 9.69928933838112e-05, 'epoch': 0.11}
{'loss': 0.7926, 'learning_rate': 9.698463103929542e-05, 'epoch': 0.11}
{'loss': 0.8193, 'learning_rate': 9.69763577124078e-05, 'epoch': 0.11}
{'loss': 0.8016, 'learning_rate': 9.696807340508221e-05, 'epoch': 0.11}
 11%|         | 721/6500 [1:22:04<11:29:21,  7.16s/it]                                                        11%|         | 721/6500 [1:22:04<11:29:21,  7.16s/it] 11%|         | 722/6500 [1:22:11<11:12:48,  6.99s/it]                                                        11%|         | 722/6500 [1:22:11<11:12:48,  6.99s/it] 11%|         | 723/6500 [1:22:17<11:01:15,  6.87s/it]                                                        11%|         | 723/6500 [1:22:17<11:01:15,  6.87s/it] 11%|         | 724/6500 [1:22:24<10:53:15,  6.79s/it]                                                        11%|         | 724/6500 [1:22:24<10:53:15,  6.79s/it] 11%|         | 725/6500 [1:22:31<10:47:47,  6.73s/it]                                                        11%|         | 725/6500 [1:22:31<10:47:47,  6.73s/it] 11%|         | 726/6500 [1:22:37<10:43:50,  6.69s/it]                                                        11%|         | 726/6500 [1:22:37<10:43:{'loss': 0.7672, 'learning_rate': 9.6959778119255e-05, 'epoch': 0.11}
{'loss': 0.7935, 'learning_rate': 9.69514718568652e-05, 'epoch': 0.11}
{'loss': 0.7754, 'learning_rate': 9.69431546198543e-05, 'epoch': 0.11}
{'loss': 0.8229, 'learning_rate': 9.693482641016645e-05, 'epoch': 0.11}
50,  6.69s/it] 11%|         | 727/6500 [1:22:44<10:40:42,  6.66s/it]                                                        11%|         | 727/6500 [1:22:44<10:40:42,  6.66s/it] 11%|         | 728/6500 [1:22:51<11:03:44,  6.90s/it]                                                        11%|         | 728/6500 [1:22:51<11:03:44,  6.90s/it] 11%|         | 729/6500 [1:22:58<10:54:29,  6.80s/it]                                                        11%|         | 729/6500 [1:22:58<10:54:29,  6.80s/it] 11%|         | 730/6500 [1:23:04<10:47:56,  6.74s/it]                                                        11%|         | 730/6500 [1:23:04<10:47:56,  6.74s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.9015311598777771, 'eval_runtime': 1.4814, 'eval_samples_per_second': 8.1, 'eval_steps_per_second': 2.025, 'epoch': 0.11}
                                                        11%|         | 730/6500 [1:23:06<10:47:56,  6.74s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-730
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-730
/projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-730
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-730
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-730
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-730/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-730/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-730/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-730/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.7748, 'learning_rate': 9.692648722974829e-05, 'epoch': 0.11}
{'loss': 1.0545, 'learning_rate': 9.691813708054904e-05, 'epoch': 0.11}
{'loss': 0.795, 'learning_rate': 9.690977596452053e-05, 'epoch': 0.11}
{'loss': 0.7571, 'learning_rate': 9.69014038836171e-05, 'epoch': 0.11}
{'loss': 0.8022, 'learning_rate': 9.689302083979568e-05, 'epoch': 0.11}
{'loss': 0.7478, 'learning_rate': 9.688462683501574e-05, 'epoch': 0.11}
 11%|         | 731/6500 [1:23:13<11:37:44,  7.26s/it]                                                        11%|         | 731/6500 [1:23:13<11:37:44,  7.26s/it] 11%|        | 732/6500 [1:23:19<11:18:11,  7.05s/it]                                                        11%|        | 732/6500 [1:23:19<11:18:11,  7.05s/it] 11%|        | 733/6500 [1:23:26<11:04:13,  6.91s/it]                                                        11%|        | 733/6500 [1:23:26<11:04:13,  6.91s/it] 11%|        | 734/6500 [1:23:33<10:54:35,  6.81s/it]                                                        11%|        | 734/6500 [1:23:33<10:54:35,  6.81s/it] 11%|        | 735/6500 [1:23:39<10:48:01,  6.74s/it]                                                        11%|        | 735/6500 [1:23:39<10:48:01,  6.74s/it] 11%|        | 736/6500 [1:23:46<10:43:22,  6.70s/it]                                                        11%|        | 736/{'loss': 0.7652, 'learning_rate': 9.687622187123936e-05, 'epoch': 0.11}
{'loss': 0.7876, 'learning_rate': 9.686780595043113e-05, 'epoch': 0.11}
{'loss': 0.787, 'learning_rate': 9.68593790745582e-05, 'epoch': 0.11}
{'loss': 0.7951, 'learning_rate': 9.685094124559034e-05, 'epoch': 0.11}
6500 [1:23:46<10:43:22,  6.70s/it] 11%|        | 737/6500 [1:23:52<10:40:29,  6.67s/it]                                                        11%|        | 737/6500 [1:23:52<10:40:29,  6.67s/it] 11%|        | 738/6500 [1:23:59<10:38:13,  6.65s/it]                                                        11%|        | 738/6500 [1:23:59<10:38:13,  6.65s/it] 11%|        | 739/6500 [1:24:06<10:36:20,  6.63s/it]                                                        11%|        | 739/6500 [1:24:06<10:36:20,  6.63s/it] 11%|        | 740/6500 [1:24:12<10:35:11,  6.62s/it]                                                        11%|        | 740/6500 [1:24:12<10:35:11,  6.62s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.9015538692474365, 'eval_runtime': 1.4712, 'eval_samples_per_second': 8.157, 'eval_steps_per_second': 2.039, 'epoch': 0.11}
                                                        11%|        | 740/6500 [1:24:14<10:35:11,  6.62s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-740
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-740
/projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-740
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-740
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-740
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-740/pytorch_model.bin
the pytorch model path is  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-740/pytorch_model.bin
/projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-740/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-740/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.7884, 'learning_rate': 9.684249246549981e-05, 'epoch': 0.11}
{'loss': 0.7679, 'learning_rate': 9.683403273626148e-05, 'epoch': 0.11}
{'loss': 0.7847, 'learning_rate': 9.682556205985274e-05, 'epoch': 0.11}
{'loss': 0.7785, 'learning_rate': 9.68170804382536e-05, 'epoch': 0.11}
{'loss': 0.8235, 'learning_rate': 9.680858787344654e-05, 'epoch': 0.11}
{'loss': 0.782, 'learning_rate': 9.680008436741665e-05, 'epoch': 0.11}
 11%|        | 741/6500 [1:24:20<11:26:16,  7.15s/it]                                                        11%|        | 741/6500 [1:24:20<11:26:16,  7.15s/it] 11%|        | 742/6500 [1:24:27<11:09:57,  6.98s/it]                                                        11%|        | 742/6500 [1:24:27<11:09:57,  6.98s/it] 11%|        | 743/6500 [1:24:34<10:58:47,  6.87s/it]                                                        11%|        | 743/6500 [1:24:34<10:58:47,  6.87s/it] 11%|        | 744/6500 [1:24:41<11:09:00,  6.97s/it]                                                        11%|        | 744/6500 [1:24:41<11:09:00,  6.97s/it] 11%|        | 745/6500 [1:24:47<10:58:03,  6.86s/it]                                                        11%|        | 745/6500 [1:24:47<10:58:03,  6.86s/it] 11%|        | 746/6500 [1:24:54<10:50:27,  6.78s/it]                                                        11%|        | {'loss': 1.0341, 'learning_rate': 9.679156992215162e-05, 'epoch': 0.11}
{'loss': 0.7969, 'learning_rate': 9.67830445396416e-05, 'epoch': 0.12}
{'loss': 0.751, 'learning_rate': 9.677450822187937e-05, 'epoch': 0.12}
{'loss': 0.7874, 'learning_rate': 9.676596097086023e-05, 'epoch': 0.12}
746/6500 [1:24:54<10:50:27,  6.78s/it] 11%|        | 747/6500 [1:25:01<10:45:06,  6.73s/it]                                                        11%|        | 747/6500 [1:25:01<10:45:06,  6.73s/it] 12%|        | 748/6500 [1:25:07<10:41:44,  6.69s/it]                                                        12%|        | 748/6500 [1:25:07<10:41:44,  6.69s/it] 12%|        | 749/6500 [1:25:14<10:38:35,  6.66s/it]                                                        12%|        | 749/6500 [1:25:14<10:38:35,  6.66s/it] 12%|        | 750/6500 [1:25:20<10:36:26,  6.64s/it]                                                        12%|        | 750/6500 [1:25:20<10:36:26,  6.64s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.9009263515472412, 'eval_runtime': 1.4955, 'eval_samples_per_second': 8.024, 'eval_steps_per_second': 2.006, 'epoch': 0.12}
                                                        12%|        | 750/6500 [1:25:22<10:36:26,  6.64s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-750
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-750/projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-750

I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-750
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-750
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-750/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-750/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-750/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-750/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.7568, 'learning_rate': 9.675740278858208e-05, 'epoch': 0.12}
{'loss': 0.7639, 'learning_rate': 9.674883367704529e-05, 'epoch': 0.12}
{'loss': 0.7883, 'learning_rate': 9.674025363825287e-05, 'epoch': 0.12}
{'loss': 0.8009, 'learning_rate': 9.673166267421037e-05, 'epoch': 0.12}
{'loss': 0.7864, 'learning_rate': 9.672306078692583e-05, 'epoch': 0.12}
{'loss': 0.7803, 'learning_rate': 9.671444797840991e-05, 'epoch': 0.12}
 12%|        | 751/6500 [1:25:29<11:29:54,  7.20s/it]                                                        12%|        | 751/6500 [1:25:29<11:29:54,  7.20s/it] 12%|        | 752/6500 [1:25:36<11:13:03,  7.03s/it]                                                        12%|        | 752/6500 [1:25:36<11:13:03,  7.03s/it] 12%|        | 753/6500 [1:25:42<11:00:29,  6.90s/it]                                                        12%|        | 753/6500 [1:25:42<11:00:29,  6.90s/it] 12%|        | 754/6500 [1:25:49<10:51:36,  6.80s/it]                                                        12%|        | 754/6500 [1:25:49<10:51:36,  6.80s/it] 12%|        | 755/6500 [1:25:55<10:45:31,  6.74s/it]                                                        12%|        | 755/6500 [1:25:55<10:45:31,  6.74s/it] 12%|        | 756/6500 [1:26:02<10:41:20,  6.70s/it]                                                        12%|        | {'loss': 0.775, 'learning_rate': 9.670582425067581e-05, 'epoch': 0.12}
{'loss': 0.7805, 'learning_rate': 9.669718960573927e-05, 'epoch': 0.12}
{'loss': 0.7625, 'learning_rate': 9.668854404561858e-05, 'epoch': 0.12}
{'loss': 0.8075, 'learning_rate': 9.66798875723346e-05, 'epoch': 0.12}
756/6500 [1:26:02<10:41:20,  6.70s/it] 12%|        | 757/6500 [1:26:09<10:38:02,  6.67s/it]                                                        12%|        | 757/6500 [1:26:09<10:38:02,  6.67s/it] 12%|        | 758/6500 [1:26:15<10:35:41,  6.64s/it]                                                        12%|        | 758/6500 [1:26:15<10:35:41,  6.64s/it] 12%|        | 759/6500 [1:26:22<10:33:50,  6.62s/it]                                                        12%|        | 759/6500 [1:26:22<10:33:50,  6.62s/it] 12%|        | 760/6500 [1:26:29<10:58:04,  6.88s/it]                                                        12%|        | 760/6500 [1:26:29<10:58:04,  6.88s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8965965509414673, 'eval_runtime': 1.4813, 'eval_samples_per_second': 8.101, 'eval_steps_per_second': 2.025, 'epoch': 0.12}
                                                        12%|        | 760/6500 [1:26:31<10:58:04,  6.88s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-760
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  I AM HERE AND I AM SAVING THE MODEL/projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-760
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-760

I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-760
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-760
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-760/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-760/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-760/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-760/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.7758, 'learning_rate': 9.667122018791071e-05, 'epoch': 0.12}
{'loss': 1.0389, 'learning_rate': 9.666254189437286e-05, 'epoch': 0.12}
{'loss': 0.7788, 'learning_rate': 9.665385269374956e-05, 'epoch': 0.12}
{'loss': 0.769, 'learning_rate': 9.664515258807185e-05, 'epoch': 0.12}
{'loss': 0.7522, 'learning_rate': 9.663644157937336e-05, 'epoch': 0.12}
{'loss': 0.7483, 'learning_rate': 9.662771966969017e-05, 'epoch': 0.12}
 12%|        | 761/6500 [1:26:38<11:40:46,  7.33s/it]                                                        12%|        | 761/6500 [1:26:38<11:40:46,  7.33s/it] 12%|        | 762/6500 [1:26:44<11:18:59,  7.10s/it]                                                        12%|        | 762/6500 [1:26:44<11:18:59,  7.10s/it] 12%|        | 763/6500 [1:26:51<11:04:20,  6.95s/it]                                                        12%|        | 763/6500 [1:26:51<11:04:20,  6.95s/it] 12%|        | 764/6500 [1:26:57<10:53:42,  6.84s/it]                                                        12%|        | 764/6500 [1:26:57<10:53:42,  6.84s/it] 12%|        | 765/6500 [1:27:04<10:46:45,  6.77s/it]                                                        12%|        | 765/6500 [1:27:04<10:46:45,  6.77s/it] 12%|        | 766/6500 [1:27:11<10:41:51,  6.72s/it]                                                        12%|        | {'loss': 0.7521, 'learning_rate': 9.661898686106101e-05, 'epoch': 0.12}
{'loss': 0.7761, 'learning_rate': 9.661024315552714e-05, 'epoch': 0.12}
{'loss': 0.7898, 'learning_rate': 9.66014885551323e-05, 'epoch': 0.12}
{'loss': 0.7915, 'learning_rate': 9.659272306192286e-05, 'epoch': 0.12}
766/6500 [1:27:11<10:41:51,  6.72s/it] 12%|        | 767/6500 [1:27:17<10:38:03,  6.68s/it]                                                        12%|        | 767/6500 [1:27:17<10:38:03,  6.68s/it] 12%|        | 768/6500 [1:27:24<10:35:27,  6.65s/it]                                                        12%|        | 768/6500 [1:27:24<10:35:27,  6.65s/it] 12%|        | 769/6500 [1:27:30<10:33:29,  6.63s/it]                                                        12%|        | 769/6500 [1:27:30<10:33:29,  6.63s/it] 12%|        | 770/6500 [1:27:37<10:32:22,  6.62s/it]                                                        12%|        | 770/6500 [1:27:37<10:32:22,  6.62s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8953363299369812, 'eval_runtime': 1.4787, 'eval_samples_per_second': 8.115, 'eval_steps_per_second': 2.029, 'epoch': 0.12}
                                                        12%|        | 770/6500 [1:27:38<10:32:22,  6.62s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-770
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-770
 /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-770
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-770
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-770
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-770/pytorch_model.bin
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-770/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-770/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-770/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.7639, 'learning_rate': 9.658394667794771e-05, 'epoch': 0.12}
{'loss': 0.7747, 'learning_rate': 9.657515940525826e-05, 'epoch': 0.12}
{'loss': 0.7682, 'learning_rate': 9.656636124590845e-05, 'epoch': 0.12}
{'loss': 0.7765, 'learning_rate': 9.655755220195486e-05, 'epoch': 0.12}
{'loss': 0.7892, 'learning_rate': 9.65487322754565e-05, 'epoch': 0.12}
{'loss': 0.7732, 'learning_rate': 9.653990146847499e-05, 'epoch': 0.12}
 12%|        | 771/6500 [1:27:45<11:21:43,  7.14s/it]                                                        12%|        | 771/6500 [1:27:45<11:21:43,  7.14s/it] 12%|        | 772/6500 [1:27:52<11:06:02,  6.98s/it]                                                        12%|        | 772/6500 [1:27:52<11:06:02,  6.98s/it] 12%|        | 773/6500 [1:27:58<10:55:01,  6.86s/it]                                                        12%|        | 773/6500 [1:27:58<10:55:01,  6.86s/it] 12%|        | 774/6500 [1:28:05<10:47:17,  6.78s/it]                                                        12%|        | 774/6500 [1:28:05<10:47:17,  6.78s/it] 12%|        | 775/6500 [1:28:12<10:41:04,  6.72s/it]                                                        12%|        | 775/6500 [1:28:12<10:41:04,  6.72s/it] 12%|        | 776/6500 [1:28:19<11:00:45,  6.93s/it]                                                        12%|        | {'loss': 1.041, 'learning_rate': 9.653105978307449e-05, 'epoch': 0.12}
{'loss': 0.7467, 'learning_rate': 9.652220722132167e-05, 'epoch': 0.12}
{'loss': 0.7812, 'learning_rate': 9.651334378528578e-05, 'epoch': 0.12}
{'loss': 0.7483, 'learning_rate': 9.650446947703857e-05, 'epoch': 0.12}
776/6500 [1:28:19<11:00:45,  6.93s/it] 12%|        | 777/6500 [1:28:26<10:50:00,  6.81s/it]                                                        12%|        | 777/6500 [1:28:26<10:50:00,  6.81s/it] 12%|        | 778/6500 [1:28:32<10:42:28,  6.74s/it]                                                        12%|        | 778/6500 [1:28:32<10:42:28,  6.74s/it] 12%|        | 779/6500 [1:28:39<10:37:01,  6.68s/it]                                                        12%|        | 779/6500 [1:28:39<10:37:01,  6.68s/it] 12%|        | 780/6500 [1:28:45<10:33:21,  6.64s/it]                                                        12%|        | 780/6500 [1:28:45<10:33:21,  6.64s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8970109820365906, 'eval_runtime': 1.4771, 'eval_samples_per_second': 8.124, 'eval_steps_per_second': 2.031, 'epoch': 0.12}
                                                        12%|        | 780/6500 [1:28:47<10:33:21,  6.64s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-780
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-780
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-780
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-780
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-780
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-780/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-780/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-780/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-780/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.739, 'learning_rate': 9.64955842986544e-05, 'epoch': 0.12}
{'loss': 0.7412, 'learning_rate': 9.648668825221006e-05, 'epoch': 0.12}
{'loss': 0.7788, 'learning_rate': 9.647778133978502e-05, 'epoch': 0.12}
{'loss': 0.7851, 'learning_rate': 9.646886356346116e-05, 'epoch': 0.12}
{'loss': 0.7731, 'learning_rate': 9.645993492532298e-05, 'epoch': 0.12}
{'loss': 0.7508, 'learning_rate': 9.64509954274575e-05, 'epoch': 0.12}
 12%|        | 781/6500 [1:28:54<11:22:02,  7.16s/it]                                                        12%|        | 781/6500 [1:28:54<11:22:02,  7.16s/it] 12%|        | 782/6500 [1:29:00<11:04:56,  6.98s/it]                                                        12%|        | 782/6500 [1:29:00<11:04:56,  6.98s/it] 12%|        | 783/6500 [1:29:07<10:53:40,  6.86s/it]                                                        12%|        | 783/6500 [1:29:07<10:53:40,  6.86s/it] 12%|        | 784/6500 [1:29:13<10:45:20,  6.77s/it]                                                        12%|        | 784/6500 [1:29:13<10:45:20,  6.77s/it] 12%|        | 785/6500 [1:29:20<10:39:31,  6.71s/it]                                                        12%|        | 785/6500 [1:29:20<10:39:31,  6.71s/it] 12%|        | 786/6500 [1:29:26<10:35:50,  6.68s/it]                                                        12%|        | {'loss': 0.7797, 'learning_rate': 9.644204507195426e-05, 'epoch': 0.12}
{'loss': 0.76, 'learning_rate': 9.643308386090537e-05, 'epoch': 0.12}
{'loss': 0.7727, 'learning_rate': 9.642411179640542e-05, 'epoch': 0.12}
{'loss': 0.7709, 'learning_rate': 9.641512888055162e-05, 'epoch': 0.12}
786/6500 [1:29:26<10:35:50,  6.68s/it] 12%|        | 787/6500 [1:29:33<10:33:08,  6.65s/it]                                                        12%|        | 787/6500 [1:29:33<10:33:08,  6.65s/it] 12%|        | 788/6500 [1:29:40<10:31:31,  6.63s/it]                                                        12%|        | 788/6500 [1:29:40<10:31:31,  6.63s/it] 12%|        | 789/6500 [1:29:46<10:30:06,  6.62s/it]                                                        12%|        | 789/6500 [1:29:46<10:30:06,  6.62s/it] 12%|        | 790/6500 [1:29:53<10:28:39,  6.61s/it]                                                        12%|        | 790/6500 [1:29:53<10:28:39,  6.61s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.894489049911499, 'eval_runtime': 1.4756, 'eval_samples_per_second': 8.132, 'eval_steps_per_second': 2.033, 'epoch': 0.12}
                                                        12%|        | 790/6500 [1:29:54<10:28:39,  6.61s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-790
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  I AM HERE AND I AM SAVING THE MODEL/projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-790
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-790

I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-790
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-790
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-790/pytorch_model.binthe pytorch model path is
 /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-790/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-790/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-790/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.7682, 'learning_rate': 9.640613511544365e-05, 'epoch': 0.12}
{'loss': 1.0381, 'learning_rate': 9.639713050318375e-05, 'epoch': 0.12}
{'loss': 0.7424, 'learning_rate': 9.638811504587669e-05, 'epoch': 0.12}
{'loss': 0.781, 'learning_rate': 9.637908874562978e-05, 'epoch': 0.12}
{'loss': 0.7404, 'learning_rate': 9.637005160455287e-05, 'epoch': 0.12}
{'loss': 0.7444, 'learning_rate': 9.636100362475832e-05, 'epoch': 0.12}
 12%|        | 791/6500 [1:30:01<11:20:10,  7.15s/it]                                                        12%|        | 791/6500 [1:30:01<11:20:10,  7.15s/it] 12%|        | 792/6500 [1:30:09<11:28:41,  7.24s/it]                                                        12%|        | 792/6500 [1:30:09<11:28:41,  7.24s/it] 12%|        | 793/6500 [1:30:15<11:10:14,  7.05s/it]                                                        12%|        | 793/6500 [1:30:15<11:10:14,  7.05s/it] 12%|        | 794/6500 [1:30:22<10:57:17,  6.91s/it]                                                        12%|        | 794/6500 [1:30:22<10:57:17,  6.91s/it] 12%|        | 795/6500 [1:30:28<10:48:32,  6.82s/it]                                                        12%|        | 795/6500 [1:30:28<10:48:32,  6.82s/it] 12%|        | 796/6500 [1:30:35<10:42:04,  6.75s/it]                                                        12%|        | {'loss': 0.7577, 'learning_rate': 9.635194480836108e-05, 'epoch': 0.12}
{'loss': 0.7633, 'learning_rate': 9.634287515747856e-05, 'epoch': 0.12}
{'loss': 0.7903, 'learning_rate': 9.633379467423072e-05, 'epoch': 0.12}
{'loss': 0.7877, 'learning_rate': 9.632470336074009e-05, 'epoch': 0.12}
796/6500 [1:30:35<10:42:04,  6.75s/it] 12%|        | 797/6500 [1:30:42<10:37:23,  6.71s/it]                                                        12%|        | 797/6500 [1:30:42<10:37:23,  6.71s/it] 12%|        | 798/6500 [1:30:48<10:34:01,  6.67s/it]                                                        12%|        | 798/6500 [1:30:48<10:34:01,  6.67s/it] 12%|        | 799/6500 [1:30:55<10:31:47,  6.65s/it]                                                        12%|        | 799/6500 [1:30:55<10:31:47,  6.65s/it] 12%|        | 800/6500 [1:31:01<10:30:11,  6.63s/it]                                                        12%|        | 800/6500 [1:31:01<10:30:11,  6.63s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8898413181304932, 'eval_runtime': 1.4806, 'eval_samples_per_second': 8.105, 'eval_steps_per_second': 2.026, 'epoch': 0.12}
                                                        12%|        | 800/6500 [1:31:03<10:30:11,  6.63s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-800
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-800
/projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-800
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-800
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-800
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-800/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-800/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-800/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-800/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.7385, 'learning_rate': 9.631560121913172e-05, 'epoch': 0.12}
{'loss': 0.7729, 'learning_rate': 9.630648825153317e-05, 'epoch': 0.12}
{'loss': 0.7497, 'learning_rate': 9.629736446007454e-05, 'epoch': 0.12}
{'loss': 0.7956, 'learning_rate': 9.628822984688845e-05, 'epoch': 0.12}
{'loss': 0.7449, 'learning_rate': 9.627908441411008e-05, 'epoch': 0.12}
{'loss': 1.0303, 'learning_rate': 9.62699281638771e-05, 'epoch': 0.12}
 12%|        | 801/6500 [1:31:10<11:19:01,  7.15s/it]                                                        12%|        | 801/6500 [1:31:10<11:19:01,  7.15s/it] 12%|        | 802/6500 [1:31:16<11:03:03,  6.98s/it]                                                        12%|        | 802/6500 [1:31:16<11:03:03,  6.98s/it] 12%|        | 803/6500 [1:31:23<10:51:54,  6.87s/it]                                                        12%|        | 803/6500 [1:31:23<10:51:54,  6.87s/it] 12%|        | 804/6500 [1:31:30<10:44:05,  6.78s/it]                                                        12%|        | 804/6500 [1:31:30<10:44:05,  6.78s/it] 12%|        | 805/6500 [1:31:36<10:38:41,  6.73s/it]                                                        12%|        | 805/6500 [1:31:36<10:38:41,  6.73s/it] 12%|        | 806/6500 [1:31:43<10:34:46,  6.69s/it]                                                        12%|        | {'loss': 0.7677, 'learning_rate': 9.626076109832975e-05, 'epoch': 0.12}
{'loss': 0.7302, 'learning_rate': 9.625158321961075e-05, 'epoch': 0.12}
{'loss': 0.7748, 'learning_rate': 9.624239452986539e-05, 'epoch': 0.12}
{'loss': 0.7212, 'learning_rate': 9.623319503124148e-05, 'epoch': 0.12}
806/6500 [1:31:43<10:34:46,  6.69s/it] 12%|        | 807/6500 [1:31:49<10:31:51,  6.66s/it]                                                        12%|        | 807/6500 [1:31:49<10:31:51,  6.66s/it] 12%|        | 808/6500 [1:31:57<10:54:49,  6.90s/it]                                                        12%|        | 808/6500 [1:31:57<10:54:49,  6.90s/it] 12%|        | 809/6500 [1:32:03<10:46:40,  6.82s/it]                                                        12%|        | 809/6500 [1:32:03<10:46:40,  6.82s/it] 12%|        | 810/6500 [1:32:10<10:40:27,  6.75s/it]                                                        12%|        | 810/6500 [1:32:10<10:40:27,  6.75s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8917073011398315, 'eval_runtime': 1.4819, 'eval_samples_per_second': 8.098, 'eval_steps_per_second': 2.024, 'epoch': 0.12}
                                                        12%|        | 810/6500 [1:32:12<10:40:27,  6.75s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-810
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-810
/projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-810
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-810
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-810
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-810/pytorch_model.bin
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-810/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-810/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-810/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.7422, 'learning_rate': 9.622398472588932e-05, 'epoch': 0.12}
{'loss': 0.7535, 'learning_rate': 9.621476361596177e-05, 'epoch': 0.12}
{'loss': 0.7615, 'learning_rate': 9.620553170361423e-05, 'epoch': 0.13}
{'loss': 0.7706, 'learning_rate': 9.619628899100459e-05, 'epoch': 0.13}
{'loss': 0.7725, 'learning_rate': 9.618703548029327e-05, 'epoch': 0.13}
{'loss': 0.7304, 'learning_rate': 9.617777117364322e-05, 'epoch': 0.13}
 12%|        | 811/6500 [1:32:18<11:27:42,  7.25s/it]                                                        12%|        | 811/6500 [1:32:18<11:27:42,  7.25s/it] 12%|        | 812/6500 [1:32:25<11:08:55,  7.06s/it]                                                        12%|        | 812/6500 [1:32:25<11:08:55,  7.06s/it] 13%|        | 813/6500 [1:32:32<10:55:42,  6.92s/it]                                                        13%|        | 813/6500 [1:32:32<10:55:42,  6.92s/it] 13%|        | 814/6500 [1:32:38<10:46:11,  6.82s/it]                                                        13%|        | 814/6500 [1:32:38<10:46:11,  6.82s/it] 13%|        | 815/6500 [1:32:45<10:39:54,  6.75s/it]                                                        13%|        | 815/6500 [1:32:45<10:39:54,  6.75s/it] 13%|        | 816/6500 [1:32:51<10:35:07,  6.70s/it]                                                        13%|        | {'loss': 0.7654, 'learning_rate': 9.616849607321994e-05, 'epoch': 0.13}
{'loss': 0.7382, 'learning_rate': 9.61592101811914e-05, 'epoch': 0.13}
{'loss': 0.8037, 'learning_rate': 9.614991349972815e-05, 'epoch': 0.13}
{'loss': 0.7448, 'learning_rate': 9.614060603100318e-05, 'epoch': 0.13}
816/6500 [1:32:51<10:35:07,  6.70s/it] 13%|        | 817/6500 [1:32:58<10:31:54,  6.67s/it]                                                        13%|        | 817/6500 [1:32:58<10:31:54,  6.67s/it] 13%|        | 818/6500 [1:33:05<10:29:59,  6.65s/it]                                                        13%|        | 818/6500 [1:33:05<10:29:59,  6.65s/it] 13%|        | 819/6500 [1:33:11<10:28:13,  6.63s/it]                                                        13%|        | 819/6500 [1:33:11<10:28:13,  6.63s/it] 13%|        | 820/6500 [1:33:18<10:26:52,  6.62s/it]                                                        13%|        | 820/6500 [1:33:18<10:26:52,  6.62s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8891260623931885, 'eval_runtime': 1.4747, 'eval_samples_per_second': 8.137, 'eval_steps_per_second': 2.034, 'epoch': 0.13}
                                                        13%|        | 820/6500 [1:33:19<10:26:52,  6.62s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-820
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-820
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-820
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-820
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-820
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-820/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-820/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-820/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-820/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 1.0208, 'learning_rate': 9.613128777719214e-05, 'epoch': 0.13}
{'loss': 0.7613, 'learning_rate': 9.612195874047302e-05, 'epoch': 0.13}
{'loss': 0.7256, 'learning_rate': 9.61126189230265e-05, 'epoch': 0.13}
{'loss': 0.7679, 'learning_rate': 9.610326832703565e-05, 'epoch': 0.13}
{'loss': 0.7219, 'learning_rate': 9.609390695468616e-05, 'epoch': 0.13}
{'loss': 0.7462, 'learning_rate': 9.608453480816617e-05, 'epoch': 0.13}
 13%|        | 821/6500 [1:33:26<11:15:51,  7.14s/it]                                                        13%|        | 821/6500 [1:33:26<11:15:51,  7.14s/it] 13%|        | 822/6500 [1:33:33<10:59:47,  6.97s/it]                                                        13%|        | 822/6500 [1:33:33<10:59:47,  6.97s/it] 13%|        | 823/6500 [1:33:39<10:48:39,  6.86s/it]                                                        13%|        | 823/6500 [1:33:39<10:48:39,  6.86s/it] 13%|        | 824/6500 [1:33:46<10:41:18,  6.78s/it]                                                        13%|        | 824/6500 [1:33:46<10:41:18,  6.78s/it] 13%|        | 825/6500 [1:33:53<11:02:05,  7.00s/it]                                                        13%|        | 825/6500 [1:33:53<11:02:05,  7.00s/it] 13%|        | 826/6500 [1:34:00<10:50:35,  6.88s/it]                                                        13%|        | {'loss': 0.7573, 'learning_rate': 9.607515188966638e-05, 'epoch': 0.13}
{'loss': 0.7667, 'learning_rate': 9.606575820137996e-05, 'epoch': 0.13}
{'loss': 0.7669, 'learning_rate': 9.605635374550263e-05, 'epoch': 0.13}
{'loss': 0.7576, 'learning_rate': 9.604693852423268e-05, 'epoch': 0.13}
826/6500 [1:34:00<10:50:35,  6.88s/it] 13%|        | 827/6500 [1:34:07<10:42:29,  6.80s/it]                                                        13%|        | 827/6500 [1:34:07<10:42:29,  6.80s/it] 13%|        | 828/6500 [1:34:13<10:36:12,  6.73s/it]                                                        13%|        | 828/6500 [1:34:13<10:36:12,  6.73s/it] 13%|        | 829/6500 [1:34:20<10:32:23,  6.69s/it]                                                        13%|        | 829/6500 [1:34:20<10:32:23,  6.69s/it] 13%|        | 830/6500 [1:34:26<10:29:31,  6.66s/it]                                                        13%|        | 830/6500 [1:34:26<10:29:31,  6.66s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8871504664421082, 'eval_runtime': 1.4864, 'eval_samples_per_second': 8.073, 'eval_steps_per_second': 2.018, 'epoch': 0.13}
                                                        13%|        | 830/6500 [1:34:28<10:29:31,  6.66s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-830
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-830
/projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-830
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-830
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-830
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-830/pytorch_model.bin
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-830/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-830/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-830/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.7465, 'learning_rate': 9.603751253977079e-05, 'epoch': 0.13}
{'loss': 0.7496, 'learning_rate': 9.602807579432027e-05, 'epoch': 0.13}
{'loss': 0.7365, 'learning_rate': 9.601862829008688e-05, 'epoch': 0.13}
{'loss': 0.7854, 'learning_rate': 9.600917002927893e-05, 'epoch': 0.13}
{'loss': 0.7478, 'learning_rate': 9.599970101410722e-05, 'epoch': 0.13}
{'loss': 1.0145, 'learning_rate': 9.59902212467851e-05, 'epoch': 0.13}
 13%|        | 831/6500 [1:34:35<11:18:22,  7.18s/it]                                                        13%|        | 831/6500 [1:34:35<11:18:22,  7.18s/it] 13%|        | 832/6500 [1:34:41<11:01:50,  7.01s/it]                                                        13%|        | 832/6500 [1:34:41<11:01:50,  7.01s/it] 13%|        | 833/6500 [1:34:48<10:50:08,  6.88s/it]                                                        13%|        | 833/6500 [1:34:48<10:50:08,  6.88s/it] 13%|        | 834/6500 [1:34:55<10:41:55,  6.80s/it]                                                        13%|        | 834/6500 [1:34:55<10:41:55,  6.80s/it] 13%|        | 835/6500 [1:35:01<10:36:00,  6.74s/it]                                                        13%|        | 835/6500 [1:35:01<10:36:00,  6.74s/it] 13%|        | 836/6500 [1:35:08<10:31:39,  6.69s/it]                                                        13%|        | {'loss': 0.7595, 'learning_rate': 9.598073072952836e-05, 'epoch': 0.13}
{'loss': 0.7342, 'learning_rate': 9.59712294645554e-05, 'epoch': 0.13}
{'loss': 0.7394, 'learning_rate': 9.596171745408705e-05, 'epoch': 0.13}
{'loss': 0.7334, 'learning_rate': 9.595219470034671e-05, 'epoch': 0.13}
836/6500 [1:35:08<10:31:39,  6.69s/it] 13%|        | 837/6500 [1:35:14<10:28:33,  6.66s/it]                                                        13%|        | 837/6500 [1:35:14<10:28:33,  6.66s/it] 13%|        | 838/6500 [1:35:21<10:26:26,  6.64s/it]                                                        13%|        | 838/6500 [1:35:21<10:26:26,  6.64s/it] 13%|        | 839/6500 [1:35:28<10:25:08,  6.63s/it]                                                        13%|        | 839/6500 [1:35:28<10:25:08,  6.63s/it] 13%|        | 840/6500 [1:35:34<10:24:03,  6.62s/it]                                                        13%|        | 840/6500 [1:35:34<10:24:03,  6.62s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8875542283058167, 'eval_runtime': 1.4864, 'eval_samples_per_second': 8.073, 'eval_steps_per_second': 2.018, 'epoch': 0.13}
                                                        13%|        | 840/6500 [1:35:36<10:24:03,  6.62s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-840
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-840
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-840
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-840
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-840
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-840/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-840/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-840/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-840/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.7288, 'learning_rate': 9.594266120556023e-05, 'epoch': 0.13}
{'loss': 0.7479, 'learning_rate': 9.593311697195605e-05, 'epoch': 0.13}
{'loss': 0.7699, 'learning_rate': 9.592356200176504e-05, 'epoch': 0.13}
{'loss': 0.7685, 'learning_rate': 9.591399629722066e-05, 'epoch': 0.13}
{'loss': 0.7441, 'learning_rate': 9.590441986055878e-05, 'epoch': 0.13}
{'loss': 0.7423, 'learning_rate': 9.589483269401786e-05, 'epoch': 0.13}
 13%|        | 841/6500 [1:35:43<11:39:34,  7.42s/it]                                                        13%|        | 841/6500 [1:35:43<11:39:34,  7.42s/it] 13%|        | 842/6500 [1:35:50<11:16:08,  7.17s/it]                                                        13%|        | 842/6500 [1:35:50<11:16:08,  7.17s/it] 13%|        | 843/6500 [1:35:57<10:59:39,  7.00s/it]                                                        13%|        | 843/6500 [1:35:57<10:59:39,  7.00s/it] 13%|        | 844/6500 [1:36:03<10:48:28,  6.88s/it]                                                        13%|        | 844/6500 [1:36:03<10:48:28,  6.88s/it] 13%|        | 845/6500 [1:36:10<10:40:31,  6.80s/it]                                                        13%|        | 845/6500 [1:36:10<10:40:31,  6.80s/it] 13%|        | 846/6500 [1:36:16<10:34:23,  6.73s/it]                                                        13%|        | {'loss': 0.7401, 'learning_rate': 9.588523479983887e-05, 'epoch': 0.13}
{'loss': 0.7476, 'learning_rate': 9.58756261802652e-05, 'epoch': 0.13}
{'loss': 0.7638, 'learning_rate': 9.586600683754287e-05, 'epoch': 0.13}
{'loss': 0.7483, 'learning_rate': 9.58563767739203e-05, 'epoch': 0.13}
846/6500 [1:36:16<10:34:23,  6.73s/it] 13%|        | 847/6500 [1:36:23<10:30:18,  6.69s/it]                                                        13%|        | 847/6500 [1:36:23<10:30:18,  6.69s/it] 13%|        | 848/6500 [1:36:30<10:27:29,  6.66s/it]                                                        13%|        | 848/6500 [1:36:30<10:27:29,  6.66s/it] 13%|        | 849/6500 [1:36:36<10:25:38,  6.64s/it]                                                        13%|        | 849/6500 [1:36:36<10:25:38,  6.64s/it] 13%|        | 850/6500 [1:36:43<10:24:13,  6.63s/it]                                                        13%|        | 850/6500 [1:36:43<10:24:13,  6.63s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.887521505355835, 'eval_runtime': 1.5032, 'eval_samples_per_second': 7.983, 'eval_steps_per_second': 1.996, 'epoch': 0.13}
                                                        13%|        | 850/6500 [1:36:44<10:24:13,  6.63s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-850
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-850
 /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-850
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-850
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-850
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-850/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-850/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-850/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-850/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 1.0222, 'learning_rate': 9.584673599164846e-05, 'epoch': 0.13}
{'loss': 0.7403, 'learning_rate': 9.583708449298083e-05, 'epoch': 0.13}
{'loss': 0.7432, 'learning_rate': 9.582742228017342e-05, 'epoch': 0.13}
{'loss': 0.7259, 'learning_rate': 9.581774935548467e-05, 'epoch': 0.13}
{'loss': 0.7114, 'learning_rate': 9.580806572117557e-05, 'epoch': 0.13}
{'loss': 0.7224, 'learning_rate': 9.579837137950966e-05, 'epoch': 0.13}
 13%|        | 851/6500 [1:36:51<11:14:36,  7.17s/it]                                                        13%|        | 851/6500 [1:36:51<11:14:36,  7.17s/it] 13%|        | 852/6500 [1:36:58<10:57:58,  6.99s/it]                                                        13%|        | 852/6500 [1:36:58<10:57:58,  6.99s/it] 13%|        | 853/6500 [1:37:04<10:46:27,  6.87s/it]                                                        13%|        | 853/6500 [1:37:04<10:46:27,  6.87s/it] 13%|        | 854/6500 [1:37:11<10:38:06,  6.78s/it]                                                        13%|        | 854/6500 [1:37:11<10:38:06,  6.78s/it] 13%|        | 855/6500 [1:37:18<10:32:29,  6.72s/it]                                                        13%|        | 855/6500 [1:37:18<10:32:29,  6.72s/it] 13%|        | 856/6500 [1:37:24<10:28:46,  6.68s/it]                                                        13%|        | {'loss': 0.7553, 'learning_rate': 9.578866633275288e-05, 'epoch': 0.13}
{'loss': 0.7634, 'learning_rate': 9.577895058317374e-05, 'epoch': 0.13}
{'loss': 0.7455, 'learning_rate': 9.576922413304326e-05, 'epoch': 0.13}
{'loss': 0.728, 'learning_rate': 9.575948698463491e-05, 'epoch': 0.13}
856/6500 [1:37:24<10:28:46,  6.68s/it] 13%|        | 857/6500 [1:37:32<10:50:06,  6.91s/it]                                                        13%|        | 857/6500 [1:37:32<10:50:06,  6.91s/it] 13%|        | 858/6500 [1:37:38<10:40:46,  6.81s/it]                                                        13%|        | 858/6500 [1:37:38<10:40:46,  6.81s/it] 13%|        | 859/6500 [1:37:45<10:34:43,  6.75s/it]                                                        13%|        | 859/6500 [1:37:45<10:34:43,  6.75s/it] 13%|        | 860/6500 [1:37:51<10:29:58,  6.70s/it]                                                        13%|        | 860/6500 [1:37:51<10:29:58,  6.70s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8856195211410522, 'eval_runtime': 1.4854, 'eval_samples_per_second': 8.079, 'eval_steps_per_second': 2.02, 'epoch': 0.13}
                                                        13%|        | 860/6500 [1:37:53<10:29:58,  6.70s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-860
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-860
/projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-860
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-860
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-860
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-860/pytorch_model.bin
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-860/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-860/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-860/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.7505, 'learning_rate': 9.574973914022469e-05, 'epoch': 0.13}
{'loss': 0.7295, 'learning_rate': 9.57399806020911e-05, 'epoch': 0.13}
{'loss': 0.7509, 'learning_rate': 9.573021137251516e-05, 'epoch': 0.13}
{'loss': 0.7516, 'learning_rate': 9.572043145378038e-05, 'epoch': 0.13}
{'loss': 0.7374, 'learning_rate': 9.571064084817271e-05, 'epoch': 0.13}
{'loss': 1.0137, 'learning_rate': 9.570083955798065e-05, 'epoch': 0.13}
 13%|        | 861/6500 [1:38:00<11:18:31,  7.22s/it]                                                        13%|        | 861/6500 [1:38:00<11:18:31,  7.22s/it] 13%|        | 862/6500 [1:38:06<11:00:35,  7.03s/it]                                                        13%|        | 862/6500 [1:38:06<11:00:35,  7.03s/it] 13%|        | 863/6500 [1:38:13<10:47:55,  6.90s/it]                                                        13%|        | 863/6500 [1:38:13<10:47:55,  6.90s/it] 13%|        | 864/6500 [1:38:20<10:39:11,  6.80s/it]                                                        13%|        | 864/6500 [1:38:20<10:39:11,  6.80s/it] 13%|        | 865/6500 [1:38:26<10:32:58,  6.74s/it]                                                        13%|        | 865/6500 [1:38:26<10:32:58,  6.74s/it] 13%|        | 866/6500 [1:38:33<10:27:55,  6.69s/it]                                                        13%|        | {'loss': 0.7157, 'learning_rate': 9.569102758549524e-05, 'epoch': 0.13}
{'loss': 0.7553, 'learning_rate': 9.568120493300993e-05, 'epoch': 0.13}
{'loss': 0.7114, 'learning_rate': 9.567137160282071e-05, 'epoch': 0.13}
{'loss': 0.7098, 'learning_rate': 9.566152759722606e-05, 'epoch': 0.13}
866/6500 [1:38:33<10:27:55,  6.69s/it] 13%|        | 867/6500 [1:38:39<10:24:49,  6.66s/it]                                                        13%|        | 867/6500 [1:38:39<10:24:49,  6.66s/it] 13%|        | 868/6500 [1:38:46<10:22:42,  6.63s/it]                                                        13%|        | 868/6500 [1:38:46<10:22:42,  6.63s/it] 13%|        | 869/6500 [1:38:52<10:21:05,  6.62s/it]                                                        13%|        | 869/6500 [1:38:52<10:21:05,  6.62s/it] 13%|        | 870/6500 [1:38:59<10:19:48,  6.61s/it]                                                        13%|        | 870/6500 [1:38:59<10:19:48,  6.61s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8836925625801086, 'eval_runtime': 1.4814, 'eval_samples_per_second': 8.101, 'eval_steps_per_second': 2.025, 'epoch': 0.13}
                                                        13%|        | 870/6500 [1:39:01<10:19:48,  6.61s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-870
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  I AM HERE AND I AM SAVING THE MODEL/projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-870
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-870

I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-870
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-870
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-870/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-870/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-870/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-870/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.7258, 'learning_rate': 9.565167291852697e-05, 'epoch': 0.13}
{'loss': 0.7261, 'learning_rate': 9.56418075690269e-05, 'epoch': 0.13}
{'loss': 0.7687, 'learning_rate': 9.563193155103181e-05, 'epoch': 0.13}
{'loss': 0.7538, 'learning_rate': 9.562204486685017e-05, 'epoch': 0.13}
{'loss': 0.7304, 'learning_rate': 9.561214751879292e-05, 'epoch': 0.13}
{'loss': 0.7375, 'learning_rate': 9.560223950917353e-05, 'epoch': 0.13}
 13%|        | 871/6500 [1:39:07<11:10:19,  7.14s/it]                                                        13%|        | 871/6500 [1:39:07<11:10:19,  7.14s/it] 13%|        | 872/6500 [1:39:14<10:54:18,  6.98s/it]                                                        13%|        | 872/6500 [1:39:14<10:54:18,  6.98s/it] 13%|        | 873/6500 [1:39:22<11:11:29,  7.16s/it]                                                        13%|        | 873/6500 [1:39:22<11:11:29,  7.16s/it] 13%|        | 874/6500 [1:39:28<10:55:00,  6.99s/it]                                                        13%|        | 874/6500 [1:39:28<10:55:00,  6.99s/it] 13%|        | 875/6500 [1:39:35<10:43:36,  6.87s/it]                                                        13%|        | 875/6500 [1:39:35<10:43:36,  6.87s/it] 13%|        | 876/6500 [1:39:41<10:35:40,  6.78s/it]                                                        13%|        | {'loss': 0.7324, 'learning_rate': 9.559232084030791e-05, 'epoch': 0.13}
{'loss': 0.7683, 'learning_rate': 9.558239151451451e-05, 'epoch': 0.14}
{'loss': 0.7159, 'learning_rate': 9.557245153411423e-05, 'epoch': 0.14}
{'loss': 0.9996, 'learning_rate': 9.556250090143049e-05, 'epoch': 0.14}
876/6500 [1:39:41<10:35:40,  6.78s/it] 13%|        | 877/6500 [1:39:48<10:29:31,  6.72s/it]                                                        13%|        | 877/6500 [1:39:48<10:29:31,  6.72s/it] 14%|        | 878/6500 [1:39:55<10:25:49,  6.68s/it]                                                        14%|        | 878/6500 [1:39:55<10:25:49,  6.68s/it] 14%|        | 879/6500 [1:40:01<10:23:08,  6.65s/it]                                                        14%|        | 879/6500 [1:40:01<10:23:08,  6.65s/it] 14%|        | 880/6500 [1:40:08<10:20:57,  6.63s/it]                                                        14%|        | 880/6500 [1:40:08<10:20:57,  6.63s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.881975531578064, 'eval_runtime': 1.4784, 'eval_samples_per_second': 8.117, 'eval_steps_per_second': 2.029, 'epoch': 0.14}
                                                        14%|        | 880/6500 [1:40:09<10:20:57,  6.63s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-880
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-880
 /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-880
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-880
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-880
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-880/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-880/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-880/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-880/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.7619, 'learning_rate': 9.55525396187892e-05, 'epoch': 0.14}
{'loss': 0.7078, 'learning_rate': 9.554256768851873e-05, 'epoch': 0.14}
{'loss': 0.7564, 'learning_rate': 9.553258511294996e-05, 'epoch': 0.14}
{'loss': 0.6979, 'learning_rate': 9.552259189441626e-05, 'epoch': 0.14}
{'loss': 0.7147, 'learning_rate': 9.55125880352535e-05, 'epoch': 0.14}
{'loss': 0.728, 'learning_rate': 9.55025735378e-05, 'epoch': 0.14}
 14%|        | 881/6500 [1:40:16<11:09:38,  7.15s/it]                                                        14%|        | 881/6500 [1:40:16<11:09:38,  7.15s/it] 14%|        | 882/6500 [1:40:23<10:53:09,  6.98s/it]                                                        14%|        | 882/6500 [1:40:23<10:53:09,  6.98s/it] 14%|        | 883/6500 [1:40:29<10:41:39,  6.85s/it]                                                        14%|        | 883/6500 [1:40:29<10:41:39,  6.85s/it] 14%|        | 884/6500 [1:40:36<10:33:49,  6.77s/it]                                                        14%|        | 884/6500 [1:40:36<10:33:49,  6.77s/it] 14%|        | 885/6500 [1:40:42<10:28:26,  6.72s/it]                                                        14%|        | 885/6500 [1:40:42<10:28:26,  6.72s/it] 14%|        | 886/6500 [1:40:49<10:24:40,  6.68s/it]                                                        14%|        | {'loss': 0.7295, 'learning_rate': 9.549254840439659e-05, 'epoch': 0.14}
{'loss': 0.7617, 'learning_rate': 9.54825126373866e-05, 'epoch': 0.14}
{'loss': 0.7456, 'learning_rate': 9.547246623911582e-05, 'epoch': 0.14}
{'loss': 0.7075, 'learning_rate': 9.546240921193253e-05, 'epoch': 0.14}
886/6500 [1:40:49<10:24:40,  6.68s/it] 14%|        | 887/6500 [1:40:56<10:21:59,  6.65s/it]                                                        14%|        | 887/6500 [1:40:56<10:21:59,  6.65s/it] 14%|        | 888/6500 [1:41:02<10:20:09,  6.63s/it]                                                        14%|        | 888/6500 [1:41:02<10:20:09,  6.63s/it] 14%|        | 889/6500 [1:41:10<10:42:36,  6.87s/it]                                                        14%|        | 889/6500 [1:41:10<10:42:36,  6.87s/it] 14%|        | 890/6500 [1:41:16<10:34:50,  6.79s/it]                                                        14%|        | 890/6500 [1:41:16<10:34:50,  6.79s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8782637119293213, 'eval_runtime': 1.4794, 'eval_samples_per_second': 8.112, 'eval_steps_per_second': 2.028, 'epoch': 0.14}
                                                        14%|        | 890/6500 [1:41:18<10:34:50,  6.79s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-890
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-890I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-890

I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-890
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-890
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-890/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-890/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-890/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-890/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.7447, 'learning_rate': 9.54523415581875e-05, 'epoch': 0.14}
{'loss': 0.7183, 'learning_rate': 9.544226328023398e-05, 'epoch': 0.14}
{'loss': 0.771, 'learning_rate': 9.543217438042773e-05, 'epoch': 0.14}
{'loss': 0.7229, 'learning_rate': 9.542207486112694e-05, 'epoch': 0.14}
{'loss': 0.9942, 'learning_rate': 9.541196472469233e-05, 'epoch': 0.14}
{'loss': 0.7469, 'learning_rate': 9.540184397348706e-05, 'epoch': 0.14}
 14%|        | 891/6500 [1:41:24<11:18:05,  7.25s/it]                                                        14%|        | 891/6500 [1:41:24<11:18:05,  7.25s/it] 14%|        | 892/6500 [1:41:31<10:59:06,  7.05s/it]                                                        14%|        | 892/6500 [1:41:31<10:59:06,  7.05s/it] 14%|        | 893/6500 [1:41:38<10:45:52,  6.91s/it]                                                        14%|        | 893/6500 [1:41:38<10:45:52,  6.91s/it] 14%|        | 894/6500 [1:41:44<10:36:38,  6.81s/it]                                                        14%|        | 894/6500 [1:41:44<10:36:38,  6.81s/it] 14%|        | 895/6500 [1:41:51<10:30:03,  6.74s/it]                                                        14%|        | 895/6500 [1:41:51<10:30:03,  6.74s/it] 14%|        | 896/6500 [1:41:57<10:25:06,  6.69s/it]                                                        14%|        | {'loss': 0.7082, 'learning_rate': 9.539171260987681e-05, 'epoch': 0.14}
{'loss': 0.7434, 'learning_rate': 9.538157063622974e-05, 'epoch': 0.14}
{'loss': 0.7074, 'learning_rate': 9.537141805491646e-05, 'epoch': 0.14}
{'loss': 0.7173, 'learning_rate': 9.536125486831005e-05, 'epoch': 0.14}
896/6500 [1:41:57<10:25:06,  6.69s/it] 14%|        | 897/6500 [1:42:04<10:21:52,  6.66s/it]                                                        14%|        | 897/6500 [1:42:04<10:21:52,  6.66s/it] 14%|        | 898/6500 [1:42:11<10:19:15,  6.63s/it]                                                        14%|        | 898/6500 [1:42:11<10:19:15,  6.63s/it] 14%|        | 899/6500 [1:42:17<10:17:46,  6.62s/it]                                                        14%|        | 899/6500 [1:42:17<10:17:46,  6.62s/it] 14%|        | 900/6500 [1:42:24<10:16:45,  6.61s/it]                                                        14%|        | 900/6500 [1:42:24<10:16:45,  6.61s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8782577514648438, 'eval_runtime': 1.4783, 'eval_samples_per_second': 8.118, 'eval_steps_per_second': 2.029, 'epoch': 0.14}
                                                        14%|        | 900/6500 [1:42:25<10:16:45,  6.61s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-900
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-900
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-900
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-900
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-900
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-900/pytorch_model.bin
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-900/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-900/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-900/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.7379, 'learning_rate': 9.535108107878612e-05, 'epoch': 0.14}
{'loss': 0.7359, 'learning_rate': 9.534089668872274e-05, 'epoch': 0.14}
{'loss': 0.7407, 'learning_rate': 9.533070170050042e-05, 'epoch': 0.14}
{'loss': 0.7366, 'learning_rate': 9.53204961165022e-05, 'epoch': 0.14}
{'loss': 0.7155, 'learning_rate': 9.531027993911356e-05, 'epoch': 0.14}
{'loss': 0.7278, 'learning_rate': 9.53000531707225e-05, 'epoch': 0.14}
 14%|        | 901/6500 [1:42:32<11:08:28,  7.16s/it]                                                        14%|        | 901/6500 [1:42:32<11:08:28,  7.16s/it] 14%|        | 902/6500 [1:42:39<10:52:03,  6.99s/it]                                                        14%|        | 902/6500 [1:42:39<10:52:03,  6.99s/it] 14%|        | 903/6500 [1:42:45<10:40:10,  6.86s/it]                                                        14%|        | 903/6500 [1:42:45<10:40:10,  6.86s/it] 14%|        | 904/6500 [1:42:52<10:31:58,  6.78s/it]                                                        14%|        | 904/6500 [1:42:52<10:31:58,  6.78s/it] 14%|        | 905/6500 [1:42:59<10:51:04,  6.98s/it]                                                        14%|        | 905/6500 [1:42:59<10:51:04,  6.98s/it] 14%|        | 906/6500 [1:43:06<10:39:27,  6.86s/it]                                                        14%|        | {'loss': 0.7126, 'learning_rate': 9.528981581371942e-05, 'epoch': 0.14}
{'loss': 0.7655, 'learning_rate': 9.527956787049727e-05, 'epoch': 0.14}
{'loss': 0.7335, 'learning_rate': 9.526930934345142e-05, 'epoch': 0.14}
{'loss': 0.9872, 'learning_rate': 9.525904023497975e-05, 'epoch': 0.14}
906/6500 [1:43:06<10:39:27,  6.86s/it] 14%|        | 907/6500 [1:43:13<10:31:30,  6.77s/it]                                                        14%|        | 907/6500 [1:43:13<10:31:30,  6.77s/it] 14%|        | 908/6500 [1:43:19<10:25:50,  6.71s/it]                                                        14%|        | 908/6500 [1:43:19<10:25:50,  6.71s/it] 14%|        | 909/6500 [1:43:26<10:21:50,  6.67s/it]                                                        14%|        | 909/6500 [1:43:26<10:21:50,  6.67s/it] 14%|        | 910/6500 [1:43:32<10:19:00,  6.64s/it]                                                        14%|        | 910/6500 [1:43:32<10:19:00,  6.64s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8776348233222961, 'eval_runtime': 1.4811, 'eval_samples_per_second': 8.102, 'eval_steps_per_second': 2.026, 'epoch': 0.14}
                                                        14%|        | 910/6500 [1:43:34<10:19:00,  6.64s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-910
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-910
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-910
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-910
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-910
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-910/pytorch_model.bin
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-910/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-910/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-910/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.7368, 'learning_rate': 9.524876054748262e-05, 'epoch': 0.14}
{'loss': 0.7149, 'learning_rate': 9.523847028336283e-05, 'epoch': 0.14}
{'loss': 0.7173, 'learning_rate': 9.522816944502565e-05, 'epoch': 0.14}
{'loss': 0.6969, 'learning_rate': 9.521785803487889e-05, 'epoch': 0.14}
{'loss': 0.714, 'learning_rate': 9.52075360553327e-05, 'epoch': 0.14}
{'loss': 0.7265, 'learning_rate': 9.519720350879985e-05, 'epoch': 0.14}
 14%|        | 911/6500 [1:43:41<11:06:29,  7.16s/it]                                                        14%|        | 911/6500 [1:43:41<11:06:29,  7.16s/it] 14%|        | 912/6500 [1:43:47<10:50:12,  6.98s/it]                                                        14%|        | 912/6500 [1:43:47<10:50:12,  6.98s/it] 14%|        | 913/6500 [1:43:54<10:38:29,  6.86s/it]                                                        14%|        | 913/6500 [1:43:54<10:38:29,  6.86s/it] 14%|        | 914/6500 [1:44:00<10:30:33,  6.77s/it]                                                        14%|        | 914/6500 [1:44:00<10:30:33,  6.77s/it] 14%|        | 915/6500 [1:44:07<10:25:35,  6.72s/it]                                                        14%|        | 915/6500 [1:44:07<10:25:35,  6.72s/it] 14%|        | 916/6500 [1:44:13<10:21:26,  6.68s/it]                                                        14%|        | {'loss': 0.7349, 'learning_rate': 9.518686039769548e-05, 'epoch': 0.14}
{'loss': 0.7371, 'learning_rate': 9.517650672443722e-05, 'epoch': 0.14}
{'loss': 0.7152, 'learning_rate': 9.51661424914452e-05, 'epoch': 0.14}
{'loss': 0.7241, 'learning_rate': 9.515576770114199e-05, 'epoch': 0.14}
916/6500 [1:44:13<10:21:26,  6.68s/it] 14%|        | 917/6500 [1:44:20<10:18:14,  6.64s/it]                                                        14%|        | 917/6500 [1:44:20<10:18:14,  6.64s/it] 14%|        | 918/6500 [1:44:27<10:16:14,  6.62s/it]                                                        14%|        | 918/6500 [1:44:27<10:16:14,  6.62s/it] 14%|        | 919/6500 [1:44:33<10:14:59,  6.61s/it]                                                        14%|        | 919/6500 [1:44:33<10:14:59,  6.61s/it] 14%|        | 920/6500 [1:44:40<10:13:57,  6.60s/it]                                                        14%|        | 920/6500 [1:44:40<10:13:57,  6.60s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8758163452148438, 'eval_runtime': 1.4842, 'eval_samples_per_second': 8.085, 'eval_steps_per_second': 2.021, 'epoch': 0.14}
                                                        14%|        | 920/6500 [1:44:41<10:13:57,  6.60s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-920
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-920I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-920

I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-920
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-920
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-920/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-920/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-920/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-920/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.7249, 'learning_rate': 9.514538235595262e-05, 'epoch': 0.14}
{'loss': 0.7148, 'learning_rate': 9.513498645830462e-05, 'epoch': 0.14}
{'loss': 0.758, 'learning_rate': 9.512458001062797e-05, 'epoch': 0.14}
{'loss': 0.7199, 'learning_rate': 9.511416301535508e-05, 'epoch': 0.14}
{'loss': 0.9951, 'learning_rate': 9.51037354749209e-05, 'epoch': 0.14}
{'loss': 0.7234, 'learning_rate': 9.509329739176278e-05, 'epoch': 0.14}
 14%|        | 921/6500 [1:44:48<11:03:40,  7.14s/it]                                                        14%|        | 921/6500 [1:44:48<11:03:40,  7.14s/it] 14%|        | 922/6500 [1:44:55<11:05:17,  7.16s/it]                                                        14%|        | 922/6500 [1:44:55<11:05:17,  7.16s/it] 14%|        | 923/6500 [1:45:02<10:49:31,  6.99s/it]                                                        14%|        | 923/6500 [1:45:02<10:49:31,  6.99s/it] 14%|        | 924/6500 [1:45:09<10:39:02,  6.88s/it]                                                        14%|        | 924/6500 [1:45:09<10:39:02,  6.88s/it] 14%|        | 925/6500 [1:45:15<10:31:05,  6.79s/it]                                                        14%|        | 925/6500 [1:45:15<10:31:05,  6.79s/it] 14%|        | 926/6500 [1:45:22<10:25:16,  6.73s/it]                                                        14%|        | {'loss': 0.7175, 'learning_rate': 9.508284876832058e-05, 'epoch': 0.14}
{'loss': 0.7089, 'learning_rate': 9.507238960703659e-05, 'epoch': 0.14}
{'loss': 0.6987, 'learning_rate': 9.506191991035556e-05, 'epoch': 0.14}
{'loss': 0.6993, 'learning_rate': 9.505143968072474e-05, 'epoch': 0.14}
926/6500 [1:45:22<10:25:16,  6.73s/it] 14%|        | 927/6500 [1:45:28<10:21:35,  6.69s/it]                                                        14%|        | 927/6500 [1:45:28<10:21:35,  6.69s/it] 14%|        | 928/6500 [1:45:35<10:18:43,  6.66s/it]                                                        14%|        | 928/6500 [1:45:35<10:18:43,  6.66s/it] 14%|        | 929/6500 [1:45:42<10:16:22,  6.64s/it]                                                        14%|        | 929/6500 [1:45:42<10:16:22,  6.64s/it] 14%|        | 930/6500 [1:45:48<10:14:56,  6.62s/it]                                                        14%|        | 930/6500 [1:45:48<10:14:56,  6.62s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8743142485618591, 'eval_runtime': 1.4878, 'eval_samples_per_second': 8.066, 'eval_steps_per_second': 2.016, 'epoch': 0.14}
                                                        14%|        | 930/6500 [1:45:50<10:14:56,  6.62s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-930
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in   /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-930
/projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-930
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-930
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-930
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-930/pytorch_model.bin
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-930/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-930/pytorch_model.binthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-930/pytorch_model.bin

/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.7344, 'learning_rate': 9.50409489205938e-05, 'epoch': 0.14}
{'loss': 0.7453, 'learning_rate': 9.50304476324149e-05, 'epoch': 0.14}
{'loss': 0.729, 'learning_rate': 9.501993581864268e-05, 'epoch': 0.14}
{'loss': 0.7232, 'learning_rate': 9.500941348173417e-05, 'epoch': 0.14}
{'loss': 0.7211, 'learning_rate': 9.499888062414893e-05, 'epoch': 0.14}
{'loss': 0.7129, 'learning_rate': 9.498833724834895e-05, 'epoch': 0.14}
 14%|        | 931/6500 [1:45:57<11:04:37,  7.16s/it]                                                        14%|        | 931/6500 [1:45:57<11:04:37,  7.16s/it] 14%|        | 932/6500 [1:46:03<10:48:11,  6.98s/it]                                                        14%|        | 932/6500 [1:46:03<10:48:11,  6.98s/it] 14%|        | 933/6500 [1:46:10<10:36:53,  6.86s/it]                                                        14%|        | 933/6500 [1:46:10<10:36:53,  6.86s/it] 14%|        | 934/6500 [1:46:16<10:29:08,  6.78s/it]                                                        14%|        | 934/6500 [1:46:16<10:29:08,  6.78s/it] 14%|        | 935/6500 [1:46:23<10:23:40,  6.72s/it]                                                        14%|        | 935/6500 [1:46:23<10:23:40,  6.72s/it] 14%|        | 936/6500 [1:46:29<10:19:32,  6.68s/it]                                                        14%|        | {'loss': 0.7172, 'learning_rate': 9.497778335679865e-05, 'epoch': 0.14}
{'loss': 0.7348, 'learning_rate': 9.496721895196497e-05, 'epoch': 0.14}
{'loss': 0.716, 'learning_rate': 9.495664403631727e-05, 'epoch': 0.14}
{'loss': 0.9929, 'learning_rate': 9.494605861232736e-05, 'epoch': 0.14}
936/6500 [1:46:29<10:19:32,  6.68s/it] 14%|        | 937/6500 [1:46:36<10:16:03,  6.64s/it]                                                        14%|        | 937/6500 [1:46:36<10:16:03,  6.64s/it] 14%|        | 938/6500 [1:46:43<10:38:21,  6.89s/it]                                                        14%|        | 938/6500 [1:46:43<10:38:21,  6.89s/it] 14%|        | 939/6500 [1:46:50<10:29:40,  6.79s/it]                                                        14%|        | 939/6500 [1:46:50<10:29:40,  6.79s/it] 14%|        | 940/6500 [1:46:57<10:23:22,  6.73s/it]                                                        14%|        | 940/6500 [1:46:57<10:23:22,  6.73s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.875603973865509, 'eval_runtime': 1.4807, 'eval_samples_per_second': 8.105, 'eval_steps_per_second': 2.026, 'epoch': 0.14}
                                                        14%|        | 940/6500 [1:46:58<10:23:22,  6.73s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-940
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-940
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-940
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-940
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-940
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-940/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-940/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-940/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-940/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.6969, 'learning_rate': 9.493546268246954e-05, 'epoch': 0.14}
{'loss': 0.7268, 'learning_rate': 9.492485624922051e-05, 'epoch': 0.14}
{'loss': 0.6984, 'learning_rate': 9.491423931505949e-05, 'epoch': 0.15}
{'loss': 0.6871, 'learning_rate': 9.49036118824681e-05, 'epoch': 0.15}
{'loss': 0.7078, 'learning_rate': 9.489297395393047e-05, 'epoch': 0.15}
{'loss': 0.7183, 'learning_rate': 9.488232553193312e-05, 'epoch': 0.15}
 14%|        | 941/6500 [1:47:05<11:09:49,  7.23s/it]                                                        14%|        | 941/6500 [1:47:05<11:09:49,  7.23s/it] 14%|        | 942/6500 [1:47:12<10:51:34,  7.03s/it]                                                        14%|        | 942/6500 [1:47:12<10:51:34,  7.03s/it] 15%|        | 943/6500 [1:47:18<10:38:49,  6.90s/it]                                                        15%|        | 943/6500 [1:47:18<10:38:49,  6.90s/it] 15%|        | 944/6500 [1:47:25<10:29:43,  6.80s/it]                                                        15%|        | 944/6500 [1:47:25<10:29:43,  6.80s/it] 15%|        | 945/6500 [1:47:31<10:23:20,  6.73s/it]                                                        15%|        | 945/6500 [1:47:31<10:23:20,  6.73s/it] 15%|        | 946/6500 [1:47:38<10:19:25,  6.69s/it]                                                        15%|        | {'loss': 0.7398, 'learning_rate': 9.487166661896507e-05, 'epoch': 0.15}
{'loss': 0.7244, 'learning_rate': 9.486099721751777e-05, 'epoch': 0.15}
{'loss': 0.7004, 'learning_rate': 9.485031733008514e-05, 'epoch': 0.15}
{'loss': 0.7222, 'learning_rate': 9.48396269591635e-05, 'epoch': 0.15}
946/6500 [1:47:38<10:19:25,  6.69s/it] 15%|        | 947/6500 [1:47:45<10:16:16,  6.66s/it]                                                        15%|        | 947/6500 [1:47:45<10:16:16,  6.66s/it] 15%|        | 948/6500 [1:47:51<10:13:53,  6.63s/it]                                                        15%|        | 948/6500 [1:47:51<10:13:53,  6.63s/it] 15%|        | 949/6500 [1:47:58<10:12:13,  6.62s/it]                                                        15%|        | 949/6500 [1:47:58<10:12:13,  6.62s/it] 15%|        | 950/6500 [1:48:04<10:11:17,  6.61s/it]                                                        15%|        | 950/6500 [1:48:04<10:11:17,  6.61s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8734184503555298, 'eval_runtime': 1.4826, 'eval_samples_per_second': 8.094, 'eval_steps_per_second': 2.023, 'epoch': 0.15}
                                                        15%|        | 950/6500 [1:48:06<10:11:17,  6.61s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-950
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-950
 /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-950
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-950
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-950
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-950/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-950/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-950/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-950/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.7146, 'learning_rate': 9.482892610725171e-05, 'epoch': 0.15}
{'loss': 0.7222, 'learning_rate': 9.481821477685101e-05, 'epoch': 0.15}
{'loss': 0.7092, 'learning_rate': 9.48074929704651e-05, 'epoch': 0.15}
{'loss': 0.7225, 'learning_rate': 9.479676069060016e-05, 'epoch': 0.15}
{'loss': 0.9972, 'learning_rate': 9.478601793976474e-05, 'epoch': 0.15}
{'loss': 0.6876, 'learning_rate': 9.477526472046995e-05, 'epoch': 0.15}
 15%|        | 951/6500 [1:48:13<11:00:30,  7.14s/it]                                                        15%|        | 951/6500 [1:48:13<11:00:30,  7.14s/it] 15%|        | 952/6500 [1:48:19<10:44:33,  6.97s/it]                                                        15%|        | 952/6500 [1:48:19<10:44:33,  6.97s/it] 15%|        | 953/6500 [1:48:26<10:33:51,  6.86s/it]                                                        15%|        | 953/6500 [1:48:26<10:33:51,  6.86s/it] 15%|        | 954/6500 [1:48:33<10:49:36,  7.03s/it]                                                        15%|        | 954/6500 [1:48:33<10:49:36,  7.03s/it] 15%|        | 955/6500 [1:48:40<10:37:11,  6.89s/it]                                                        15%|        | 955/6500 [1:48:40<10:37:11,  6.89s/it] 15%|        | 956/6500 [1:48:46<10:28:41,  6.80s/it]                                                        15%|        | {'loss': 0.7275, 'learning_rate': 9.476450103522927e-05, 'epoch': 0.15}
{'loss': 0.6744, 'learning_rate': 9.475372688655864e-05, 'epoch': 0.15}
{'loss': 0.6886, 'learning_rate': 9.474294227697647e-05, 'epoch': 0.15}
{'loss': 0.7047, 'learning_rate': 9.473214720900356e-05, 'epoch': 0.15}
956/6500 [1:48:46<10:28:41,  6.80s/it] 15%|        | 957/6500 [1:48:53<10:22:17,  6.74s/it]                                                        15%|        | 957/6500 [1:48:53<10:22:17,  6.74s/it] 15%|        | 958/6500 [1:49:00<10:17:45,  6.69s/it]                                                        15%|        | 958/6500 [1:49:00<10:17:45,  6.69s/it] 15%|        | 959/6500 [1:49:06<10:14:37,  6.66s/it]                                                        15%|        | 959/6500 [1:49:06<10:14:37,  6.66s/it] 15%|        | 960/6500 [1:49:13<10:12:40,  6.64s/it]                                                        15%|        | 960/6500 [1:49:13<10:12:40,  6.64s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8722940683364868, 'eval_runtime': 1.4804, 'eval_samples_per_second': 8.106, 'eval_steps_per_second': 2.026, 'epoch': 0.15}
                                                        15%|        | 960/6500 [1:49:14<10:12:40,  6.64s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-960
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  I AM HERE AND I AM SAVING THE MODEL/projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-960
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-960

I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-960
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-960
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-960/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-960/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-960/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-960/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.7053, 'learning_rate': 9.472134168516324e-05, 'epoch': 0.15}
{'loss': 0.7392, 'learning_rate': 9.47105257079812e-05, 'epoch': 0.15}
{'loss': 0.7249, 'learning_rate': 9.469969927998564e-05, 'epoch': 0.15}
{'loss': 0.6908, 'learning_rate': 9.468886240370712e-05, 'epoch': 0.15}
{'loss': 0.7184, 'learning_rate': 9.467801508167875e-05, 'epoch': 0.15}
{'loss': 0.7017, 'learning_rate': 9.466715731643598e-05, 'epoch': 0.15}
 15%|        | 961/6500 [1:49:21<11:01:09,  7.16s/it]                                                        15%|        | 961/6500 [1:49:21<11:01:09,  7.16s/it] 15%|        | 962/6500 [1:49:28<10:44:57,  6.99s/it]                                                        15%|        | 962/6500 [1:49:28<10:44:57,  6.99s/it] 15%|        | 963/6500 [1:49:34<10:33:52,  6.87s/it]                                                        15%|        | 963/6500 [1:49:34<10:33:52,  6.87s/it] 15%|        | 964/6500 [1:49:41<10:25:46,  6.78s/it]                                                        15%|        | 964/6500 [1:49:41<10:25:46,  6.78s/it] 15%|        | 965/6500 [1:49:47<10:20:03,  6.72s/it]                                                        15%|        | 965/6500 [1:49:47<10:20:03,  6.72s/it] 15%|        | 966/6500 [1:49:54<10:15:43,  6.68s/it]                                                        15%|        | {'loss': 0.7454, 'learning_rate': 9.465628911051679e-05, 'epoch': 0.15}
{'loss': 0.6927, 'learning_rate': 9.464541046646152e-05, 'epoch': 0.15}
{'loss': 0.9776, 'learning_rate': 9.463452138681301e-05, 'epoch': 0.15}
{'loss': 0.7211, 'learning_rate': 9.462362187411651e-05, 'epoch': 0.15}
966/6500 [1:49:54<10:15:43,  6.68s/it] 15%|        | 967/6500 [1:50:01<10:13:19,  6.65s/it]                                                        15%|        | 967/6500 [1:50:01<10:13:19,  6.65s/it] 15%|        | 968/6500 [1:50:07<10:11:33,  6.63s/it]                                                        15%|        | 968/6500 [1:50:07<10:11:33,  6.63s/it] 15%|        | 969/6500 [1:50:14<10:10:09,  6.62s/it]                                                        15%|        | 969/6500 [1:50:14<10:10:09,  6.62s/it] 15%|        | 970/6500 [1:50:21<10:33:16,  6.87s/it]                                                        15%|        | 970/6500 [1:50:21<10:33:16,  6.87s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8734133243560791, 'eval_runtime': 1.4798, 'eval_samples_per_second': 8.109, 'eval_steps_per_second': 2.027, 'epoch': 0.15}
                                                        15%|        | 970/6500 [1:50:23<10:33:16,  6.87s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-970
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-970
/projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-970
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-970
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-970
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-970/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-970/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-970/pytorch_model.binthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-970/pytorch_model.bin

/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.6848, 'learning_rate': 9.46127119309197e-05, 'epoch': 0.15}
{'loss': 0.7251, 'learning_rate': 9.460179155977274e-05, 'epoch': 0.15}
{'loss': 0.6815, 'learning_rate': 9.459086076322818e-05, 'epoch': 0.15}
{'loss': 0.6989, 'learning_rate': 9.457991954384105e-05, 'epoch': 0.15}
{'loss': 0.7165, 'learning_rate': 9.456896790416875e-05, 'epoch': 0.15}
{'loss': 0.7209, 'learning_rate': 9.455800584677119e-05, 'epoch': 0.15}
 15%|        | 971/6500 [1:50:30<11:17:47,  7.36s/it]                                                        15%|        | 971/6500 [1:50:30<11:17:47,  7.36s/it] 15%|        | 972/6500 [1:50:36<10:56:06,  7.12s/it]                                                        15%|        | 972/6500 [1:50:36<10:56:06,  7.12s/it] 15%|        | 973/6500 [1:50:43<10:41:18,  6.96s/it]                                                        15%|        | 973/6500 [1:50:43<10:41:18,  6.96s/it] 15%|        | 974/6500 [1:50:49<10:30:22,  6.84s/it]                                                        15%|        | 974/6500 [1:50:49<10:30:22,  6.84s/it] 15%|        | 975/6500 [1:50:56<10:23:32,  6.77s/it]                                                        15%|        | 975/6500 [1:50:56<10:23:32,  6.77s/it] 15%|        | 976/6500 [1:51:03<10:18:08,  6.71s/it]                                                        15%|        | {'loss': 0.7186, 'learning_rate': 9.454703337421069e-05, 'epoch': 0.15}
{'loss': 0.7143, 'learning_rate': 9.453605048905199e-05, 'epoch': 0.15}
{'loss': 0.6976, 'learning_rate': 9.452505719386227e-05, 'epoch': 0.15}
{'loss': 0.7019, 'learning_rate': 9.451405349121115e-05, 'epoch': 0.15}
976/6500 [1:51:03<10:18:08,  6.71s/it] 15%|        | 977/6500 [1:51:09<10:14:05,  6.67s/it]                                                        15%|        | 977/6500 [1:51:09<10:14:05,  6.67s/it] 15%|        | 978/6500 [1:51:16<10:11:29,  6.64s/it]                                                        15%|        | 978/6500 [1:51:16<10:11:29,  6.64s/it] 15%|        | 979/6500 [1:51:22<10:09:48,  6.63s/it]                                                        15%|        | 979/6500 [1:51:22<10:09:48,  6.63s/it] 15%|        | 980/6500 [1:51:29<10:08:23,  6.61s/it]                                                        15%|        | 980/6500 [1:51:29<10:08:23,  6.61s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8707183599472046, 'eval_runtime': 1.4762, 'eval_samples_per_second': 8.129, 'eval_steps_per_second': 2.032, 'epoch': 0.15}
                                                        15%|        | 980/6500 [1:51:30<10:08:23,  6.61s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-980
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  I AM HERE AND I AM SAVING THE MODEL
/projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-980the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-980

I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-980
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-980
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-980/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-980/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-980/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-980/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.691, 'learning_rate': 9.450303938367067e-05, 'epoch': 0.15}
{'loss': 0.7428, 'learning_rate': 9.449201487381532e-05, 'epoch': 0.15}
{'loss': 0.6983, 'learning_rate': 9.448097996422201e-05, 'epoch': 0.15}
{'loss': 0.9715, 'learning_rate': 9.446993465747006e-05, 'epoch': 0.15}
{'loss': 0.7245, 'learning_rate': 9.44588789561413e-05, 'epoch': 0.15}
{'loss': 0.6726, 'learning_rate': 9.44478128628199e-05, 'epoch': 0.15}
 15%|        | 981/6500 [1:51:38<11:01:47,  7.19s/it]                                                        15%|        | 981/6500 [1:51:38<11:01:47,  7.19s/it] 15%|        | 982/6500 [1:51:44<10:44:45,  7.01s/it]                                                        15%|        | 982/6500 [1:51:44<10:44:45,  7.01s/it] 15%|        | 983/6500 [1:51:51<10:32:51,  6.88s/it]                                                        15%|        | 983/6500 [1:51:51<10:32:51,  6.88s/it] 15%|        | 984/6500 [1:51:57<10:24:17,  6.79s/it]                                                        15%|        | 984/6500 [1:51:57<10:24:17,  6.79s/it] 15%|        | 985/6500 [1:52:04<10:18:15,  6.73s/it]                                                        15%|        | 985/6500 [1:52:04<10:18:15,  6.73s/it] 15%|        | 986/6500 [1:52:11<10:38:12,  6.94s/it]                                                        15%|        | {'loss': 0.7153, 'learning_rate': 9.443673638009247e-05, 'epoch': 0.15}
{'loss': 0.675, 'learning_rate': 9.442564951054809e-05, 'epoch': 0.15}
{'loss': 0.6876, 'learning_rate': 9.441455225677827e-05, 'epoch': 0.15}
{'loss': 0.7046, 'learning_rate': 9.440344462137689e-05, 'epoch': 0.15}
986/6500 [1:52:11<10:38:12,  6.94s/it] 15%|        | 987/6500 [1:52:18<10:27:40,  6.83s/it]                                                        15%|        | 987/6500 [1:52:18<10:27:40,  6.83s/it] 15%|        | 988/6500 [1:52:24<10:19:44,  6.75s/it]                                                        15%|        | 988/6500 [1:52:24<10:19:44,  6.75s/it] 15%|        | 989/6500 [1:52:31<10:14:01,  6.69s/it]                                                        15%|        | 989/6500 [1:52:31<10:14:01,  6.69s/it] 15%|        | 990/6500 [1:52:37<10:09:48,  6.64s/it]                                                        15%|        | 990/6500 [1:52:37<10:09:48,  6.64s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8706419467926025, 'eval_runtime': 1.4749, 'eval_samples_per_second': 8.136, 'eval_steps_per_second': 2.034, 'epoch': 0.15}
                                                        15%|        | 990/6500 [1:52:39<10:09:48,  6.64s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-990
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-990I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-990

I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-990
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-990
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-990/pytorch_model.bin
the pytorch model path is  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-990/pytorch_model.bin
/projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-990/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-990/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.713, 'learning_rate': 9.43923266069403e-05, 'epoch': 0.15}
{'loss': 0.7111, 'learning_rate': 9.438119821606727e-05, 'epoch': 0.15}
{'loss': 0.7038, 'learning_rate': 9.437005945135903e-05, 'epoch': 0.15}
{'loss': 0.6947, 'learning_rate': 9.435891031541915e-05, 'epoch': 0.15}
{'loss': 0.7066, 'learning_rate': 9.434775081085368e-05, 'epoch': 0.15}
{'loss': 0.6916, 'learning_rate': 9.433658094027111e-05, 'epoch': 0.15}
 15%|        | 991/6500 [1:52:46<10:58:31,  7.17s/it]                                                        15%|        | 991/6500 [1:52:46<10:58:31,  7.17s/it] 15%|        | 992/6500 [1:52:52<10:40:59,  6.98s/it]                                                        15%|        | 992/6500 [1:52:52<10:40:59,  6.98s/it] 15%|        | 993/6500 [1:52:59<10:28:57,  6.85s/it]                                                        15%|        | 993/6500 [1:52:59<10:28:57,  6.85s/it] 15%|        | 994/6500 [1:53:06<10:20:20,  6.76s/it]                                                        15%|        | 994/6500 [1:53:06<10:20:20,  6.76s/it] 15%|        | 995/6500 [1:53:12<10:15:08,  6.70s/it]                                                        15%|        | 995/6500 [1:53:12<10:15:08,  6.70s/it] 15%|        | 996/6500 [1:53:19<10:10:03,  6.65s/it]                                                        15%|        | {'loss': 0.7378, 'learning_rate': 9.432540070628231e-05, 'epoch': 0.15}
{'loss': 0.6996, 'learning_rate': 9.431421011150062e-05, 'epoch': 0.15}
{'loss': 0.973, 'learning_rate': 9.430300915854172e-05, 'epoch': 0.15}
{'loss': 0.7094, 'learning_rate': 9.42917978500238e-05, 'epoch': 0.15}
996/6500 [1:53:19<10:10:03,  6.65s/it] 15%|        | 997/6500 [1:53:25<10:06:58,  6.62s/it]                                                        15%|        | 997/6500 [1:53:25<10:06:58,  6.62s/it] 15%|        | 998/6500 [1:53:32<10:04:52,  6.60s/it]                                                        15%|        | 998/6500 [1:53:32<10:04:52,  6.60s/it] 15%|        | 999/6500 [1:53:38<10:03:13,  6.58s/it]                                                        15%|        | 999/6500 [1:53:38<10:03:13,  6.58s/it] 15%|        | 1000/6500 [1:53:45<10:01:50,  6.57s/it]                                                         15%|        | 1000/6500 [1:53:45<10:01:50,  6.57s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8697776794433594, 'eval_runtime': 1.4755, 'eval_samples_per_second': 8.133, 'eval_steps_per_second': 2.033, 'epoch': 0.15}
                                                         15%|        | 1000/6500 [1:53:46<10:01:50,  6.57s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-1000
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1000I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1000

I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1000
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1000
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1000/pytorch_model.bin
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1000/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1000/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1000/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.6918, 'learning_rate': 9.428057618856745e-05, 'epoch': 0.15}
{'loss': 0.6899, 'learning_rate': 9.426934417679563e-05, 'epoch': 0.15}
{'loss': 0.6714, 'learning_rate': 9.425810181733377e-05, 'epoch': 0.15}
{'loss': 0.6843, 'learning_rate': 9.424684911280972e-05, 'epoch': 0.15}
{'loss': 0.6968, 'learning_rate': 9.423558606585369e-05, 'epoch': 0.15}
{'loss': 0.7234, 'learning_rate': 9.42243126790984e-05, 'epoch': 0.15}
 15%|        | 1001/6500 [1:53:53<10:52:20,  7.12s/it]                                                         15%|        | 1001/6500 [1:53:53<10:52:20,  7.12s/it] 15%|        | 1002/6500 [1:54:00<10:52:37,  7.12s/it]                                                         15%|        | 1002/6500 [1:54:00<10:52:37,  7.12s/it] 15%|        | 1003/6500 [1:54:07<10:36:42,  6.95s/it]                                                         15%|        | 1003/6500 [1:54:07<10:36:42,  6.95s/it] 15%|        | 1004/6500 [1:54:13<10:25:25,  6.83s/it]                                                         15%|        | 1004/6500 [1:54:13<10:25:25,  6.83s/it] 15%|        | 1005/6500 [1:54:20<10:17:29,  6.74s/it]                                                         15%|        | 1005/6500 [1:54:20<10:17:29,  6.74s/it] 15%|        | 1006/6500 [1:54:27<10:12:30,  6.69s/it]                                                         15%{'loss': 0.7159, 'learning_rate': 9.42130289551789e-05, 'epoch': 0.15}
{'loss': 0.6946, 'learning_rate': 9.420173489673269e-05, 'epoch': 0.16}
{'loss': 0.7024, 'learning_rate': 9.419043050639973e-05, 'epoch': 0.16}
{'loss': 0.6947, 'learning_rate': 9.417911578682229e-05, 'epoch': 0.16}
|        | 1006/6500 [1:54:27<10:12:30,  6.69s/it] 15%|        | 1007/6500 [1:54:33<10:08:25,  6.65s/it]                                                         15%|        | 1007/6500 [1:54:33<10:08:25,  6.65s/it] 16%|        | 1008/6500 [1:54:40<10:05:52,  6.62s/it]                                                         16%|        | 1008/6500 [1:54:40<10:05:52,  6.62s/it] 16%|        | 1009/6500 [1:54:46<10:03:49,  6.60s/it]                                                         16%|        | 1009/6500 [1:54:46<10:03:49,  6.60s/it] 16%|        | 1010/6500 [1:54:53<10:02:37,  6.59s/it]                                                         16%|        | 1010/6500 [1:54:53<10:02:37,  6.59s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8668479323387146, 'eval_runtime': 1.74, 'eval_samples_per_second': 6.896, 'eval_steps_per_second': 1.724, 'epoch': 0.16}
                                                         16%|        | 1010/6500 [1:54:54<10:02:37,  6.59s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-1010
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  I AM HERE AND I AM SAVING THE MODEL/projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1010
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1010

I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1010
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1010
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1010/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1010/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1010/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1010/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.6991, 'learning_rate': 9.416779074064517e-05, 'epoch': 0.16}
{'loss': 0.7196, 'learning_rate': 9.415645537051549e-05, 'epoch': 0.16}
{'loss': 0.6958, 'learning_rate': 9.414510967908286e-05, 'epoch': 0.16}
{'loss': 0.9761, 'learning_rate': 9.413375366899923e-05, 'epoch': 0.16}
{'loss': 0.6749, 'learning_rate': 9.412238734291903e-05, 'epoch': 0.16}
{'loss': 0.7111, 'learning_rate': 9.411101070349905e-05, 'epoch': 0.16}
 16%|        | 1011/6500 [1:55:01<10:59:05,  7.20s/it]                                                         16%|        | 1011/6500 [1:55:01<10:59:05,  7.20s/it] 16%|        | 1012/6500 [1:55:08<10:41:15,  7.01s/it]                                                         16%|        | 1012/6500 [1:55:08<10:41:15,  7.01s/it] 16%|        | 1013/6500 [1:55:15<10:28:45,  6.88s/it]                                                         16%|        | 1013/6500 [1:55:15<10:28:45,  6.88s/it] 16%|        | 1014/6500 [1:55:21<10:19:52,  6.78s/it]                                                         16%|        | 1014/6500 [1:55:21<10:19:52,  6.78s/it] 16%|        | 1015/6500 [1:55:28<10:12:59,  6.71s/it]                                                         16%|        | 1015/6500 [1:55:28<10:12:59,  6.71s/it] 16%|        | 1016/6500 [1:55:34<10:08:40,  6.66s/it]                                                         16%{'loss': 0.6822, 'learning_rate': 9.409962375339851e-05, 'epoch': 0.16}
{'loss': 0.6695, 'learning_rate': 9.408822649527906e-05, 'epoch': 0.16}
{'loss': 0.6747, 'learning_rate': 9.407681893180473e-05, 'epoch': 0.16}
{'loss': 0.7113, 'learning_rate': 9.406540106564196e-05, 'epoch': 0.16}
|        | 1016/6500 [1:55:34<10:08:40,  6.66s/it] 16%|        | 1017/6500 [1:55:41<10:05:23,  6.62s/it]                                                         16%|        | 1017/6500 [1:55:41<10:05:23,  6.62s/it] 16%|        | 1018/6500 [1:55:47<10:02:56,  6.60s/it]                                                         16%|        | 1018/6500 [1:55:47<10:02:56,  6.60s/it] 16%|        | 1019/6500 [1:55:54<10:17:47,  6.76s/it]                                                         16%|        | 1019/6500 [1:55:54<10:17:47,  6.76s/it] 16%|        | 1020/6500 [1:56:01<10:11:23,  6.69s/it]                                                         16%|        | 1020/6500 [1:56:01<10:11:23,  6.69s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8667111396789551, 'eval_runtime': 1.5009, 'eval_samples_per_second': 7.995, 'eval_steps_per_second': 1.999, 'epoch': 0.16}
                                                         16%|        | 1020/6500 [1:56:02<10:11:23,  6.69s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-1020
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1020
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1020
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1020
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1020
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1020/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1020/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1020/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1020/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.7236, 'learning_rate': 9.405397289945963e-05, 'epoch': 0.16}
{'loss': 0.7026, 'learning_rate': 9.404253443592896e-05, 'epoch': 0.16}
{'loss': 0.6818, 'learning_rate': 9.403108567772367e-05, 'epoch': 0.16}
{'loss': 0.7117, 'learning_rate': 9.40196266275198e-05, 'epoch': 0.16}
{'loss': 0.6809, 'learning_rate': 9.400815728799586e-05, 'epoch': 0.16}
{'loss': 0.7062, 'learning_rate': 9.399667766183274e-05, 'epoch': 0.16}
 16%|        | 1021/6500 [1:56:09<11:03:09,  7.26s/it]                                                         16%|        | 1021/6500 [1:56:09<11:03:09,  7.26s/it] 16%|        | 1022/6500 [1:56:16<10:43:16,  7.05s/it]                                                         16%|        | 1022/6500 [1:56:16<10:43:16,  7.05s/it] 16%|        | 1023/6500 [1:56:23<10:29:33,  6.90s/it]                                                         16%|        | 1023/6500 [1:56:23<10:29:33,  6.90s/it] 16%|        | 1024/6500 [1:56:29<10:19:33,  6.79s/it]                                                         16%|        | 1024/6500 [1:56:29<10:19:33,  6.79s/it] 16%|        | 1025/6500 [1:56:36<10:12:36,  6.71s/it]                                                         16%|        | 1025/6500 [1:56:36<10:12:36,  6.71s/it] 16%|        | 1026/6500 [1:56:42<10:07:37,  6.66s/it]                                                         16%{'loss': 0.7017, 'learning_rate': 9.39851877517137e-05, 'epoch': 0.16}
{'loss': 0.6961, 'learning_rate': 9.397368756032445e-05, 'epoch': 0.16}
{'loss': 0.9726, 'learning_rate': 9.396217709035312e-05, 'epoch': 0.16}
{'loss': 0.676, 'learning_rate': 9.395065634449018e-05, 'epoch': 0.16}
|        | 1026/6500 [1:56:42<10:07:37,  6.66s/it] 16%|        | 1027/6500 [1:56:49<10:04:11,  6.62s/it]                                                         16%|        | 1027/6500 [1:56:49<10:04:11,  6.62s/it] 16%|        | 1028/6500 [1:56:55<10:01:39,  6.60s/it]                                                         16%|        | 1028/6500 [1:56:55<10:01:39,  6.60s/it] 16%|        | 1029/6500 [1:57:02<9:59:58,  6.58s/it]                                                         16%|        | 1029/6500 [1:57:02<9:59:58,  6.58s/it] 16%|        | 1030/6500 [1:57:08<9:59:05,  6.57s/it]                                                        16%|        | 1030/6500 [1:57:08<9:59:05,  6.57s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8696789741516113, 'eval_runtime': 1.4865, 'eval_samples_per_second': 8.073, 'eval_steps_per_second': 2.018, 'epoch': 0.16}
                                                        16%|        | 1030/6500 [1:57:10<9:59:05,  6.57s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-1030
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1030
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1030
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1030
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1030
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1030/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1030/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1030/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1030/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.7148, 'learning_rate': 9.393912532542854e-05, 'epoch': 0.16}
{'loss': 0.6569, 'learning_rate': 9.392758403586352e-05, 'epoch': 0.16}
{'loss': 0.6666, 'learning_rate': 9.391603247849281e-05, 'epoch': 0.16}
{'loss': 0.6858, 'learning_rate': 9.390447065601651e-05, 'epoch': 0.16}
{'loss': 0.6925, 'learning_rate': 9.389289857113715e-05, 'epoch': 0.16}
{'loss': 0.7148, 'learning_rate': 9.388131622655962e-05, 'epoch': 0.16}
 16%|        | 1031/6500 [1:57:17<10:48:29,  7.11s/it]                                                         16%|        | 1031/6500 [1:57:17<10:48:29,  7.11s/it] 16%|        | 1032/6500 [1:57:23<10:32:51,  6.94s/it]                                                         16%|        | 1032/6500 [1:57:23<10:32:51,  6.94s/it] 16%|        | 1033/6500 [1:57:30<10:22:05,  6.83s/it]                                                         16%|        | 1033/6500 [1:57:30<10:22:05,  6.83s/it] 16%|        | 1034/6500 [1:57:36<10:14:25,  6.74s/it]                                                         16%|        | 1034/6500 [1:57:36<10:14:25,  6.74s/it] 16%|        | 1035/6500 [1:57:44<10:31:44,  6.94s/it]                                                         16%|        | 1035/6500 [1:57:44<10:31:44,  6.94s/it] 16%|        | 1036/6500 [1:57:50<10:21:17,  6.82s/it]                                                         16%{'loss': 0.7099, 'learning_rate': 9.386972362499123e-05, 'epoch': 0.16}
{'loss': 0.6664, 'learning_rate': 9.385812076914167e-05, 'epoch': 0.16}
{'loss': 0.6987, 'learning_rate': 9.384650766172305e-05, 'epoch': 0.16}
{'loss': 0.6738, 'learning_rate': 9.383488430544984e-05, 'epoch': 0.16}
|        | 1036/6500 [1:57:50<10:21:17,  6.82s/it] 16%|        | 1037/6500 [1:57:57<10:13:47,  6.74s/it]                                                         16%|        | 1037/6500 [1:57:57<10:13:47,  6.74s/it] 16%|        | 1038/6500 [1:58:03<10:08:52,  6.69s/it]                                                         16%|        | 1038/6500 [1:58:03<10:08:52,  6.69s/it] 16%|        | 1039/6500 [1:58:10<10:05:12,  6.65s/it]                                                         16%|        | 1039/6500 [1:58:10<10:05:12,  6.65s/it] 16%|        | 1040/6500 [1:58:17<10:02:18,  6.62s/it]                                                         16%|        | 1040/6500 [1:58:17<10:02:18,  6.62s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.865571916103363, 'eval_runtime': 1.4878, 'eval_samples_per_second': 8.065, 'eval_steps_per_second': 2.016, 'epoch': 0.16}
                                                         16%|        | 1040/6500 [1:58:18<10:02:18,  6.62s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-1040
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1040I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1040

I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1040
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1040
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1040/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1040/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1040/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1040/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.7275, 'learning_rate': 9.382325070303896e-05, 'epoch': 0.16}
{'loss': 0.6678, 'learning_rate': 9.381160685720967e-05, 'epoch': 0.16}
{'loss': 0.9563, 'learning_rate': 9.379995277068365e-05, 'epoch': 0.16}
{'loss': 0.7047, 'learning_rate': 9.378828844618499e-05, 'epoch': 0.16}
{'loss': 0.6627, 'learning_rate': 9.377661388644014e-05, 'epoch': 0.16}
{'loss': 0.7022, 'learning_rate': 9.376492909417795e-05, 'epoch': 0.16}
 16%|        | 1041/6500 [1:58:25<10:51:02,  7.16s/it]                                                         16%|        | 1041/6500 [1:58:25<10:51:02,  7.16s/it] 16%|        | 1042/6500 [1:58:32<10:34:49,  6.98s/it]                                                         16%|        | 1042/6500 [1:58:32<10:34:49,  6.98s/it] 16%|        | 1043/6500 [1:58:38<10:22:57,  6.85s/it]                                                         16%|        | 1043/6500 [1:58:38<10:22:57,  6.85s/it] 16%|        | 1044/6500 [1:58:45<10:14:44,  6.76s/it]                                                         16%|        | 1044/6500 [1:58:45<10:14:44,  6.76s/it] 16%|        | 1045/6500 [1:58:51<10:08:50,  6.70s/it]                                                         16%|        | 1045/6500 [1:58:51<10:08:50,  6.70s/it] 16%|        | 1046/6500 [1:58:58<10:04:25,  6.65s/it]                                                         16%{'loss': 0.6577, 'learning_rate': 9.375323407212969e-05, 'epoch': 0.16}
{'loss': 0.6691, 'learning_rate': 9.374152882302898e-05, 'epoch': 0.16}
{'loss': 0.68, 'learning_rate': 9.372981334961187e-05, 'epoch': 0.16}
{'loss': 0.677, 'learning_rate': 9.371808765461677e-05, 'epoch': 0.16}
|        | 1046/6500 [1:58:58<10:04:25,  6.65s/it] 16%|        | 1047/6500 [1:59:04<10:01:39,  6.62s/it]                                                         16%|        | 1047/6500 [1:59:04<10:01:39,  6.62s/it] 16%|        | 1048/6500 [1:59:11<9:59:53,  6.60s/it]                                                         16%|        | 1048/6500 [1:59:11<9:59:53,  6.60s/it] 16%|        | 1049/6500 [1:59:17<9:58:20,  6.59s/it]                                                        16%|        | 1049/6500 [1:59:17<9:58:20,  6.59s/it] 16%|        | 1050/6500 [1:59:24<9:56:49,  6.57s/it]                                                        16%|        | 1050/6500 [1:59:24<9:56:49,  6.57s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8651223182678223, 'eval_runtime': 1.4856, 'eval_samples_per_second': 8.077, 'eval_steps_per_second': 2.019, 'epoch': 0.16}
                                                        16%|        | 1050/6500 [1:59:25<9:56:49,  6.57s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-1050
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1050I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1050

I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1050
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1050
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1050/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1050/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1050/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1050/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.7136, 'learning_rate': 9.370635174078448e-05, 'epoch': 0.16}
{'loss': 0.6951, 'learning_rate': 9.369460561085823e-05, 'epoch': 0.16}
{'loss': 0.6695, 'learning_rate': 9.368284926758357e-05, 'epoch': 0.16}
{'loss': 0.6948, 'learning_rate': 9.36710827137085e-05, 'epoch': 0.16}
{'loss': 0.6777, 'learning_rate': 9.365930595198336e-05, 'epoch': 0.16}
{'loss': 0.7184, 'learning_rate': 9.364751898516091e-05, 'epoch': 0.16}
 16%|        | 1051/6500 [1:59:33<11:14:19,  7.43s/it]                                                         16%|        | 1051/6500 [1:59:33<11:14:19,  7.43s/it] 16%|        | 1052/6500 [1:59:40<10:50:23,  7.16s/it]                                                         16%|        | 1052/6500 [1:59:40<10:50:23,  7.16s/it] 16%|        | 1053/6500 [1:59:46<10:33:05,  6.97s/it]                                                         16%|        | 1053/6500 [1:59:46<10:33:05,  6.97s/it] 16%|        | 1054/6500 [1:59:53<10:20:55,  6.84s/it]                                                         16%|        | 1054/6500 [1:59:53<10:20:55,  6.84s/it] 16%|        | 1055/6500 [1:59:59<10:12:14,  6.75s/it]                                                         16%|        | 1055/6500 [1:59:59<10:12:14,  6.75s/it] 16%|        | 1056/6500 [2:00:06<10:06:28,  6.68s/it]                                                         16%{'loss': 0.6764, 'learning_rate': 9.363572181599628e-05, 'epoch': 0.16}
{'loss': 0.9458, 'learning_rate': 9.362391444724699e-05, 'epoch': 0.16}
{'loss': 0.7062, 'learning_rate': 9.361209688167292e-05, 'epoch': 0.16}
{'loss': 0.6629, 'learning_rate': 9.36002691220364e-05, 'epoch': 0.16}
|        | 1056/6500 [2:00:06<10:06:28,  6.68s/it] 16%|        | 1057/6500 [2:00:13<10:02:35,  6.64s/it]                                                         16%|        | 1057/6500 [2:00:13<10:02:35,  6.64s/it] 16%|        | 1058/6500 [2:00:19<9:59:45,  6.61s/it]                                                         16%|        | 1058/6500 [2:00:19<9:59:45,  6.61s/it] 16%|        | 1059/6500 [2:00:26<9:57:48,  6.59s/it]                                                        16%|        | 1059/6500 [2:00:26<9:57:48,  6.59s/it] 16%|        | 1060/6500 [2:00:32<9:56:19,  6.58s/it]                                                        16%|        | 1060/6500 [2:00:32<9:56:19,  6.58s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8672515749931335, 'eval_runtime': 1.4875, 'eval_samples_per_second': 8.067, 'eval_steps_per_second': 2.017, 'epoch': 0.16}
                                                        16%|        | 1060/6500 [2:00:34<9:56:19,  6.58s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-1060
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1060
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1060
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1060
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1060
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1060/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1060/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1060/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1060/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.6961, 'learning_rate': 9.358843117110204e-05, 'epoch': 0.16}
{'loss': 0.66, 'learning_rate': 9.357658303163693e-05, 'epoch': 0.16}
{'loss': 0.6714, 'learning_rate': 9.356472470641047e-05, 'epoch': 0.16}
{'loss': 0.6864, 'learning_rate': 9.35528561981945e-05, 'epoch': 0.16}
{'loss': 0.6966, 'learning_rate': 9.354097750976319e-05, 'epoch': 0.16}
{'loss': 0.69, 'learning_rate': 9.352908864389312e-05, 'epoch': 0.16}
 16%|        | 1061/6500 [2:00:41<10:45:50,  7.12s/it]                                                         16%|        | 1061/6500 [2:00:41<10:45:50,  7.12s/it] 16%|        | 1062/6500 [2:00:47<10:30:12,  6.95s/it]                                                         16%|        | 1062/6500 [2:00:47<10:30:12,  6.95s/it] 16%|        | 1063/6500 [2:00:54<10:19:11,  6.83s/it]                                                         16%|        | 1063/6500 [2:00:54<10:19:11,  6.83s/it] 16%|        | 1064/6500 [2:01:00<10:11:15,  6.75s/it]                                                         16%|        | 1064/6500 [2:01:00<10:11:15,  6.75s/it] 16%|        | 1065/6500 [2:01:07<10:05:44,  6.69s/it]                                                         16%|        | 1065/6500 [2:01:07<10:05:44,  6.69s/it] 16%|        | 1066/6500 [2:01:13<10:02:07,  6.65s/it]                                                         16%{'loss': 0.6898, 'learning_rate': 9.351718960336325e-05, 'epoch': 0.16}
{'loss': 0.6684, 'learning_rate': 9.35052803909549e-05, 'epoch': 0.16}
{'loss': 0.6837, 'learning_rate': 9.349336100945176e-05, 'epoch': 0.16}
{'loss': 0.6738, 'learning_rate': 9.348143146163994e-05, 'epoch': 0.16}
|        | 1066/6500 [2:01:13<10:02:07,  6.65s/it] 16%|        | 1067/6500 [2:01:21<10:23:18,  6.88s/it]                                                         16%|        | 1067/6500 [2:01:21<10:23:18,  6.88s/it] 16%|        | 1068/6500 [2:01:27<10:14:17,  6.79s/it]                                                         16%|        | 1068/6500 [2:01:27<10:14:17,  6.79s/it] 16%|        | 1069/6500 [2:01:34<10:08:14,  6.72s/it]                                                         16%|        | 1069/6500 [2:01:34<10:08:14,  6.72s/it] 16%|        | 1070/6500 [2:01:40<10:03:22,  6.67s/it]                                                         16%|        | 1070/6500 [2:01:40<10:03:22,  6.67s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.862988293170929, 'eval_runtime': 1.4819, 'eval_samples_per_second': 8.098, 'eval_steps_per_second': 2.024, 'epoch': 0.16}
                                                         16%|        | 1070/6500 [2:01:42<10:03:22,  6.67s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-1070
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1070
/projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1070
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1070
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1070
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1070/pytorch_model.bin
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1070/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1070/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1070/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.7171, 'learning_rate': 9.346949175030791e-05, 'epoch': 0.16}
{'loss': 0.6793, 'learning_rate': 9.345754187824644e-05, 'epoch': 0.16}
{'loss': 0.9434, 'learning_rate': 9.34455818482488e-05, 'epoch': 0.17}
{'loss': 0.6969, 'learning_rate': 9.343361166311057e-05, 'epoch': 0.17}
{'loss': 0.671, 'learning_rate': 9.342163132562967e-05, 'epoch': 0.17}
{'loss': 0.668, 'learning_rate': 9.340964083860648e-05, 'epoch': 0.17}
 16%|        | 1071/6500 [2:01:49<10:47:05,  7.15s/it]                                                         16%|        | 1071/6500 [2:01:49<10:47:05,  7.15s/it] 16%|        | 1072/6500 [2:01:55<10:30:47,  6.97s/it]                                                         16%|        | 1072/6500 [2:01:55<10:30:47,  6.97s/it] 17%|        | 1073/6500 [2:02:02<10:18:58,  6.84s/it]                                                         17%|        | 1073/6500 [2:02:02<10:18:58,  6.84s/it] 17%|        | 1074/6500 [2:02:08<10:10:57,  6.76s/it]                                                         17%|        | 1074/6500 [2:02:08<10:10:57,  6.76s/it] 17%|        | 1075/6500 [2:02:15<10:05:37,  6.70s/it]                                                         17%|        | 1075/6500 [2:02:15<10:05:37,  6.70s/it] 17%|        | 1076/6500 [2:02:21<10:01:31,  6.65s/it]                                                         17%{'loss': 0.6684, 'learning_rate': 9.339764020484366e-05, 'epoch': 0.17}
{'loss': 0.6691, 'learning_rate': 9.338562942714631e-05, 'epoch': 0.17}
{'loss': 0.687, 'learning_rate': 9.337360850832187e-05, 'epoch': 0.17}
{'loss': 0.6976, 'learning_rate': 9.336157745118016e-05, 'epoch': 0.17}
|        | 1076/6500 [2:02:21<10:01:31,  6.65s/it] 17%|        | 1077/6500 [2:02:28<9:59:16,  6.63s/it]                                                         17%|        | 1077/6500 [2:02:28<9:59:16,  6.63s/it] 17%|        | 1078/6500 [2:02:35<9:57:31,  6.61s/it]                                                        17%|        | 1078/6500 [2:02:35<9:57:31,  6.61s/it] 17%|        | 1079/6500 [2:02:41<9:56:02,  6.60s/it]                                                        17%|        | 1079/6500 [2:02:41<9:56:02,  6.60s/it] 17%|        | 1080/6500 [2:02:48<9:55:10,  6.59s/it]                                                        17%|        | 1080/6500 [2:02:48<9:55:10,  6.59s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8598710894584656, 'eval_runtime': 1.4865, 'eval_samples_per_second': 8.073, 'eval_steps_per_second': 2.018, 'epoch': 0.17}
                                                        17%|        | 1080/6500 [2:02:49<9:55:10,  6.59s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-1080
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1080I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1080

I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1080
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1080
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1080/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1080/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1080/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1080/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.689, 'learning_rate': 9.334953625853335e-05, 'epoch': 0.17}
{'loss': 0.6819, 'learning_rate': 9.333748493319603e-05, 'epoch': 0.17}
{'loss': 0.6786, 'learning_rate': 9.332542347798509e-05, 'epoch': 0.17}
{'loss': 0.6692, 'learning_rate': 9.331335189571984e-05, 'epoch': 0.17}
{'loss': 0.6802, 'learning_rate': 9.330127018922194e-05, 'epoch': 0.17}
{'loss': 0.6906, 'learning_rate': 9.32891783613154e-05, 'epoch': 0.17}
 17%|        | 1081/6500 [2:02:56<10:43:42,  7.13s/it]                                                         17%|        | 1081/6500 [2:02:56<10:43:42,  7.13s/it] 17%|        | 1082/6500 [2:03:03<10:28:25,  6.96s/it]                                                         17%|        | 1082/6500 [2:03:03<10:28:25,  6.96s/it] 17%|        | 1083/6500 [2:03:10<10:41:51,  7.11s/it]                                                         17%|        | 1083/6500 [2:03:10<10:41:51,  7.11s/it] 17%|        | 1084/6500 [2:03:17<10:27:28,  6.95s/it]                                                         17%|        | 1084/6500 [2:03:17<10:27:28,  6.95s/it] 17%|        | 1085/6500 [2:03:23<10:16:33,  6.83s/it]                                                         17%|        | 1085/6500 [2:03:23<10:16:33,  6.83s/it] 17%|        | 1086/6500 [2:03:30<10:08:36,  6.74s/it]                                                         17%{'loss': 0.6812, 'learning_rate': 9.327707641482662e-05, 'epoch': 0.17}
{'loss': 0.9565, 'learning_rate': 9.326496435258437e-05, 'epoch': 0.17}
{'loss': 0.6732, 'learning_rate': 9.325284217741974e-05, 'epoch': 0.17}
{'loss': 0.6728, 'learning_rate': 9.324070989216625e-05, 'epoch': 0.17}
|        | 1086/6500 [2:03:30<10:08:36,  6.74s/it] 17%|        | 1087/6500 [2:03:36<10:03:20,  6.69s/it]                                                         17%|        | 1087/6500 [2:03:36<10:03:20,  6.69s/it] 17%|        | 1088/6500 [2:03:43<9:59:07,  6.64s/it]                                                         17%|        | 1088/6500 [2:03:43<9:59:07,  6.64s/it] 17%|        | 1089/6500 [2:03:49<9:56:36,  6.62s/it]                                                        17%|        | 1089/6500 [2:03:50<9:56:36,  6.62s/it] 17%|        | 1090/6500 [2:03:56<9:54:28,  6.59s/it]                                                        17%|        | 1090/6500 [2:03:56<9:54:28,  6.59s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.863781750202179, 'eval_runtime': 1.4858, 'eval_samples_per_second': 8.076, 'eval_steps_per_second': 2.019, 'epoch': 0.17}
                                                        17%|        | 1090/6500 [2:03:58<9:54:28,  6.59s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-1090
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1090
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1090
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1090
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1090
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1090/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1090/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1090/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1090/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.6672, 'learning_rate': 9.322856749965971e-05, 'epoch': 0.17}
{'loss': 0.6502, 'learning_rate': 9.321641500273836e-05, 'epoch': 0.17}
{'loss': 0.6581, 'learning_rate': 9.320425240424277e-05, 'epoch': 0.17}
{'loss': 0.6805, 'learning_rate': 9.319207970701586e-05, 'epoch': 0.17}
{'loss': 0.6886, 'learning_rate': 9.317989691390291e-05, 'epoch': 0.17}
{'loss': 0.688, 'learning_rate': 9.316770402775164e-05, 'epoch': 0.17}
 17%|        | 1091/6500 [2:04:04<10:42:01,  7.12s/it]                                                         17%|        | 1091/6500 [2:04:04<10:42:01,  7.12s/it] 17%|        | 1092/6500 [2:04:11<10:26:07,  6.95s/it]                                                         17%|        | 1092/6500 [2:04:11<10:26:07,  6.95s/it] 17%|        | 1093/6500 [2:04:17<10:15:40,  6.83s/it]                                                         17%|        | 1093/6500 [2:04:17<10:15:40,  6.83s/it] 17%|        | 1094/6500 [2:04:24<10:07:35,  6.74s/it]                                                         17%|        | 1094/6500 [2:04:24<10:07:35,  6.74s/it] 17%|        | 1095/6500 [2:04:31<10:02:29,  6.69s/it]                                                         17%|        | 1095/6500 [2:04:31<10:02:29,  6.69s/it] 17%|        | 1096/6500 [2:04:37<9:59:11,  6.65s/it]                                                         17%|{'loss': 0.6645, 'learning_rate': 9.315550105141199e-05, 'epoch': 0.17}
{'loss': 0.6855, 'learning_rate': 9.314328798773636e-05, 'epoch': 0.17}
{'loss': 0.6715, 'learning_rate': 9.313106483957948e-05, 'epoch': 0.17}
{'loss': 0.6823, 'learning_rate': 9.311883160979844e-05, 'epoch': 0.17}
        | 1096/6500 [2:04:37<9:59:11,  6.65s/it] 17%|        | 1097/6500 [2:04:44<9:56:36,  6.63s/it]                                                        17%|        | 1097/6500 [2:04:44<9:56:36,  6.63s/it] 17%|        | 1098/6500 [2:04:50<9:54:38,  6.60s/it]                                                        17%|        | 1098/6500 [2:04:50<9:54:38,  6.60s/it] 17%|        | 1099/6500 [2:04:57<10:09:54,  6.78s/it]                                                         17%|        | 1099/6500 [2:04:57<10:09:54,  6.78s/it] 17%|        | 1100/6500 [2:05:04<10:03:59,  6.71s/it]                                                         17%|        | 1100/6500 [2:05:04<10:03:59,  6.71s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8617375493049622, 'eval_runtime': 1.5105, 'eval_samples_per_second': 7.945, 'eval_steps_per_second': 1.986, 'epoch': 0.17}
                                                         17%|        | 1100/6500 [2:05:06<10:03:59,  6.71s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-1100
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1100I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1100

I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1100
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1100
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1100/pytorch_model.bin
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1100/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1100/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1100/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.696, 'learning_rate': 9.310658830125267e-05, 'epoch': 0.17}
{'loss': 0.6768, 'learning_rate': 9.309433491680398e-05, 'epoch': 0.17}
{'loss': 0.9533, 'learning_rate': 9.308207145931653e-05, 'epoch': 0.17}
{'loss': 0.6564, 'learning_rate': 9.306979793165681e-05, 'epoch': 0.17}
{'loss': 0.692, 'learning_rate': 9.305751433669369e-05, 'epoch': 0.17}
{'loss': 0.6507, 'learning_rate': 9.304522067729839e-05, 'epoch': 0.17}
 17%|        | 1101/6500 [2:05:12<10:50:27,  7.23s/it]                                                         17%|        | 1101/6500 [2:05:12<10:50:27,  7.23s/it] 17%|        | 1102/6500 [2:05:19<10:32:18,  7.03s/it]                                                         17%|        | 1102/6500 [2:05:19<10:32:18,  7.03s/it] 17%|        | 1103/6500 [2:05:26<10:19:45,  6.89s/it]                                                         17%|        | 1103/6500 [2:05:26<10:19:45,  6.89s/it] 17%|        | 1104/6500 [2:05:32<10:10:50,  6.79s/it]                                                         17%|        | 1104/6500 [2:05:32<10:10:50,  6.79s/it] 17%|        | 1105/6500 [2:05:39<10:04:35,  6.72s/it]                                                         17%|        | 1105/6500 [2:05:39<10:04:35,  6.72s/it] 17%|        | 1106/6500 [2:05:45<10:00:06,  6.68s/it]                                                         17%{'loss': 0.6596, 'learning_rate': 9.303291695634449e-05, 'epoch': 0.17}
{'loss': 0.6546, 'learning_rate': 9.302060317670787e-05, 'epoch': 0.17}
{'loss': 0.6856, 'learning_rate': 9.300827934126683e-05, 'epoch': 0.17}
{'loss': 0.7015, 'learning_rate': 9.299594545290202e-05, 'epoch': 0.17}
|        | 1106/6500 [2:05:45<10:00:06,  6.68s/it] 17%|        | 1107/6500 [2:05:52<9:57:02,  6.64s/it]                                                         17%|        | 1107/6500 [2:05:52<9:57:02,  6.64s/it] 17%|        | 1108/6500 [2:05:58<9:54:36,  6.62s/it]                                                        17%|        | 1108/6500 [2:05:58<9:54:36,  6.62s/it] 17%|        | 1109/6500 [2:06:05<9:52:38,  6.60s/it]                                                        17%|        | 1109/6500 [2:06:05<9:52:38,  6.60s/it] 17%|        | 1110/6500 [2:06:12<9:51:55,  6.59s/it]                                                        17%|        | 1110/6500 [2:06:12<9:51:55,  6.59s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8624659776687622, 'eval_runtime': 1.485, 'eval_samples_per_second': 8.081, 'eval_steps_per_second': 2.02, 'epoch': 0.17}
                                                        17%|        | 1110/6500 [2:06:13<9:51:55,  6.59s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-1110
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1110
/projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1110
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1110
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1110
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1110/pytorch_model.bin
the pytorch model path is the pytorch model path is/projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1110/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1110/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1110/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.6833, 'learning_rate': 9.298360151449635e-05, 'epoch': 0.17}
{'loss': 0.6587, 'learning_rate': 9.297124752893518e-05, 'epoch': 0.17}
{'loss': 0.6804, 'learning_rate': 9.295888349910618e-05, 'epoch': 0.17}
{'loss': 0.6567, 'learning_rate': 9.294650942789933e-05, 'epoch': 0.17}
{'loss': 0.7055, 'learning_rate': 9.293412531820704e-05, 'epoch': 0.17}
{'loss': 0.6505, 'learning_rate': 9.292173117292399e-05, 'epoch': 0.17}
 17%|        | 1111/6500 [2:06:20<10:40:00,  7.13s/it]                                                         17%|        | 1111/6500 [2:06:20<10:40:00,  7.13s/it] 17%|        | 1112/6500 [2:06:26<10:25:04,  6.96s/it]                                                         17%|        | 1112/6500 [2:06:26<10:25:04,  6.96s/it] 17%|        | 1113/6500 [2:06:33<10:14:01,  6.84s/it]                                                         17%|        | 1113/6500 [2:06:33<10:14:01,  6.84s/it] 17%|        | 1114/6500 [2:06:40<10:06:26,  6.76s/it]                                                         17%|        | 1114/6500 [2:06:40<10:06:26,  6.76s/it] 17%|        | 1115/6500 [2:06:47<10:23:55,  6.95s/it]                                                         17%|        | 1115/6500 [2:06:47<10:23:55,  6.95s/it] 17%|        | 1116/6500 [2:06:54<10:13:39,  6.84s/it]                                                         17%{'loss': 0.8871, 'learning_rate': 9.290932699494726e-05, 'epoch': 0.17}
{'loss': 0.749, 'learning_rate': 9.289691278717623e-05, 'epoch': 0.17}
{'loss': 0.6416, 'learning_rate': 9.288448855251265e-05, 'epoch': 0.17}
{'loss': 0.6896, 'learning_rate': 9.287205429386063e-05, 'epoch': 0.17}
|        | 1116/6500 [2:06:54<10:13:39,  6.84s/it] 17%|        | 1117/6500 [2:07:00<10:06:01,  6.75s/it]                                                         17%|        | 1117/6500 [2:07:00<10:06:01,  6.75s/it] 17%|        | 1118/6500 [2:07:07<10:00:54,  6.70s/it]                                                         17%|        | 1118/6500 [2:07:07<10:00:54,  6.70s/it] 17%|        | 1119/6500 [2:07:13<9:57:37,  6.66s/it]                                                         17%|        | 1119/6500 [2:07:13<9:57:37,  6.66s/it] 17%|        | 1120/6500 [2:07:20<9:54:37,  6.63s/it]                                                        17%|        | 1120/6500 [2:07:20<9:54:37,  6.63s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8648548126220703, 'eval_runtime': 1.4859, 'eval_samples_per_second': 8.076, 'eval_steps_per_second': 2.019, 'epoch': 0.17}
                                                        17%|        | 1120/6500 [2:07:21<9:54:37,  6.63s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-1120
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1120I AM HERE AND I AM SAVING THE MODEL

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1120
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1120
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1120
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1120/pytorch_model.bin 
/projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1120/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1120/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1120/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.6376, 'learning_rate': 9.285961001412657e-05, 'epoch': 0.17}
{'loss': 0.6493, 'learning_rate': 9.284715571621927e-05, 'epoch': 0.17}
{'loss': 0.6626, 'learning_rate': 9.283469140304983e-05, 'epoch': 0.17}
{'loss': 0.6607, 'learning_rate': 9.28222170775317e-05, 'epoch': 0.17}
{'loss': 0.6984, 'learning_rate': 9.280973274258071e-05, 'epoch': 0.17}
{'loss': 0.6847, 'learning_rate': 9.279723840111496e-05, 'epoch': 0.17}
 17%|        | 1121/6500 [2:07:28<10:41:21,  7.15s/it]                                                         17%|        | 1121/6500 [2:07:28<10:41:21,  7.15s/it] 17%|        | 1122/6500 [2:07:35<10:25:48,  6.98s/it]                                                         17%|        | 1122/6500 [2:07:35<10:25:48,  6.98s/it] 17%|        | 1123/6500 [2:07:41<10:14:53,  6.86s/it]                                                         17%|        | 1123/6500 [2:07:41<10:14:53,  6.86s/it] 17%|        | 1124/6500 [2:07:48<10:06:46,  6.77s/it]                                                         17%|        | 1124/6500 [2:07:48<10:06:46,  6.77s/it] 17%|        | 1125/6500 [2:07:55<10:01:20,  6.71s/it]                                                         17%|        | 1125/6500 [2:07:55<10:01:20,  6.71s/it] 17%|        | 1126/6500 [2:08:01<9:57:11,  6.67s/it]                                                         17%|{'loss': 0.6437, 'learning_rate': 9.278473405605497e-05, 'epoch': 0.17}
{'loss': 0.6775, 'learning_rate': 9.277221971032351e-05, 'epoch': 0.17}
{'loss': 0.6498, 'learning_rate': 9.275969536684577e-05, 'epoch': 0.17}
{'loss': 0.7057, 'learning_rate': 9.274716102854922e-05, 'epoch': 0.17}
        | 1126/6500 [2:08:01<9:57:11,  6.67s/it] 17%|        | 1127/6500 [2:08:08<9:54:45,  6.64s/it]                                                        17%|        | 1127/6500 [2:08:08<9:54:45,  6.64s/it] 17%|        | 1128/6500 [2:08:14<9:52:31,  6.62s/it]                                                        17%|        | 1128/6500 [2:08:14<9:52:31,  6.62s/it] 17%|        | 1129/6500 [2:08:21<9:50:44,  6.60s/it]                                                        17%|        | 1129/6500 [2:08:21<9:50:44,  6.60s/it] 17%|        | 1130/6500 [2:08:27<9:49:31,  6.59s/it]                                                        17%|        | 1130/6500 [2:08:27<9:49:31,  6.59s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8584859371185303, 'eval_runtime': 2.2357, 'eval_samples_per_second': 5.367, 'eval_steps_per_second': 1.342, 'epoch': 0.17}
                                                        17%|        | 1130/6500 [2:08:30<9:49:31,  6.59s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-1130
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1130
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1130
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1130
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1130
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1130/pytorch_model.bin
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1130/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1130/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1130/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.6519, 'learning_rate': 9.273461669836366e-05, 'epoch': 0.17}
{'loss': 0.9362, 'learning_rate': 9.272206237922129e-05, 'epoch': 0.17}
{'loss': 0.685, 'learning_rate': 9.270949807405662e-05, 'epoch': 0.17}
{'loss': 0.6438, 'learning_rate': 9.269692378580642e-05, 'epoch': 0.17}
{'loss': 0.6751, 'learning_rate': 9.26843395174099e-05, 'epoch': 0.17}
{'loss': 0.634, 'learning_rate': 9.267174527180853e-05, 'epoch': 0.17}
 17%|        | 1131/6500 [2:08:37<11:01:44,  7.40s/it]                                                         17%|        | 1131/6500 [2:08:37<11:01:44,  7.40s/it] 17%|        | 1132/6500 [2:08:45<11:18:10,  7.58s/it]                                                         17%|        | 1132/6500 [2:08:45<11:18:10,  7.58s/it] 17%|        | 1133/6500 [2:08:51<10:51:49,  7.29s/it]                                                         17%|        | 1133/6500 [2:08:51<10:51:49,  7.29s/it] 17%|        | 1134/6500 [2:08:58<10:33:30,  7.08s/it]                                                         17%|        | 1134/6500 [2:08:58<10:33:30,  7.08s/it] 17%|        | 1135/6500 [2:09:04<10:20:34,  6.94s/it]                                                         17%|        | 1135/6500 [2:09:04<10:20:34,  6.94s/it] 17%|        | 1136/6500 [2:09:11<10:11:34,  6.84s/it]                                                         17%{'loss': 0.6417, 'learning_rate': 9.265914105194617e-05, 'epoch': 0.17}
{'loss': 0.6768, 'learning_rate': 9.264652686076895e-05, 'epoch': 0.18}
{'loss': 0.666, 'learning_rate': 9.263390270122538e-05, 'epoch': 0.18}
{'loss': 0.6716, 'learning_rate': 9.262126857626627e-05, 'epoch': 0.18}
|        | 1136/6500 [2:09:11<10:11:34,  6.84s/it] 17%|        | 1137/6500 [2:09:18<10:08:38,  6.81s/it]                                                         17%|        | 1137/6500 [2:09:18<10:08:38,  6.81s/it] 18%|        | 1138/6500 [2:09:24<10:03:13,  6.75s/it]                                                         18%|        | 1138/6500 [2:09:24<10:03:13,  6.75s/it] 18%|        | 1139/6500 [2:09:31<9:58:52,  6.70s/it]                                                         18%|        | 1139/6500 [2:09:31<9:58:52,  6.70s/it] 18%|        | 1140/6500 [2:09:38<9:58:32,  6.70s/it]                                                        18%|        | 1140/6500 [2:09:38<9:58:32,  6.70s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8562698364257812, 'eval_runtime': 1.513, 'eval_samples_per_second': 7.931, 'eval_steps_per_second': 1.983, 'epoch': 0.18}
                                                        18%|        | 1140/6500 [2:09:39<9:58:32,  6.70s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-1140
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1140I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1140

I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1140
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1140
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1140/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1140/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1140/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1140/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.6717, 'learning_rate': 9.260862448884477e-05, 'epoch': 0.18}
{'loss': 0.6519, 'learning_rate': 9.259597044191636e-05, 'epoch': 0.18}
{'loss': 0.6665, 'learning_rate': 9.258330643843884e-05, 'epoch': 0.18}
{'loss': 0.6511, 'learning_rate': 9.257063248137236e-05, 'epoch': 0.18}
{'loss': 0.6965, 'learning_rate': 9.255794857367936e-05, 'epoch': 0.18}
{'loss': 0.6648, 'learning_rate': 9.254525471832464e-05, 'epoch': 0.18}
 18%|        | 1141/6500 [2:09:49<12:04:02,  8.11s/it]                                                         18%|        | 1141/6500 [2:09:49<12:04:02,  8.11s/it] 18%|        | 1142/6500 [2:09:56<11:23:51,  7.66s/it]                                                         18%|        | 1142/6500 [2:09:56<11:23:51,  7.66s/it] 18%|        | 1143/6500 [2:10:02<10:55:21,  7.34s/it]                                                         18%|        | 1143/6500 [2:10:02<10:55:21,  7.34s/it] 18%|        | 1144/6500 [2:10:09<10:35:36,  7.12s/it]                                                         18%|        | 1144/6500 [2:10:09<10:35:36,  7.12s/it] 18%|        | 1145/6500 [2:10:15<10:21:22,  6.96s/it]                                                         18%|        | 1145/6500 [2:10:15<10:21:22,  6.96s/it] 18%|        | 1146/6500 [2:10:22<10:11:18,  6.85s/it]                                                         18%{'loss': 0.9235, 'learning_rate': 9.253255091827533e-05, 'epoch': 0.18}
{'loss': 0.6798, 'learning_rate': 9.251983717650084e-05, 'epoch': 0.18}
{'loss': 0.6551, 'learning_rate': 9.250711349597291e-05, 'epoch': 0.18}
{'loss': 0.6529, 'learning_rate': 9.249437987966567e-05, 'epoch': 0.18}
|        | 1146/6500 [2:10:22<10:11:18,  6.85s/it] 18%|        | 1147/6500 [2:10:29<10:04:00,  6.77s/it]                                                         18%|        | 1147/6500 [2:10:29<10:04:00,  6.77s/it] 18%|        | 1148/6500 [2:10:37<10:45:14,  7.23s/it]                                                         18%|        | 1148/6500 [2:10:37<10:45:14,  7.23s/it] 18%|        | 1149/6500 [2:10:44<10:28:02,  7.04s/it]                                                         18%|        | 1149/6500 [2:10:44<10:28:02,  7.04s/it] 18%|        | 1150/6500 [2:10:50<10:16:23,  6.91s/it]                                                         18%|        | 1150/6500 [2:10:50<10:16:23,  6.91s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.859830379486084, 'eval_runtime': 1.5271, 'eval_samples_per_second': 7.858, 'eval_steps_per_second': 1.965, 'epoch': 0.18}
                                                         18%|        | 1150/6500 [2:10:52<10:16:23,  6.91s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-1150
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1150I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1150

I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1150
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1150
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1150/pytorch_model.bin
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1150/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1150/pytorch_model.binthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1150/pytorch_model.bin

/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.6419, 'learning_rate': 9.248163633055549e-05, 'epoch': 0.18}
{'loss': 0.6497, 'learning_rate': 9.246888285162112e-05, 'epoch': 0.18}
{'loss': 0.6702, 'learning_rate': 9.24561194458436e-05, 'epoch': 0.18}
{'loss': 0.6779, 'learning_rate': 9.244334611620629e-05, 'epoch': 0.18}
{'loss': 0.6746, 'learning_rate': 9.243056286569488e-05, 'epoch': 0.18}
{'loss': 0.6611, 'learning_rate': 9.241776969729739e-05, 'epoch': 0.18}
 18%|        | 1151/6500 [2:10:59<10:58:24,  7.39s/it]                                                         18%|        | 1151/6500 [2:10:59<10:58:24,  7.39s/it] 18%|        | 1152/6500 [2:11:05<10:37:02,  7.15s/it]                                                         18%|        | 1152/6500 [2:11:05<10:37:02,  7.15s/it] 18%|        | 1153/6500 [2:11:12<10:21:43,  6.98s/it]                                                         18%|        | 1153/6500 [2:11:12<10:21:43,  6.98s/it] 18%|        | 1154/6500 [2:11:18<10:10:53,  6.86s/it]                                                         18%|        | 1154/6500 [2:11:18<10:10:53,  6.86s/it] 18%|        | 1155/6500 [2:11:28<11:28:54,  7.73s/it]                                                         18%|        | 1155/6500 [2:11:29<11:28:54,  7.73s/it] 18%|        | 1156/6500 [2:11:35<11:11:49,  7.54s/it]                                                         18%{'loss': 0.6591, 'learning_rate': 9.240496661400414e-05, 'epoch': 0.18}
{'loss': 0.6571, 'learning_rate': 9.239215361880776e-05, 'epoch': 0.18}
{'loss': 0.6446, 'learning_rate': 9.237933071470323e-05, 'epoch': 0.18}
{'loss': 0.6876, 'learning_rate': 9.23664979046878e-05, 'epoch': 0.18}
|        | 1156/6500 [2:11:35<11:11:49,  7.54s/it] 18%|        | 1157/6500 [2:11:42<10:45:44,  7.25s/it]                                                         18%|        | 1157/6500 [2:11:42<10:45:44,  7.25s/it] 18%|        | 1158/6500 [2:11:48<10:27:26,  7.05s/it]                                                         18%|        | 1158/6500 [2:11:48<10:27:26,  7.05s/it] 18%|        | 1159/6500 [2:11:55<10:14:30,  6.90s/it]                                                         18%|        | 1159/6500 [2:11:55<10:14:30,  6.90s/it] 18%|        | 1160/6500 [2:12:02<10:05:05,  6.80s/it]                                                         18%|        | 1160/6500 [2:12:02<10:05:05,  6.80s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8557372093200684, 'eval_runtime': 1.6945, 'eval_samples_per_second': 7.082, 'eval_steps_per_second': 1.77, 'epoch': 0.18}
                                                         18%|        | 1160/6500 [2:12:03<10:05:05,  6.80s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-1160
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1160I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1160

I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1160
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1160
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1160/pytorch_model.bin
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1160/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1160/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1160/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.6574, 'learning_rate': 9.23536551917611e-05, 'epoch': 0.18}
{'loss': 0.935, 'learning_rate': 9.2340802578925e-05, 'epoch': 0.18}
{'loss': 0.6639, 'learning_rate': 9.232794006918375e-05, 'epoch': 0.18}
{'loss': 0.6604, 'learning_rate': 9.231506766554384e-05, 'epoch': 0.18}
{'loss': 0.6412, 'learning_rate': 9.230218537101416e-05, 'epoch': 0.18}
{'loss': 0.6298, 'learning_rate': 9.228929318860584e-05, 'epoch': 0.18}
 18%|        | 1161/6500 [2:12:10<10:56:04,  7.37s/it]                                                         18%|        | 1161/6500 [2:12:10<10:56:04,  7.37s/it] 18%|        | 1162/6500 [2:12:17<10:34:02,  7.13s/it]                                                         18%|        | 1162/6500 [2:12:17<10:34:02,  7.13s/it] 18%|        | 1163/6500 [2:12:23<10:18:57,  6.96s/it]                                                         18%|        | 1163/6500 [2:12:23<10:18:57,  6.96s/it] 18%|        | 1164/6500 [2:12:31<10:33:32,  7.12s/it]                                                         18%|        | 1164/6500 [2:12:31<10:33:32,  7.12s/it] 18%|        | 1165/6500 [2:12:37<10:18:28,  6.96s/it]                                                         18%|        | 1165/6500 [2:12:37<10:18:28,  6.96s/it] 18%|        | 1166/6500 [2:12:44<10:07:46,  6.84s/it]                                                         18%{'loss': 0.6348, 'learning_rate': 9.227639112133238e-05, 'epoch': 0.18}
{'loss': 0.656, 'learning_rate': 9.226347917220953e-05, 'epoch': 0.18}
{'loss': 0.6762, 'learning_rate': 9.225055734425539e-05, 'epoch': 0.18}
{'loss': 0.6682, 'learning_rate': 9.223762564049035e-05, 'epoch': 0.18}
|        | 1166/6500 [2:12:44<10:07:46,  6.84s/it] 18%|        | 1167/6500 [2:12:51<10:00:34,  6.76s/it]                                                         18%|        | 1167/6500 [2:12:51<10:00:34,  6.76s/it] 18%|        | 1168/6500 [2:12:57<9:55:06,  6.70s/it]                                                         18%|        | 1168/6500 [2:12:57<9:55:06,  6.70s/it] 18%|        | 1169/6500 [2:13:04<9:51:50,  6.66s/it]                                                        18%|        | 1169/6500 [2:13:04<9:51:50,  6.66s/it] 18%|        | 1170/6500 [2:13:10<9:50:02,  6.64s/it]                                                        18%|        | 1170/6500 [2:13:10<9:50:02,  6.64s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8527745008468628, 'eval_runtime': 1.4844, 'eval_samples_per_second': 8.084, 'eval_steps_per_second': 2.021, 'epoch': 0.18}
                                                        18%|        | 1170/6500 [2:13:12<9:50:02,  6.64s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-1170
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1170I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1170

I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1170
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1170
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1170/pytorch_model.bin
the pytorch model path isthe pytorch model path is  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1170/pytorch_model.bin/projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1170/pytorch_model.bin

the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1170/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.6619, 'learning_rate': 9.222468406393713e-05, 'epoch': 0.18}
{'loss': 0.6578, 'learning_rate': 9.221173261762073e-05, 'epoch': 0.18}
{'loss': 0.6486, 'learning_rate': 9.219877130456851e-05, 'epoch': 0.18}
{'loss': 0.6539, 'learning_rate': 9.218580012781005e-05, 'epoch': 0.18}
{'loss': 0.6792, 'learning_rate': 9.217281909037732e-05, 'epoch': 0.18}
{'loss': 0.653, 'learning_rate': 9.215982819530451e-05, 'epoch': 0.18}
 18%|        | 1171/6500 [2:13:19<10:36:11,  7.16s/it]                                                         18%|        | 1171/6500 [2:13:19<10:36:11,  7.16s/it] 18%|        | 1172/6500 [2:13:25<10:20:16,  6.99s/it]                                                         18%|        | 1172/6500 [2:13:25<10:20:16,  6.99s/it] 18%|        | 1173/6500 [2:13:32<10:08:57,  6.86s/it]                                                         18%|        | 1173/6500 [2:13:32<10:08:57,  6.86s/it] 18%|        | 1174/6500 [2:13:38<10:00:46,  6.77s/it]                                                         18%|        | 1174/6500 [2:13:38<10:00:46,  6.77s/it] 18%|        | 1175/6500 [2:13:45<9:55:07,  6.71s/it]                                                         18%|        | 1175/6500 [2:13:45<9:55:07,  6.71s/it] 18%|        | 1176/6500 [2:13:52<9:52:42,  6.68s/it]                                                        18%|{'loss': 0.9365, 'learning_rate': 9.214682744562823e-05, 'epoch': 0.18}
{'loss': 0.6389, 'learning_rate': 9.213381684438726e-05, 'epoch': 0.18}
{'loss': 0.6657, 'learning_rate': 9.212079639462281e-05, 'epoch': 0.18}
{'loss': 0.6423, 'learning_rate': 9.210776609937829e-05, 'epoch': 0.18}
        | 1176/6500 [2:13:52<9:52:42,  6.68s/it] 18%|        | 1177/6500 [2:13:58<9:49:24,  6.64s/it]                                                        18%|        | 1177/6500 [2:13:58<9:49:24,  6.64s/it] 18%|        | 1178/6500 [2:14:05<9:47:13,  6.62s/it]                                                        18%|        | 1178/6500 [2:14:05<9:47:13,  6.62s/it] 18%|        | 1179/6500 [2:14:11<9:45:53,  6.61s/it]                                                        18%|        | 1179/6500 [2:14:11<9:45:53,  6.61s/it] 18%|        | 1180/6500 [2:14:18<10:01:00,  6.78s/it]                                                         18%|        | 1180/6500 [2:14:18<10:01:00,  6.78s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8579555153846741, 'eval_runtime': 1.5123, 'eval_samples_per_second': 7.935, 'eval_steps_per_second': 1.984, 'epoch': 0.18}
                                                         18%|        | 1180/6500 [2:14:20<10:01:00,  6.78s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-1180
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in   /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1180
/projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1180
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1180
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1180
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1180/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1180/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1180/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1180/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.6357, 'learning_rate': 9.209472596169946e-05, 'epoch': 0.18}
{'loss': 0.6413, 'learning_rate': 9.208167598463439e-05, 'epoch': 0.18}
{'loss': 0.6591, 'learning_rate': 9.206861617123341e-05, 'epoch': 0.18}
{'loss': 0.6797, 'learning_rate': 9.205554652454918e-05, 'epoch': 0.18}
{'loss': 0.6599, 'learning_rate': 9.204246704763665e-05, 'epoch': 0.18}
{'loss': 0.6508, 'learning_rate': 9.202937774355307e-05, 'epoch': 0.18}
 18%|        | 1181/6500 [2:14:27<10:43:44,  7.26s/it]                                                         18%|        | 1181/6500 [2:14:27<10:43:44,  7.26s/it] 18%|        | 1182/6500 [2:14:33<10:25:14,  7.05s/it]                                                         18%|        | 1182/6500 [2:14:33<10:25:14,  7.05s/it] 18%|        | 1183/6500 [2:14:40<10:11:52,  6.90s/it]                                                         18%|        | 1183/6500 [2:14:40<10:11:52,  6.90s/it] 18%|        | 1184/6500 [2:14:47<10:03:08,  6.81s/it]                                                         18%|        | 1184/6500 [2:14:47<10:03:08,  6.81s/it] 18%|        | 1185/6500 [2:14:53<9:57:00,  6.74s/it]                                                         18%|        | 1185/6500 [2:14:53<9:57:00,  6.74s/it] 18%|        | 1186/6500 [2:15:00<9:52:34,  6.69s/it]                                                        18%|{'loss': 0.6647, 'learning_rate': 9.201627861535799e-05, 'epoch': 0.18}
{'loss': 0.6462, 'learning_rate': 9.200316966611324e-05, 'epoch': 0.18}
{'loss': 0.6591, 'learning_rate': 9.199005089888297e-05, 'epoch': 0.18}
{'loss': 0.6591, 'learning_rate': 9.197692231673361e-05, 'epoch': 0.18}
        | 1186/6500 [2:15:00<9:52:34,  6.69s/it] 18%|        | 1187/6500 [2:15:06<9:49:05,  6.65s/it]                                                        18%|        | 1187/6500 [2:15:06<9:49:05,  6.65s/it] 18%|        | 1188/6500 [2:15:13<9:46:26,  6.62s/it]                                                        18%|        | 1188/6500 [2:15:13<9:46:26,  6.62s/it] 18%|        | 1189/6500 [2:15:19<9:44:53,  6.61s/it]                                                        18%|        | 1189/6500 [2:15:19<9:44:53,  6.61s/it] 18%|        | 1190/6500 [2:15:26<9:43:27,  6.59s/it]                                                        18%|        | 1190/6500 [2:15:26<9:43:27,  6.59s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8545814752578735, 'eval_runtime': 1.7386, 'eval_samples_per_second': 6.902, 'eval_steps_per_second': 1.726, 'epoch': 0.18}
                                                        18%|        | 1190/6500 [2:15:28<9:43:27,  6.59s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-1190
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  I AM HERE AND I AM SAVING THE MODEL
/projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1190the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1190

I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1190
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1190
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1190/pytorch_model.bin
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1190/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1190/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1190/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.6594, 'learning_rate': 9.196378392273387e-05, 'epoch': 0.18}
{'loss': 0.9325, 'learning_rate': 9.195063571995479e-05, 'epoch': 0.18}
{'loss': 0.6374, 'learning_rate': 9.193747771146968e-05, 'epoch': 0.18}
{'loss': 0.6723, 'learning_rate': 9.192430990035413e-05, 'epoch': 0.18}
{'loss': 0.6252, 'learning_rate': 9.191113228968604e-05, 'epoch': 0.18}
{'loss': 0.6374, 'learning_rate': 9.189794488254561e-05, 'epoch': 0.18}
 18%|        | 1191/6500 [2:15:35<10:36:13,  7.19s/it]                                                         18%|        | 1191/6500 [2:15:35<10:36:13,  7.19s/it] 18%|        | 1192/6500 [2:15:41<10:19:13,  7.00s/it]                                                         18%|        | 1192/6500 [2:15:41<10:19:13,  7.00s/it] 18%|        | 1193/6500 [2:15:48<10:07:28,  6.87s/it]                                                         18%|        | 1193/6500 [2:15:48<10:07:28,  6.87s/it] 18%|        | 1194/6500 [2:15:54<9:59:12,  6.78s/it]                                                         18%|        | 1194/6500 [2:15:54<9:59:12,  6.78s/it] 18%|        | 1195/6500 [2:16:01<9:53:32,  6.71s/it]                                                        18%|        | 1195/6500 [2:16:01<9:53:32,  6.71s/it] 18%|        | 1196/6500 [2:16:08<10:05:31,  6.85s/it]                                                         18%|{'loss': 0.646, 'learning_rate': 9.188474768201532e-05, 'epoch': 0.18}
{'loss': 0.6422, 'learning_rate': 9.18715406911799e-05, 'epoch': 0.18}
{'loss': 0.6897, 'learning_rate': 9.185832391312644e-05, 'epoch': 0.18}
{'loss': 0.6748, 'learning_rate': 9.184509735094427e-05, 'epoch': 0.18}
        | 1196/6500 [2:16:08<10:05:31,  6.85s/it] 18%|        | 1197/6500 [2:16:15<9:57:46,  6.76s/it]                                                         18%|        | 1197/6500 [2:16:15<9:57:46,  6.76s/it] 18%|        | 1198/6500 [2:16:21<9:52:24,  6.70s/it]                                                        18%|        | 1198/6500 [2:16:21<9:52:24,  6.70s/it] 18%|        | 1199/6500 [2:16:28<9:48:47,  6.66s/it]                                                        18%|        | 1199/6500 [2:16:28<9:48:47,  6.66s/it] 18%|        | 1200/6500 [2:16:34<9:46:29,  6.64s/it]                                                        18%|        | 1200/6500 [2:16:34<9:46:29,  6.64s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8512656092643738, 'eval_runtime': 1.5055, 'eval_samples_per_second': 7.971, 'eval_steps_per_second': 1.993, 'epoch': 0.18}
                                                        18%|        | 1200/6500 [2:16:36<9:46:29,  6.64s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-1200
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1200I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1200

I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1200
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1200
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1200/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1200/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1200/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1200/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.627, 'learning_rate': 9.1831861007725e-05, 'epoch': 0.18}
{'loss': 0.6605, 'learning_rate': 9.181861488656256e-05, 'epoch': 0.18}
{'loss': 0.6359, 'learning_rate': 9.180535899055316e-05, 'epoch': 0.19}
{'loss': 0.6899, 'learning_rate': 9.17920933227953e-05, 'epoch': 0.19}
{'loss': 0.6311, 'learning_rate': 9.177881788638969e-05, 'epoch': 0.19}
{'loss': 0.9228, 'learning_rate': 9.176553268443943e-05, 'epoch': 0.19}
 18%|        | 1201/6500 [2:16:43<10:36:22,  7.21s/it]                                                         18%|        | 1201/6500 [2:16:43<10:36:22,  7.21s/it] 18%|        | 1202/6500 [2:16:49<10:19:08,  7.01s/it]                                                         18%|        | 1202/6500 [2:16:49<10:19:08,  7.01s/it] 19%|        | 1203/6500 [2:16:56<10:06:56,  6.87s/it]                                                         19%|        | 1203/6500 [2:16:56<10:06:56,  6.87s/it] 19%|        | 1204/6500 [2:17:02<9:58:38,  6.78s/it]                                                         19%|        | 1204/6500 [2:17:02<9:58:38,  6.78s/it] 19%|        | 1205/6500 [2:17:09<9:52:48,  6.72s/it]                                                        19%|        | 1205/6500 [2:17:09<9:52:48,  6.72s/it] 19%|        | 1206/6500 [2:17:16<9:48:15,  6.67s/it]                                                        19%|{'loss': 0.6629, 'learning_rate': 9.175223772004986e-05, 'epoch': 0.19}
{'loss': 0.6297, 'learning_rate': 9.173893299632856e-05, 'epoch': 0.19}
{'loss': 0.6581, 'learning_rate': 9.172561851638545e-05, 'epoch': 0.19}
{'loss': 0.6209, 'learning_rate': 9.171229428333272e-05, 'epoch': 0.19}
        | 1206/6500 [2:17:16<9:48:15,  6.67s/it] 19%|        | 1207/6500 [2:17:22<9:45:26,  6.64s/it]                                                        19%|        | 1207/6500 [2:17:22<9:45:26,  6.64s/it] 19%|        | 1208/6500 [2:17:29<9:43:22,  6.61s/it]                                                        19%|        | 1208/6500 [2:17:29<9:43:22,  6.61s/it] 19%|        | 1209/6500 [2:17:35<9:42:10,  6.60s/it]                                                        19%|        | 1209/6500 [2:17:35<9:42:10,  6.60s/it] 19%|        | 1210/6500 [2:17:42<9:41:07,  6.59s/it]                                                        19%|        | 1210/6500 [2:17:42<9:41:07,  6.59s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8563529253005981, 'eval_runtime': 1.6104, 'eval_samples_per_second': 7.451, 'eval_steps_per_second': 1.863, 'epoch': 0.19}
                                                        19%|        | 1210/6500 [2:17:43<9:41:07,  6.59s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-1210
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1210I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1210

I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1210
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1210
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1210/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1210/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1210/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1210/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.6324, 'learning_rate': 9.169896030028482e-05, 'epoch': 0.19}
{'loss': 0.65, 'learning_rate': 9.168561657035848e-05, 'epoch': 0.19}
{'loss': 0.6584, 'learning_rate': 9.16722630966727e-05, 'epoch': 0.19}
{'loss': 0.6627, 'learning_rate': 9.165889988234881e-05, 'epoch': 0.19}
{'loss': 0.652, 'learning_rate': 9.164552693051035e-05, 'epoch': 0.19}
{'loss': 0.6341, 'learning_rate': 9.16321442442832e-05, 'epoch': 0.19}
 19%|        | 1211/6500 [2:17:50<10:31:48,  7.17s/it]                                                         19%|        | 1211/6500 [2:17:50<10:31:48,  7.17s/it] 19%|        | 1212/6500 [2:17:58<10:41:16,  7.28s/it]                                                         19%|        | 1212/6500 [2:17:58<10:41:16,  7.28s/it] 19%|        | 1213/6500 [2:18:04<10:22:42,  7.07s/it]                                                         19%|        | 1213/6500 [2:18:04<10:22:42,  7.07s/it] 19%|        | 1214/6500 [2:18:11<10:09:35,  6.92s/it]                                                         19%|        | 1214/6500 [2:18:11<10:09:35,  6.92s/it] 19%|        | 1215/6500 [2:18:18<10:00:12,  6.81s/it]                                                         19%|        | 1215/6500 [2:18:18<10:00:12,  6.81s/it] 19%|        | 1216/6500 [2:18:24<9:53:21,  6.74s/it]                                                         19%|{'loss': 0.6442, 'learning_rate': 9.161875182679546e-05, 'epoch': 0.19}
{'loss': 0.6303, 'learning_rate': 9.160534968117752e-05, 'epoch': 0.19}
{'loss': 0.6832, 'learning_rate': 9.159193781056203e-05, 'epoch': 0.19}
{'loss': 0.6362, 'learning_rate': 9.1578516218084e-05, 'epoch': 0.19}
        | 1216/6500 [2:18:24<9:53:21,  6.74s/it] 19%|        | 1217/6500 [2:18:31<9:48:33,  6.68s/it]                                                        19%|        | 1217/6500 [2:18:31<9:48:33,  6.68s/it] 19%|        | 1218/6500 [2:18:37<9:45:08,  6.65s/it]                                                        19%|        | 1218/6500 [2:18:37<9:45:08,  6.65s/it] 19%|        | 1219/6500 [2:18:44<9:42:38,  6.62s/it]                                                        19%|        | 1219/6500 [2:18:44<9:42:38,  6.62s/it] 19%|        | 1220/6500 [2:18:50<9:42:08,  6.62s/it]                                                        19%|        | 1220/6500 [2:18:50<9:42:08,  6.62s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8527622222900391, 'eval_runtime': 1.8622, 'eval_samples_per_second': 6.444, 'eval_steps_per_second': 1.611, 'epoch': 0.19}
                                                        19%|        | 1220/6500 [2:18:52<9:42:08,  6.62s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-1220
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1220I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1220

I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1220
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1220
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1220/pytorch_model.bin
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1220/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1220/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1220/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.9125, 'learning_rate': 9.156508490688058e-05, 'epoch': 0.19}
{'loss': 0.6618, 'learning_rate': 9.15516438800913e-05, 'epoch': 0.19}
{'loss': 0.6178, 'learning_rate': 9.153819314085787e-05, 'epoch': 0.19}
{'loss': 0.6591, 'learning_rate': 9.152473269232437e-05, 'epoch': 0.19}
{'loss': 0.6135, 'learning_rate': 9.151126253763708e-05, 'epoch': 0.19}
{'loss': 0.6315, 'learning_rate': 9.149778267994457e-05, 'epoch': 0.19}
 19%|        | 1221/6500 [2:18:59<10:37:16,  7.24s/it]                                                         19%|        | 1221/6500 [2:18:59<10:37:16,  7.24s/it] 19%|        | 1222/6500 [2:19:06<10:18:54,  7.04s/it]                                                         19%|        | 1222/6500 [2:19:06<10:18:54,  7.04s/it] 19%|        | 1223/6500 [2:19:12<10:06:00,  6.89s/it]                                                         19%|        | 1223/6500 [2:19:12<10:06:00,  6.89s/it] 19%|        | 1224/6500 [2:19:19<9:57:23,  6.79s/it]                                                         19%|        | 1224/6500 [2:19:19<9:57:23,  6.79s/it] 19%|        | 1225/6500 [2:19:25<9:51:17,  6.73s/it]                                                        19%|        | 1225/6500 [2:19:25<9:51:17,  6.73s/it] 19%|        | 1226/6500 [2:19:32<9:46:50,  6.68s/it]                                                        19%|{'loss': 0.6448, 'learning_rate': 9.148429312239767e-05, 'epoch': 0.19}
{'loss': 0.6525, 'learning_rate': 9.147079386814947e-05, 'epoch': 0.19}
{'loss': 0.6516, 'learning_rate': 9.145728492035536e-05, 'epoch': 0.19}
{'loss': 0.6478, 'learning_rate': 9.144376628217295e-05, 'epoch': 0.19}
        | 1226/6500 [2:19:32<9:46:50,  6.68s/it] 19%|        | 1227/6500 [2:19:38<9:43:45,  6.64s/it]                                                        19%|        | 1227/6500 [2:19:38<9:43:45,  6.64s/it] 19%|        | 1228/6500 [2:19:45<9:41:34,  6.62s/it]                                                        19%|        | 1228/6500 [2:19:45<9:41:34,  6.62s/it] 19%|        | 1229/6500 [2:19:53<10:09:13,  6.93s/it]                                                         19%|        | 1229/6500 [2:19:53<10:09:13,  6.93s/it] 19%|        | 1230/6500 [2:19:59<9:59:39,  6.83s/it]                                                         19%|        | 1230/6500 [2:19:59<9:59:39,  6.83s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8524448275566101, 'eval_runtime': 1.4866, 'eval_samples_per_second': 8.072, 'eval_steps_per_second': 2.018, 'epoch': 0.19}
                                                        19%|        | 1230/6500 [2:20:01<9:59:39,  6.83s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-1230
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1230
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1230
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1230
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1230
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1230/pytorch_model.bin
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1230/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1230/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1230/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.6391, 'learning_rate': 9.143023795676217e-05, 'epoch': 0.19}
{'loss': 0.6452, 'learning_rate': 9.141669994728517e-05, 'epoch': 0.19}
{'loss': 0.6273, 'learning_rate': 9.140315225690636e-05, 'epoch': 0.19}
{'loss': 0.6781, 'learning_rate': 9.138959488879244e-05, 'epoch': 0.19}
{'loss': 0.636, 'learning_rate': 9.137602784611239e-05, 'epoch': 0.19}
{'loss': 0.9161, 'learning_rate': 9.136245113203739e-05, 'epoch': 0.19}
 19%|        | 1231/6500 [2:20:08<10:40:17,  7.29s/it]                                                         19%|        | 1231/6500 [2:20:08<10:40:17,  7.29s/it] 19%|        | 1232/6500 [2:20:14<10:21:17,  7.08s/it]                                                         19%|        | 1232/6500 [2:20:14<10:21:17,  7.08s/it] 19%|        | 1233/6500 [2:20:21<10:07:30,  6.92s/it]                                                         19%|        | 1233/6500 [2:20:21<10:07:30,  6.92s/it] 19%|        | 1234/6500 [2:20:27<9:58:11,  6.82s/it]                                                         19%|        | 1234/6500 [2:20:27<9:58:11,  6.82s/it] 19%|        | 1235/6500 [2:20:34<9:51:22,  6.74s/it]                                                        19%|        | 1235/6500 [2:20:34<9:51:22,  6.74s/it] 19%|        | 1236/6500 [2:20:40<9:46:30,  6.69s/it]                                                        19%|{'loss': 0.6594, 'learning_rate': 9.134886474974091e-05, 'epoch': 0.19}
{'loss': 0.6319, 'learning_rate': 9.133526870239873e-05, 'epoch': 0.19}
{'loss': 0.6336, 'learning_rate': 9.132166299318878e-05, 'epoch': 0.19}
{'loss': 0.6241, 'learning_rate': 9.130804762529137e-05, 'epoch': 0.19}
        | 1236/6500 [2:20:40<9:46:30,  6.69s/it] 19%|        | 1237/6500 [2:20:47<9:43:15,  6.65s/it]                                                        19%|        | 1237/6500 [2:20:47<9:43:15,  6.65s/it] 19%|        | 1238/6500 [2:20:54<9:40:58,  6.62s/it]                                                        19%|        | 1238/6500 [2:20:54<9:40:58,  6.62s/it] 19%|        | 1239/6500 [2:21:00<9:39:30,  6.61s/it]                                                        19%|        | 1239/6500 [2:21:00<9:39:30,  6.61s/it] 19%|        | 1240/6500 [2:21:07<9:38:15,  6.60s/it]                                                        19%|        | 1240/6500 [2:21:07<9:38:15,  6.60s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8558815121650696, 'eval_runtime': 1.4833, 'eval_samples_per_second': 8.09, 'eval_steps_per_second': 2.023, 'epoch': 0.19}
                                                        19%|        | 1240/6500 [2:21:08<9:38:15,  6.60s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-1240
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1240
/projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1240
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1240
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1240
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1240/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1240/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1240/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1240/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.6296, 'learning_rate': 9.129442260188899e-05, 'epoch': 0.19}
{'loss': 0.6373, 'learning_rate': 9.128078792616638e-05, 'epoch': 0.19}
{'loss': 0.6602, 'learning_rate': 9.126714360131059e-05, 'epoch': 0.19}
{'loss': 0.6545, 'learning_rate': 9.12534896305109e-05, 'epoch': 0.19}
{'loss': 0.6401, 'learning_rate': 9.123982601695882e-05, 'epoch': 0.19}
{'loss': 0.6395, 'learning_rate': 9.122615276384816e-05, 'epoch': 0.19}
 19%|        | 1241/6500 [2:21:15<10:25:33,  7.14s/it]                                                         19%|        | 1241/6500 [2:21:15<10:25:33,  7.14s/it] 19%|        | 1242/6500 [2:21:22<10:09:49,  6.96s/it]                                                         19%|        | 1242/6500 [2:21:22<10:09:49,  6.96s/it] 19%|        | 1243/6500 [2:21:28<9:59:08,  6.84s/it]                                                         19%|        | 1243/6500 [2:21:28<9:59:08,  6.84s/it] 19%|        | 1244/6500 [2:21:35<9:51:25,  6.75s/it]                                                        19%|        | 1244/6500 [2:21:35<9:51:25,  6.75s/it] 19%|        | 1245/6500 [2:21:42<10:09:21,  6.96s/it]                                                         19%|        | 1245/6500 [2:21:42<10:09:21,  6.96s/it] 19%|        | 1246/6500 [2:21:49<9:58:52,  6.84s/it]                                                         19%|{'loss': 0.6348, 'learning_rate': 9.121246987437496e-05, 'epoch': 0.19}
{'loss': 0.6412, 'learning_rate': 9.119877735173748e-05, 'epoch': 0.19}
{'loss': 0.662, 'learning_rate': 9.118507519913631e-05, 'epoch': 0.19}
{'loss': 0.6357, 'learning_rate': 9.11713634197742e-05, 'epoch': 0.19}
        | 1246/6500 [2:21:49<9:58:52,  6.84s/it] 19%|        | 1247/6500 [2:21:55<9:51:50,  6.76s/it]                                                        19%|        | 1247/6500 [2:21:55<9:51:50,  6.76s/it] 19%|        | 1248/6500 [2:22:02<9:46:55,  6.71s/it]                                                        19%|        | 1248/6500 [2:22:02<9:46:55,  6.71s/it] 19%|        | 1249/6500 [2:22:09<9:43:28,  6.67s/it]                                                        19%|        | 1249/6500 [2:22:09<9:43:28,  6.67s/it] 19%|        | 1250/6500 [2:22:15<9:41:00,  6.64s/it]                                                        19%|        | 1250/6500 [2:22:15<9:41:00,  6.64s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8502087593078613, 'eval_runtime': 1.4887, 'eval_samples_per_second': 8.061, 'eval_steps_per_second': 2.015, 'epoch': 0.19}
                                                        19%|        | 1250/6500 [2:22:17<9:41:00,  6.64s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-1250
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1250
/projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1250
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1250
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1250
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1250/pytorch_model.bin
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1250/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1250/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1250/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.9168, 'learning_rate': 9.115764201685623e-05, 'epoch': 0.19}
{'loss': 0.6385, 'learning_rate': 9.114391099358968e-05, 'epoch': 0.19}
{'loss': 0.6456, 'learning_rate': 9.113017035318409e-05, 'epoch': 0.19}
{'loss': 0.6236, 'learning_rate': 9.111642009885127e-05, 'epoch': 0.19}
{'loss': 0.6216, 'learning_rate': 9.110266023380523e-05, 'epoch': 0.19}
{'loss': 0.6247, 'learning_rate': 9.108889076126226e-05, 'epoch': 0.19}
 19%|        | 1251/6500 [2:22:24<10:28:03,  7.18s/it]                                                         19%|        | 1251/6500 [2:22:24<10:28:03,  7.18s/it] 19%|        | 1252/6500 [2:22:30<10:11:57,  7.00s/it]                                                         19%|        | 1252/6500 [2:22:30<10:11:57,  7.00s/it] 19%|        | 1253/6500 [2:22:37<10:00:16,  6.86s/it]                                                         19%|        | 1253/6500 [2:22:37<10:00:16,  6.86s/it] 19%|        | 1254/6500 [2:22:43<9:52:45,  6.78s/it]                                                         19%|        | 1254/6500 [2:22:43<9:52:45,  6.78s/it] 19%|        | 1255/6500 [2:22:50<9:47:01,  6.72s/it]                                                        19%|        | 1255/6500 [2:22:50<9:47:01,  6.72s/it] 19%|        | 1256/6500 [2:22:56<9:42:48,  6.67s/it]                                                        19%|{'loss': 0.6467, 'learning_rate': 9.107511168444092e-05, 'epoch': 0.19}
{'loss': 0.6605, 'learning_rate': 9.106132300656196e-05, 'epoch': 0.19}
{'loss': 0.6455, 'learning_rate': 9.104752473084838e-05, 'epoch': 0.19}
{'loss': 0.6256, 'learning_rate': 9.103371686052548e-05, 'epoch': 0.19}
        | 1256/6500 [2:22:56<9:42:48,  6.67s/it] 19%|        | 1257/6500 [2:23:03<9:40:03,  6.64s/it]                                                        19%|        | 1257/6500 [2:23:03<9:40:03,  6.64s/it] 19%|        | 1258/6500 [2:23:10<9:38:10,  6.62s/it]                                                        19%|        | 1258/6500 [2:23:10<9:38:10,  6.62s/it] 19%|        | 1259/6500 [2:23:16<9:36:55,  6.60s/it]                                                        19%|        | 1259/6500 [2:23:16<9:36:55,  6.60s/it] 19%|        | 1260/6500 [2:23:23<9:35:35,  6.59s/it]                                                        19%|        | 1260/6500 [2:23:23<9:35:35,  6.59s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8495685458183289, 'eval_runtime': 1.4852, 'eval_samples_per_second': 8.08, 'eval_steps_per_second': 2.02, 'epoch': 0.19}
                                                        19%|        | 1260/6500 [2:23:24<9:35:35,  6.59s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-1260
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1260
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1260
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1260
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1260
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1260/pytorch_model.bin
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1260/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1260/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1260/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.654, 'learning_rate': 9.101989939882076e-05, 'epoch': 0.19}
{'loss': 0.6233, 'learning_rate': 9.100607234896397e-05, 'epoch': 0.19}
{'loss': 0.6435, 'learning_rate': 9.099223571418707e-05, 'epoch': 0.19}
{'loss': 0.6497, 'learning_rate': 9.097838949772432e-05, 'epoch': 0.19}
{'loss': 0.6362, 'learning_rate': 9.096453370281219e-05, 'epoch': 0.19}
{'loss': 0.9176, 'learning_rate': 9.095066833268935e-05, 'epoch': 0.19}
 19%|        | 1261/6500 [2:23:32<10:49:25,  7.44s/it]                                                         19%|        | 1261/6500 [2:23:32<10:49:25,  7.44s/it] 19%|        | 1262/6500 [2:23:39<10:26:26,  7.18s/it]                                                         19%|        | 1262/6500 [2:23:39<10:26:26,  7.18s/it] 19%|        | 1263/6500 [2:23:45<10:09:52,  6.99s/it]                                                         19%|        | 1263/6500 [2:23:45<10:09:52,  6.99s/it] 19%|        | 1264/6500 [2:23:52<9:58:33,  6.86s/it]                                                         19%|        | 1264/6500 [2:23:52<9:58:33,  6.86s/it] 19%|        | 1265/6500 [2:23:58<9:50:06,  6.76s/it]                                                        19%|        | 1265/6500 [2:23:58<9:50:06,  6.76s/it] 19%|        | 1266/6500 [2:24:05<9:44:12,  6.70s/it]                                                        19%|{'loss': 0.6193, 'learning_rate': 9.093679339059678e-05, 'epoch': 0.19}
{'loss': 0.6577, 'learning_rate': 9.092290887977765e-05, 'epoch': 0.2}
{'loss': 0.6136, 'learning_rate': 9.090901480347739e-05, 'epoch': 0.2}
{'loss': 0.6129, 'learning_rate': 9.089511116494367e-05, 'epoch': 0.2}
        | 1266/6500 [2:24:05<9:44:12,  6.70s/it] 19%|        | 1267/6500 [2:24:11<9:39:48,  6.65s/it]                                                        19%|        | 1267/6500 [2:24:11<9:39:48,  6.65s/it] 20%|        | 1268/6500 [2:24:18<9:37:14,  6.62s/it]                                                        20%|        | 1268/6500 [2:24:18<9:37:14,  6.62s/it] 20%|        | 1269/6500 [2:24:24<9:35:28,  6.60s/it]                                                        20%|        | 1269/6500 [2:24:24<9:35:28,  6.60s/it] 20%|        | 1270/6500 [2:24:31<9:34:03,  6.59s/it]                                                        20%|        | 1270/6500 [2:24:31<9:34:03,  6.59s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.853855550289154, 'eval_runtime': 1.4831, 'eval_samples_per_second': 8.091, 'eval_steps_per_second': 2.023, 'epoch': 0.2}
                                                        20%|        | 1270/6500 [2:24:33<9:34:03,  6.59s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-1270
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1270I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1270

I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1270
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1270
the pytorch model path isthe pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1270/pytorch_model.bin 
/projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1270/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1270/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1270/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.6279, 'learning_rate': 9.088119796742633e-05, 'epoch': 0.2}
{'loss': 0.6325, 'learning_rate': 9.086727521417755e-05, 'epoch': 0.2}
{'loss': 0.6527, 'learning_rate': 9.085334290845164e-05, 'epoch': 0.2}
{'loss': 0.652, 'learning_rate': 9.083940105350524e-05, 'epoch': 0.2}
{'loss': 0.6217, 'learning_rate': 9.082544965259715e-05, 'epoch': 0.2}
{'loss': 0.6357, 'learning_rate': 9.081148870898842e-05, 'epoch': 0.2}
 20%|        | 1271/6500 [2:24:39<10:22:03,  7.14s/it]                                                         20%|        | 1271/6500 [2:24:39<10:22:03,  7.14s/it] 20%|        | 1272/6500 [2:24:46<10:06:49,  6.96s/it]                                                         20%|        | 1272/6500 [2:24:46<10:06:49,  6.96s/it] 20%|        | 1273/6500 [2:24:53<9:55:37,  6.84s/it]                                                         20%|        | 1273/6500 [2:24:53<9:55:37,  6.84s/it] 20%|        | 1274/6500 [2:24:59<9:48:09,  6.75s/it]                                                        20%|        | 1274/6500 [2:24:59<9:48:09,  6.75s/it] 20%|        | 1275/6500 [2:25:06<9:42:37,  6.69s/it]                                                        20%|        | 1275/6500 [2:25:06<9:42:37,  6.69s/it] 20%|        | 1276/6500 [2:25:12<9:39:19,  6.65s/it]                                                        20%|   {'loss': 0.6369, 'learning_rate': 9.079751822594235e-05, 'epoch': 0.2}
{'loss': 0.6689, 'learning_rate': 9.078353820672443e-05, 'epoch': 0.2}
{'loss': 0.618, 'learning_rate': 9.076954865460243e-05, 'epoch': 0.2}
{'loss': 0.9017, 'learning_rate': 9.075554957284633e-05, 'epoch': 0.2}
     | 1276/6500 [2:25:12<9:39:19,  6.65s/it] 20%|        | 1277/6500 [2:25:19<9:52:56,  6.81s/it]                                                        20%|        | 1277/6500 [2:25:19<9:52:56,  6.81s/it] 20%|        | 1278/6500 [2:25:26<9:46:01,  6.73s/it]                                                        20%|        | 1278/6500 [2:25:26<9:46:01,  6.73s/it] 20%|        | 1279/6500 [2:25:33<9:41:32,  6.68s/it]                                                        20%|        | 1279/6500 [2:25:33<9:41:32,  6.68s/it] 20%|        | 1280/6500 [2:25:39<9:38:06,  6.64s/it]                                                        20%|        | 1280/6500 [2:25:39<9:38:06,  6.64s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8481313586235046, 'eval_runtime': 1.5085, 'eval_samples_per_second': 7.955, 'eval_steps_per_second': 1.989, 'epoch': 0.2}
                                                        20%|        | 1280/6500 [2:25:41<9:38:06,  6.64s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-1280
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1280I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1280

I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1280
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1280
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1280/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1280/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1280/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1280/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.6649, 'learning_rate': 9.07415409647283e-05, 'epoch': 0.2}
{'loss': 0.6097, 'learning_rate': 9.072752283352279e-05, 'epoch': 0.2}
{'loss': 0.6527, 'learning_rate': 9.071349518250643e-05, 'epoch': 0.2}
{'loss': 0.6068, 'learning_rate': 9.069945801495813e-05, 'epoch': 0.2}
{'loss': 0.6196, 'learning_rate': 9.068541133415897e-05, 'epoch': 0.2}
{'loss': 0.628, 'learning_rate': 9.067135514339229e-05, 'epoch': 0.2}
 20%|        | 1281/6500 [2:25:48<10:26:04,  7.20s/it]                                                         20%|        | 1281/6500 [2:25:48<10:26:04,  7.20s/it] 20%|        | 1282/6500 [2:25:54<10:08:52,  7.00s/it]                                                         20%|        | 1282/6500 [2:25:54<10:08:52,  7.00s/it] 20%|        | 1283/6500 [2:26:01<9:57:11,  6.87s/it]                                                         20%|        | 1283/6500 [2:26:01<9:57:11,  6.87s/it] 20%|        | 1284/6500 [2:26:07<9:50:15,  6.79s/it]                                                        20%|        | 1284/6500 [2:26:07<9:50:15,  6.79s/it] 20%|        | 1285/6500 [2:26:14<9:44:14,  6.72s/it]                                                        20%|        | 1285/6500 [2:26:14<9:44:14,  6.72s/it] 20%|        | 1286/6500 [2:26:20<9:39:58,  6.67s/it]                                                        20%|   {'loss': 0.6415, 'learning_rate': 9.065728944594362e-05, 'epoch': 0.2}
{'loss': 0.6547, 'learning_rate': 9.064321424510074e-05, 'epoch': 0.2}
{'loss': 0.6442, 'learning_rate': 9.062912954415366e-05, 'epoch': 0.2}
{'loss': 0.615, 'learning_rate': 9.061503534639457e-05, 'epoch': 0.2}
     | 1286/6500 [2:26:20<9:39:58,  6.67s/it] 20%|        | 1287/6500 [2:26:27<9:37:12,  6.64s/it]                                                        20%|        | 1287/6500 [2:26:27<9:37:12,  6.64s/it] 20%|        | 1288/6500 [2:26:34<9:35:20,  6.62s/it]                                                        20%|        | 1288/6500 [2:26:34<9:35:20,  6.62s/it] 20%|        | 1289/6500 [2:26:40<9:34:18,  6.61s/it]                                                        20%|        | 1289/6500 [2:26:40<9:34:18,  6.61s/it] 20%|        | 1290/6500 [2:26:47<9:33:08,  6.60s/it]                                                        20%|        | 1290/6500 [2:26:47<9:33:08,  6.60s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8525456190109253, 'eval_runtime': 1.737, 'eval_samples_per_second': 6.908, 'eval_steps_per_second': 1.727, 'epoch': 0.2}
                                                        20%|        | 1290/6500 [2:26:48<9:33:08,  6.60s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-1290
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1290I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1290

I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1290
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1290
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1290/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1290/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1290/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1290/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.641, 'learning_rate': 9.060093165511789e-05, 'epoch': 0.2}
{'loss': 0.6198, 'learning_rate': 9.058681847362032e-05, 'epoch': 0.2}
{'loss': 0.6642, 'learning_rate': 9.05726958052007e-05, 'epoch': 0.2}
{'loss': 0.6201, 'learning_rate': 9.055856365316011e-05, 'epoch': 0.2}
{'loss': 0.9031, 'learning_rate': 9.054442202080188e-05, 'epoch': 0.2}
{'loss': 0.6485, 'learning_rate': 9.053027091143151e-05, 'epoch': 0.2}
 20%|        | 1291/6500 [2:26:55<10:26:36,  7.22s/it]                                                         20%|        | 1291/6500 [2:26:55<10:26:36,  7.22s/it] 20%|        | 1292/6500 [2:27:02<10:09:34,  7.02s/it]                                                         20%|        | 1292/6500 [2:27:02<10:09:34,  7.02s/it] 20%|        | 1293/6500 [2:27:09<10:13:34,  7.07s/it]                                                         20%|        | 1293/6500 [2:27:09<10:13:34,  7.07s/it] 20%|        | 1294/6500 [2:27:16<10:00:14,  6.92s/it]                                                         20%|        | 1294/6500 [2:27:16<10:00:14,  6.92s/it] 20%|        | 1295/6500 [2:27:22<9:50:26,  6.81s/it]                                                         20%|        | 1295/6500 [2:27:22<9:50:26,  6.81s/it] 20%|        | 1296/6500 [2:27:29<9:43:38,  6.73s/it]                                                        20%|{'loss': 0.6049, 'learning_rate': 9.051611032835675e-05, 'epoch': 0.2}
{'loss': 0.6393, 'learning_rate': 9.050194027488754e-05, 'epoch': 0.2}
{'loss': 0.6071, 'learning_rate': 9.048776075433604e-05, 'epoch': 0.2}
{'loss': 0.6104, 'learning_rate': 9.047357177001663e-05, 'epoch': 0.2}
        | 1296/6500 [2:27:29<9:43:38,  6.73s/it] 20%|        | 1297/6500 [2:27:35<9:38:57,  6.68s/it]                                                        20%|        | 1297/6500 [2:27:35<9:38:57,  6.68s/it] 20%|        | 1298/6500 [2:27:42<9:35:59,  6.64s/it]                                                        20%|        | 1298/6500 [2:27:42<9:35:59,  6.64s/it] 20%|        | 1299/6500 [2:27:48<9:33:38,  6.62s/it]                                                        20%|        | 1299/6500 [2:27:48<9:33:38,  6.62s/it] 20%|        | 1300/6500 [2:27:55<9:31:41,  6.60s/it]                                                        20%|        | 1300/6500 [2:27:55<9:31:41,  6.60s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8540288805961609, 'eval_runtime': 1.5018, 'eval_samples_per_second': 7.99, 'eval_steps_per_second': 1.998, 'epoch': 0.2}
                                                        20%|        | 1300/6500 [2:27:56<9:31:41,  6.60s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-1300
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1300I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1300

I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1300
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1300
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1300/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1300/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1300/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1300/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.6363, 'learning_rate': 9.045937332524592e-05, 'epoch': 0.2}
{'loss': 0.6294, 'learning_rate': 9.044516542334267e-05, 'epoch': 0.2}
{'loss': 0.6349, 'learning_rate': 9.043094806762793e-05, 'epoch': 0.2}
{'loss': 0.6381, 'learning_rate': 9.04167212614249e-05, 'epoch': 0.2}
{'loss': 0.6166, 'learning_rate': 9.0402485008059e-05, 'epoch': 0.2}
{'loss': 0.6292, 'learning_rate': 9.038823931085789e-05, 'epoch': 0.2}
 20%|        | 1301/6500 [2:28:03<10:16:58,  7.12s/it]                                                         20%|        | 1301/6500 [2:28:03<10:16:58,  7.12s/it] 20%|        | 1302/6500 [2:28:10<10:03:29,  6.97s/it]                                                         20%|        | 1302/6500 [2:28:10<10:03:29,  6.97s/it] 20%|        | 1303/6500 [2:28:17<9:53:00,  6.85s/it]                                                         20%|        | 1303/6500 [2:28:17<9:53:00,  6.85s/it] 20%|        | 1304/6500 [2:28:23<9:45:30,  6.76s/it]                                                        20%|        | 1304/6500 [2:28:23<9:45:30,  6.76s/it] 20%|        | 1305/6500 [2:28:30<9:39:59,  6.70s/it]                                                        20%|        | 1305/6500 [2:28:30<9:39:59,  6.70s/it] 20%|        | 1306/6500 [2:28:36<9:36:16,  6.66s/it]                                                        20%|   {'loss': 0.6209, 'learning_rate': 9.037398417315142e-05, 'epoch': 0.2}
{'loss': 0.6584, 'learning_rate': 9.03597195982716e-05, 'epoch': 0.2}
{'loss': 0.6252, 'learning_rate': 9.034544558955274e-05, 'epoch': 0.2}
{'loss': 0.9012, 'learning_rate': 9.033116215033126e-05, 'epoch': 0.2}
     | 1306/6500 [2:28:36<9:36:16,  6.66s/it] 20%|        | 1307/6500 [2:28:43<9:33:26,  6.63s/it]                                                        20%|        | 1307/6500 [2:28:43<9:33:26,  6.63s/it] 20%|        | 1308/6500 [2:28:49<9:31:26,  6.60s/it]                                                        20%|        | 1308/6500 [2:28:49<9:31:26,  6.60s/it] 20%|        | 1309/6500 [2:28:57<9:53:30,  6.86s/it]                                                        20%|        | 1309/6500 [2:28:57<9:53:30,  6.86s/it] 20%|        | 1310/6500 [2:29:03<9:45:21,  6.77s/it]                                                        20%|        | 1310/6500 [2:29:03<9:45:21,  6.77s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8492521047592163, 'eval_runtime': 1.4929, 'eval_samples_per_second': 8.038, 'eval_steps_per_second': 2.01, 'epoch': 0.2}
                                                        20%|        | 1310/6500 [2:29:05<9:45:21,  6.77s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-1310
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1310I AM HERE AND I AM SAVING THE MODEL

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1310
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1310
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1310
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1310/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1310/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1310/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1310/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.6383, 'learning_rate': 9.031686928394584e-05, 'epoch': 0.2}
{'loss': 0.6185, 'learning_rate': 9.030256699373738e-05, 'epoch': 0.2}
{'loss': 0.6049, 'learning_rate': 9.028825528304892e-05, 'epoch': 0.2}
{'loss': 0.6062, 'learning_rate': 9.027393415522574e-05, 'epoch': 0.2}
{'loss': 0.6107, 'learning_rate': 9.025960361361531e-05, 'epoch': 0.2}
{'loss': 0.6245, 'learning_rate': 9.024526366156732e-05, 'epoch': 0.2}
 20%|        | 1311/6500 [2:29:12<10:30:27,  7.29s/it]                                                         20%|        | 1311/6500 [2:29:12<10:30:27,  7.29s/it] 20%|        | 1312/6500 [2:29:18<10:11:10,  7.07s/it]                                                         20%|        | 1312/6500 [2:29:18<10:11:10,  7.07s/it] 20%|        | 1313/6500 [2:29:25<9:57:45,  6.91s/it]                                                         20%|        | 1313/6500 [2:29:25<9:57:45,  6.91s/it] 20%|        | 1314/6500 [2:29:31<9:48:34,  6.81s/it]                                                        20%|        | 1314/6500 [2:29:31<9:48:34,  6.81s/it] 20%|        | 1315/6500 [2:29:38<9:41:49,  6.73s/it]                                                        20%|        | 1315/6500 [2:29:38<9:41:49,  6.73s/it] 20%|        | 1316/6500 [2:29:45<9:36:53,  6.68s/it]                                                        20%|   {'loss': 0.6405, 'learning_rate': 9.023091430243367e-05, 'epoch': 0.2}
{'loss': 0.6369, 'learning_rate': 9.021655553956839e-05, 'epoch': 0.2}
{'loss': 0.6264, 'learning_rate': 9.020218737632778e-05, 'epoch': 0.2}
{'loss': 0.6208, 'learning_rate': 9.018780981607029e-05, 'epoch': 0.2}
     | 1316/6500 [2:29:45<9:36:53,  6.68s/it] 20%|        | 1317/6500 [2:29:51<9:33:33,  6.64s/it]                                                        20%|        | 1317/6500 [2:29:51<9:33:33,  6.64s/it] 20%|        | 1318/6500 [2:29:58<9:31:06,  6.61s/it]                                                        20%|        | 1318/6500 [2:29:58<9:31:06,  6.61s/it] 20%|        | 1319/6500 [2:30:04<9:29:51,  6.60s/it]                                                        20%|        | 1319/6500 [2:30:04<9:29:51,  6.60s/it] 20%|        | 1320/6500 [2:30:11<9:28:21,  6.58s/it]                                                        20%|        | 1320/6500 [2:30:11<9:28:21,  6.58s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8519079685211182, 'eval_runtime': 1.4844, 'eval_samples_per_second': 8.084, 'eval_steps_per_second': 2.021, 'epoch': 0.2}
                                                        20%|        | 1320/6500 [2:30:12<9:28:21,  6.58s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-1320
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1320
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1320
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1320
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1320
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1320/pytorch_model.bin
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1320/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1320/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1320/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.6225, 'learning_rate': 9.01734228621566e-05, 'epoch': 0.2}
{'loss': 0.6169, 'learning_rate': 9.01590265179496e-05, 'epoch': 0.2}
{'loss': 0.6462, 'learning_rate': 9.014462078681431e-05, 'epoch': 0.2}
{'loss': 0.625, 'learning_rate': 9.013020567211799e-05, 'epoch': 0.2}
{'loss': 0.903, 'learning_rate': 9.01157811772301e-05, 'epoch': 0.2}
{'loss': 0.6254, 'learning_rate': 9.010134730552224e-05, 'epoch': 0.2}
 20%|        | 1321/6500 [2:30:19<10:16:32,  7.14s/it]                                                         20%|        | 1321/6500 [2:30:19<10:16:32,  7.14s/it] 20%|        | 1322/6500 [2:30:26<10:01:19,  6.97s/it]                                                         20%|        | 1322/6500 [2:30:26<10:01:19,  6.97s/it] 20%|        | 1323/6500 [2:30:32<9:50:24,  6.84s/it]                                                         20%|        | 1323/6500 [2:30:32<9:50:24,  6.84s/it] 20%|        | 1324/6500 [2:30:39<9:42:44,  6.76s/it]                                                        20%|        | 1324/6500 [2:30:39<9:42:44,  6.76s/it] 20%|        | 1325/6500 [2:30:45<9:37:10,  6.69s/it]                                                        20%|        | 1325/6500 [2:30:45<9:37:10,  6.69s/it] 20%|        | 1326/6500 [2:30:53<9:57:02,  6.92s/it]                                                        20%|   {'loss': 0.6227, 'learning_rate': 9.008690406036829e-05, 'epoch': 0.2}
{'loss': 0.6127, 'learning_rate': 9.007245144514425e-05, 'epoch': 0.2}
{'loss': 0.6071, 'learning_rate': 9.005798946322832e-05, 'epoch': 0.2}
{'loss': 0.6044, 'learning_rate': 9.004351811800091e-05, 'epoch': 0.2}
     | 1326/6500 [2:30:53<9:57:02,  6.92s/it] 20%|        | 1327/6500 [2:30:59<9:47:30,  6.81s/it]                                                        20%|        | 1327/6500 [2:30:59<9:47:30,  6.81s/it] 20%|        | 1328/6500 [2:31:06<9:41:44,  6.75s/it]                                                        20%|        | 1328/6500 [2:31:06<9:41:44,  6.75s/it] 20%|        | 1329/6500 [2:31:13<9:38:09,  6.71s/it]                                                        20%|        | 1329/6500 [2:31:13<9:38:09,  6.71s/it] 20%|        | 1330/6500 [2:31:19<9:33:53,  6.66s/it]                                                        20%|        | 1330/6500 [2:31:19<9:33:53,  6.66s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8482038378715515, 'eval_runtime': 1.4875, 'eval_samples_per_second': 8.067, 'eval_steps_per_second': 2.017, 'epoch': 0.2}
                                                        20%|        | 1330/6500 [2:31:21<9:33:53,  6.66s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-1330
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1330
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1330
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1330
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1330
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1330/pytorch_model.bin
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1330/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1330/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1330/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.6369, 'learning_rate': 9.002903741284463e-05, 'epoch': 0.2}
{'loss': 0.642, 'learning_rate': 9.001454735114421e-05, 'epoch': 0.2}
{'loss': 0.6276, 'learning_rate': 9.000004793628665e-05, 'epoch': 0.21}
{'loss': 0.6184, 'learning_rate': 8.998553917166108e-05, 'epoch': 0.21}
{'loss': 0.626, 'learning_rate': 8.997102106065884e-05, 'epoch': 0.21}
{'loss': 0.6126, 'learning_rate': 8.995649360667348e-05, 'epoch': 0.21}
 20%|        | 1331/6500 [2:31:28<10:17:15,  7.16s/it]                                                         20%|        | 1331/6500 [2:31:28<10:17:15,  7.16s/it] 20%|        | 1332/6500 [2:31:34<10:01:41,  6.99s/it]                                                         20%|        | 1332/6500 [2:31:34<10:01:41,  6.99s/it] 21%|        | 1333/6500 [2:31:41<9:50:32,  6.86s/it]                                                         21%|        | 1333/6500 [2:31:41<9:50:32,  6.86s/it] 21%|        | 1334/6500 [2:31:47<9:42:26,  6.76s/it]                                                        21%|        | 1334/6500 [2:31:47<9:42:26,  6.76s/it] 21%|        | 1335/6500 [2:31:54<9:36:45,  6.70s/it]                                                        21%|        | 1335/6500 [2:31:54<9:36:45,  6.70s/it] 21%|        | 1336/6500 [2:32:00<9:32:55,  6.66s/it]                                                        21%|   {'loss': 0.6196, 'learning_rate': 8.994195681310067e-05, 'epoch': 0.21}
{'loss': 0.6463, 'learning_rate': 8.99274106833383e-05, 'epoch': 0.21}
{'loss': 0.6189, 'learning_rate': 8.991285522078644e-05, 'epoch': 0.21}
{'loss': 0.9024, 'learning_rate': 8.989829042884735e-05, 'epoch': 0.21}
     | 1336/6500 [2:32:00<9:32:55,  6.66s/it] 21%|        | 1337/6500 [2:32:07<9:31:35,  6.64s/it]                                                        21%|        | 1337/6500 [2:32:07<9:31:35,  6.64s/it] 21%|        | 1338/6500 [2:32:14<9:29:31,  6.62s/it]                                                        21%|        | 1338/6500 [2:32:14<9:29:31,  6.62s/it] 21%|        | 1339/6500 [2:32:20<9:28:08,  6.61s/it]                                                        21%|        | 1339/6500 [2:32:20<9:28:08,  6.61s/it] 21%|        | 1340/6500 [2:32:27<9:26:47,  6.59s/it]                                                        21%|        | 1340/6500 [2:32:27<9:26:47,  6.59s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8457147479057312, 'eval_runtime': 1.49, 'eval_samples_per_second': 8.054, 'eval_steps_per_second': 2.013, 'epoch': 0.21}
                                                        21%|        | 1340/6500 [2:32:28<9:26:47,  6.59s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-1340
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1340I AM HERE AND I AM SAVING THE MODEL

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1340
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1340
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1340
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1340/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1340/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1340/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1340/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.6031, 'learning_rate': 8.988371631092547e-05, 'epoch': 0.21}
{'loss': 0.6412, 'learning_rate': 8.986913287042739e-05, 'epoch': 0.21}
{'loss': 0.595, 'learning_rate': 8.985454011076191e-05, 'epoch': 0.21}
{'loss': 0.5955, 'learning_rate': 8.983993803533999e-05, 'epoch': 0.21}
{'loss': 0.6022, 'learning_rate': 8.98253266475748e-05, 'epoch': 0.21}
{'loss': 0.6193, 'learning_rate': 8.981070595088164e-05, 'epoch': 0.21}
 21%|        | 1341/6500 [2:32:35<10:13:53,  7.14s/it]                                                         21%|        | 1341/6500 [2:32:35<10:13:53,  7.14s/it] 21%|        | 1342/6500 [2:32:43<10:21:44,  7.23s/it]                                                         21%|        | 1342/6500 [2:32:43<10:21:44,  7.23s/it] 21%|        | 1343/6500 [2:32:49<10:04:40,  7.04s/it]                                                         21%|        | 1343/6500 [2:32:49<10:04:40,  7.04s/it] 21%|        | 1344/6500 [2:32:56<9:52:15,  6.89s/it]                                                         21%|        | 1344/6500 [2:32:56<9:52:15,  6.89s/it] 21%|        | 1345/6500 [2:33:02<9:43:45,  6.79s/it]                                                        21%|        | 1345/6500 [2:33:02<9:43:45,  6.79s/it] 21%|        | 1346/6500 [2:33:09<9:37:32,  6.72s/it]                                                        21%|{'loss': 0.6403, 'learning_rate': 8.979607594867802e-05, 'epoch': 0.21}
{'loss': 0.6309, 'learning_rate': 8.978143664438361e-05, 'epoch': 0.21}
{'loss': 0.611, 'learning_rate': 8.976678804142025e-05, 'epoch': 0.21}
{'loss': 0.6205, 'learning_rate': 8.975213014321198e-05, 'epoch': 0.21}
        | 1346/6500 [2:33:09<9:37:32,  6.72s/it] 21%|        | 1347/6500 [2:33:15<9:33:16,  6.68s/it]                                                        21%|        | 1347/6500 [2:33:15<9:33:16,  6.68s/it] 21%|        | 1348/6500 [2:33:22<9:30:14,  6.64s/it]                                                        21%|        | 1348/6500 [2:33:22<9:30:14,  6.64s/it] 21%|        | 1349/6500 [2:33:28<9:27:49,  6.61s/it]                                                        21%|        | 1349/6500 [2:33:28<9:27:49,  6.61s/it] 21%|        | 1350/6500 [2:33:35<9:26:27,  6.60s/it]                                                        21%|        | 1350/6500 [2:33:35<9:26:27,  6.60s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8472617864608765, 'eval_runtime': 1.4894, 'eval_samples_per_second': 8.057, 'eval_steps_per_second': 2.014, 'epoch': 0.21}
                                                        21%|        | 1350/6500 [2:33:37<9:26:27,  6.60s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-1350
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1350 /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1350

I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1350
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1350
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1350/pytorch_model.bin
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1350/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1350/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1350/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.6096, 'learning_rate': 8.9737462953185e-05, 'epoch': 0.21}
{'loss': 0.6317, 'learning_rate': 8.972278647476764e-05, 'epoch': 0.21}
{'loss': 0.6183, 'learning_rate': 8.970810071139047e-05, 'epoch': 0.21}
{'loss': 0.7249, 'learning_rate': 8.969340566648619e-05, 'epoch': 0.21}
{'loss': 0.8125, 'learning_rate': 8.967870134348966e-05, 'epoch': 0.21}
{'loss': 0.5993, 'learning_rate': 8.966398774583795e-05, 'epoch': 0.21}
 21%|        | 1351/6500 [2:33:43<10:10:53,  7.12s/it]                                                         21%|        | 1351/6500 [2:33:43<10:10:53,  7.12s/it] 21%|        | 1352/6500 [2:33:50<9:56:27,  6.95s/it]                                                         21%|        | 1352/6500 [2:33:50<9:56:27,  6.95s/it] 21%|        | 1353/6500 [2:33:56<9:46:02,  6.83s/it]                                                        21%|        | 1353/6500 [2:33:56<9:46:02,  6.83s/it] 21%|        | 1354/6500 [2:34:03<9:38:47,  6.75s/it]                                                        21%|        | 1354/6500 [2:34:03<9:38:47,  6.75s/it] 21%|        | 1355/6500 [2:34:10<9:33:37,  6.69s/it]                                                        21%|        | 1355/6500 [2:34:10<9:33:37,  6.69s/it] 21%|        | 1356/6500 [2:34:16<9:30:22,  6.65s/it]                                                        21%|      {'loss': 0.6363, 'learning_rate': 8.964926487697027e-05, 'epoch': 0.21}
{'loss': 0.5915, 'learning_rate': 8.9634532740328e-05, 'epoch': 0.21}
{'loss': 0.6001, 'learning_rate': 8.961979133935468e-05, 'epoch': 0.21}
{'loss': 0.6133, 'learning_rate': 8.960504067749602e-05, 'epoch': 0.21}
  | 1356/6500 [2:34:16<9:30:22,  6.65s/it] 21%|        | 1357/6500 [2:34:23<9:27:41,  6.62s/it]                                                        21%|        | 1357/6500 [2:34:23<9:27:41,  6.62s/it] 21%|        | 1358/6500 [2:34:30<9:41:34,  6.79s/it]                                                        21%|        | 1358/6500 [2:34:30<9:41:34,  6.79s/it] 21%|        | 1359/6500 [2:34:36<9:35:42,  6.72s/it]                                                        21%|        | 1359/6500 [2:34:36<9:35:42,  6.72s/it] 21%|        | 1360/6500 [2:34:43<9:31:32,  6.67s/it]                                                        21%|        | 1360/6500 [2:34:43<9:31:32,  6.67s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8455912470817566, 'eval_runtime': 1.5059, 'eval_samples_per_second': 7.968, 'eval_steps_per_second': 1.992, 'epoch': 0.21}
                                                        21%|        | 1360/6500 [2:34:44<9:31:32,  6.67s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-1360
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1360
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1360
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1360
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1360
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1360/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1360/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1360/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1360/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.61, 'learning_rate': 8.959028075819992e-05, 'epoch': 0.21}
{'loss': 0.6478, 'learning_rate': 8.957551158491639e-05, 'epoch': 0.21}
{'loss': 0.6359, 'learning_rate': 8.956073316109766e-05, 'epoch': 0.21}
{'loss': 0.6054, 'learning_rate': 8.954594549019808e-05, 'epoch': 0.21}
{'loss': 0.6172, 'learning_rate': 8.953114857567419e-05, 'epoch': 0.21}
{'loss': 0.6021, 'learning_rate': 8.951634242098468e-05, 'epoch': 0.21}
 21%|        | 1361/6500 [2:34:51<10:14:47,  7.18s/it]                                                         21%|        | 1361/6500 [2:34:51<10:14:47,  7.18s/it] 21%|        | 1362/6500 [2:34:58<9:58:57,  6.99s/it]                                                         21%|        | 1362/6500 [2:34:58<9:58:57,  6.99s/it] 21%|        | 1363/6500 [2:35:04<9:47:31,  6.86s/it]                                                        21%|        | 1363/6500 [2:35:04<9:47:31,  6.86s/it] 21%|        | 1364/6500 [2:35:11<9:39:49,  6.77s/it]                                                        21%|        | 1364/6500 [2:35:11<9:39:49,  6.77s/it] 21%|        | 1365/6500 [2:35:18<9:34:14,  6.71s/it]                                                        21%|        | 1365/6500 [2:35:18<9:34:14,  6.71s/it] 21%|        | 1366/6500 [2:35:24<9:29:56,  6.66s/it]                                                        21%|      {'loss': 0.65, 'learning_rate': 8.950152702959038e-05, 'epoch': 0.21}
{'loss': 0.6024, 'learning_rate': 8.94867024049543e-05, 'epoch': 0.21}
{'loss': 0.8853, 'learning_rate': 8.947186855054164e-05, 'epoch': 0.21}
{'loss': 0.6302, 'learning_rate': 8.945702546981969e-05, 'epoch': 0.21}
  | 1366/6500 [2:35:24<9:29:56,  6.66s/it] 21%|        | 1367/6500 [2:35:31<9:27:19,  6.63s/it]                                                        21%|        | 1367/6500 [2:35:31<9:27:19,  6.63s/it] 21%|        | 1368/6500 [2:35:37<9:25:00,  6.61s/it]                                                        21%|        | 1368/6500 [2:35:37<9:25:00,  6.61s/it] 21%|        | 1369/6500 [2:35:44<9:23:35,  6.59s/it]                                                        21%|        | 1369/6500 [2:35:44<9:23:35,  6.59s/it] 21%|        | 1370/6500 [2:35:50<9:22:34,  6.58s/it]                                                        21%|        | 1370/6500 [2:35:50<9:22:34,  6.58s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8476278781890869, 'eval_runtime': 1.4908, 'eval_samples_per_second': 8.05, 'eval_steps_per_second': 2.012, 'epoch': 0.21}
                                                        21%|        | 1370/6500 [2:35:52<9:22:34,  6.58s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-1370
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1370I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1370

I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1370
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1370
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1370/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1370/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1370/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1370/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.5984, 'learning_rate': 8.944217316625793e-05, 'epoch': 0.21}
{'loss': 0.6276, 'learning_rate': 8.942731164332802e-05, 'epoch': 0.21}
{'loss': 0.5899, 'learning_rate': 8.941244090450372e-05, 'epoch': 0.21}
{'loss': 0.6032, 'learning_rate': 8.9397560953261e-05, 'epoch': 0.21}
{'loss': 0.6187, 'learning_rate': 8.938267179307796e-05, 'epoch': 0.21}
{'loss': 0.627, 'learning_rate': 8.936777342743481e-05, 'epoch': 0.21}
 21%|        | 1371/6500 [2:35:59<10:07:07,  7.10s/it]                                                         21%|        | 1371/6500 [2:35:59<10:07:07,  7.10s/it] 21%|        | 1372/6500 [2:36:05<9:53:06,  6.94s/it]                                                         21%|        | 1372/6500 [2:36:05<9:53:06,  6.94s/it] 21%|        | 1373/6500 [2:36:12<9:43:11,  6.83s/it]                                                        21%|        | 1373/6500 [2:36:12<9:43:11,  6.83s/it] 21%|        | 1374/6500 [2:36:19<9:58:08,  7.00s/it]                                                        21%|        | 1374/6500 [2:36:19<9:58:08,  7.00s/it] 21%|        | 1375/6500 [2:36:26<9:46:39,  6.87s/it]                                                        21%|        | 1375/6500 [2:36:26<9:46:39,  6.87s/it] 21%|        | 1376/6500 [2:36:32<9:38:48,  6.78s/it]                                                        21%|      {'loss': 0.6229, 'learning_rate': 8.935286585981399e-05, 'epoch': 0.21}
{'loss': 0.6247, 'learning_rate': 8.933794909370006e-05, 'epoch': 0.21}
{'loss': 0.6033, 'learning_rate': 8.93230231325797e-05, 'epoch': 0.21}
{'loss': 0.608, 'learning_rate': 8.930808797994177e-05, 'epoch': 0.21}
  | 1376/6500 [2:36:32<9:38:48,  6.78s/it] 21%|        | 1377/6500 [2:36:39<9:33:08,  6.71s/it]                                                        21%|        | 1377/6500 [2:36:39<9:33:08,  6.71s/it] 21%|        | 1378/6500 [2:36:45<9:29:16,  6.67s/it]                                                        21%|        | 1378/6500 [2:36:45<9:29:16,  6.67s/it] 21%|        | 1379/6500 [2:36:52<9:26:08,  6.63s/it]                                                        21%|        | 1379/6500 [2:36:52<9:26:08,  6.63s/it] 21%|        | 1380/6500 [2:36:59<9:24:24,  6.61s/it]                                                        21%|        | 1380/6500 [2:36:59<9:24:24,  6.61s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8484303951263428, 'eval_runtime': 1.4903, 'eval_samples_per_second': 8.052, 'eval_steps_per_second': 2.013, 'epoch': 0.21}
                                                        21%|        | 1380/6500 [2:37:00<9:24:24,  6.61s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-1380
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1380I AM HERE AND I AM SAVING THE MODEL

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1380
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1380
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1380
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1380/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1380/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1380/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1380/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.604, 'learning_rate': 8.929314363927727e-05, 'epoch': 0.21}
{'loss': 0.6517, 'learning_rate': 8.927819011407937e-05, 'epoch': 0.21}
{'loss': 0.6168, 'learning_rate': 8.926322740784332e-05, 'epoch': 0.21}
{'loss': 0.8732, 'learning_rate': 8.92482555240666e-05, 'epoch': 0.21}
{'loss': 0.6254, 'learning_rate': 8.923327446624878e-05, 'epoch': 0.21}
 21%|        | 1381/6500 [2:37:07<10:10:09,  7.15s/it]                                                         21%|        | 1381/6500 [2:37:07<10:10:09,  7.15s/it] 21%|       | 1382/6500 [2:37:14<9:55:27,  6.98s/it]                                                         21%|       | 1382/6500 [2:37:14<9:55:27,  6.98s/it] 21%|       | 1383/6500 [2:37:20<9:44:36,  6.85s/it]                                                        21%|       | 1383/6500 [2:37:20<9:44:36,  6.85s/it] 21%|       | 1384/6500 [2:37:27<9:36:27,  6.76s/it]                                                        21%|       | 1384/6500 [2:37:27<9:36:27,  6.76s/it] 21%|       | 1385/6500 [2:37:33<9:30:48,  6.70s/it]                                                        21%|       | 1385/6500 [2:37:33<9:30:48,  6.70s/it] 21%|       | 1386/6500 [2:37:40<9:26:54,  6.65s/it]                                                       {'loss': 0.5922, 'learning_rate': 8.921828423789158e-05, 'epoch': 0.21}
{'loss': 0.6109, 'learning_rate': 8.920328484249892e-05, 'epoch': 0.21}
{'loss': 0.5891, 'learning_rate': 8.918827628357677e-05, 'epoch': 0.21}
{'loss': 0.5959, 'learning_rate': 8.917325856463331e-05, 'epoch': 0.21}
{'loss': 0.6152, 'learning_rate': 8.915823168917884e-05, 'epoch': 0.21}
 21%|       | 1386/6500 [2:37:40<9:26:54,  6.65s/it] 21%|       | 1387/6500 [2:37:46<9:23:55,  6.62s/it]                                                        21%|       | 1387/6500 [2:37:46<9:23:55,  6.62s/it] 21%|       | 1388/6500 [2:37:53<9:22:23,  6.60s/it]                                                        21%|       | 1388/6500 [2:37:53<9:22:23,  6.60s/it] 21%|       | 1389/6500 [2:37:59<9:21:06,  6.59s/it]                                                        21%|       | 1389/6500 [2:37:59<9:21:06,  6.59s/it] 21%|       | 1390/6500 [2:38:07<9:42:31,  6.84s/it]                                                        21%|       | 1390/6500 [2:38:07<9:42:31,  6.84s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8459194302558899, 'eval_runtime': 1.5212, 'eval_samples_per_second': 7.888, 'eval_steps_per_second': 1.972, 'epoch': 0.21}
                                                        21%|       | 1390/6500 [2:38:08<9:42:31,  6.84s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-1390
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1390
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1390
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1390
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1390
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1390/pytorch_model.bin
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1390/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1390/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1390/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.6312, 'learning_rate': 8.91431956607258e-05, 'epoch': 0.21}
{'loss': 0.6131, 'learning_rate': 8.912815048278876e-05, 'epoch': 0.21}
{'loss': 0.6128, 'learning_rate': 8.911309615888446e-05, 'epoch': 0.21}
{'loss': 0.6045, 'learning_rate': 8.909803269253174e-05, 'epoch': 0.21}
{'loss': 0.6046, 'learning_rate': 8.908296008725161e-05, 'epoch': 0.21}
 21%|       | 1391/6500 [2:38:15<10:21:44,  7.30s/it]                                                         21%|       | 1391/6500 [2:38:15<10:21:44,  7.30s/it] 21%|       | 1392/6500 [2:38:22<10:02:23,  7.08s/it]                                                         21%|       | 1392/6500 [2:38:22<10:02:23,  7.08s/it] 21%|       | 1393/6500 [2:38:28<9:48:57,  6.92s/it]                                                         21%|       | 1393/6500 [2:38:28<9:48:57,  6.92s/it] 21%|       | 1394/6500 [2:38:35<9:39:32,  6.81s/it]                                                        21%|       | 1394/6500 [2:38:35<9:39:32,  6.81s/it] 21%|       | 1395/6500 [2:38:41<9:33:11,  6.74s/it]                                                        21%|       | 1395/6500 [2:38:41<9:33:11,  6.74s/it] 21%|       | 1396/6500 [2:38:48<9:28:30,  6.68s/it]                                                 {'loss': 0.5933, 'learning_rate': 8.906787834656717e-05, 'epoch': 0.21}
{'loss': 0.6422, 'learning_rate': 8.905278747400369e-05, 'epoch': 0.21}
{'loss': 0.607, 'learning_rate': 8.903768747308861e-05, 'epoch': 0.22}
{'loss': 0.8818, 'learning_rate': 8.902257834735144e-05, 'epoch': 0.22}
{'loss': 0.6183, 'learning_rate': 8.900746010032383e-05, 'epoch': 0.22}
       21%|       | 1396/6500 [2:38:48<9:28:30,  6.68s/it] 21%|       | 1397/6500 [2:38:55<9:25:09,  6.65s/it]                                                        21%|       | 1397/6500 [2:38:55<9:25:09,  6.65s/it] 22%|       | 1398/6500 [2:39:01<9:22:56,  6.62s/it]                                                        22%|       | 1398/6500 [2:39:01<9:22:56,  6.62s/it] 22%|       | 1399/6500 [2:39:08<9:20:48,  6.60s/it]                                                        22%|       | 1399/6500 [2:39:08<9:20:48,  6.60s/it] 22%|       | 1400/6500 [2:39:14<9:19:33,  6.58s/it]                                                        22%|       | 1400/6500 [2:39:14<9:19:33,  6.58s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8442631363868713, 'eval_runtime': 1.4885, 'eval_samples_per_second': 8.062, 'eval_steps_per_second': 2.015, 'epoch': 0.22}
                                                        22%|       | 1400/6500 [2:39:16<9:19:33,  6.58s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-1400
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1400
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1400
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1400
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1400
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1400/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1400/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1400/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1400/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.6055, 'learning_rate': 8.899233273553958e-05, 'epoch': 0.22}
{'loss': 0.5964, 'learning_rate': 8.897719625653465e-05, 'epoch': 0.22}
{'loss': 0.5798, 'learning_rate': 8.896205066684707e-05, 'epoch': 0.22}
{'loss': 0.5945, 'learning_rate': 8.894689597001704e-05, 'epoch': 0.22}
{'loss': 0.5974, 'learning_rate': 8.893173216958687e-05, 'epoch': 0.22}
 22%|       | 1401/6500 [2:39:23<10:05:35,  7.13s/it]                                                         22%|       | 1401/6500 [2:39:23<10:05:35,  7.13s/it] 22%|       | 1402/6500 [2:39:29<9:50:43,  6.95s/it]                                                         22%|       | 1402/6500 [2:39:29<9:50:43,  6.95s/it] 22%|       | 1403/6500 [2:39:36<9:40:26,  6.83s/it]                                                        22%|       | 1403/6500 [2:39:36<9:40:26,  6.83s/it] 22%|       | 1404/6500 [2:39:42<9:33:15,  6.75s/it]                                                        22%|       | 1404/6500 [2:39:42<9:33:15,  6.75s/it] 22%|       | 1405/6500 [2:39:49<9:28:00,  6.69s/it]                                                        22%|       | 1405/6500 [2:39:49<9:28:00,  6.69s/it] 22%|       | 1406/6500 [2:39:56<9:49:58,  6.95s/it]                                                    {'loss': 0.6199, 'learning_rate': 8.891655926910103e-05, 'epoch': 0.22}
{'loss': 0.6229, 'learning_rate': 8.890137727210607e-05, 'epoch': 0.22}
{'loss': 0.6062, 'learning_rate': 8.88861861821507e-05, 'epoch': 0.22}
{'loss': 0.6113, 'learning_rate': 8.887098600278573e-05, 'epoch': 0.22}
{'loss': 0.6, 'learning_rate': 8.885577673756414e-05, 'epoch': 0.22}
    22%|       | 1406/6500 [2:39:56<9:49:58,  6.95s/it] 22%|       | 1407/6500 [2:40:03<9:40:07,  6.83s/it]                                                        22%|       | 1407/6500 [2:40:03<9:40:07,  6.83s/it] 22%|       | 1408/6500 [2:40:09<9:32:56,  6.75s/it]                                                        22%|       | 1408/6500 [2:40:09<9:32:56,  6.75s/it] 22%|       | 1409/6500 [2:40:16<9:27:48,  6.69s/it]                                                        22%|       | 1409/6500 [2:40:16<9:27:48,  6.69s/it] 22%|       | 1410/6500 [2:40:23<9:24:18,  6.65s/it]                                                        22%|       | 1410/6500 [2:40:23<9:24:18,  6.65s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8447564244270325, 'eval_runtime': 1.4863, 'eval_samples_per_second': 8.074, 'eval_steps_per_second': 2.018, 'epoch': 0.22}
                                                        22%|       | 1410/6500 [2:40:24<9:24:18,  6.65s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-1410
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  I AM HERE AND I AM SAVING THE MODEL
/projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1410the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1410

I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1410
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1410
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1410/pytorch_model.bin
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1410/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1410/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1410/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.6028, 'learning_rate': 8.884055839004098e-05, 'epoch': 0.22}
{'loss': 0.6337, 'learning_rate': 8.882533096377344e-05, 'epoch': 0.22}
{'loss': 0.6043, 'learning_rate': 8.881009446232086e-05, 'epoch': 0.22}
{'loss': 0.8895, 'learning_rate': 8.879484888924467e-05, 'epoch': 0.22}
{'loss': 0.5917, 'learning_rate': 8.877959424810843e-05, 'epoch': 0.22}
 22%|       | 1411/6500 [2:40:31<10:09:25,  7.19s/it]                                                         22%|       | 1411/6500 [2:40:31<10:09:25,  7.19s/it] 22%|       | 1412/6500 [2:40:38<9:53:03,  6.99s/it]                                                         22%|       | 1412/6500 [2:40:38<9:53:03,  6.99s/it] 22%|       | 1413/6500 [2:40:44<9:41:36,  6.86s/it]                                                        22%|       | 1413/6500 [2:40:44<9:41:36,  6.86s/it] 22%|       | 1414/6500 [2:40:51<9:33:42,  6.77s/it]                                                        22%|       | 1414/6500 [2:40:51<9:33:42,  6.77s/it] 22%|       | 1415/6500 [2:40:57<9:28:05,  6.70s/it]                                                        22%|       | 1415/6500 [2:40:57<9:28:05,  6.70s/it] 22%|       | 1416/6500 [2:41:04<9:24:03,  6.66s/it]                                                    {'loss': 0.6213, 'learning_rate': 8.87643305424778e-05, 'epoch': 0.22}
{'loss': 0.5908, 'learning_rate': 8.87490577759206e-05, 'epoch': 0.22}
{'loss': 0.5859, 'learning_rate': 8.873377595200676e-05, 'epoch': 0.22}
{'loss': 0.5845, 'learning_rate': 8.871848507430829e-05, 'epoch': 0.22}
{'loss': 0.615, 'learning_rate': 8.870318514639935e-05, 'epoch': 0.22}
    22%|       | 1416/6500 [2:41:04<9:24:03,  6.66s/it] 22%|       | 1417/6500 [2:41:10<9:21:34,  6.63s/it]                                                        22%|       | 1417/6500 [2:41:10<9:21:34,  6.63s/it] 22%|       | 1418/6500 [2:41:17<9:19:37,  6.61s/it]                                                        22%|       | 1418/6500 [2:41:17<9:19:37,  6.61s/it] 22%|       | 1419/6500 [2:41:23<9:18:07,  6.59s/it]                                                        22%|       | 1419/6500 [2:41:23<9:18:07,  6.59s/it] 22%|       | 1420/6500 [2:41:30<9:16:51,  6.58s/it]                                                        22%|       | 1420/6500 [2:41:30<9:16:51,  6.58s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.845432698726654, 'eval_runtime': 1.4848, 'eval_samples_per_second': 8.082, 'eval_steps_per_second': 2.02, 'epoch': 0.22}
                                                        22%|       | 1420/6500 [2:41:31<9:16:51,  6.58s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-1420
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1420I AM HERE AND I AM SAVING THE MODEL

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1420
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1420
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1420
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1420/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1420/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1420/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1420/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.6272, 'learning_rate': 8.868787617185619e-05, 'epoch': 0.22}
{'loss': 0.6074, 'learning_rate': 8.86725581542572e-05, 'epoch': 0.22}
{'loss': 0.5945, 'learning_rate': 8.865723109718288e-05, 'epoch': 0.22}
{'loss': 0.6185, 'learning_rate': 8.864189500421582e-05, 'epoch': 0.22}
{'loss': 0.5946, 'learning_rate': 8.862654987894076e-05, 'epoch': 0.22}
 22%|       | 1421/6500 [2:41:38<10:01:44,  7.11s/it]                                                         22%|       | 1421/6500 [2:41:38<10:01:44,  7.11s/it] 22%|       | 1422/6500 [2:41:45<9:47:26,  6.94s/it]                                                         22%|       | 1422/6500 [2:41:45<9:47:26,  6.94s/it] 22%|       | 1423/6500 [2:41:52<9:59:40,  7.09s/it]                                                        22%|       | 1423/6500 [2:41:52<9:59:40,  7.09s/it] 22%|       | 1424/6500 [2:41:59<9:46:20,  6.93s/it]                                                        22%|       | 1424/6500 [2:41:59<9:46:20,  6.93s/it] 22%|       | 1425/6500 [2:42:05<9:36:25,  6.81s/it]                                                        22%|       | 1425/6500 [2:42:05<9:36:25,  6.81s/it] 22%|       | 1426/6500 [2:42:12<9:30:39,  6.75s/it]                                                    {'loss': 0.6086, 'learning_rate': 8.861119572494453e-05, 'epoch': 0.22}
{'loss': 0.6163, 'learning_rate': 8.859583254581605e-05, 'epoch': 0.22}
{'loss': 0.6066, 'learning_rate': 8.858046034514637e-05, 'epoch': 0.22}
{'loss': 0.8859, 'learning_rate': 8.856507912652867e-05, 'epoch': 0.22}
{'loss': 0.594, 'learning_rate': 8.85496888935582e-05, 'epoch': 0.22}
    22%|       | 1426/6500 [2:42:12<9:30:39,  6.75s/it] 22%|       | 1427/6500 [2:42:19<9:25:27,  6.69s/it]                                                        22%|       | 1427/6500 [2:42:19<9:25:27,  6.69s/it] 22%|       | 1428/6500 [2:42:25<9:21:33,  6.64s/it]                                                        22%|       | 1428/6500 [2:42:25<9:21:33,  6.64s/it] 22%|       | 1429/6500 [2:42:32<9:18:48,  6.61s/it]                                                        22%|       | 1429/6500 [2:42:32<9:18:48,  6.61s/it] 22%|       | 1430/6500 [2:42:38<9:17:05,  6.59s/it]                                                        22%|       | 1430/6500 [2:42:38<9:17:05,  6.59s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8442719578742981, 'eval_runtime': 1.4855, 'eval_samples_per_second': 8.078, 'eval_steps_per_second': 2.02, 'epoch': 0.22}
                                                        22%|       | 1430/6500 [2:42:40<9:17:05,  6.59s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-1430
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1430I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1430

I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1430
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1430
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1430/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1430/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1430/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1430/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.624, 'learning_rate': 8.853428964983233e-05, 'epoch': 0.22}
{'loss': 0.5794, 'learning_rate': 8.851888139895057e-05, 'epoch': 0.22}
{'loss': 0.587, 'learning_rate': 8.850346414451445e-05, 'epoch': 0.22}
{'loss': 0.5984, 'learning_rate': 8.84880378901277e-05, 'epoch': 0.22}
{'loss': 0.6001, 'learning_rate': 8.847260263939612e-05, 'epoch': 0.22}
 22%|       | 1431/6500 [2:42:47<10:02:57,  7.14s/it]                                                         22%|       | 1431/6500 [2:42:47<10:02:57,  7.14s/it] 22%|       | 1432/6500 [2:42:53<9:48:10,  6.96s/it]                                                         22%|       | 1432/6500 [2:42:53<9:48:10,  6.96s/it] 22%|       | 1433/6500 [2:43:00<9:37:54,  6.84s/it]                                                        22%|       | 1433/6500 [2:43:00<9:37:54,  6.84s/it] 22%|       | 1434/6500 [2:43:06<9:31:36,  6.77s/it]                                                        22%|       | 1434/6500 [2:43:06<9:31:36,  6.77s/it] 22%|       | 1435/6500 [2:43:13<9:26:20,  6.71s/it]                                                        22%|       | 1435/6500 [2:43:13<9:26:20,  6.71s/it] 22%|       | 1436/6500 [2:43:19<9:22:19,  6.66s/it]                                                    {'loss': 0.6261, 'learning_rate': 8.845715839592758e-05, 'epoch': 0.22}
{'loss': 0.6265, 'learning_rate': 8.844170516333208e-05, 'epoch': 0.22}
{'loss': 0.5849, 'learning_rate': 8.842624294522174e-05, 'epoch': 0.22}
{'loss': 0.6083, 'learning_rate': 8.841077174521075e-05, 'epoch': 0.22}
{'loss': 0.5878, 'learning_rate': 8.83952915669154e-05, 'epoch': 0.22}
    22%|       | 1436/6500 [2:43:19<9:22:19,  6.66s/it] 22%|       | 1437/6500 [2:43:26<9:19:38,  6.63s/it]                                                        22%|       | 1437/6500 [2:43:26<9:19:38,  6.63s/it] 22%|       | 1438/6500 [2:43:33<9:17:32,  6.61s/it]                                                        22%|       | 1438/6500 [2:43:33<9:17:32,  6.61s/it] 22%|       | 1439/6500 [2:43:40<9:31:51,  6.78s/it]                                                        22%|       | 1439/6500 [2:43:40<9:31:51,  6.78s/it] 22%|       | 1440/6500 [2:43:46<9:26:09,  6.71s/it]                                                        22%|       | 1440/6500 [2:43:46<9:26:09,  6.71s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.845733642578125, 'eval_runtime': 1.7538, 'eval_samples_per_second': 6.842, 'eval_steps_per_second': 1.711, 'epoch': 0.22}
                                                        22%|       | 1440/6500 [2:43:48<9:26:09,  6.71s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-1440
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1440I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in 
 /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1440
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1440
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1440
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1440/pytorch_model.bin
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1440/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1440/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1440/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.6337, 'learning_rate': 8.837980241395408e-05, 'epoch': 0.22}
{'loss': 0.5922, 'learning_rate': 8.836430428994732e-05, 'epoch': 0.22}
{'loss': 0.8783, 'learning_rate': 8.834879719851768e-05, 'epoch': 0.22}
{'loss': 0.6141, 'learning_rate': 8.833328114328986e-05, 'epoch': 0.22}
{'loss': 0.585, 'learning_rate': 8.831775612789063e-05, 'epoch': 0.22}
 22%|       | 1441/6500 [2:43:55<10:15:29,  7.30s/it]                                                         22%|       | 1441/6500 [2:43:55<10:15:29,  7.30s/it] 22%|       | 1442/6500 [2:44:02<9:56:43,  7.08s/it]                                                         22%|       | 1442/6500 [2:44:02<9:56:43,  7.08s/it] 22%|       | 1443/6500 [2:44:08<9:44:28,  6.93s/it]                                                        22%|       | 1443/6500 [2:44:08<9:44:28,  6.93s/it] 22%|       | 1444/6500 [2:44:15<9:34:56,  6.82s/it]                                                        22%|       | 1444/6500 [2:44:15<9:34:56,  6.82s/it] 22%|       | 1445/6500 [2:44:21<9:28:11,  6.74s/it]                                                        22%|       | 1445/6500 [2:44:21<9:28:11,  6.74s/it] 22%|       | 1446/6500 [2:44:28<9:23:34,  6.69s/it]                                                    {'loss': 0.6152, 'learning_rate': 8.83022221559489e-05, 'epoch': 0.22}
{'loss': 0.5665, 'learning_rate': 8.828667923109563e-05, 'epoch': 0.22}
{'loss': 0.5893, 'learning_rate': 8.827112735696385e-05, 'epoch': 0.22}
{'loss': 0.5973, 'learning_rate': 8.825556653718876e-05, 'epoch': 0.22}
{'loss': 0.5971, 'learning_rate': 8.82399967754076e-05, 'epoch': 0.22}
    22%|       | 1446/6500 [2:44:28<9:23:34,  6.69s/it] 22%|       | 1447/6500 [2:44:34<9:20:18,  6.65s/it]                                                        22%|       | 1447/6500 [2:44:34<9:20:18,  6.65s/it] 22%|       | 1448/6500 [2:44:41<9:17:59,  6.63s/it]                                                        22%|       | 1448/6500 [2:44:41<9:17:59,  6.63s/it] 22%|       | 1449/6500 [2:44:48<9:16:34,  6.61s/it]                                                        22%|       | 1449/6500 [2:44:48<9:16:34,  6.61s/it] 22%|       | 1450/6500 [2:44:54<9:15:05,  6.60s/it]                                                        22%|       | 1450/6500 [2:44:54<9:15:05,  6.60s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8435587882995605, 'eval_runtime': 1.4882, 'eval_samples_per_second': 8.064, 'eval_steps_per_second': 2.016, 'epoch': 0.22}
                                                        22%|       | 1450/6500 [2:44:56<9:15:05,  6.60s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-1450
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1450
/projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1450
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1450
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1450
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1450/pytorch_model.bin
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1450/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1450/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1450/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.6081, 'learning_rate': 8.822441807525967e-05, 'epoch': 0.22}
{'loss': 0.6094, 'learning_rate': 8.820883044038644e-05, 'epoch': 0.22}
{'loss': 0.583, 'learning_rate': 8.81932338744314e-05, 'epoch': 0.22}
{'loss': 0.6105, 'learning_rate': 8.817762838104016e-05, 'epoch': 0.22}
{'loss': 0.5881, 'learning_rate': 8.816201396386042e-05, 'epoch': 0.22}
 22%|       | 1451/6500 [2:45:03<10:01:31,  7.15s/it]                                                         22%|       | 1451/6500 [2:45:03<10:01:31,  7.15s/it] 22%|       | 1452/6500 [2:45:09<9:46:41,  6.97s/it]                                                         22%|       | 1452/6500 [2:45:09<9:46:41,  6.97s/it] 22%|       | 1453/6500 [2:45:16<9:36:11,  6.85s/it]                                                        22%|       | 1453/6500 [2:45:16<9:36:11,  6.85s/it] 22%|       | 1454/6500 [2:45:22<9:28:51,  6.76s/it]                                                        22%|       | 1454/6500 [2:45:22<9:28:51,  6.76s/it] 22%|       | 1455/6500 [2:45:29<9:39:30,  6.89s/it]                                                        22%|       | 1455/6500 [2:45:29<9:39:30,  6.89s/it] 22%|       | 1456/6500 [2:45:36<9:31:17,  6.80s/it]                                                    {'loss': 0.6469, 'learning_rate': 8.814639062654194e-05, 'epoch': 0.22}
{'loss': 0.5851, 'learning_rate': 8.813075837273658e-05, 'epoch': 0.22}
{'loss': 0.8716, 'learning_rate': 8.81151172060983e-05, 'epoch': 0.22}
{'loss': 0.6109, 'learning_rate': 8.80994671302831e-05, 'epoch': 0.22}
{'loss': 0.5768, 'learning_rate': 8.808380814894912e-05, 'epoch': 0.22}
    22%|       | 1456/6500 [2:45:36<9:31:17,  6.80s/it] 22%|       | 1457/6500 [2:45:43<9:25:16,  6.73s/it]                                                        22%|       | 1457/6500 [2:45:43<9:25:16,  6.73s/it] 22%|       | 1458/6500 [2:45:49<9:20:52,  6.67s/it]                                                        22%|       | 1458/6500 [2:45:49<9:20:52,  6.67s/it] 22%|       | 1459/6500 [2:45:56<9:18:13,  6.64s/it]                                                        22%|       | 1459/6500 [2:45:56<9:18:13,  6.64s/it] 22%|       | 1460/6500 [2:46:02<9:15:59,  6.62s/it]                                                        22%|       | 1460/6500 [2:46:02<9:15:59,  6.62s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8429150581359863, 'eval_runtime': 1.5001, 'eval_samples_per_second': 7.999, 'eval_steps_per_second': 2.0, 'epoch': 0.22}
                                                        22%|       | 1460/6500 [2:46:04<9:15:59,  6.62s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-1460
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1460
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1460
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1460
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1460
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1460/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1460/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1460/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1460/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.6126, 'learning_rate': 8.806814026575654e-05, 'epoch': 0.22}
{'loss': 0.5758, 'learning_rate': 8.805246348436762e-05, 'epoch': 0.22}
{'loss': 0.5879, 'learning_rate': 8.803677780844674e-05, 'epoch': 0.23}
{'loss': 0.605, 'learning_rate': 8.80210832416603e-05, 'epoch': 0.23}
{'loss': 0.6121, 'learning_rate': 8.800537978767682e-05, 'epoch': 0.23}
 22%|       | 1461/6500 [2:46:11<10:01:25,  7.16s/it]                                                         22%|       | 1461/6500 [2:46:11<10:01:25,  7.16s/it] 22%|       | 1462/6500 [2:46:17<9:47:01,  6.99s/it]                                                         22%|       | 1462/6500 [2:46:17<9:47:01,  6.99s/it] 23%|       | 1463/6500 [2:46:24<9:37:05,  6.87s/it]                                                        23%|       | 1463/6500 [2:46:24<9:37:05,  6.87s/it] 23%|       | 1464/6500 [2:46:30<9:29:23,  6.78s/it]                                                        23%|       | 1464/6500 [2:46:30<9:29:23,  6.78s/it] 23%|       | 1465/6500 [2:46:37<9:24:06,  6.72s/it]                                                        23%|       | 1465/6500 [2:46:37<9:24:06,  6.72s/it] 23%|       | 1466/6500 [2:46:44<9:20:54,  6.69s/it]                                                    {'loss': 0.6027, 'learning_rate': 8.79896674501669e-05, 'epoch': 0.23}
{'loss': 0.6078, 'learning_rate': 8.797394623280319e-05, 'epoch': 0.23}
{'loss': 0.5925, 'learning_rate': 8.795821613926045e-05, 'epoch': 0.23}
{'loss': 0.597, 'learning_rate': 8.794247717321547e-05, 'epoch': 0.23}
{'loss': 0.5794, 'learning_rate': 8.792672933834713e-05, 'epoch': 0.23}
    23%|       | 1466/6500 [2:46:44<9:20:54,  6.69s/it] 23%|       | 1467/6500 [2:46:50<9:18:18,  6.66s/it]                                                        23%|       | 1467/6500 [2:46:50<9:18:18,  6.66s/it] 23%|       | 1468/6500 [2:46:57<9:16:12,  6.63s/it]                                                        23%|       | 1468/6500 [2:46:57<9:16:12,  6.63s/it] 23%|       | 1469/6500 [2:47:03<9:14:45,  6.62s/it]                                                        23%|       | 1469/6500 [2:47:03<9:14:45,  6.62s/it] 23%|       | 1470/6500 [2:47:10<9:13:58,  6.61s/it]                                                        23%|       | 1470/6500 [2:47:10<9:13:58,  6.61s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.84336918592453, 'eval_runtime': 1.7277, 'eval_samples_per_second': 6.946, 'eval_steps_per_second': 1.736, 'epoch': 0.23}
                                                        23%|       | 1470/6500 [2:47:12<9:13:58,  6.61s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-1470
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1470I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in 
 /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1470
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1470
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1470
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1470/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1470/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1470/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1470/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.6264, 'learning_rate': 8.791097263833643e-05, 'epoch': 0.23}
{'loss': 0.5921, 'learning_rate': 8.789520707686635e-05, 'epoch': 0.23}
{'loss': 0.869, 'learning_rate': 8.787943265762204e-05, 'epoch': 0.23}
{'loss': 0.6124, 'learning_rate': 8.786364938429065e-05, 'epoch': 0.23}
{'loss': 0.5828, 'learning_rate': 8.784785726056143e-05, 'epoch': 0.23}
 23%|       | 1471/6500 [2:47:19<10:21:04,  7.41s/it]                                                         23%|       | 1471/6500 [2:47:19<10:21:04,  7.41s/it] 23%|       | 1472/6500 [2:47:26<10:00:33,  7.17s/it]                                                         23%|       | 1472/6500 [2:47:26<10:00:33,  7.17s/it] 23%|       | 1473/6500 [2:47:32<9:45:20,  6.99s/it]                                                         23%|       | 1473/6500 [2:47:32<9:45:20,  6.99s/it] 23%|       | 1474/6500 [2:47:39<9:35:03,  6.86s/it]                                                        23%|       | 1474/6500 [2:47:39<9:35:03,  6.86s/it] 23%|       | 1475/6500 [2:47:46<9:27:36,  6.78s/it]                                                        23%|       | 1475/6500 [2:47:46<9:27:36,  6.78s/it] 23%|       | 1476/6500 [2:47:52<9:22:52,  6.72s/it]                                                 {'loss': 0.5819, 'learning_rate': 8.78320562901257e-05, 'epoch': 0.23}
{'loss': 0.5756, 'learning_rate': 8.781624647667684e-05, 'epoch': 0.23}
{'loss': 0.5773, 'learning_rate': 8.780042782391028e-05, 'epoch': 0.23}
{'loss': 0.5866, 'learning_rate': 8.778460033552356e-05, 'epoch': 0.23}
{'loss': 0.6093, 'learning_rate': 8.776876401521624e-05, 'epoch': 0.23}
       23%|       | 1476/6500 [2:47:52<9:22:52,  6.72s/it] 23%|       | 1477/6500 [2:47:59<9:19:12,  6.68s/it]                                                        23%|       | 1477/6500 [2:47:59<9:19:12,  6.68s/it] 23%|       | 1478/6500 [2:48:05<9:16:36,  6.65s/it]                                                        23%|       | 1478/6500 [2:48:05<9:16:36,  6.65s/it] 23%|       | 1479/6500 [2:48:12<9:14:45,  6.63s/it]                                                        23%|       | 1479/6500 [2:48:12<9:14:45,  6.63s/it] 23%|       | 1480/6500 [2:48:18<9:13:42,  6.62s/it]                                                        23%|       | 1480/6500 [2:48:18<9:13:42,  6.62s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.843248188495636, 'eval_runtime': 1.5015, 'eval_samples_per_second': 7.992, 'eval_steps_per_second': 1.998, 'epoch': 0.23}
                                                        23%|       | 1480/6500 [2:48:20<9:13:42,  6.62s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-1480
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1480
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1480
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1480
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1480
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1480/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1480/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1480/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1480/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.6111, 'learning_rate': 8.775291886668995e-05, 'epoch': 0.23}
{'loss': 0.5891, 'learning_rate': 8.773706489364843e-05, 'epoch': 0.23}
{'loss': 0.595, 'learning_rate': 8.772120209979745e-05, 'epoch': 0.23}
{'loss': 0.593, 'learning_rate': 8.770533048884482e-05, 'epoch': 0.23}
{'loss': 0.5854, 'learning_rate': 8.768945006450043e-05, 'epoch': 0.23}
 23%|       | 1481/6500 [2:48:27<9:57:07,  7.14s/it]                                                        23%|       | 1481/6500 [2:48:27<9:57:07,  7.14s/it] 23%|       | 1482/6500 [2:48:33<9:43:08,  6.97s/it]                                                        23%|       | 1482/6500 [2:48:33<9:43:08,  6.97s/it] 23%|       | 1483/6500 [2:48:40<9:33:21,  6.86s/it]                                                        23%|       | 1483/6500 [2:48:40<9:33:21,  6.86s/it] 23%|       | 1484/6500 [2:48:47<9:26:38,  6.78s/it]                                                        23%|       | 1484/6500 [2:48:47<9:26:38,  6.78s/it] 23%|       | 1485/6500 [2:48:53<9:21:35,  6.72s/it]                                                        23%|       | 1485/6500 [2:48:53<9:21:35,  6.72s/it] 23%|       | 1486/6500 [2:49:00<9:17:57,  6.68s/it]                                                       {'loss': 0.616, 'learning_rate': 8.767356083047626e-05, 'epoch': 0.23}
{'loss': 0.5873, 'learning_rate': 8.765766279048629e-05, 'epoch': 0.23}
{'loss': 0.8764, 'learning_rate': 8.764175594824662e-05, 'epoch': 0.23}
{'loss': 0.5932, 'learning_rate': 8.762584030747534e-05, 'epoch': 0.23}
{'loss': 0.5888, 'learning_rate': 8.760991587189268e-05, 'epoch': 0.23}
 23%|       | 1486/6500 [2:49:00<9:17:57,  6.68s/it] 23%|       | 1487/6500 [2:49:07<9:37:12,  6.91s/it]                                                        23%|       | 1487/6500 [2:49:07<9:37:12,  6.91s/it] 23%|       | 1488/6500 [2:49:14<9:28:41,  6.81s/it]                                                        23%|       | 1488/6500 [2:49:14<9:28:41,  6.81s/it] 23%|       | 1489/6500 [2:49:20<9:23:19,  6.75s/it]                                                        23%|       | 1489/6500 [2:49:20<9:23:19,  6.75s/it] 23%|       | 1490/6500 [2:49:27<9:19:10,  6.70s/it]                                                        23%|       | 1490/6500 [2:49:27<9:19:10,  6.70s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8447660207748413, 'eval_runtime': 1.4863, 'eval_samples_per_second': 8.074, 'eval_steps_per_second': 2.018, 'epoch': 0.23}
                                                        23%|       | 1490/6500 [2:49:28<9:19:10,  6.70s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-1490
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1490
 /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1490
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1490
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1490
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1490/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1490/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1490/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1490/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.5765, 'learning_rate': 8.759398264522085e-05, 'epoch': 0.23}
{'loss': 0.5671, 'learning_rate': 8.757804063118415e-05, 'epoch': 0.23}
{'loss': 0.5744, 'learning_rate': 8.756208983350893e-05, 'epoch': 0.23}
{'loss': 0.5915, 'learning_rate': 8.754613025592359e-05, 'epoch': 0.23}
{'loss': 0.6167, 'learning_rate': 8.753016190215859e-05, 'epoch': 0.23}
 23%|       | 1491/6500 [2:49:35<10:00:34,  7.19s/it]                                                         23%|       | 1491/6500 [2:49:35<10:00:34,  7.19s/it] 23%|       | 1492/6500 [2:49:42<9:45:16,  7.01s/it]                                                         23%|       | 1492/6500 [2:49:42<9:45:16,  7.01s/it] 23%|       | 1493/6500 [2:49:48<9:34:28,  6.88s/it]                                                        23%|       | 1493/6500 [2:49:48<9:34:28,  6.88s/it] 23%|       | 1494/6500 [2:49:55<9:26:59,  6.80s/it]                                                        23%|       | 1494/6500 [2:49:55<9:26:59,  6.80s/it] 23%|       | 1495/6500 [2:50:02<9:21:18,  6.73s/it]                                                        23%|       | 1495/6500 [2:50:02<9:21:18,  6.73s/it] 23%|       | 1496/6500 [2:50:08<9:17:48,  6.69s/it]                                                    {'loss': 0.592, 'learning_rate': 8.751418477594645e-05, 'epoch': 0.23}
{'loss': 0.5806, 'learning_rate': 8.749819888102166e-05, 'epoch': 0.23}
{'loss': 0.6016, 'learning_rate': 8.748220422112092e-05, 'epoch': 0.23}
{'loss': 0.5805, 'learning_rate': 8.746620079998282e-05, 'epoch': 0.23}
{'loss': 0.5887, 'learning_rate': 8.745018862134808e-05, 'epoch': 0.23}
    23%|       | 1496/6500 [2:50:08<9:17:48,  6.69s/it] 23%|       | 1497/6500 [2:50:15<9:15:20,  6.66s/it]                                                        23%|       | 1497/6500 [2:50:15<9:15:20,  6.66s/it] 23%|       | 1498/6500 [2:50:21<9:13:13,  6.64s/it]                                                        23%|       | 1498/6500 [2:50:21<9:13:13,  6.64s/it] 23%|       | 1499/6500 [2:50:28<9:11:27,  6.62s/it]                                                        23%|       | 1499/6500 [2:50:28<9:11:27,  6.62s/it] 23%|       | 1500/6500 [2:50:35<9:10:31,  6.61s/it]                                                        23%|       | 1500/6500 [2:50:35<9:10:31,  6.61s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8438727259635925, 'eval_runtime': 1.4853, 'eval_samples_per_second': 8.079, 'eval_steps_per_second': 2.02, 'epoch': 0.23}
                                                        23%|       | 1500/6500 [2:50:36<9:10:31,  6.61s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-1500
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in   /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1500
/projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1500
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1500
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1500
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1500/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1500/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1500/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1500/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.6041, 'learning_rate': 8.743416768895947e-05, 'epoch': 0.23}
{'loss': 0.5864, 'learning_rate': 8.741813800656174e-05, 'epoch': 0.23}
{'loss': 0.8775, 'learning_rate': 8.740209957790178e-05, 'epoch': 0.23}
{'loss': 0.5738, 'learning_rate': 8.738605240672843e-05, 'epoch': 0.23}
{'loss': 0.6078, 'learning_rate': 8.736999649679264e-05, 'epoch': 0.23}
 23%|       | 1501/6500 [2:50:43<9:53:19,  7.12s/it]                                                        23%|       | 1501/6500 [2:50:43<9:53:19,  7.12s/it] 23%|       | 1502/6500 [2:50:49<9:39:29,  6.96s/it]                                                        23%|       | 1502/6500 [2:50:49<9:39:29,  6.96s/it] 23%|       | 1503/6500 [2:50:57<9:51:42,  7.10s/it]                                                        23%|       | 1503/6500 [2:50:57<9:51:42,  7.10s/it] 23%|       | 1504/6500 [2:51:04<9:39:13,  6.96s/it]                                                        23%|       | 1504/6500 [2:51:04<9:39:13,  6.96s/it] 23%|       | 1505/6500 [2:51:10<9:29:32,  6.84s/it]                                                        23%|       | 1505/6500 [2:51:10<9:29:32,  6.84s/it] 23%|       | 1506/6500 [2:51:17<9:23:14,  6.77s/it]                                                       {'loss': 0.5702, 'learning_rate': 8.735393185184741e-05, 'epoch': 0.23}
{'loss': 0.5739, 'learning_rate': 8.73378584756477e-05, 'epoch': 0.23}
{'loss': 0.5794, 'learning_rate': 8.73217763719506e-05, 'epoch': 0.23}
{'loss': 0.5952, 'learning_rate': 8.73056855445152e-05, 'epoch': 0.23}
{'loss': 0.614, 'learning_rate': 8.728958599710262e-05, 'epoch': 0.23}
 23%|       | 1506/6500 [2:51:17<9:23:14,  6.77s/it] 23%|       | 1507/6500 [2:51:23<9:18:44,  6.71s/it]                                                        23%|       | 1507/6500 [2:51:23<9:18:44,  6.71s/it] 23%|       | 1508/6500 [2:51:30<9:15:30,  6.68s/it]                                                        23%|       | 1508/6500 [2:51:30<9:15:30,  6.68s/it] 23%|       | 1509/6500 [2:51:36<9:13:17,  6.65s/it]                                                        23%|       | 1509/6500 [2:51:36<9:13:17,  6.65s/it] 23%|       | 1510/6500 [2:51:43<9:11:54,  6.64s/it]                                                        23%|       | 1510/6500 [2:51:43<9:11:54,  6.64s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8442568778991699, 'eval_runtime': 1.4928, 'eval_samples_per_second': 8.038, 'eval_steps_per_second': 2.01, 'epoch': 0.23}
                                                        23%|       | 1510/6500 [2:51:45<9:11:54,  6.64s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-1510
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1510I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1510

I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1510
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1510
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1510/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1510/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1510/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1510/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.602, 'learning_rate': 8.727347773347603e-05, 'epoch': 0.23}
{'loss': 0.5801, 'learning_rate': 8.725736075740066e-05, 'epoch': 0.23}
{'loss': 0.5886, 'learning_rate': 8.724123507264372e-05, 'epoch': 0.23}
{'loss': 0.5765, 'learning_rate': 8.722510068297454e-05, 'epoch': 0.23}
{'loss': 0.6192, 'learning_rate': 8.720895759216439e-05, 'epoch': 0.23}
 23%|       | 1511/6500 [2:51:51<9:54:28,  7.15s/it]                                                        23%|       | 1511/6500 [2:51:51<9:54:28,  7.15s/it] 23%|       | 1512/6500 [2:51:58<9:40:34,  6.98s/it]                                                        23%|       | 1512/6500 [2:51:58<9:40:34,  6.98s/it] 23%|       | 1513/6500 [2:52:05<9:30:27,  6.86s/it]                                                        23%|       | 1513/6500 [2:52:05<9:30:27,  6.86s/it] 23%|       | 1514/6500 [2:52:11<9:23:28,  6.78s/it]                                                        23%|       | 1514/6500 [2:52:11<9:23:28,  6.78s/it] 23%|       | 1515/6500 [2:52:18<9:18:13,  6.72s/it]                                                        23%|       | 1515/6500 [2:52:18<9:18:13,  6.72s/it] 23%|       | 1516/6500 [2:52:24<9:14:35,  6.68s/it]                                                       {'loss': 0.5742, 'learning_rate': 8.719280580398663e-05, 'epoch': 0.23}
{'loss': 0.8507, 'learning_rate': 8.717664532221668e-05, 'epoch': 0.23}
{'loss': 0.6149, 'learning_rate': 8.71604761506319e-05, 'epoch': 0.23}
{'loss': 0.5673, 'learning_rate': 8.714429829301176e-05, 'epoch': 0.23}
{'loss': 0.6077, 'learning_rate': 8.712811175313773e-05, 'epoch': 0.23}
 23%|       | 1516/6500 [2:52:24<9:14:35,  6.68s/it] 23%|       | 1517/6500 [2:52:31<9:11:37,  6.64s/it]                                                        23%|       | 1517/6500 [2:52:31<9:11:37,  6.64s/it] 23%|       | 1518/6500 [2:52:37<9:09:49,  6.62s/it]                                                        23%|       | 1518/6500 [2:52:37<9:09:49,  6.62s/it] 23%|       | 1519/6500 [2:52:44<9:08:14,  6.60s/it]                                                        23%|       | 1519/6500 [2:52:44<9:08:14,  6.60s/it] 23%|       | 1520/6500 [2:52:52<9:30:12,  6.87s/it]                                                        23%|       | 1520/6500 [2:52:52<9:30:12,  6.87s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8452857732772827, 'eval_runtime': 1.4898, 'eval_samples_per_second': 8.055, 'eval_steps_per_second': 2.014, 'epoch': 0.23}
                                                        23%|       | 1520/6500 [2:52:53<9:30:12,  6.87s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-1520
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1520
/projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1520
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1520
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1520
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1520/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1520/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1520/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1520/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.553, 'learning_rate': 8.711191653479333e-05, 'epoch': 0.23}
{'loss': 0.5685, 'learning_rate': 8.709571264176409e-05, 'epoch': 0.23}
{'loss': 0.5749, 'learning_rate': 8.707950007783755e-05, 'epoch': 0.23}
{'loss': 0.586, 'learning_rate': 8.706327884680332e-05, 'epoch': 0.23}
{'loss': 0.6054, 'learning_rate': 8.704704895245301e-05, 'epoch': 0.23}
 23%|       | 1521/6500 [2:53:00<10:07:03,  7.32s/it]                                                         23%|       | 1521/6500 [2:53:00<10:07:03,  7.32s/it] 23%|       | 1522/6500 [2:53:06<9:48:56,  7.10s/it]                                                         23%|       | 1522/6500 [2:53:06<9:48:56,  7.10s/it] 23%|       | 1523/6500 [2:53:13<9:36:37,  6.95s/it]                                                        23%|       | 1523/6500 [2:53:13<9:36:37,  6.95s/it] 23%|       | 1524/6500 [2:53:20<9:27:15,  6.84s/it]                                                        23%|       | 1524/6500 [2:53:20<9:27:15,  6.84s/it] 23%|       | 1525/6500 [2:53:26<9:20:17,  6.76s/it]                                                        23%|       | 1525/6500 [2:53:26<9:20:17,  6.76s/it] 23%|       | 1526/6500 [2:53:33<9:15:55,  6.71s/it]                                                    {'loss': 0.5935, 'learning_rate': 8.703081039858026e-05, 'epoch': 0.23}
{'loss': 0.5715, 'learning_rate': 8.701456318898073e-05, 'epoch': 0.23}
{'loss': 0.5945, 'learning_rate': 8.69983073274521e-05, 'epoch': 0.24}
{'loss': 0.5725, 'learning_rate': 8.69820428177941e-05, 'epoch': 0.24}
{'loss': 0.6187, 'learning_rate': 8.696576966380843e-05, 'epoch': 0.24}
    23%|       | 1526/6500 [2:53:33<9:15:55,  6.71s/it] 23%|       | 1527/6500 [2:53:39<9:12:56,  6.67s/it]                                                        23%|       | 1527/6500 [2:53:39<9:12:56,  6.67s/it] 24%|       | 1528/6500 [2:53:46<9:10:53,  6.65s/it]                                                        24%|       | 1528/6500 [2:53:46<9:10:53,  6.65s/it] 24%|       | 1529/6500 [2:53:53<9:09:10,  6.63s/it]                                                        24%|       | 1529/6500 [2:53:53<9:09:10,  6.63s/it] 24%|       | 1530/6500 [2:53:59<9:08:06,  6.62s/it]                                                        24%|       | 1530/6500 [2:53:59<9:08:06,  6.62s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8440078496932983, 'eval_runtime': 1.4862, 'eval_samples_per_second': 8.074, 'eval_steps_per_second': 2.019, 'epoch': 0.24}
                                                        24%|       | 1530/6500 [2:54:01<9:08:06,  6.62s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-1530
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1530
/projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1530
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1530
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1530
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1530/pytorch_model.bin
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1530/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1530/pytorch_model.binthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1530/pytorch_model.bin

/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.5763, 'learning_rate': 8.694948786929886e-05, 'epoch': 0.24}
{'loss': 0.8548, 'learning_rate': 8.693319743807116e-05, 'epoch': 0.24}
{'loss': 0.6008, 'learning_rate': 8.691689837393311e-05, 'epoch': 0.24}
{'loss': 0.5681, 'learning_rate': 8.690059068069454e-05, 'epoch': 0.24}
{'loss': 0.594, 'learning_rate': 8.688427436216724e-05, 'epoch': 0.24}
 24%|       | 1531/6500 [2:54:08<9:51:17,  7.14s/it]                                                        24%|       | 1531/6500 [2:54:08<9:51:17,  7.14s/it] 24%|       | 1532/6500 [2:54:14<9:37:19,  6.97s/it]                                                        24%|       | 1532/6500 [2:54:14<9:37:19,  6.97s/it] 24%|       | 1533/6500 [2:54:21<9:27:42,  6.86s/it]                                                        24%|       | 1533/6500 [2:54:21<9:27:42,  6.86s/it] 24%|       | 1534/6500 [2:54:27<9:20:46,  6.78s/it]                                                        24%|       | 1534/6500 [2:54:27<9:20:46,  6.78s/it] 24%|       | 1535/6500 [2:54:34<9:16:13,  6.72s/it]                                                        24%|       | 1535/6500 [2:54:34<9:16:13,  6.72s/it] 24%|       | 1536/6500 [2:54:41<9:28:03,  6.87s/it]                                                       {'loss': 0.5606, 'learning_rate': 8.686794942216508e-05, 'epoch': 0.24}
{'loss': 0.5704, 'learning_rate': 8.685161586450388e-05, 'epoch': 0.24}
{'loss': 0.5881, 'learning_rate': 8.683527369300156e-05, 'epoch': 0.24}
{'loss': 0.5929, 'learning_rate': 8.681892291147798e-05, 'epoch': 0.24}
{'loss': 0.5883, 'learning_rate': 8.6802563523755e-05, 'epoch': 0.24}
 24%|       | 1536/6500 [2:54:41<9:28:03,  6.87s/it] 24%|       | 1537/6500 [2:54:48<9:21:02,  6.78s/it]                                                        24%|       | 1537/6500 [2:54:48<9:21:02,  6.78s/it] 24%|       | 1538/6500 [2:54:54<9:15:53,  6.72s/it]                                                        24%|       | 1538/6500 [2:54:54<9:15:53,  6.72s/it] 24%|       | 1539/6500 [2:55:01<9:12:15,  6.68s/it]                                                        24%|       | 1539/6500 [2:55:01<9:12:15,  6.68s/it] 24%|       | 1540/6500 [2:55:07<9:10:07,  6.65s/it]                                                        24%|       | 1540/6500 [2:55:07<9:10:07,  6.65s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.841295599937439, 'eval_runtime': 1.5016, 'eval_samples_per_second': 7.991, 'eval_steps_per_second': 1.998, 'epoch': 0.24}
                                                        24%|       | 1540/6500 [2:55:09<9:10:07,  6.65s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-1540
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1540
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1540
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1540
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1540
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1540/pytorch_model.bin
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1540/pytorch_model.bin /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1540/pytorch_model.bin

the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1540/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.5924, 'learning_rate': 8.678619553365659e-05, 'epoch': 0.24}
{'loss': 0.5808, 'learning_rate': 8.676981894500862e-05, 'epoch': 0.24}
{'loss': 0.5804, 'learning_rate': 8.675343376163905e-05, 'epoch': 0.24}
{'loss': 0.571, 'learning_rate': 8.673703998737778e-05, 'epoch': 0.24}
{'loss': 0.6172, 'learning_rate': 8.67206376260568e-05, 'epoch': 0.24}
 24%|       | 1541/6500 [2:55:16<9:55:04,  7.20s/it]                                                        24%|       | 1541/6500 [2:55:16<9:55:04,  7.20s/it] 24%|       | 1542/6500 [2:55:22<9:39:39,  7.01s/it]                                                        24%|       | 1542/6500 [2:55:22<9:39:39,  7.01s/it] 24%|       | 1543/6500 [2:55:29<9:28:52,  6.89s/it]                                                        24%|       | 1543/6500 [2:55:29<9:28:52,  6.89s/it] 24%|       | 1544/6500 [2:55:36<9:21:11,  6.79s/it]                                                        24%|       | 1544/6500 [2:55:36<9:21:11,  6.79s/it] 24%|       | 1545/6500 [2:55:42<9:15:45,  6.73s/it]                                                        24%|       | 1545/6500 [2:55:42<9:15:45,  6.73s/it] 24%|       | 1546/6500 [2:55:49<9:11:51,  6.68s/it]                                                       {'loss': 0.5839, 'learning_rate': 8.670422668151003e-05, 'epoch': 0.24}
{'loss': 0.8493, 'learning_rate': 8.668780715757345e-05, 'epoch': 0.24}
{'loss': 0.5948, 'learning_rate': 8.6671379058085e-05, 'epoch': 0.24}
{'loss': 0.5817, 'learning_rate': 8.665494238688467e-05, 'epoch': 0.24}
{'loss': 0.571, 'learning_rate': 8.663849714781442e-05, 'epoch': 0.24}
 24%|       | 1546/6500 [2:55:49<9:11:51,  6.68s/it] 24%|       | 1547/6500 [2:55:55<9:09:02,  6.65s/it]                                                        24%|       | 1547/6500 [2:55:55<9:09:02,  6.65s/it] 24%|       | 1548/6500 [2:56:02<9:06:59,  6.63s/it]                                                        24%|       | 1548/6500 [2:56:02<9:06:59,  6.63s/it] 24%|       | 1549/6500 [2:56:08<9:05:17,  6.61s/it]                                                        24%|       | 1549/6500 [2:56:08<9:05:17,  6.61s/it] 24%|       | 1550/6500 [2:56:15<9:04:24,  6.60s/it]                                                        24%|       | 1550/6500 [2:56:15<9:04:24,  6.60s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8422642350196838, 'eval_runtime': 1.4888, 'eval_samples_per_second': 8.06, 'eval_steps_per_second': 2.015, 'epoch': 0.24}
                                                        24%|       | 1550/6500 [2:56:17<9:04:24,  6.60s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-1550
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1550I AM HERE AND I AM SAVING THE MODEL

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1550
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1550
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1550
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1550/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1550/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1550/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1550/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.5701, 'learning_rate': 8.662204334471822e-05, 'epoch': 0.24}
{'loss': 0.5682, 'learning_rate': 8.660558098144206e-05, 'epoch': 0.24}
{'loss': 0.5791, 'learning_rate': 8.658911006183391e-05, 'epoch': 0.24}
{'loss': 0.6041, 'learning_rate': 8.657263058974375e-05, 'epoch': 0.24}
{'loss': 0.5947, 'learning_rate': 8.655614256902355e-05, 'epoch': 0.24}
 24%|       | 1551/6500 [2:56:24<9:50:20,  7.16s/it]                                                        24%|       | 1551/6500 [2:56:24<9:50:20,  7.16s/it] 24%|       | 1552/6500 [2:56:31<9:58:02,  7.25s/it]                                                        24%|       | 1552/6500 [2:56:31<9:58:02,  7.25s/it] 24%|       | 1553/6500 [2:56:38<9:41:04,  7.05s/it]                                                        24%|       | 1553/6500 [2:56:38<9:41:04,  7.05s/it] 24%|       | 1554/6500 [2:56:44<9:29:38,  6.91s/it]                                                        24%|       | 1554/6500 [2:56:44<9:29:38,  6.91s/it] 24%|       | 1555/6500 [2:56:51<9:21:37,  6.81s/it]                                                        24%|       | 1555/6500 [2:56:51<9:21:37,  6.81s/it] 24%|       | 1556/6500 [2:56:57<9:15:58,  6.75s/it]                                                       {'loss': 0.5834, 'learning_rate': 8.65396460035273e-05, 'epoch': 0.24}
{'loss': 0.5823, 'learning_rate': 8.652314089711095e-05, 'epoch': 0.24}
{'loss': 0.57, 'learning_rate': 8.650662725363249e-05, 'epoch': 0.24}
{'loss': 0.5739, 'learning_rate': 8.649010507695187e-05, 'epoch': 0.24}
{'loss': 0.6138, 'learning_rate': 8.647357437093105e-05, 'epoch': 0.24}
 24%|       | 1556/6500 [2:56:57<9:15:58,  6.75s/it] 24%|       | 1557/6500 [2:57:04<9:11:49,  6.70s/it]                                                        24%|       | 1557/6500 [2:57:04<9:11:49,  6.70s/it] 24%|       | 1558/6500 [2:57:11<9:09:32,  6.67s/it]                                                        24%|       | 1558/6500 [2:57:11<9:09:32,  6.67s/it] 24%|       | 1559/6500 [2:57:17<9:07:16,  6.65s/it]                                                        24%|       | 1559/6500 [2:57:17<9:07:16,  6.65s/it] 24%|       | 1560/6500 [2:57:24<9:05:38,  6.63s/it]                                                        24%|       | 1560/6500 [2:57:24<9:05:38,  6.63s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.840571939945221, 'eval_runtime': 1.4906, 'eval_samples_per_second': 8.05, 'eval_steps_per_second': 2.013, 'epoch': 0.24}
                                                        24%|       | 1560/6500 [2:57:25<9:05:38,  6.63s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-1560
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1560
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1560
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1560
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1560
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1560/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1560/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1560/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1560/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.5789, 'learning_rate': 8.645703513943397e-05, 'epoch': 0.24}
{'loss': 0.8549, 'learning_rate': 8.64404873863266e-05, 'epoch': 0.24}
{'loss': 0.5902, 'learning_rate': 8.642393111547687e-05, 'epoch': 0.24}
{'loss': 0.5847, 'learning_rate': 8.64073663307547e-05, 'epoch': 0.24}
{'loss': 0.5627, 'learning_rate': 8.639079303603202e-05, 'epoch': 0.24}
 24%|       | 1561/6500 [2:57:32<9:49:10,  7.16s/it]                                                        24%|       | 1561/6500 [2:57:32<9:49:10,  7.16s/it] 24%|       | 1562/6500 [2:57:39<9:34:42,  6.98s/it]                                                        24%|       | 1562/6500 [2:57:39<9:34:42,  6.98s/it] 24%|       | 1563/6500 [2:57:45<9:24:38,  6.86s/it]                                                        24%|       | 1563/6500 [2:57:45<9:24:38,  6.86s/it] 24%|       | 1564/6500 [2:57:52<9:17:18,  6.77s/it]                                                        24%|       | 1564/6500 [2:57:52<9:17:18,  6.77s/it] 24%|       | 1565/6500 [2:57:58<9:12:16,  6.71s/it]                                                        24%|       | 1565/6500 [2:57:58<9:12:16,  6.71s/it] 24%|       | 1566/6500 [2:58:05<9:09:18,  6.68s/it]                                                       {'loss': 0.5528, 'learning_rate': 8.637421123518272e-05, 'epoch': 0.24}
{'loss': 0.5629, 'learning_rate': 8.635762093208269e-05, 'epoch': 0.24}
{'loss': 0.5862, 'learning_rate': 8.634102213060984e-05, 'epoch': 0.24}
{'loss': 0.6002, 'learning_rate': 8.632441483464402e-05, 'epoch': 0.24}
{'loss': 0.582, 'learning_rate': 8.630779904806709e-05, 'epoch': 0.24}
 24%|       | 1566/6500 [2:58:05<9:09:18,  6.68s/it] 24%|       | 1567/6500 [2:58:12<9:07:08,  6.65s/it]                                                        24%|       | 1567/6500 [2:58:12<9:07:08,  6.65s/it] 24%|       | 1568/6500 [2:58:19<9:26:11,  6.89s/it]                                                        24%|       | 1568/6500 [2:58:19<9:26:11,  6.89s/it] 24%|       | 1569/6500 [2:58:26<9:18:39,  6.80s/it]                                                        24%|       | 1569/6500 [2:58:26<9:18:39,  6.80s/it] 24%|       | 1570/6500 [2:58:32<9:13:39,  6.74s/it]                                                        24%|       | 1570/6500 [2:58:32<9:13:39,  6.74s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.842470109462738, 'eval_runtime': 1.4938, 'eval_samples_per_second': 8.033, 'eval_steps_per_second': 2.008, 'epoch': 0.24}
                                                        24%|       | 1570/6500 [2:58:34<9:13:39,  6.74s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-1570
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1570
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1570
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1570
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1570
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1570/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1570/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1570/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1570/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.5798, 'learning_rate': 8.629117477476289e-05, 'epoch': 0.24}
{'loss': 0.5767, 'learning_rate': 8.627454201861724e-05, 'epoch': 0.24}
{'loss': 0.5661, 'learning_rate': 8.625790078351794e-05, 'epoch': 0.24}
{'loss': 0.5747, 'learning_rate': 8.624125107335478e-05, 'epoch': 0.24}
{'loss': 0.6021, 'learning_rate': 8.622459289201954e-05, 'epoch': 0.24}
 24%|       | 1571/6500 [2:58:41<9:55:26,  7.25s/it]                                                        24%|       | 1571/6500 [2:58:41<9:55:26,  7.25s/it] 24%|       | 1572/6500 [2:58:47<9:39:16,  7.05s/it]                                                        24%|       | 1572/6500 [2:58:47<9:39:16,  7.05s/it] 24%|       | 1573/6500 [2:58:54<9:27:54,  6.92s/it]                                                        24%|       | 1573/6500 [2:58:54<9:27:54,  6.92s/it] 24%|       | 1574/6500 [2:59:00<9:19:46,  6.82s/it]                                                        24%|       | 1574/6500 [2:59:00<9:19:46,  6.82s/it] 24%|       | 1575/6500 [2:59:07<9:14:06,  6.75s/it]                                                        24%|       | 1575/6500 [2:59:07<9:14:06,  6.75s/it] 24%|       | 1576/6500 [2:59:14<9:09:56,  6.70s/it]                                                       {'loss': 0.5737, 'learning_rate': 8.620792624340596e-05, 'epoch': 0.24}
{'loss': 0.8552, 'learning_rate': 8.619125113140975e-05, 'epoch': 0.24}
{'loss': 0.5648, 'learning_rate': 8.617456755992867e-05, 'epoch': 0.24}
{'loss': 0.5917, 'learning_rate': 8.615787553286234e-05, 'epoch': 0.24}
{'loss': 0.5588, 'learning_rate': 8.614117505411246e-05, 'epoch': 0.24}
 24%|       | 1576/6500 [2:59:14<9:09:56,  6.70s/it] 24%|       | 1577/6500 [2:59:20<9:06:48,  6.66s/it]                                                        24%|       | 1577/6500 [2:59:20<9:06:48,  6.66s/it] 24%|       | 1578/6500 [2:59:27<9:04:24,  6.64s/it]                                                        24%|       | 1578/6500 [2:59:27<9:04:24,  6.64s/it] 24%|       | 1579/6500 [2:59:33<9:03:11,  6.62s/it]                                                        24%|       | 1579/6500 [2:59:33<9:03:11,  6.62s/it] 24%|       | 1580/6500 [2:59:40<9:02:12,  6.61s/it]                                                        24%|       | 1580/6500 [2:59:40<9:02:12,  6.61s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8446318507194519, 'eval_runtime': 1.4883, 'eval_samples_per_second': 8.063, 'eval_steps_per_second': 2.016, 'epoch': 0.24}
                                                        24%|       | 1580/6500 [2:59:41<9:02:12,  6.61s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-1580
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1580I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in 
 /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1580
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1580
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1580
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1580/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1580/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1580/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1580/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.5507, 'learning_rate': 8.612446612758265e-05, 'epoch': 0.24}
{'loss': 0.5649, 'learning_rate': 8.610774875717851e-05, 'epoch': 0.24}
{'loss': 0.57, 'learning_rate': 8.609102294680766e-05, 'epoch': 0.24}
{'loss': 0.5948, 'learning_rate': 8.607428870037962e-05, 'epoch': 0.24}
{'loss': 0.5836, 'learning_rate': 8.605754602180594e-05, 'epoch': 0.24}
 24%|       | 1581/6500 [2:59:48<9:45:32,  7.14s/it]                                                        24%|       | 1581/6500 [2:59:48<9:45:32,  7.14s/it] 24%|       | 1582/6500 [2:59:55<9:31:59,  6.98s/it]                                                        24%|       | 1582/6500 [2:59:55<9:31:59,  6.98s/it] 24%|       | 1583/6500 [3:00:02<9:22:17,  6.86s/it]                                                        24%|       | 1583/6500 [3:00:02<9:22:17,  6.86s/it] 24%|       | 1584/6500 [3:00:09<9:41:01,  7.09s/it]                                                        24%|       | 1584/6500 [3:00:09<9:41:01,  7.09s/it] 24%|       | 1585/6500 [3:00:16<9:28:47,  6.94s/it]                                                        24%|       | 1585/6500 [3:00:16<9:28:47,  6.94s/it] 24%|       | 1586/6500 [3:00:22<9:20:01,  6.84s/it]                                                       {'loss': 0.5746, 'learning_rate': 8.60407949150001e-05, 'epoch': 0.24}
{'loss': 0.5795, 'learning_rate': 8.602403538387758e-05, 'epoch': 0.24}
{'loss': 0.5729, 'learning_rate': 8.600726743235583e-05, 'epoch': 0.24}
{'loss': 0.5801, 'learning_rate': 8.599049106435424e-05, 'epoch': 0.24}
{'loss': 0.5816, 'learning_rate': 8.59737062837942e-05, 'epoch': 0.24}
 24%|       | 1586/6500 [3:00:22<9:20:01,  6.84s/it] 24%|       | 1587/6500 [3:00:29<9:13:57,  6.77s/it]                                                        24%|       | 1587/6500 [3:00:29<9:13:57,  6.77s/it] 24%|       | 1588/6500 [3:00:36<9:09:20,  6.71s/it]                                                        24%|       | 1588/6500 [3:00:36<9:09:20,  6.71s/it] 24%|       | 1589/6500 [3:00:42<9:06:15,  6.67s/it]                                                        24%|       | 1589/6500 [3:00:42<9:06:15,  6.67s/it] 24%|       | 1590/6500 [3:00:49<9:04:13,  6.65s/it]                                                        24%|       | 1590/6500 [3:00:49<9:04:13,  6.65s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8414657711982727, 'eval_runtime': 1.4896, 'eval_samples_per_second': 8.056, 'eval_steps_per_second': 2.014, 'epoch': 0.24}
                                                        24%|       | 1590/6500 [3:00:50<9:04:13,  6.65s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-1590
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1590
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1590
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1590
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1590
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1590/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1590/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1590/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1590/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.5765, 'learning_rate': 8.595691309459901e-05, 'epoch': 0.24}
{'loss': 0.8665, 'learning_rate': 8.594011150069403e-05, 'epoch': 0.24}
{'loss': 0.5568, 'learning_rate': 8.59233015060065e-05, 'epoch': 0.25}
{'loss': 0.5938, 'learning_rate': 8.590648311446567e-05, 'epoch': 0.25}
{'loss': 0.5455, 'learning_rate': 8.58896563300027e-05, 'epoch': 0.25}
 24%|       | 1591/6500 [3:00:57<9:47:47,  7.18s/it]                                                        24%|       | 1591/6500 [3:00:57<9:47:47,  7.18s/it] 24%|       | 1592/6500 [3:01:04<9:32:42,  7.00s/it]                                                        24%|       | 1592/6500 [3:01:04<9:32:42,  7.00s/it] 25%|       | 1593/6500 [3:01:10<9:22:21,  6.88s/it]                                                        25%|       | 1593/6500 [3:01:10<9:22:21,  6.88s/it] 25%|       | 1594/6500 [3:01:17<9:14:48,  6.79s/it]                                                        25%|       | 1594/6500 [3:01:17<9:14:48,  6.79s/it] 25%|       | 1595/6500 [3:01:23<9:09:38,  6.72s/it]                                                        25%|       | 1595/6500 [3:01:23<9:09:38,  6.72s/it] 25%|       | 1596/6500 [3:01:30<9:06:00,  6.68s/it]                                                       {'loss': 0.5572, 'learning_rate': 8.58728211565508e-05, 'epoch': 0.25}
{'loss': 0.5737, 'learning_rate': 8.585597759804506e-05, 'epoch': 0.25}
{'loss': 0.5711, 'learning_rate': 8.583912565842257e-05, 'epoch': 0.25}
{'loss': 0.5946, 'learning_rate': 8.582226534162235e-05, 'epoch': 0.25}
{'loss': 0.5881, 'learning_rate': 8.580539665158542e-05, 'epoch': 0.25}
 25%|       | 1596/6500 [3:01:30<9:06:00,  6.68s/it] 25%|       | 1597/6500 [3:01:37<9:03:25,  6.65s/it]                                                        25%|       | 1597/6500 [3:01:37<9:03:25,  6.65s/it] 25%|       | 1598/6500 [3:01:43<9:01:10,  6.62s/it]                                                        25%|       | 1598/6500 [3:01:43<9:01:10,  6.62s/it] 25%|       | 1599/6500 [3:01:50<8:59:17,  6.60s/it]                                                        25%|       | 1599/6500 [3:01:50<8:59:17,  6.60s/it] 25%|       | 1600/6500 [3:01:57<9:19:57,  6.86s/it]                                                        25%|       | 1600/6500 [3:01:57<9:19:57,  6.86s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8417582511901855, 'eval_runtime': 1.5029, 'eval_samples_per_second': 7.985, 'eval_steps_per_second': 1.996, 'epoch': 0.25}
                                                        25%|       | 1600/6500 [3:01:59<9:19:57,  6.86s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-1600
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1600I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1600

I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1600
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1600
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1600/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1600/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1600/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1600/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.5561, 'learning_rate': 8.578851959225472e-05, 'epoch': 0.25}
{'loss': 0.5762, 'learning_rate': 8.577163416757517e-05, 'epoch': 0.25}
{'loss': 0.5568, 'learning_rate': 8.57547403814936e-05, 'epoch': 0.25}
{'loss': 0.6112, 'learning_rate': 8.573783823795889e-05, 'epoch': 0.25}
{'loss': 0.5591, 'learning_rate': 8.572092774092176e-05, 'epoch': 0.25}
 25%|       | 1601/6500 [3:02:05<9:56:12,  7.30s/it]                                                        25%|       | 1601/6500 [3:02:05<9:56:12,  7.30s/it] 25%|       | 1602/6500 [3:02:12<9:38:15,  7.08s/it]                                                        25%|       | 1602/6500 [3:02:12<9:38:15,  7.08s/it] 25%|       | 1603/6500 [3:02:19<9:24:54,  6.92s/it]                                                        25%|       | 1603/6500 [3:02:19<9:24:54,  6.92s/it] 25%|       | 1604/6500 [3:02:25<9:15:26,  6.81s/it]                                                        25%|       | 1604/6500 [3:02:25<9:15:26,  6.81s/it] 25%|       | 1605/6500 [3:02:32<9:08:56,  6.73s/it]                                                        25%|       | 1605/6500 [3:02:32<9:08:56,  6.73s/it] 25%|       | 1606/6500 [3:02:38<9:04:24,  6.67s/it]                                                       {'loss': 0.8454, 'learning_rate': 8.570400889433497e-05, 'epoch': 0.25}
{'loss': 0.5883, 'learning_rate': 8.568708170215319e-05, 'epoch': 0.25}
{'loss': 0.5596, 'learning_rate': 8.567014616833302e-05, 'epoch': 0.25}
{'loss': 0.5812, 'learning_rate': 8.56532022968331e-05, 'epoch': 0.25}
{'loss': 0.553, 'learning_rate': 8.563625009161387e-05, 'epoch': 0.25}
 25%|       | 1606/6500 [3:02:38<9:04:24,  6.67s/it] 25%|       | 1607/6500 [3:02:45<9:01:07,  6.64s/it]                                                        25%|       | 1607/6500 [3:02:45<9:01:07,  6.64s/it] 25%|       | 1608/6500 [3:02:51<8:58:57,  6.61s/it]                                                        25%|       | 1608/6500 [3:02:51<8:58:57,  6.61s/it] 25%|       | 1609/6500 [3:02:58<8:57:06,  6.59s/it]                                                        25%|       | 1609/6500 [3:02:58<8:57:06,  6.59s/it] 25%|       | 1610/6500 [3:03:04<8:56:09,  6.58s/it]                                                        25%|       | 1610/6500 [3:03:04<8:56:09,  6.58s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8441477417945862, 'eval_runtime': 1.4985, 'eval_samples_per_second': 8.008, 'eval_steps_per_second': 2.002, 'epoch': 0.25}
                                                        25%|       | 1610/6500 [3:03:06<8:56:09,  6.58s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-1610
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1610
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1610
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1610
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1610
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1610/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1610/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1610/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1610/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.5622, 'learning_rate': 8.56192895566379e-05, 'epoch': 0.25}
{'loss': 0.5767, 'learning_rate': 8.560232069586953e-05, 'epoch': 0.25}
{'loss': 0.5791, 'learning_rate': 8.558534351327517e-05, 'epoch': 0.25}
{'loss': 0.5866, 'learning_rate': 8.556835801282314e-05, 'epoch': 0.25}
{'loss': 0.5769, 'learning_rate': 8.555136419848368e-05, 'epoch': 0.25}
 25%|       | 1611/6500 [3:03:13<9:40:31,  7.12s/it]                                                        25%|       | 1611/6500 [3:03:13<9:40:31,  7.12s/it] 25%|       | 1612/6500 [3:03:19<9:26:31,  6.95s/it]                                                        25%|       | 1612/6500 [3:03:19<9:26:31,  6.95s/it] 25%|       | 1613/6500 [3:03:26<9:16:40,  6.83s/it]                                                        25%|       | 1613/6500 [3:03:26<9:16:40,  6.83s/it] 25%|       | 1614/6500 [3:03:32<9:09:39,  6.75s/it]                                                        25%|       | 1614/6500 [3:03:32<9:09:39,  6.75s/it] 25%|       | 1615/6500 [3:03:39<9:04:47,  6.69s/it]                                                        25%|       | 1615/6500 [3:03:39<9:04:47,  6.69s/it] 25%|       | 1616/6500 [3:03:46<9:01:34,  6.65s/it]                                                       {'loss': 0.5648, 'learning_rate': 8.553436207422898e-05, 'epoch': 0.25}
{'loss': 0.5646, 'learning_rate': 8.55173516440332e-05, 'epoch': 0.25}
{'loss': 0.5563, 'learning_rate': 8.550033291187243e-05, 'epoch': 0.25}
{'loss': 0.6059, 'learning_rate': 8.548330588172469e-05, 'epoch': 0.25}
{'loss': 0.5625, 'learning_rate': 8.546627055756994e-05, 'epoch': 0.25}
 25%|       | 1616/6500 [3:03:46<9:01:34,  6.65s/it] 25%|       | 1617/6500 [3:03:53<9:13:52,  6.81s/it]                                                        25%|       | 1617/6500 [3:03:53<9:13:52,  6.81s/it] 25%|       | 1618/6500 [3:03:59<9:07:21,  6.73s/it]                                                        25%|       | 1618/6500 [3:03:59<9:07:21,  6.73s/it] 25%|       | 1619/6500 [3:04:06<9:02:56,  6.67s/it]                                                        25%|       | 1619/6500 [3:04:06<9:02:56,  6.67s/it] 25%|       | 1620/6500 [3:04:12<9:00:15,  6.64s/it]                                                        25%|       | 1620/6500 [3:04:12<9:00:15,  6.64s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.838802695274353, 'eval_runtime': 1.495, 'eval_samples_per_second': 8.027, 'eval_steps_per_second': 2.007, 'epoch': 0.25}
                                                        25%|       | 1620/6500 [3:04:14<9:00:15,  6.64s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-1620
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1620
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1620
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1620
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1620
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1620/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1620/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1620/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1620/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.8471, 'learning_rate': 8.544922694339008e-05, 'epoch': 0.25}
{'loss': 0.588, 'learning_rate': 8.543217504316896e-05, 'epoch': 0.25}
{'loss': 0.5463, 'learning_rate': 8.541511486089237e-05, 'epoch': 0.25}
{'loss': 0.5799, 'learning_rate': 8.539804640054798e-05, 'epoch': 0.25}
{'loss': 0.5459, 'learning_rate': 8.538096966612548e-05, 'epoch': 0.25}
 25%|       | 1621/6500 [3:04:21<9:43:35,  7.18s/it]                                                        25%|       | 1621/6500 [3:04:21<9:43:35,  7.18s/it] 25%|       | 1622/6500 [3:04:27<9:28:10,  6.99s/it]                                                        25%|       | 1622/6500 [3:04:27<9:28:10,  6.99s/it] 25%|       | 1623/6500 [3:04:34<9:17:41,  6.86s/it]                                                        25%|       | 1623/6500 [3:04:34<9:17:41,  6.86s/it] 25%|       | 1624/6500 [3:04:41<9:09:56,  6.77s/it]                                                        25%|       | 1624/6500 [3:04:41<9:09:56,  6.77s/it] 25%|       | 1625/6500 [3:04:47<9:04:42,  6.70s/it]                                                        25%|       | 1625/6500 [3:04:47<9:04:42,  6.70s/it] 25%|       | 1626/6500 [3:04:54<9:01:16,  6.66s/it]                                                       {'loss': 0.556, 'learning_rate': 8.536388466161645e-05, 'epoch': 0.25}
{'loss': 0.5708, 'learning_rate': 8.534679139101439e-05, 'epoch': 0.25}
{'loss': 0.58, 'learning_rate': 8.532968985831476e-05, 'epoch': 0.25}
{'loss': 0.5673, 'learning_rate': 8.531258006751492e-05, 'epoch': 0.25}
{'loss': 0.5699, 'learning_rate': 8.529546202261421e-05, 'epoch': 0.25}
 25%|       | 1626/6500 [3:04:54<9:01:16,  6.66s/it] 25%|       | 1627/6500 [3:05:00<8:58:31,  6.63s/it]                                                        25%|       | 1627/6500 [3:05:00<8:58:31,  6.63s/it] 25%|       | 1628/6500 [3:05:07<8:56:39,  6.61s/it]                                                        25%|       | 1628/6500 [3:05:07<8:56:39,  6.61s/it] 25%|       | 1629/6500 [3:05:13<8:55:23,  6.59s/it]                                                        25%|       | 1629/6500 [3:05:13<8:55:23,  6.59s/it] 25%|       | 1630/6500 [3:05:20<8:54:15,  6.58s/it]                                                        25%|       | 1630/6500 [3:05:20<8:54:15,  6.58s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8405555486679077, 'eval_runtime': 1.488, 'eval_samples_per_second': 8.064, 'eval_steps_per_second': 2.016, 'epoch': 0.25}
                                                        25%|       | 1630/6500 [3:05:21<8:54:15,  6.58s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-1630
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1630
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1630
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1630
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1630
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1630/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1630/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1630/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1630/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.5623, 'learning_rate': 8.527833572761383e-05, 'epoch': 0.25}
{'loss': 0.5757, 'learning_rate': 8.526120118651699e-05, 'epoch': 0.25}
{'loss': 0.5615, 'learning_rate': 8.524405840332875e-05, 'epoch': 0.25}
{'loss': 0.6077, 'learning_rate': 8.522690738205617e-05, 'epoch': 0.25}
{'loss': 0.5638, 'learning_rate': 8.520974812670814e-05, 'epoch': 0.25}
 25%|       | 1631/6500 [3:05:28<9:37:31,  7.12s/it]                                                        25%|       | 1631/6500 [3:05:28<9:37:31,  7.12s/it] 25%|       | 1632/6500 [3:05:35<9:23:35,  6.95s/it]                                                        25%|       | 1632/6500 [3:05:35<9:23:35,  6.95s/it] 25%|       | 1633/6500 [3:05:42<9:34:52,  7.09s/it]                                                        25%|       | 1633/6500 [3:05:42<9:34:52,  7.09s/it] 25%|       | 1634/6500 [3:05:49<9:21:50,  6.93s/it]                                                        25%|       | 1634/6500 [3:05:49<9:21:50,  6.93s/it] 25%|       | 1635/6500 [3:05:55<9:12:46,  6.82s/it]                                                        25%|       | 1635/6500 [3:05:55<9:12:46,  6.82s/it] 25%|       | 1636/6500 [3:06:02<9:06:20,  6.74s/it]                                                       {'loss': 0.8446, 'learning_rate': 8.519258064129558e-05, 'epoch': 0.25}
{'loss': 0.5818, 'learning_rate': 8.517540492983127e-05, 'epoch': 0.25}
{'loss': 0.5604, 'learning_rate': 8.515822099632993e-05, 'epoch': 0.25}
{'loss': 0.5585, 'learning_rate': 8.514102884480819e-05, 'epoch': 0.25}
{'loss': 0.5477, 'learning_rate': 8.512382847928461e-05, 'epoch': 0.25}
 25%|       | 1636/6500 [3:06:02<9:06:20,  6.74s/it] 25%|       | 1637/6500 [3:06:08<9:01:38,  6.68s/it]                                                        25%|       | 1637/6500 [3:06:08<9:01:38,  6.68s/it] 25%|       | 1638/6500 [3:06:15<8:58:21,  6.64s/it]                                                        25%|       | 1638/6500 [3:06:15<8:58:21,  6.64s/it] 25%|       | 1639/6500 [3:06:22<8:56:14,  6.62s/it]                                                        25%|       | 1639/6500 [3:06:22<8:56:14,  6.62s/it] 25%|       | 1640/6500 [3:06:28<8:54:41,  6.60s/it]                                                        25%|       | 1640/6500 [3:06:28<8:54:41,  6.60s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.843742311000824, 'eval_runtime': 1.491, 'eval_samples_per_second': 8.049, 'eval_steps_per_second': 2.012, 'epoch': 0.25}
                                                        25%|       | 1640/6500 [3:06:30<8:54:41,  6.60s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-1640
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1640I AM HERE AND I AM SAVING THE MODEL

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1640
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1640
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1640
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1640/pytorch_model.bin
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1640/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1640/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1640/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.5562, 'learning_rate': 8.510661990377971e-05, 'epoch': 0.25}
{'loss': 0.5714, 'learning_rate': 8.508940312231584e-05, 'epoch': 0.25}
{'loss': 0.5916, 'learning_rate': 8.507217813891733e-05, 'epoch': 0.25}
{'loss': 0.5786, 'learning_rate': 8.505494495761043e-05, 'epoch': 0.25}
{'loss': 0.5654, 'learning_rate': 8.503770358242329e-05, 'epoch': 0.25}
 25%|       | 1641/6500 [3:06:36<9:37:50,  7.14s/it]                                                        25%|       | 1641/6500 [3:06:36<9:37:50,  7.14s/it] 25%|       | 1642/6500 [3:06:43<9:23:29,  6.96s/it]                                                        25%|       | 1642/6500 [3:06:43<9:23:29,  6.96s/it] 25%|       | 1643/6500 [3:06:50<9:13:48,  6.84s/it]                                                        25%|       | 1643/6500 [3:06:50<9:13:48,  6.84s/it] 25%|       | 1644/6500 [3:06:56<9:06:50,  6.76s/it]                                                        25%|       | 1644/6500 [3:06:56<9:06:50,  6.76s/it] 25%|       | 1645/6500 [3:07:03<9:02:11,  6.70s/it]                                                        25%|       | 1645/6500 [3:07:03<9:02:11,  6.70s/it] 25%|       | 1646/6500 [3:07:09<8:58:14,  6.65s/it]                                                       {'loss': 0.5705, 'learning_rate': 8.502045401738595e-05, 'epoch': 0.25}
{'loss': 0.562, 'learning_rate': 8.500319626653042e-05, 'epoch': 0.25}
{'loss': 0.5632, 'learning_rate': 8.49859303338906e-05, 'epoch': 0.25}
{'loss': 0.5837, 'learning_rate': 8.496865622350227e-05, 'epoch': 0.25}
{'loss': 0.5647, 'learning_rate': 8.495137393940317e-05, 'epoch': 0.25}
 25%|       | 1646/6500 [3:07:09<8:58:14,  6.65s/it] 25%|       | 1647/6500 [3:07:16<8:55:50,  6.62s/it]                                                        25%|       | 1647/6500 [3:07:16<8:55:50,  6.62s/it] 25%|       | 1648/6500 [3:07:22<8:54:11,  6.61s/it]                                                        25%|       | 1648/6500 [3:07:22<8:54:11,  6.61s/it] 25%|       | 1649/6500 [3:07:30<9:13:47,  6.85s/it]                                                        25%|       | 1649/6500 [3:07:30<9:13:47,  6.85s/it] 25%|       | 1650/6500 [3:07:36<9:06:55,  6.77s/it]                                                        25%|       | 1650/6500 [3:07:36<9:06:55,  6.77s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8405213356018066, 'eval_runtime': 1.4891, 'eval_samples_per_second': 8.059, 'eval_steps_per_second': 2.015, 'epoch': 0.25}
                                                        25%|       | 1650/6500 [3:07:38<9:06:55,  6.77s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-1650
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1650I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1650

I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1650
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1650
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1650/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1650/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1650/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1650/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.8461, 'learning_rate': 8.493408348563291e-05, 'epoch': 0.25}
{'loss': 0.5536, 'learning_rate': 8.491678486623304e-05, 'epoch': 0.25}
{'loss': 0.5782, 'learning_rate': 8.489947808524701e-05, 'epoch': 0.25}
{'loss': 0.5517, 'learning_rate': 8.488216314672018e-05, 'epoch': 0.25}
{'loss': 0.54, 'learning_rate': 8.486484005469977e-05, 'epoch': 0.25}
 25%|       | 1651/6500 [3:07:45<9:46:51,  7.26s/it]                                                        25%|       | 1651/6500 [3:07:45<9:46:51,  7.26s/it] 25%|       | 1652/6500 [3:07:51<9:29:35,  7.05s/it]                                                        25%|       | 1652/6500 [3:07:51<9:29:35,  7.05s/it] 25%|       | 1653/6500 [3:07:58<9:17:15,  6.90s/it]                                                        25%|       | 1653/6500 [3:07:58<9:17:15,  6.90s/it] 25%|       | 1654/6500 [3:08:04<9:08:55,  6.80s/it]                                                        25%|       | 1654/6500 [3:08:04<9:08:55,  6.80s/it] 25%|       | 1655/6500 [3:08:11<9:03:05,  6.73s/it]                                                        25%|       | 1655/6500 [3:08:11<9:03:05,  6.73s/it] 25%|       | 1656/6500 [3:08:18<8:58:44,  6.67s/it]                                                       {'loss': 0.5495, 'learning_rate': 8.4847508813235e-05, 'epoch': 0.25}
{'loss': 0.5671, 'learning_rate': 8.483016942637691e-05, 'epoch': 0.25}
{'loss': 0.5829, 'learning_rate': 8.48128218981785e-05, 'epoch': 0.26}
{'loss': 0.5698, 'learning_rate': 8.479546623269463e-05, 'epoch': 0.26}
{'loss': 0.5526, 'learning_rate': 8.47781024339821e-05, 'epoch': 0.26}
 25%|       | 1656/6500 [3:08:18<8:58:44,  6.67s/it] 25%|       | 1657/6500 [3:08:24<8:55:46,  6.64s/it]                                                        25%|       | 1657/6500 [3:08:24<8:55:46,  6.64s/it] 26%|       | 1658/6500 [3:08:31<8:53:44,  6.61s/it]                                                        26%|       | 1658/6500 [3:08:31<8:53:44,  6.61s/it] 26%|       | 1659/6500 [3:08:37<8:52:13,  6.60s/it]                                                        26%|       | 1659/6500 [3:08:37<8:52:13,  6.60s/it] 26%|       | 1660/6500 [3:08:44<8:51:12,  6.59s/it]                                                        26%|       | 1660/6500 [3:08:44<8:51:12,  6.59s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8395660519599915, 'eval_runtime': 1.4847, 'eval_samples_per_second': 8.082, 'eval_steps_per_second': 2.021, 'epoch': 0.26}
                                                        26%|       | 1660/6500 [3:08:45<8:51:12,  6.59s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-1660
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1660
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1660
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1660
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1660
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1660/pytorch_model.bin
the pytorch model path isthe pytorch model path is  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1660/pytorch_model.bin/projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1660/pytorch_model.bin

the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1660/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.5779, 'learning_rate': 8.476073050609958e-05, 'epoch': 0.26}
{'loss': 0.5519, 'learning_rate': 8.474335045310764e-05, 'epoch': 0.26}
{'loss': 0.5656, 'learning_rate': 8.472596227906876e-05, 'epoch': 0.26}
{'loss': 0.5748, 'learning_rate': 8.470856598804737e-05, 'epoch': 0.26}
{'loss': 0.5715, 'learning_rate': 8.469116158410969e-05, 'epoch': 0.26}
 26%|       | 1661/6500 [3:08:52<9:35:25,  7.13s/it]                                                        26%|       | 1661/6500 [3:08:52<9:35:25,  7.13s/it] 26%|       | 1662/6500 [3:08:59<9:21:36,  6.96s/it]                                                        26%|       | 1662/6500 [3:08:59<9:21:36,  6.96s/it] 26%|       | 1663/6500 [3:09:05<9:11:44,  6.84s/it]                                                        26%|       | 1663/6500 [3:09:05<9:11:44,  6.84s/it] 26%|       | 1664/6500 [3:09:12<9:05:06,  6.76s/it]                                                        26%|       | 1664/6500 [3:09:12<9:05:06,  6.76s/it] 26%|       | 1665/6500 [3:09:19<9:21:57,  6.97s/it]                                                        26%|       | 1665/6500 [3:09:19<9:21:57,  6.97s/it] 26%|       | 1666/6500 [3:09:26<9:12:09,  6.85s/it]                                                       {'loss': 0.8429, 'learning_rate': 8.467374907132392e-05, 'epoch': 0.26}
{'loss': 0.5552, 'learning_rate': 8.465632845376013e-05, 'epoch': 0.26}
{'loss': 0.5792, 'learning_rate': 8.463889973549027e-05, 'epoch': 0.26}
{'loss': 0.5279, 'learning_rate': 8.462146292058822e-05, 'epoch': 0.26}
{'loss': 0.5445, 'learning_rate': 8.460401801312969e-05, 'epoch': 0.26}
 26%|       | 1666/6500 [3:09:26<9:12:09,  6.85s/it] 26%|       | 1667/6500 [3:09:33<9:05:00,  6.77s/it]                                                        26%|       | 1667/6500 [3:09:33<9:05:00,  6.77s/it] 26%|       | 1668/6500 [3:09:39<9:00:22,  6.71s/it]                                                        26%|       | 1668/6500 [3:09:39<9:00:22,  6.71s/it] 26%|       | 1669/6500 [3:09:46<8:56:52,  6.67s/it]                                                        26%|       | 1669/6500 [3:09:46<8:56:52,  6.67s/it] 26%|       | 1670/6500 [3:09:52<8:54:21,  6.64s/it]                                                        26%|       | 1670/6500 [3:09:52<8:54:21,  6.64s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8435071706771851, 'eval_runtime': 1.4921, 'eval_samples_per_second': 8.042, 'eval_steps_per_second': 2.011, 'epoch': 0.26}
                                                        26%|       | 1670/6500 [3:09:54<8:54:21,  6.64s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-1670
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1670I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in 
 /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1670
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1670
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1670
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1670/pytorch_model.bin
the pytorch model path isthe pytorch model path is  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1670/pytorch_model.bin/projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1670/pytorch_model.bin

the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1670/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.5561, 'learning_rate': 8.458656501719235e-05, 'epoch': 0.26}
{'loss': 0.5489, 'learning_rate': 8.456910393685573e-05, 'epoch': 0.26}
{'loss': 0.5832, 'learning_rate': 8.455163477620126e-05, 'epoch': 0.26}
{'loss': 0.5832, 'learning_rate': 8.453415753931222e-05, 'epoch': 0.26}
{'loss': 0.5392, 'learning_rate': 8.451667223027384e-05, 'epoch': 0.26}
 26%|       | 1671/6500 [3:10:01<9:36:22,  7.16s/it]                                                        26%|       | 1671/6500 [3:10:01<9:36:22,  7.16s/it] 26%|       | 1672/6500 [3:10:07<9:22:01,  6.98s/it]                                                        26%|       | 1672/6500 [3:10:07<9:22:01,  6.98s/it] 26%|       | 1673/6500 [3:10:14<9:11:45,  6.86s/it]                                                        26%|       | 1673/6500 [3:10:14<9:11:45,  6.86s/it] 26%|       | 1674/6500 [3:10:20<9:04:38,  6.77s/it]                                                        26%|       | 1674/6500 [3:10:20<9:04:38,  6.77s/it] 26%|       | 1675/6500 [3:10:27<8:59:32,  6.71s/it]                                                        26%|       | 1675/6500 [3:10:27<8:59:32,  6.71s/it] 26%|       | 1676/6500 [3:10:33<8:56:01,  6.67s/it]                                                       {'loss': 0.5699, 'learning_rate': 8.44991788531732e-05, 'epoch': 0.26}
{'loss': 0.5468, 'learning_rate': 8.448167741209925e-05, 'epoch': 0.26}
{'loss': 0.5896, 'learning_rate': 8.446416791114284e-05, 'epoch': 0.26}
{'loss': 0.5432, 'learning_rate': 8.444665035439674e-05, 'epoch': 0.26}
{'loss': 0.8367, 'learning_rate': 8.442912474595558e-05, 'epoch': 0.26}
 26%|       | 1676/6500 [3:10:33<8:56:01,  6.67s/it] 26%|       | 1677/6500 [3:10:40<8:53:24,  6.64s/it]                                                        26%|       | 1677/6500 [3:10:40<8:53:24,  6.64s/it] 26%|       | 1678/6500 [3:10:47<8:51:36,  6.61s/it]                                                        26%|       | 1678/6500 [3:10:47<8:51:36,  6.61s/it] 26%|       | 1679/6500 [3:10:53<8:50:24,  6.60s/it]                                                        26%|       | 1679/6500 [3:10:53<8:50:24,  6.60s/it] 26%|       | 1680/6500 [3:11:00<8:49:32,  6.59s/it]                                                        26%|       | 1680/6500 [3:11:00<8:49:32,  6.59s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8408759832382202, 'eval_runtime': 1.495, 'eval_samples_per_second': 8.027, 'eval_steps_per_second': 2.007, 'epoch': 0.26}
                                                        26%|       | 1680/6500 [3:11:01<8:49:32,  6.59s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-1680
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1680
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1680
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1680
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1680
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1680/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1680/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1680/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1680/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.5736, 'learning_rate': 8.441159108991583e-05, 'epoch': 0.26}
{'loss': 0.5474, 'learning_rate': 8.439404939037587e-05, 'epoch': 0.26}
{'loss': 0.5731, 'learning_rate': 8.437649965143601e-05, 'epoch': 0.26}
{'loss': 0.541, 'learning_rate': 8.435894187719834e-05, 'epoch': 0.26}
{'loss': 0.5519, 'learning_rate': 8.434137607176693e-05, 'epoch': 0.26}
 26%|       | 1681/6500 [3:11:09<9:54:16,  7.40s/it]                                                        26%|       | 1681/6500 [3:11:09<9:54:16,  7.40s/it] 26%|       | 1682/6500 [3:11:16<9:34:39,  7.16s/it]                                                        26%|       | 1682/6500 [3:11:16<9:34:39,  7.16s/it] 26%|       | 1683/6500 [3:11:22<9:20:24,  6.98s/it]                                                        26%|       | 1683/6500 [3:11:22<9:20:24,  6.98s/it] 26%|       | 1684/6500 [3:11:29<9:10:39,  6.86s/it]                                                        26%|       | 1684/6500 [3:11:29<9:10:39,  6.86s/it] 26%|       | 1685/6500 [3:11:35<9:03:36,  6.77s/it]                                                        26%|       | 1685/6500 [3:11:35<9:03:36,  6.77s/it] 26%|       | 1686/6500 [3:11:42<8:58:44,  6.71s/it]                                                       {'loss': 0.5628, 'learning_rate': 8.432380223924766e-05, 'epoch': 0.26}
{'loss': 0.5634, 'learning_rate': 8.430622038374831e-05, 'epoch': 0.26}
{'loss': 0.5704, 'learning_rate': 8.428863050937853e-05, 'epoch': 0.26}
{'loss': 0.5708, 'learning_rate': 8.427103262024985e-05, 'epoch': 0.26}
{'loss': 0.5456, 'learning_rate': 8.425342672047567e-05, 'epoch': 0.26}
 26%|       | 1686/6500 [3:11:42<8:58:44,  6.71s/it] 26%|       | 1687/6500 [3:11:48<8:55:37,  6.68s/it]                                                        26%|       | 1687/6500 [3:11:48<8:55:37,  6.68s/it] 26%|       | 1688/6500 [3:11:55<8:52:57,  6.65s/it]                                                        26%|       | 1688/6500 [3:11:55<8:52:57,  6.65s/it] 26%|       | 1689/6500 [3:12:02<8:51:12,  6.62s/it]                                                        26%|       | 1689/6500 [3:12:02<8:51:12,  6.62s/it] 26%|       | 1690/6500 [3:12:08<8:49:53,  6.61s/it]                                                        26%|       | 1690/6500 [3:12:08<8:49:53,  6.61s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.842246413230896, 'eval_runtime': 1.4947, 'eval_samples_per_second': 8.028, 'eval_steps_per_second': 2.007, 'epoch': 0.26}
                                                        26%|       | 1690/6500 [3:12:10<8:49:53,  6.61s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-1690
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1690
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1690
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1690
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1690
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1690/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1690/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1690/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1690/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.5648, 'learning_rate': 8.423581281417124e-05, 'epoch': 0.26}
{'loss': 0.5429, 'learning_rate': 8.421819090545373e-05, 'epoch': 0.26}
{'loss': 0.5922, 'learning_rate': 8.420056099844216e-05, 'epoch': 0.26}
{'loss': 0.5541, 'learning_rate': 8.418292309725738e-05, 'epoch': 0.26}
{'loss': 0.825, 'learning_rate': 8.416527720602218e-05, 'epoch': 0.26}
 26%|       | 1691/6500 [3:12:17<9:33:58,  7.16s/it]                                                        26%|       | 1691/6500 [3:12:17<9:33:58,  7.16s/it] 26%|       | 1692/6500 [3:12:23<9:19:12,  6.98s/it]                                                        26%|       | 1692/6500 [3:12:23<9:19:12,  6.98s/it] 26%|       | 1693/6500 [3:12:30<9:09:22,  6.86s/it]                                                        26%|       | 1693/6500 [3:12:30<9:09:22,  6.86s/it] 26%|       | 1694/6500 [3:12:36<9:02:10,  6.77s/it]                                                        26%|       | 1694/6500 [3:12:36<9:02:10,  6.77s/it] 26%|       | 1695/6500 [3:12:43<8:56:46,  6.70s/it]                                                        26%|       | 1695/6500 [3:12:43<8:56:46,  6.70s/it] 26%|       | 1696/6500 [3:12:49<8:53:39,  6.67s/it]                                                       {'loss': 0.576, 'learning_rate': 8.414762332886115e-05, 'epoch': 0.26}
{'loss': 0.5367, 'learning_rate': 8.412996146990079e-05, 'epoch': 0.26}
{'loss': 0.5719, 'learning_rate': 8.411229163326944e-05, 'epoch': 0.26}
{'loss': 0.5281, 'learning_rate': 8.409461382309733e-05, 'epoch': 0.26}
{'loss': 0.5444, 'learning_rate': 8.407692804351656e-05, 'epoch': 0.26}
 26%|       | 1696/6500 [3:12:49<8:53:39,  6.67s/it] 26%|       | 1697/6500 [3:12:57<9:05:56,  6.82s/it]                                                        26%|       | 1697/6500 [3:12:57<9:05:56,  6.82s/it] 26%|       | 1698/6500 [3:13:03<8:59:13,  6.74s/it]                                                        26%|       | 1698/6500 [3:13:03<8:59:13,  6.74s/it] 26%|       | 1699/6500 [3:13:10<8:54:44,  6.68s/it]                                                        26%|       | 1699/6500 [3:13:10<8:54:44,  6.68s/it] 26%|       | 1700/6500 [3:13:16<8:51:26,  6.64s/it]                                                        26%|       | 1700/6500 [3:13:16<8:51:26,  6.64s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8422040343284607, 'eval_runtime': 1.7545, 'eval_samples_per_second': 6.84, 'eval_steps_per_second': 1.71, 'epoch': 0.26}
                                                        26%|       | 1700/6500 [3:13:18<8:51:26,  6.64s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-1700
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  I AM HERE AND I AM SAVING THE MODEL
/projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1700the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1700

I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1700
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1700
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1700/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1700/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1700/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1700/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.5503, 'learning_rate': 8.405923429866103e-05, 'epoch': 0.26}
{'loss': 0.5625, 'learning_rate': 8.404153259266657e-05, 'epoch': 0.26}
{'loss': 0.5586, 'learning_rate': 8.402382292967084e-05, 'epoch': 0.26}
{'loss': 0.564, 'learning_rate': 8.400610531381338e-05, 'epoch': 0.26}
{'loss': 0.5502, 'learning_rate': 8.398837974923555e-05, 'epoch': 0.26}
 26%|       | 1701/6500 [3:13:25<9:38:42,  7.24s/it]                                                        26%|       | 1701/6500 [3:13:25<9:38:42,  7.24s/it] 26%|       | 1702/6500 [3:13:31<9:22:25,  7.03s/it]                                                        26%|       | 1702/6500 [3:13:31<9:22:25,  7.03s/it] 26%|       | 1703/6500 [3:13:38<9:10:52,  6.89s/it]                                                        26%|       | 1703/6500 [3:13:38<9:10:52,  6.89s/it] 26%|       | 1704/6500 [3:13:45<9:03:45,  6.80s/it]                                                        26%|       | 1704/6500 [3:13:45<9:03:45,  6.80s/it] 26%|       | 1705/6500 [3:13:53<9:29:05,  7.12s/it]                                                        26%|       | 1705/6500 [3:13:53<9:29:05,  7.12s/it] 26%|       | 1706/6500 [3:13:59<9:15:44,  6.96s/it]                                                       {'loss': 0.5605, 'learning_rate': 8.397064624008062e-05, 'epoch': 0.26}
{'loss': 0.5427, 'learning_rate': 8.395290479049367e-05, 'epoch': 0.26}
{'loss': 0.5932, 'learning_rate': 8.393515540462164e-05, 'epoch': 0.26}
{'loss': 0.5536, 'learning_rate': 8.391739808661339e-05, 'epoch': 0.26}
{'loss': 0.8352, 'learning_rate': 8.389963284061955e-05, 'epoch': 0.26}
 26%|       | 1706/6500 [3:13:59<9:15:44,  6.96s/it] 26%|       | 1707/6500 [3:14:06<9:05:52,  6.83s/it]                                                        26%|       | 1707/6500 [3:14:06<9:05:52,  6.83s/it] 26%|       | 1708/6500 [3:14:12<8:59:07,  6.75s/it]                                                        26%|       | 1708/6500 [3:14:12<8:59:07,  6.75s/it] 26%|       | 1709/6500 [3:14:19<8:54:29,  6.69s/it]                                                        26%|       | 1709/6500 [3:14:19<8:54:29,  6.69s/it] 26%|       | 1710/6500 [3:14:25<8:51:11,  6.65s/it]                                                        26%|       | 1710/6500 [3:14:25<8:51:11,  6.65s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8394728899002075, 'eval_runtime': 1.4995, 'eval_samples_per_second': 8.003, 'eval_steps_per_second': 2.001, 'epoch': 0.26}
                                                        26%|       | 1710/6500 [3:14:27<8:51:11,  6.65s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-1710
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1710
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1710
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1710
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1710
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1710/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1710/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1710/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1710/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.5738, 'learning_rate': 8.388185967079265e-05, 'epoch': 0.26}
{'loss': 0.5453, 'learning_rate': 8.386407858128706e-05, 'epoch': 0.26}
{'loss': 0.5461, 'learning_rate': 8.384628957625899e-05, 'epoch': 0.26}
{'loss': 0.5421, 'learning_rate': 8.382849265986653e-05, 'epoch': 0.26}
{'loss': 0.5477, 'learning_rate': 8.381068783626959e-05, 'epoch': 0.26}
 26%|       | 1711/6500 [3:14:34<9:32:56,  7.18s/it]                                                        26%|       | 1711/6500 [3:14:34<9:32:56,  7.18s/it] 26%|       | 1712/6500 [3:14:40<9:17:56,  6.99s/it]                                                        26%|       | 1712/6500 [3:14:40<9:17:56,  6.99s/it] 26%|       | 1713/6500 [3:14:47<9:22:34,  7.05s/it]                                                        26%|       | 1713/6500 [3:14:47<9:22:34,  7.05s/it] 26%|       | 1714/6500 [3:14:54<9:11:29,  6.91s/it]                                                        26%|       | 1714/6500 [3:14:54<9:11:29,  6.91s/it] 26%|       | 1715/6500 [3:15:01<9:02:52,  6.81s/it]                                                        26%|       | 1715/6500 [3:15:01<9:02:52,  6.81s/it] 26%|       | 1716/6500 [3:15:07<8:57:46,  6.74s/it]                                                       {'loss': 0.5457, 'learning_rate': 8.379287510962993e-05, 'epoch': 0.26}
{'loss': 0.5786, 'learning_rate': 8.377505448411118e-05, 'epoch': 0.26}
{'loss': 0.5644, 'learning_rate': 8.375722596387881e-05, 'epoch': 0.26}
{'loss': 0.5588, 'learning_rate': 8.373938955310011e-05, 'epoch': 0.26}
{'loss': 0.5606, 'learning_rate': 8.372154525594424e-05, 'epoch': 0.26}
 26%|       | 1716/6500 [3:15:07<8:57:46,  6.74s/it] 26%|       | 1717/6500 [3:15:14<8:53:26,  6.69s/it]                                                        26%|       | 1717/6500 [3:15:14<8:53:26,  6.69s/it] 26%|       | 1718/6500 [3:15:20<8:49:56,  6.65s/it]                                                        26%|       | 1718/6500 [3:15:20<8:49:56,  6.65s/it] 26%|       | 1719/6500 [3:15:27<8:47:47,  6.62s/it]                                                        26%|       | 1719/6500 [3:15:27<8:47:47,  6.62s/it] 26%|       | 1720/6500 [3:15:33<8:46:42,  6.61s/it]                                                        26%|       | 1720/6500 [3:15:33<8:46:42,  6.61s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8429202437400818, 'eval_runtime': 2.0679, 'eval_samples_per_second': 5.803, 'eval_steps_per_second': 1.451, 'epoch': 0.26}
                                                        26%|       | 1720/6500 [3:15:36<8:46:42,  6.61s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-1720
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1720
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1720
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1720
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1720
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1720/pytorch_model.binthe pytorch model path is
 /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1720/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1720/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1720/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.5507, 'learning_rate': 8.370369307658219e-05, 'epoch': 0.26}
{'loss': 0.5466, 'learning_rate': 8.368583301918682e-05, 'epoch': 0.26}
{'loss': 0.5779, 'learning_rate': 8.36679650879328e-05, 'epoch': 0.27}
{'loss': 0.5559, 'learning_rate': 8.365008928699662e-05, 'epoch': 0.27}
{'loss': 0.8383, 'learning_rate': 8.363220562055669e-05, 'epoch': 0.27}
 26%|       | 1721/6500 [3:15:43<9:44:42,  7.34s/it]                                                        26%|       | 1721/6500 [3:15:43<9:44:42,  7.34s/it] 26%|       | 1722/6500 [3:15:49<9:25:41,  7.10s/it]                                                        26%|       | 1722/6500 [3:15:49<9:25:41,  7.10s/it] 27%|       | 1723/6500 [3:15:56<9:12:26,  6.94s/it]                                                        27%|       | 1723/6500 [3:15:56<9:12:26,  6.94s/it] 27%|       | 1724/6500 [3:16:02<9:03:17,  6.83s/it]                                                        27%|       | 1724/6500 [3:16:02<9:03:17,  6.83s/it] 27%|       | 1725/6500 [3:16:09<8:56:36,  6.74s/it]                                                        27%|       | 1725/6500 [3:16:09<8:56:36,  6.74s/it] 27%|       | 1726/6500 [3:16:15<8:51:47,  6.68s/it]                                                       {'loss': 0.5557, 'learning_rate': 8.361431409279317e-05, 'epoch': 0.27}
{'loss': 0.5575, 'learning_rate': 8.359641470788811e-05, 'epoch': 0.27}
{'loss': 0.5445, 'learning_rate': 8.357850747002538e-05, 'epoch': 0.27}
{'loss': 0.5341, 'learning_rate': 8.35605923833907e-05, 'epoch': 0.27}
{'loss': 0.5395, 'learning_rate': 8.35426694521716e-05, 'epoch': 0.27}
 27%|       | 1726/6500 [3:16:15<8:51:47,  6.68s/it] 27%|       | 1727/6500 [3:16:22<8:48:42,  6.65s/it]                                                        27%|       | 1727/6500 [3:16:22<8:48:42,  6.65s/it] 27%|       | 1728/6500 [3:16:28<8:46:13,  6.62s/it]                                                        27%|       | 1728/6500 [3:16:28<8:46:13,  6.62s/it] 27%|       | 1729/6500 [3:16:35<8:44:52,  6.60s/it]                                                        27%|       | 1729/6500 [3:16:35<8:44:52,  6.60s/it] 27%|       | 1730/6500 [3:16:43<9:21:41,  7.07s/it]                                                        27%|       | 1730/6500 [3:16:43<9:21:41,  7.07s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8416395783424377, 'eval_runtime': 1.4879, 'eval_samples_per_second': 8.065, 'eval_steps_per_second': 2.016, 'epoch': 0.27}
                                                        27%|       | 1730/6500 [3:16:45<9:21:41,  7.07s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-1730
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in I AM HERE AND I AM SAVING THE MODEL 
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1730
/projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1730
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1730
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1730
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1730/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1730/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1730/pytorch_model.binthe pytorch model path is
 /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1730/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.5612, 'learning_rate': 8.352473868055746e-05, 'epoch': 0.27}
{'loss': 0.5775, 'learning_rate': 8.350680007273947e-05, 'epoch': 0.27}
{'loss': 0.5635, 'learning_rate': 8.348885363291071e-05, 'epoch': 0.27}
{'loss': 0.5467, 'learning_rate': 8.3470899365266e-05, 'epoch': 0.27}
{'loss': 0.5629, 'learning_rate': 8.345293727400209e-05, 'epoch': 0.27}
 27%|       | 1731/6500 [3:16:51<9:53:25,  7.47s/it]                                                        27%|       | 1731/6500 [3:16:51<9:53:25,  7.47s/it] 27%|       | 1732/6500 [3:16:58<9:31:39,  7.19s/it]                                                        27%|       | 1732/6500 [3:16:58<9:31:39,  7.19s/it] 27%|       | 1733/6500 [3:17:05<9:16:36,  7.01s/it]                                                        27%|       | 1733/6500 [3:17:05<9:16:36,  7.01s/it] 27%|       | 1734/6500 [3:17:11<9:07:05,  6.89s/it]                                                        27%|       | 1734/6500 [3:17:11<9:07:05,  6.89s/it] 27%|       | 1735/6500 [3:17:18<8:58:53,  6.79s/it]                                                        27%|       | 1735/6500 [3:17:18<8:58:53,  6.79s/it] 27%|       | 1736/6500 [3:17:24<8:53:07,  6.71s/it]                                                       {'loss': 0.536, 'learning_rate': 8.343496736331749e-05, 'epoch': 0.27}
{'loss': 0.5564, 'learning_rate': 8.341698963741256e-05, 'epoch': 0.27}
{'loss': 0.5715, 'learning_rate': 8.339900410048947e-05, 'epoch': 0.27}
{'loss': 0.5552, 'learning_rate': 8.338101075675225e-05, 'epoch': 0.27}
{'loss': 0.8342, 'learning_rate': 8.336300961040673e-05, 'epoch': 0.27}
 27%|       | 1736/6500 [3:17:24<8:53:07,  6.71s/it] 27%|       | 1737/6500 [3:17:31<8:49:05,  6.67s/it]                                                        27%|       | 1737/6500 [3:17:31<8:49:05,  6.67s/it] 27%|       | 1738/6500 [3:17:37<8:46:17,  6.63s/it]                                                        27%|       | 1738/6500 [3:17:37<8:46:17,  6.63s/it] 27%|       | 1739/6500 [3:17:44<8:44:01,  6.60s/it]                                                        27%|       | 1739/6500 [3:17:44<8:44:01,  6.60s/it] 27%|       | 1740/6500 [3:17:51<8:42:48,  6.59s/it]                                                        27%|       | 1740/6500 [3:17:51<8:42:48,  6.59s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8386362195014954, 'eval_runtime': 1.5009, 'eval_samples_per_second': 7.995, 'eval_steps_per_second': 1.999, 'epoch': 0.27}
                                                        27%|       | 1740/6500 [3:17:52<8:42:48,  6.59s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-1740
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1740
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1740
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1740
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1740
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1740/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1740/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1740/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1740/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.544, 'learning_rate': 8.334500066566054e-05, 'epoch': 0.27}
{'loss': 0.5752, 'learning_rate': 8.33269839267232e-05, 'epoch': 0.27}
{'loss': 0.5302, 'learning_rate': 8.330895939780601e-05, 'epoch': 0.27}
{'loss': 0.5264, 'learning_rate': 8.329092708312207e-05, 'epoch': 0.27}
{'loss': 0.5414, 'learning_rate': 8.327288698688634e-05, 'epoch': 0.27}
 27%|       | 1741/6500 [3:17:59<9:25:55,  7.13s/it]                                                        27%|       | 1741/6500 [3:17:59<9:25:55,  7.13s/it] 27%|       | 1742/6500 [3:18:05<9:12:15,  6.96s/it]                                                        27%|       | 1742/6500 [3:18:05<9:12:15,  6.96s/it] 27%|       | 1743/6500 [3:18:12<9:03:01,  6.85s/it]                                                        27%|       | 1743/6500 [3:18:12<9:03:01,  6.85s/it] 27%|       | 1744/6500 [3:18:19<8:55:48,  6.76s/it]                                                        27%|       | 1744/6500 [3:18:19<8:55:48,  6.76s/it] 27%|       | 1745/6500 [3:18:25<8:50:42,  6.70s/it]                                                        27%|       | 1745/6500 [3:18:25<8:50:42,  6.70s/it] 27%|       | 1746/6500 [3:18:33<9:09:16,  6.93s/it]                                                       {'loss': 0.5525, 'learning_rate': 8.325483911331557e-05, 'epoch': 0.27}
{'loss': 0.5753, 'learning_rate': 8.323678346662835e-05, 'epoch': 0.27}
{'loss': 0.5571, 'learning_rate': 8.321872005104509e-05, 'epoch': 0.27}
{'loss': 0.5464, 'learning_rate': 8.3200648870788e-05, 'epoch': 0.27}
{'loss': 0.5492, 'learning_rate': 8.318256993008107e-05, 'epoch': 0.27}
 27%|       | 1746/6500 [3:18:33<9:09:16,  6.93s/it] 27%|       | 1747/6500 [3:18:39<9:00:02,  6.82s/it]                                                        27%|       | 1747/6500 [3:18:39<9:00:02,  6.82s/it] 27%|       | 1748/6500 [3:18:46<8:53:44,  6.74s/it]                                                        27%|       | 1748/6500 [3:18:46<8:53:44,  6.74s/it] 27%|       | 1749/6500 [3:18:52<8:49:36,  6.69s/it]                                                        27%|       | 1749/6500 [3:18:52<8:49:36,  6.69s/it] 27%|       | 1750/6500 [3:18:59<8:46:04,  6.65s/it]                                                        27%|       | 1750/6500 [3:18:59<8:46:04,  6.65s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8443529009819031, 'eval_runtime': 1.4919, 'eval_samples_per_second': 8.044, 'eval_steps_per_second': 2.011, 'epoch': 0.27}
                                                        27%|       | 1750/6500 [3:19:00<8:46:04,  6.65s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-1750
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1750
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1750
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1750
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1750
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1750/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1750/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1750/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1750/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.5358, 'learning_rate': 8.316448323315021e-05, 'epoch': 0.27}
{'loss': 0.5811, 'learning_rate': 8.3146388784223e-05, 'epoch': 0.27}
{'loss': 0.5337, 'learning_rate': 8.312828658752896e-05, 'epoch': 0.27}
{'loss': 0.7747, 'learning_rate': 8.311017664729935e-05, 'epoch': 0.27}
{'loss': 0.6136, 'learning_rate': 8.309205896776727e-05, 'epoch': 0.27}
 27%|       | 1751/6500 [3:19:07<9:26:43,  7.16s/it]                                                        27%|       | 1751/6500 [3:19:07<9:26:43,  7.16s/it] 27%|       | 1752/6500 [3:19:14<9:11:58,  6.98s/it]                                                        27%|       | 1752/6500 [3:19:14<9:11:58,  6.98s/it] 27%|       | 1753/6500 [3:19:20<9:02:01,  6.85s/it]                                                        27%|       | 1753/6500 [3:19:20<9:02:01,  6.85s/it] 27%|       | 1754/6500 [3:19:27<8:54:35,  6.76s/it]                                                        27%|       | 1754/6500 [3:19:27<8:54:35,  6.76s/it] 27%|       | 1755/6500 [3:19:33<8:49:40,  6.70s/it]                                                        27%|       | 1755/6500 [3:19:33<8:49:40,  6.70s/it] 27%|       | 1756/6500 [3:19:40<8:46:08,  6.65s/it]                                                       {'loss': 0.5283, 'learning_rate': 8.307393355316761e-05, 'epoch': 0.27}
{'loss': 0.5731, 'learning_rate': 8.305580040773706e-05, 'epoch': 0.27}
{'loss': 0.5219, 'learning_rate': 8.303765953571417e-05, 'epoch': 0.27}
{'loss': 0.5303, 'learning_rate': 8.30195109413392e-05, 'epoch': 0.27}
{'loss': 0.5367, 'learning_rate': 8.300135462885435e-05, 'epoch': 0.27}
 27%|       | 1756/6500 [3:19:40<8:46:08,  6.65s/it] 27%|       | 1757/6500 [3:19:47<8:43:25,  6.62s/it]                                                        27%|       | 1757/6500 [3:19:47<8:43:25,  6.62s/it] 27%|       | 1758/6500 [3:19:53<8:41:24,  6.60s/it]                                                        27%|       | 1758/6500 [3:19:53<8:41:24,  6.60s/it] 27%|       | 1759/6500 [3:20:00<8:39:57,  6.58s/it]                                                        27%|       | 1759/6500 [3:20:00<8:39:57,  6.58s/it] 27%|       | 1760/6500 [3:20:06<8:39:06,  6.57s/it]                                                        27%|       | 1760/6500 [3:20:06<8:39:06,  6.57s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8376907110214233, 'eval_runtime': 1.4888, 'eval_samples_per_second': 8.06, 'eval_steps_per_second': 2.015, 'epoch': 0.27}
                                                        27%|       | 1760/6500 [3:20:08<8:39:06,  6.57s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-1760
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1760
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1760
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1760
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1760
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1760/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1760/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1760/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1760/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.5359, 'learning_rate': 8.298319060250348e-05, 'epoch': 0.27}
{'loss': 0.5767, 'learning_rate': 8.296501886653236e-05, 'epoch': 0.27}
{'loss': 0.5606, 'learning_rate': 8.29468394251885e-05, 'epoch': 0.27}
{'loss': 0.5346, 'learning_rate': 8.292865228272126e-05, 'epoch': 0.27}
{'loss': 0.5513, 'learning_rate': 8.291045744338175e-05, 'epoch': 0.27}
 27%|       | 1761/6500 [3:20:15<9:21:24,  7.11s/it]                                                        27%|       | 1761/6500 [3:20:15<9:21:24,  7.11s/it] 27%|       | 1762/6500 [3:20:22<9:31:55,  7.24s/it]                                                        27%|       | 1762/6500 [3:20:22<9:31:55,  7.24s/it] 27%|       | 1763/6500 [3:20:29<9:15:27,  7.04s/it]                                                        27%|       | 1763/6500 [3:20:29<9:15:27,  7.04s/it] 27%|       | 1764/6500 [3:20:35<9:04:04,  6.89s/it]                                                        27%|       | 1764/6500 [3:20:35<9:04:04,  6.89s/it] 27%|       | 1765/6500 [3:20:42<8:55:48,  6.79s/it]                                                        27%|       | 1765/6500 [3:20:42<8:55:48,  6.79s/it] 27%|       | 1766/6500 [3:20:48<8:49:49,  6.72s/it]                                                       {'loss': 0.5336, 'learning_rate': 8.289225491142292e-05, 'epoch': 0.27}
{'loss': 0.582, 'learning_rate': 8.287404469109947e-05, 'epoch': 0.27}
{'loss': 0.5395, 'learning_rate': 8.285582678666797e-05, 'epoch': 0.27}
{'loss': 0.8195, 'learning_rate': 8.283760120238672e-05, 'epoch': 0.27}
{'loss': 0.5668, 'learning_rate': 8.281936794251586e-05, 'epoch': 0.27}
 27%|       | 1766/6500 [3:20:48<8:49:49,  6.72s/it] 27%|       | 1767/6500 [3:20:55<8:45:24,  6.66s/it]                                                        27%|       | 1767/6500 [3:20:55<8:45:24,  6.66s/it] 27%|       | 1768/6500 [3:21:01<8:42:38,  6.63s/it]                                                        27%|       | 1768/6500 [3:21:01<8:42:38,  6.63s/it] 27%|       | 1769/6500 [3:21:08<8:40:33,  6.60s/it]                                                        27%|       | 1769/6500 [3:21:08<8:40:33,  6.60s/it] 27%|       | 1770/6500 [3:21:14<8:39:26,  6.59s/it]                                                        27%|       | 1770/6500 [3:21:14<8:39:26,  6.59s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8387519121170044, 'eval_runtime': 1.4833, 'eval_samples_per_second': 8.09, 'eval_steps_per_second': 2.023, 'epoch': 0.27}
                                                        27%|       | 1770/6500 [3:21:16<8:39:26,  6.59s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-1770
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1770
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1770
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1770
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1770
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1770/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1770/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1770/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1770/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.5326, 'learning_rate': 8.280112701131726e-05, 'epoch': 0.27}
{'loss': 0.5609, 'learning_rate': 8.278287841305468e-05, 'epoch': 0.27}
{'loss': 0.5235, 'learning_rate': 8.276462215199357e-05, 'epoch': 0.27}
{'loss': 0.5364, 'learning_rate': 8.274635823240127e-05, 'epoch': 0.27}
{'loss': 0.5527, 'learning_rate': 8.27280866585468e-05, 'epoch': 0.27}
 27%|       | 1771/6500 [3:21:23<9:22:36,  7.14s/it]                                                        27%|       | 1771/6500 [3:21:23<9:22:36,  7.14s/it] 27%|       | 1772/6500 [3:21:29<9:08:38,  6.96s/it]                                                        27%|       | 1772/6500 [3:21:29<9:08:38,  6.96s/it] 27%|       | 1773/6500 [3:21:36<8:58:37,  6.84s/it]                                                        27%|       | 1773/6500 [3:21:36<8:58:37,  6.84s/it] 27%|       | 1774/6500 [3:21:43<8:51:49,  6.75s/it]                                                        27%|       | 1774/6500 [3:21:43<8:51:49,  6.75s/it] 27%|       | 1775/6500 [3:21:49<8:46:47,  6.69s/it]                                                        27%|       | 1775/6500 [3:21:49<8:46:47,  6.69s/it] 27%|       | 1776/6500 [3:21:56<8:43:17,  6.65s/it]                                                       {'loss': 0.5557, 'learning_rate': 8.27098074347011e-05, 'epoch': 0.27}
{'loss': 0.5493, 'learning_rate': 8.269152056513678e-05, 'epoch': 0.27}
{'loss': 0.5544, 'learning_rate': 8.26732260541283e-05, 'epoch': 0.27}
{'loss': 0.5361, 'learning_rate': 8.265492390595186e-05, 'epoch': 0.27}
{'loss': 0.546, 'learning_rate': 8.263661412488552e-05, 'epoch': 0.27}
 27%|       | 1776/6500 [3:21:56<8:43:17,  6.65s/it] 27%|       | 1777/6500 [3:22:02<8:40:44,  6.62s/it]                                                        27%|       | 1777/6500 [3:22:02<8:40:44,  6.62s/it] 27%|       | 1778/6500 [3:22:10<9:00:11,  6.86s/it]                                                        27%|       | 1778/6500 [3:22:10<9:00:11,  6.86s/it] 27%|       | 1779/6500 [3:22:16<8:53:02,  6.77s/it]                                                        27%|       | 1779/6500 [3:22:16<8:53:02,  6.77s/it] 27%|       | 1780/6500 [3:22:23<8:47:33,  6.71s/it]                                                        27%|       | 1780/6500 [3:22:23<8:47:33,  6.71s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8445983529090881, 'eval_runtime': 1.4895, 'eval_samples_per_second': 8.056, 'eval_steps_per_second': 2.014, 'epoch': 0.27}
                                                        27%|       | 1780/6500 [3:22:24<8:47:33,  6.71s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-1780
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1780
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1780
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1780
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1780
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1780/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1780/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1780/pytorch_model.binthe pytorch model path is
 /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1780/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.5319, 'learning_rate': 8.261829671520907e-05, 'epoch': 0.27}
{'loss': 0.5827, 'learning_rate': 8.259997168120409e-05, 'epoch': 0.27}
{'loss': 0.5449, 'learning_rate': 8.258163902715392e-05, 'epoch': 0.27}
{'loss': 0.8128, 'learning_rate': 8.256329875734375e-05, 'epoch': 0.27}
{'loss': 0.5615, 'learning_rate': 8.254495087606045e-05, 'epoch': 0.27}
 27%|       | 1781/6500 [3:22:31<9:26:38,  7.20s/it]                                                        27%|       | 1781/6500 [3:22:31<9:26:38,  7.20s/it] 27%|       | 1782/6500 [3:22:38<9:10:56,  7.01s/it]                                                        27%|       | 1782/6500 [3:22:38<9:10:56,  7.01s/it] 27%|       | 1783/6500 [3:22:44<8:59:57,  6.87s/it]                                                        27%|       | 1783/6500 [3:22:44<8:59:57,  6.87s/it] 27%|       | 1784/6500 [3:22:51<8:52:22,  6.77s/it]                                                        27%|       | 1784/6500 [3:22:51<8:52:22,  6.77s/it] 27%|       | 1785/6500 [3:22:57<8:48:53,  6.73s/it]                                                        27%|       | 1785/6500 [3:22:57<8:48:53,  6.73s/it] 27%|       | 1786/6500 [3:23:04<8:44:38,  6.68s/it]                                                       {'loss': 0.5465, 'learning_rate': 8.252659538759279e-05, 'epoch': 0.27}
{'loss': 0.5347, 'learning_rate': 8.250823229623122e-05, 'epoch': 0.27}
{'loss': 0.5367, 'learning_rate': 8.2489861606268e-05, 'epoch': 0.28}
{'loss': 0.5324, 'learning_rate': 8.247148332199715e-05, 'epoch': 0.28}
{'loss': 0.551, 'learning_rate': 8.245309744771452e-05, 'epoch': 0.28}
 27%|       | 1786/6500 [3:23:04<8:44:38,  6.68s/it] 27%|       | 1787/6500 [3:23:11<8:42:28,  6.65s/it]                                                        27%|       | 1787/6500 [3:23:11<8:42:28,  6.65s/it] 28%|       | 1788/6500 [3:23:17<8:39:58,  6.62s/it]                                                        28%|       | 1788/6500 [3:23:17<8:39:58,  6.62s/it] 28%|       | 1789/6500 [3:23:24<8:37:54,  6.60s/it]                                                        28%|       | 1789/6500 [3:23:24<8:37:54,  6.60s/it] 28%|       | 1790/6500 [3:23:30<8:36:21,  6.58s/it]                                                        28%|       | 1790/6500 [3:23:30<8:36:21,  6.58s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8362829089164734, 'eval_runtime': 1.6356, 'eval_samples_per_second': 7.337, 'eval_steps_per_second': 1.834, 'epoch': 0.28}
                                                        28%|       | 1790/6500 [3:23:32<8:36:21,  6.58s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-1790
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1790
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1790
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1790
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1790
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1790/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1790/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1790/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1790/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.5583, 'learning_rate': 8.24347039877177e-05, 'epoch': 0.28}
{'loss': 0.552, 'learning_rate': 8.241630294630599e-05, 'epoch': 0.28}
{'loss': 0.5478, 'learning_rate': 8.239789432778058e-05, 'epoch': 0.28}
{'loss': 0.5458, 'learning_rate': 8.237947813644436e-05, 'epoch': 0.28}
{'loss': 0.5376, 'learning_rate': 8.236105437660198e-05, 'epoch': 0.28}
 28%|       | 1791/6500 [3:23:39<9:20:14,  7.14s/it]                                                        28%|       | 1791/6500 [3:23:39<9:20:14,  7.14s/it] 28%|       | 1792/6500 [3:23:45<9:05:47,  6.96s/it]                                                        28%|       | 1792/6500 [3:23:45<9:05:47,  6.96s/it] 28%|       | 1793/6500 [3:23:52<8:55:57,  6.83s/it]                                                        28%|       | 1793/6500 [3:23:52<8:55:57,  6.83s/it] 28%|       | 1794/6500 [3:24:00<9:22:32,  7.17s/it]                                                        28%|       | 1794/6500 [3:24:00<9:22:32,  7.17s/it] 28%|       | 1795/6500 [3:24:06<9:07:32,  6.98s/it]                                                        28%|       | 1795/6500 [3:24:06<9:07:32,  6.98s/it] 28%|       | 1796/6500 [3:24:13<8:57:05,  6.85s/it]                                                       {'loss': 0.5299, 'learning_rate': 8.234262305255991e-05, 'epoch': 0.28}
{'loss': 0.5731, 'learning_rate': 8.232418416862633e-05, 'epoch': 0.28}
{'loss': 0.5415, 'learning_rate': 8.230573772911126e-05, 'epoch': 0.28}
{'loss': 0.8281, 'learning_rate': 8.228728373832642e-05, 'epoch': 0.28}
{'loss': 0.5503, 'learning_rate': 8.226882220058529e-05, 'epoch': 0.28}
 28%|       | 1796/6500 [3:24:13<8:57:05,  6.85s/it] 28%|       | 1797/6500 [3:24:19<8:49:39,  6.76s/it]                                                        28%|       | 1797/6500 [3:24:19<8:49:39,  6.76s/it] 28%|       | 1798/6500 [3:24:26<8:44:31,  6.69s/it]                                                        28%|       | 1798/6500 [3:24:26<8:44:31,  6.69s/it] 28%|       | 1799/6500 [3:24:32<8:40:29,  6.64s/it]                                                        28%|       | 1799/6500 [3:24:32<8:40:29,  6.64s/it] 28%|       | 1800/6500 [3:24:39<8:37:54,  6.61s/it]                                                        28%|       | 1800/6500 [3:24:39<8:37:54,  6.61s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8397880792617798, 'eval_runtime': 1.494, 'eval_samples_per_second': 8.032, 'eval_steps_per_second': 2.008, 'epoch': 0.28}
                                                        28%|       | 1800/6500 [3:24:40<8:37:54,  6.61s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-1800
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1800
/projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1800
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1800
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1800
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1800/pytorch_model.bin
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1800/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1800/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1800/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.5424, 'learning_rate': 8.225035312020318e-05, 'epoch': 0.28}
{'loss': 0.5292, 'learning_rate': 8.223187650149712e-05, 'epoch': 0.28}
{'loss': 0.5206, 'learning_rate': 8.221339234878589e-05, 'epoch': 0.28}
{'loss': 0.5327, 'learning_rate': 8.219490066639007e-05, 'epoch': 0.28}
{'loss': 0.5352, 'learning_rate': 8.217640145863197e-05, 'epoch': 0.28}
 28%|       | 1801/6500 [3:24:47<9:19:05,  7.14s/it]                                                        28%|       | 1801/6500 [3:24:47<9:19:05,  7.14s/it] 28%|       | 1802/6500 [3:24:54<9:04:54,  6.96s/it]                                                        28%|       | 1802/6500 [3:24:54<9:04:54,  6.96s/it] 28%|       | 1803/6500 [3:25:00<8:54:43,  6.83s/it]                                                        28%|       | 1803/6500 [3:25:00<8:54:43,  6.83s/it] 28%|       | 1804/6500 [3:25:07<8:47:41,  6.74s/it]                                                        28%|       | 1804/6500 [3:25:07<8:47:41,  6.74s/it] 28%|       | 1805/6500 [3:25:13<8:42:40,  6.68s/it]                                                        28%|       | 1805/6500 [3:25:13<8:42:40,  6.68s/it] 28%|       | 1806/6500 [3:25:20<8:39:41,  6.64s/it]                                                       {'loss': 0.5598, 'learning_rate': 8.215789472983565e-05, 'epoch': 0.28}
{'loss': 0.549, 'learning_rate': 8.213938048432697e-05, 'epoch': 0.28}
{'loss': 0.5399, 'learning_rate': 8.212085872643351e-05, 'epoch': 0.28}
{'loss': 0.5468, 'learning_rate': 8.210232946048462e-05, 'epoch': 0.28}
{'loss': 0.5389, 'learning_rate': 8.208379269081141e-05, 'epoch': 0.28}
 28%|       | 1806/6500 [3:25:20<8:39:41,  6.64s/it] 28%|       | 1807/6500 [3:25:26<8:37:09,  6.61s/it]                                                        28%|       | 1807/6500 [3:25:26<8:37:09,  6.61s/it] 28%|       | 1808/6500 [3:25:33<8:35:29,  6.59s/it]                                                        28%|       | 1808/6500 [3:25:33<8:35:29,  6.59s/it] 28%|       | 1809/6500 [3:25:40<8:34:23,  6.58s/it]                                                        28%|       | 1809/6500 [3:25:40<8:34:23,  6.58s/it] 28%|       | 1810/6500 [3:25:47<8:55:32,  6.85s/it]                                                        28%|       | 1810/6500 [3:25:47<8:55:32,  6.85s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8441357612609863, 'eval_runtime': 1.4915, 'eval_samples_per_second': 8.046, 'eval_steps_per_second': 2.011, 'epoch': 0.28}
                                                        28%|       | 1810/6500 [3:25:49<8:55:32,  6.85s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-1810
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1810
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1810
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1810
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1810
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1810/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1810/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1810/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1810/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.5421, 'learning_rate': 8.206524842174672e-05, 'epoch': 0.28}
{'loss': 0.568, 'learning_rate': 8.204669665762519e-05, 'epoch': 0.28}
{'loss': 0.5394, 'learning_rate': 8.202813740278314e-05, 'epoch': 0.28}
{'loss': 0.824, 'learning_rate': 8.200957066155872e-05, 'epoch': 0.28}
{'loss': 0.5324, 'learning_rate': 8.199099643829177e-05, 'epoch': 0.28}
 28%|       | 1811/6500 [3:25:55<9:29:50,  7.29s/it]                                                        28%|       | 1811/6500 [3:25:55<9:29:50,  7.29s/it] 28%|       | 1812/6500 [3:26:02<9:12:14,  7.07s/it]                                                        28%|       | 1812/6500 [3:26:02<9:12:14,  7.07s/it] 28%|       | 1813/6500 [3:26:08<8:59:46,  6.91s/it]                                                        28%|       | 1813/6500 [3:26:08<8:59:46,  6.91s/it] 28%|       | 1814/6500 [3:26:15<8:51:07,  6.80s/it]                                                        28%|       | 1814/6500 [3:26:15<8:51:07,  6.80s/it] 28%|       | 1815/6500 [3:26:22<8:45:12,  6.73s/it]                                                        28%|       | 1815/6500 [3:26:22<8:45:12,  6.73s/it] 28%|       | 1816/6500 [3:26:28<8:40:39,  6.67s/it]                                                       {'loss': 0.5513, 'learning_rate': 8.197241473732392e-05, 'epoch': 0.28}
{'loss': 0.5361, 'learning_rate': 8.195382556299852e-05, 'epoch': 0.28}
{'loss': 0.5233, 'learning_rate': 8.193522891966067e-05, 'epoch': 0.28}
{'loss': 0.5291, 'learning_rate': 8.191662481165724e-05, 'epoch': 0.28}
{'loss': 0.5493, 'learning_rate': 8.189801324333681e-05, 'epoch': 0.28}
 28%|       | 1816/6500 [3:26:28<8:40:39,  6.67s/it] 28%|       | 1817/6500 [3:26:35<8:37:57,  6.64s/it]                                                        28%|       | 1817/6500 [3:26:35<8:37:57,  6.64s/it] 28%|       | 1818/6500 [3:26:41<8:35:55,  6.61s/it]                                                        28%|       | 1818/6500 [3:26:41<8:35:55,  6.61s/it] 28%|       | 1819/6500 [3:26:48<8:34:15,  6.59s/it]                                                        28%|       | 1819/6500 [3:26:48<8:34:15,  6.59s/it] 28%|       | 1820/6500 [3:26:54<8:32:53,  6.58s/it]                                                        28%|       | 1820/6500 [3:26:54<8:32:53,  6.58s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8368576169013977, 'eval_runtime': 1.4828, 'eval_samples_per_second': 8.093, 'eval_steps_per_second': 2.023, 'epoch': 0.28}
                                                        28%|       | 1820/6500 [3:26:56<8:32:53,  6.58s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-1820
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1820
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1820
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1820
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1820
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1820/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1820/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1820/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1820/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.5668, 'learning_rate': 8.187939421904973e-05, 'epoch': 0.28}
{'loss': 0.5421, 'learning_rate': 8.186076774314809e-05, 'epoch': 0.28}
{'loss': 0.5372, 'learning_rate': 8.184213381998568e-05, 'epoch': 0.28}
{'loss': 0.5489, 'learning_rate': 8.182349245391812e-05, 'epoch': 0.28}
{'loss': 0.5303, 'learning_rate': 8.180484364930267e-05, 'epoch': 0.28}
 28%|       | 1821/6500 [3:27:03<9:15:53,  7.13s/it]                                                        28%|       | 1821/6500 [3:27:03<9:15:53,  7.13s/it] 28%|       | 1822/6500 [3:27:09<9:02:32,  6.96s/it]                                                        28%|       | 1822/6500 [3:27:09<9:02:32,  6.96s/it] 28%|       | 1823/6500 [3:27:16<8:53:03,  6.84s/it]                                                        28%|       | 1823/6500 [3:27:16<8:53:03,  6.84s/it] 28%|       | 1824/6500 [3:27:22<8:45:53,  6.75s/it]                                                        28%|       | 1824/6500 [3:27:22<8:45:53,  6.75s/it] 28%|       | 1825/6500 [3:27:29<8:40:44,  6.68s/it]                                                        28%|       | 1825/6500 [3:27:29<8:40:44,  6.68s/it] 28%|       | 1826/6500 [3:27:35<8:37:30,  6.64s/it]                                                       {'loss': 0.5434, 'learning_rate': 8.178618741049842e-05, 'epoch': 0.28}
{'loss': 0.5446, 'learning_rate': 8.17675237418661e-05, 'epoch': 0.28}
{'loss': 0.5465, 'learning_rate': 8.174885264776826e-05, 'epoch': 0.28}
{'loss': 0.8219, 'learning_rate': 8.173017413256915e-05, 'epoch': 0.28}
{'loss': 0.5283, 'learning_rate': 8.171148820063476e-05, 'epoch': 0.28}
 28%|       | 1826/6500 [3:27:35<8:37:30,  6.64s/it] 28%|       | 1827/6500 [3:27:43<8:55:37,  6.88s/it]                                                        28%|       | 1827/6500 [3:27:43<8:55:37,  6.88s/it] 28%|       | 1828/6500 [3:27:49<8:47:53,  6.78s/it]                                                        28%|       | 1828/6500 [3:27:49<8:47:53,  6.78s/it] 28%|       | 1829/6500 [3:27:56<8:42:46,  6.72s/it]                                                        28%|       | 1829/6500 [3:27:56<8:42:46,  6.72s/it] 28%|       | 1830/6500 [3:28:03<8:38:56,  6.67s/it]                                                        28%|       | 1830/6500 [3:28:03<8:38:56,  6.67s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8362083435058594, 'eval_runtime': 1.4906, 'eval_samples_per_second': 8.05, 'eval_steps_per_second': 2.013, 'epoch': 0.28}
                                                        28%|       | 1830/6500 [3:28:04<8:38:56,  6.67s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-1830
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1830I AM HERE AND I AM SAVING THE MODEL

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1830
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1830
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1830
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1830/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1830/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1830/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1830/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.551, 'learning_rate': 8.169279485633282e-05, 'epoch': 0.28}
{'loss': 0.5111, 'learning_rate': 8.167409410403277e-05, 'epoch': 0.28}
{'loss': 0.522, 'learning_rate': 8.16553859481058e-05, 'epoch': 0.28}
{'loss': 0.5351, 'learning_rate': 8.163667039292484e-05, 'epoch': 0.28}
{'loss': 0.5253, 'learning_rate': 8.161794744286453e-05, 'epoch': 0.28}
 28%|       | 1831/6500 [3:28:11<9:19:02,  7.18s/it]                                                        28%|       | 1831/6500 [3:28:11<9:19:02,  7.18s/it] 28%|       | 1832/6500 [3:28:17<9:03:54,  6.99s/it]                                                        28%|       | 1832/6500 [3:28:17<9:03:54,  6.99s/it] 28%|       | 1833/6500 [3:28:24<8:53:57,  6.86s/it]                                                        28%|       | 1833/6500 [3:28:24<8:53:57,  6.86s/it] 28%|       | 1834/6500 [3:28:31<8:46:19,  6.77s/it]                                                        28%|       | 1834/6500 [3:28:31<8:46:19,  6.77s/it] 28%|       | 1835/6500 [3:28:37<8:41:12,  6.70s/it]                                                        28%|       | 1835/6500 [3:28:37<8:41:12,  6.70s/it] 28%|       | 1836/6500 [3:28:44<8:37:24,  6.66s/it]                                                       {'loss': 0.5646, 'learning_rate': 8.159921710230125e-05, 'epoch': 0.28}
{'loss': 0.5544, 'learning_rate': 8.158047937561309e-05, 'epoch': 0.28}
{'loss': 0.5195, 'learning_rate': 8.156173426717988e-05, 'epoch': 0.28}
{'loss': 0.5444, 'learning_rate': 8.15429817813832e-05, 'epoch': 0.28}
{'loss': 0.5178, 'learning_rate': 8.152422192260631e-05, 'epoch': 0.28}
 28%|       | 1836/6500 [3:28:44<8:37:24,  6.66s/it] 28%|       | 1837/6500 [3:28:50<8:34:52,  6.63s/it]                                                        28%|       | 1837/6500 [3:28:50<8:34:52,  6.63s/it] 28%|       | 1838/6500 [3:28:57<8:33:12,  6.61s/it]                                                        28%|       | 1838/6500 [3:28:57<8:33:12,  6.61s/it] 28%|       | 1839/6500 [3:29:03<8:31:47,  6.59s/it]                                                        28%|       | 1839/6500 [3:29:03<8:31:47,  6.59s/it] 28%|       | 1840/6500 [3:29:10<8:30:43,  6.58s/it]                                                        28%|       | 1840/6500 [3:29:10<8:30:43,  6.58s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.844028651714325, 'eval_runtime': 1.4763, 'eval_samples_per_second': 8.128, 'eval_steps_per_second': 2.032, 'epoch': 0.28}
                                                        28%|       | 1840/6500 [3:29:11<8:30:43,  6.58s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-1840
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1840I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1840

I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1840
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1840
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1840/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1840/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1840/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1840/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.5746, 'learning_rate': 8.150545469523421e-05, 'epoch': 0.28}
{'loss': 0.5215, 'learning_rate': 8.148668010365364e-05, 'epoch': 0.28}
{'loss': 0.8169, 'learning_rate': 8.146789815225303e-05, 'epoch': 0.28}
{'loss': 0.5538, 'learning_rate': 8.144910884542256e-05, 'epoch': 0.28}
{'loss': 0.5257, 'learning_rate': 8.14303121875541e-05, 'epoch': 0.28}
 28%|       | 1841/6500 [3:29:18<9:11:35,  7.10s/it]                                                        28%|       | 1841/6500 [3:29:18<9:11:35,  7.10s/it] 28%|       | 1842/6500 [3:29:25<8:58:47,  6.94s/it]                                                        28%|       | 1842/6500 [3:29:25<8:58:47,  6.94s/it] 28%|       | 1843/6500 [3:29:32<9:10:39,  7.09s/it]                                                        28%|       | 1843/6500 [3:29:32<9:10:39,  7.09s/it] 28%|       | 1844/6500 [3:29:39<8:57:53,  6.93s/it]                                                        28%|       | 1844/6500 [3:29:39<8:57:53,  6.93s/it] 28%|       | 1845/6500 [3:29:45<8:48:54,  6.82s/it]                                                        28%|       | 1845/6500 [3:29:45<8:48:54,  6.82s/it] 28%|       | 1846/6500 [3:29:52<8:42:21,  6.73s/it]                                                       {'loss': 0.5437, 'learning_rate': 8.141150818304129e-05, 'epoch': 0.28}
{'loss': 0.5135, 'learning_rate': 8.139269683627942e-05, 'epoch': 0.28}
{'loss': 0.5175, 'learning_rate': 8.137387815166551e-05, 'epoch': 0.28}
{'loss': 0.5399, 'learning_rate': 8.135505213359835e-05, 'epoch': 0.28}
{'loss': 0.535, 'learning_rate': 8.133621878647842e-05, 'epoch': 0.28}
 28%|       | 1846/6500 [3:29:52<8:42:21,  6.73s/it] 28%|       | 1847/6500 [3:29:58<8:37:55,  6.68s/it]                                                        28%|       | 1847/6500 [3:29:58<8:37:55,  6.68s/it] 28%|       | 1848/6500 [3:30:05<8:34:56,  6.64s/it]                                                        28%|       | 1848/6500 [3:30:05<8:34:56,  6.64s/it] 28%|       | 1849/6500 [3:30:12<8:32:48,  6.62s/it]                                                        28%|       | 1849/6500 [3:30:12<8:32:48,  6.62s/it] 28%|       | 1850/6500 [3:30:18<8:31:16,  6.60s/it]                                                        28%|       | 1850/6500 [3:30:18<8:31:16,  6.60s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8362184762954712, 'eval_runtime': 1.481, 'eval_samples_per_second': 8.103, 'eval_steps_per_second': 2.026, 'epoch': 0.28}
                                                        28%|       | 1850/6500 [3:30:20<8:31:16,  6.60s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-1850
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1850I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1850

I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1850
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1850
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1850/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1850/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1850/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1850/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.5446, 'learning_rate': 8.131737811470786e-05, 'epoch': 0.28}
{'loss': 0.5411, 'learning_rate': 8.129853012269056e-05, 'epoch': 0.28}
{'loss': 0.5281, 'learning_rate': 8.127967481483217e-05, 'epoch': 0.29}
{'loss': 0.5337, 'learning_rate': 8.126081219553995e-05, 'epoch': 0.29}
{'loss': 0.5182, 'learning_rate': 8.124194226922296e-05, 'epoch': 0.29}
 28%|       | 1851/6500 [3:30:26<9:11:10,  7.11s/it]                                                        28%|       | 1851/6500 [3:30:26<9:11:10,  7.11s/it] 28%|       | 1852/6500 [3:30:33<8:58:15,  6.95s/it]                                                        28%|       | 1852/6500 [3:30:33<8:58:15,  6.95s/it] 29%|       | 1853/6500 [3:30:40<8:49:11,  6.83s/it]                                                        29%|       | 1853/6500 [3:30:40<8:49:11,  6.83s/it] 29%|       | 1854/6500 [3:30:46<8:42:51,  6.75s/it]                                                        29%|       | 1854/6500 [3:30:46<8:42:51,  6.75s/it] 29%|       | 1855/6500 [3:30:53<8:38:02,  6.69s/it]                                                        29%|       | 1855/6500 [3:30:53<8:38:02,  6.69s/it] 29%|       | 1856/6500 [3:30:59<8:34:47,  6.65s/it]                                                       {'loss': 0.5727, 'learning_rate': 8.122306504029194e-05, 'epoch': 0.29}
{'loss': 0.5216, 'learning_rate': 8.120418051315927e-05, 'epoch': 0.29}
{'loss': 0.8122, 'learning_rate': 8.118528869223914e-05, 'epoch': 0.29}
{'loss': 0.5568, 'learning_rate': 8.11663895819474e-05, 'epoch': 0.29}
{'loss': 0.5134, 'learning_rate': 8.114748318670159e-05, 'epoch': 0.29}
 29%|       | 1856/6500 [3:30:59<8:34:47,  6.65s/it] 29%|       | 1857/6500 [3:31:06<8:32:39,  6.62s/it]                                                        29%|       | 1857/6500 [3:31:06<8:32:39,  6.62s/it] 29%|       | 1858/6500 [3:31:12<8:30:24,  6.60s/it]                                                        29%|       | 1858/6500 [3:31:12<8:30:24,  6.60s/it] 29%|       | 1859/6500 [3:31:20<8:50:40,  6.86s/it]                                                        29%|       | 1859/6500 [3:31:20<8:50:40,  6.86s/it] 29%|       | 1860/6500 [3:31:26<8:43:04,  6.76s/it]                                                        29%|       | 1860/6500 [3:31:26<8:43:04,  6.76s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8410325050354004, 'eval_runtime': 1.4776, 'eval_samples_per_second': 8.121, 'eval_steps_per_second': 2.03, 'epoch': 0.29}
                                                        29%|       | 1860/6500 [3:31:28<8:43:04,  6.76s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-1860
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1860
/projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1860
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1860
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1860
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1860/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1860/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1860/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1860/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.5501, 'learning_rate': 8.112856951092097e-05, 'epoch': 0.29}
{'loss': 0.5133, 'learning_rate': 8.110964855902647e-05, 'epoch': 0.29}
{'loss': 0.5297, 'learning_rate': 8.109072033544079e-05, 'epoch': 0.29}
{'loss': 0.5423, 'learning_rate': 8.107178484458824e-05, 'epoch': 0.29}
{'loss': 0.5493, 'learning_rate': 8.105284209089492e-05, 'epoch': 0.29}
 29%|       | 1861/6500 [3:31:35<9:18:54,  7.23s/it]                                                        29%|       | 1861/6500 [3:31:35<9:18:54,  7.23s/it] 29%|       | 1862/6500 [3:31:41<9:03:25,  7.03s/it]                                                        29%|       | 1862/6500 [3:31:41<9:03:25,  7.03s/it] 29%|       | 1863/6500 [3:31:48<8:52:14,  6.89s/it]                                                        29%|       | 1863/6500 [3:31:48<8:52:14,  6.89s/it] 29%|       | 1864/6500 [3:31:54<8:44:31,  6.79s/it]                                                        29%|       | 1864/6500 [3:31:54<8:44:31,  6.79s/it] 29%|       | 1865/6500 [3:32:01<8:38:57,  6.72s/it]                                                        29%|       | 1865/6500 [3:32:01<8:38:57,  6.72s/it] 29%|       | 1866/6500 [3:32:07<8:34:51,  6.67s/it]                                                       {'loss': 0.5346, 'learning_rate': 8.103389207878856e-05, 'epoch': 0.29}
{'loss': 0.5382, 'learning_rate': 8.101493481269862e-05, 'epoch': 0.29}
{'loss': 0.5274, 'learning_rate': 8.099597029705625e-05, 'epoch': 0.29}
{'loss': 0.5311, 'learning_rate': 8.097699853629426e-05, 'epoch': 0.29}
{'loss': 0.5176, 'learning_rate': 8.095801953484723e-05, 'epoch': 0.29}
 29%|       | 1866/6500 [3:32:07<8:34:51,  6.67s/it] 29%|       | 1867/6500 [3:32:14<8:32:06,  6.63s/it]                                                        29%|       | 1867/6500 [3:32:14<8:32:06,  6.63s/it] 29%|       | 1868/6500 [3:32:20<8:29:51,  6.60s/it]                                                        29%|       | 1868/6500 [3:32:20<8:29:51,  6.60s/it] 29%|       | 1869/6500 [3:32:27<8:28:18,  6.59s/it]                                                        29%|       | 1869/6500 [3:32:27<8:28:18,  6.59s/it] 29%|       | 1870/6500 [3:32:34<8:27:33,  6.58s/it]                                                        29%|       | 1870/6500 [3:32:34<8:27:33,  6.58s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8442622423171997, 'eval_runtime': 1.4777, 'eval_samples_per_second': 8.121, 'eval_steps_per_second': 2.03, 'epoch': 0.29}
                                                        29%|       | 1870/6500 [3:32:35<8:27:33,  6.58s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-1870
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1870
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1870
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1870
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1870
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1870/pytorch_model.bin
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1870/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1870/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1870/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.567, 'learning_rate': 8.093903329715135e-05, 'epoch': 0.29}
{'loss': 0.5279, 'learning_rate': 8.092003982764455e-05, 'epoch': 0.29}
{'loss': 0.8101, 'learning_rate': 8.090103913076642e-05, 'epoch': 0.29}
{'loss': 0.5469, 'learning_rate': 8.08820312109583e-05, 'epoch': 0.29}
{'loss': 0.5268, 'learning_rate': 8.086301607266314e-05, 'epoch': 0.29}
 29%|       | 1871/6500 [3:32:42<9:09:17,  7.12s/it]                                                        29%|       | 1871/6500 [3:32:42<9:09:17,  7.12s/it] 29%|       | 1872/6500 [3:32:49<8:56:00,  6.95s/it]                                                        29%|       | 1872/6500 [3:32:49<8:56:00,  6.95s/it] 29%|       | 1873/6500 [3:32:55<8:46:44,  6.83s/it]                                                        29%|       | 1873/6500 [3:32:55<8:46:44,  6.83s/it] 29%|       | 1874/6500 [3:33:02<8:40:32,  6.75s/it]                                                        29%|       | 1874/6500 [3:33:02<8:40:32,  6.75s/it] 29%|       | 1875/6500 [3:33:09<8:49:11,  6.87s/it]                                                        29%|       | 1875/6500 [3:33:09<8:49:11,  6.87s/it] 29%|       | 1876/6500 [3:33:15<8:41:47,  6.77s/it]                                                       {'loss': 0.519, 'learning_rate': 8.084399372032562e-05, 'epoch': 0.29}
{'loss': 0.5071, 'learning_rate': 8.082496415839212e-05, 'epoch': 0.29}
{'loss': 0.5215, 'learning_rate': 8.080592739131063e-05, 'epoch': 0.29}
{'loss': 0.5237, 'learning_rate': 8.078688342353095e-05, 'epoch': 0.29}
{'loss': 0.5438, 'learning_rate': 8.076783225950444e-05, 'epoch': 0.29}
 29%|       | 1876/6500 [3:33:15<8:41:47,  6.77s/it] 29%|       | 1877/6500 [3:33:22<8:36:34,  6.70s/it]                                                        29%|       | 1877/6500 [3:33:22<8:36:34,  6.70s/it] 29%|       | 1878/6500 [3:33:28<8:33:14,  6.66s/it]                                                        29%|       | 1878/6500 [3:33:28<8:33:14,  6.66s/it] 29%|       | 1879/6500 [3:33:35<8:30:47,  6.63s/it]                                                        29%|       | 1879/6500 [3:33:35<8:30:47,  6.63s/it] 29%|       | 1880/6500 [3:33:42<8:28:43,  6.61s/it]                                                        29%|       | 1880/6500 [3:33:42<8:28:43,  6.61s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8378265500068665, 'eval_runtime': 1.4956, 'eval_samples_per_second': 8.024, 'eval_steps_per_second': 2.006, 'epoch': 0.29}
                                                        29%|       | 1880/6500 [3:33:43<8:28:43,  6.61s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-1880
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  I AM HERE AND I AM SAVING THE MODEL
/projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1880the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1880

I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1880
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1880
the pytorch model path isthe pytorch model path is the pytorch model path is/projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1880/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1880/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1880/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1880/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.5443, 'learning_rate': 8.074877390368423e-05, 'epoch': 0.29}
{'loss': 0.5336, 'learning_rate': 8.072970836052504e-05, 'epoch': 0.29}
{'loss': 0.5347, 'learning_rate': 8.07106356344834e-05, 'epoch': 0.29}
{'loss': 0.5277, 'learning_rate': 8.069155573001739e-05, 'epoch': 0.29}
{'loss': 0.5257, 'learning_rate': 8.067246865158682e-05, 'epoch': 0.29}
 29%|       | 1881/6500 [3:33:50<9:09:38,  7.14s/it]                                                        29%|       | 1881/6500 [3:33:50<9:09:38,  7.14s/it] 29%|       | 1882/6500 [3:33:57<8:56:21,  6.97s/it]                                                        29%|       | 1882/6500 [3:33:57<8:56:21,  6.97s/it] 29%|       | 1883/6500 [3:34:03<8:46:45,  6.85s/it]                                                        29%|       | 1883/6500 [3:34:03<8:46:45,  6.85s/it] 29%|       | 1884/6500 [3:34:10<8:39:54,  6.76s/it]                                                        29%|       | 1884/6500 [3:34:10<8:39:54,  6.76s/it] 29%|       | 1885/6500 [3:34:16<8:34:23,  6.69s/it]                                                        29%|       | 1885/6500 [3:34:16<8:34:23,  6.69s/it] 29%|       | 1886/6500 [3:34:23<8:30:54,  6.64s/it]                                                       {'loss': 0.5548, 'learning_rate': 8.065337440365321e-05, 'epoch': 0.29}
{'loss': 0.5335, 'learning_rate': 8.06342729906797e-05, 'epoch': 0.29}
{'loss': 0.8159, 'learning_rate': 8.061516441713115e-05, 'epoch': 0.29}
{'loss': 0.5279, 'learning_rate': 8.059604868747405e-05, 'epoch': 0.29}
{'loss': 0.5389, 'learning_rate': 8.057692580617659e-05, 'epoch': 0.29}
 29%|       | 1886/6500 [3:34:23<8:30:54,  6.64s/it] 29%|       | 1887/6500 [3:34:29<8:28:28,  6.61s/it]                                                        29%|       | 1887/6500 [3:34:29<8:28:28,  6.61s/it] 29%|       | 1888/6500 [3:34:36<8:26:45,  6.59s/it]                                                        29%|       | 1888/6500 [3:34:36<8:26:45,  6.59s/it] 29%|       | 1889/6500 [3:34:42<8:25:14,  6.57s/it]                                                        29%|       | 1889/6500 [3:34:42<8:25:14,  6.57s/it] 29%|       | 1890/6500 [3:34:49<8:24:06,  6.56s/it]                                                        29%|       | 1890/6500 [3:34:49<8:24:06,  6.56s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.843045175075531, 'eval_runtime': 1.4735, 'eval_samples_per_second': 8.144, 'eval_steps_per_second': 2.036, 'epoch': 0.29}
                                                        29%|       | 1890/6500 [3:34:50<8:24:06,  6.56s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-1890
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  I AM HERE AND I AM SAVING THE MODEL/projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1890
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1890

I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1890
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1890
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1890/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1890/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1890/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1890/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.5148, 'learning_rate': 8.055779577770866e-05, 'epoch': 0.29}
{'loss': 0.5144, 'learning_rate': 8.053865860654175e-05, 'epoch': 0.29}
{'loss': 0.5151, 'learning_rate': 8.051951429714906e-05, 'epoch': 0.29}
{'loss': 0.5368, 'learning_rate': 8.050036285400546e-05, 'epoch': 0.29}
{'loss': 0.5548, 'learning_rate': 8.04812042815875e-05, 'epoch': 0.29}
 29%|       | 1891/6500 [3:34:58<9:25:55,  7.37s/it]                                                        29%|       | 1891/6500 [3:34:58<9:25:55,  7.37s/it] 29%|       | 1892/6500 [3:35:05<9:06:54,  7.12s/it]                                                        29%|       | 1892/6500 [3:35:05<9:06:54,  7.12s/it] 29%|       | 1893/6500 [3:35:11<8:53:37,  6.95s/it]                                                        29%|       | 1893/6500 [3:35:11<8:53:37,  6.95s/it] 29%|       | 1894/6500 [3:35:18<8:43:55,  6.82s/it]                                                        29%|       | 1894/6500 [3:35:18<8:43:55,  6.82s/it] 29%|       | 1895/6500 [3:35:24<8:37:12,  6.74s/it]                                                        29%|       | 1895/6500 [3:35:24<8:37:12,  6.74s/it] 29%|       | 1896/6500 [3:35:31<8:32:30,  6.68s/it]                                                       {'loss': 0.5324, 'learning_rate': 8.046203858437337e-05, 'epoch': 0.29}
{'loss': 0.5263, 'learning_rate': 8.044286576684293e-05, 'epoch': 0.29}
{'loss': 0.5435, 'learning_rate': 8.04236858334777e-05, 'epoch': 0.29}
{'loss': 0.5174, 'learning_rate': 8.04044987887609e-05, 'epoch': 0.29}
{'loss': 0.5302, 'learning_rate': 8.038530463717738e-05, 'epoch': 0.29}
 29%|       | 1896/6500 [3:35:31<8:32:30,  6.68s/it] 29%|       | 1897/6500 [3:35:37<8:29:11,  6.64s/it]                                                        29%|       | 1897/6500 [3:35:37<8:29:11,  6.64s/it] 29%|       | 1898/6500 [3:35:44<8:26:32,  6.60s/it]                                                        29%|       | 1898/6500 [3:35:44<8:26:32,  6.60s/it] 29%|       | 1899/6500 [3:35:50<8:24:37,  6.58s/it]                                                        29%|       | 1899/6500 [3:35:50<8:24:37,  6.58s/it] 29%|       | 1900/6500 [3:35:57<8:23:26,  6.57s/it]                                                        29%|       | 1900/6500 [3:35:57<8:23:26,  6.57s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.842362105846405, 'eval_runtime': 1.4774, 'eval_samples_per_second': 8.122, 'eval_steps_per_second': 2.031, 'epoch': 0.29}
                                                        29%|       | 1900/6500 [3:35:58<8:23:26,  6.57s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-1900
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1900
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1900
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1900
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1900
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1900/pytorch_model.bin
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1900/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1900/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1900/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.5514, 'learning_rate': 8.036610338321362e-05, 'epoch': 0.29}
{'loss': 0.5285, 'learning_rate': 8.034689503135783e-05, 'epoch': 0.29}
{'loss': 0.8147, 'learning_rate': 8.032767958609986e-05, 'epoch': 0.29}
{'loss': 0.5192, 'learning_rate': 8.030845705193116e-05, 'epoch': 0.29}
{'loss': 0.5486, 'learning_rate': 8.028922743334492e-05, 'epoch': 0.29}
 29%|       | 1901/6500 [3:36:05<9:04:14,  7.10s/it]                                                        29%|       | 1901/6500 [3:36:05<9:04:14,  7.10s/it] 29%|       | 1902/6500 [3:36:12<8:51:11,  6.93s/it]                                                        29%|       | 1902/6500 [3:36:12<8:51:11,  6.93s/it] 29%|       | 1903/6500 [3:36:18<8:42:14,  6.82s/it]                                                        29%|       | 1903/6500 [3:36:18<8:42:14,  6.82s/it] 29%|       | 1904/6500 [3:36:25<8:35:29,  6.73s/it]                                                        29%|       | 1904/6500 [3:36:25<8:35:29,  6.73s/it] 29%|       | 1905/6500 [3:36:31<8:31:13,  6.68s/it]                                                        29%|       | 1905/6500 [3:36:31<8:31:13,  6.68s/it] 29%|       | 1906/6500 [3:36:38<8:28:13,  6.64s/it]                                                       {'loss': 0.5086, 'learning_rate': 8.026999073483593e-05, 'epoch': 0.29}
{'loss': 0.5157, 'learning_rate': 8.025074696090063e-05, 'epoch': 0.29}
{'loss': 0.5212, 'learning_rate': 8.023149611603717e-05, 'epoch': 0.29}
{'loss': 0.5242, 'learning_rate': 8.021223820474529e-05, 'epoch': 0.29}
{'loss': 0.5601, 'learning_rate': 8.019297323152642e-05, 'epoch': 0.29}
 29%|       | 1906/6500 [3:36:38<8:28:13,  6.64s/it] 29%|       | 1907/6500 [3:36:45<8:44:56,  6.86s/it]                                                        29%|       | 1907/6500 [3:36:45<8:44:56,  6.86s/it] 29%|       | 1908/6500 [3:36:52<8:37:33,  6.76s/it]                                                        29%|       | 1908/6500 [3:36:52<8:37:33,  6.76s/it] 29%|       | 1909/6500 [3:36:58<8:32:04,  6.69s/it]                                                        29%|       | 1909/6500 [3:36:58<8:32:04,  6.69s/it] 29%|       | 1910/6500 [3:37:05<8:28:31,  6.65s/it]                                                        29%|       | 1910/6500 [3:37:05<8:28:31,  6.65s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8340104222297668, 'eval_runtime': 1.4765, 'eval_samples_per_second': 8.128, 'eval_steps_per_second': 2.032, 'epoch': 0.29}
                                                        29%|       | 1910/6500 [3:37:06<8:28:31,  6.65s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-1910
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1910
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1910
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1910
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1910
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1910/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1910/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1910/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1910/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.5519, 'learning_rate': 8.017370120088365e-05, 'epoch': 0.29}
{'loss': 0.5159, 'learning_rate': 8.015442211732167e-05, 'epoch': 0.29}
{'loss': 0.5328, 'learning_rate': 8.013513598534688e-05, 'epoch': 0.29}
{'loss': 0.5181, 'learning_rate': 8.011584280946726e-05, 'epoch': 0.29}
{'loss': 0.5639, 'learning_rate': 8.00965425941925e-05, 'epoch': 0.29}
 29%|       | 1911/6500 [3:37:13<9:09:06,  7.18s/it]                                                        29%|       | 1911/6500 [3:37:13<9:09:06,  7.18s/it] 29%|       | 1912/6500 [3:37:20<8:54:19,  6.99s/it]                                                        29%|       | 1912/6500 [3:37:20<8:54:19,  6.99s/it] 29%|       | 1913/6500 [3:37:26<8:43:48,  6.85s/it]                                                        29%|       | 1913/6500 [3:37:26<8:43:48,  6.85s/it] 29%|       | 1914/6500 [3:37:33<8:36:13,  6.75s/it]                                                        29%|       | 1914/6500 [3:37:33<8:36:13,  6.75s/it] 29%|       | 1915/6500 [3:37:40<8:31:15,  6.69s/it]                                                        29%|       | 1915/6500 [3:37:40<8:31:15,  6.69s/it] 29%|       | 1916/6500 [3:37:46<8:27:39,  6.64s/it]                                                       {'loss': 0.5144, 'learning_rate': 8.007723534403389e-05, 'epoch': 0.29}
{'loss': 0.8061, 'learning_rate': 8.005792106350441e-05, 'epoch': 0.29}
{'loss': 0.5481, 'learning_rate': 8.003859975711862e-05, 'epoch': 0.3}
{'loss': 0.5147, 'learning_rate': 8.001927142939278e-05, 'epoch': 0.3}
{'loss': 0.5403, 'learning_rate': 7.999993608484477e-05, 'epoch': 0.3}
 29%|       | 1916/6500 [3:37:46<8:27:39,  6.64s/it] 29%|       | 1917/6500 [3:37:53<8:25:03,  6.61s/it]                                                        29%|       | 1917/6500 [3:37:53<8:25:03,  6.61s/it] 30%|       | 1918/6500 [3:37:59<8:23:07,  6.59s/it]                                                        30%|       | 1918/6500 [3:37:59<8:23:07,  6.59s/it] 30%|       | 1919/6500 [3:38:06<8:21:59,  6.57s/it]                                                        30%|       | 1919/6500 [3:38:06<8:21:59,  6.57s/it] 30%|       | 1920/6500 [3:38:12<8:21:25,  6.57s/it]                                                        30%|       | 1920/6500 [3:38:12<8:21:25,  6.57s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8421029448509216, 'eval_runtime': 1.4759, 'eval_samples_per_second': 8.131, 'eval_steps_per_second': 2.033, 'epoch': 0.3}
                                                        30%|       | 1920/6500 [3:38:14<8:21:25,  6.57s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-1920
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1920
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1920
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1920
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1920
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1920/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1920/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1920/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1920/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.4971, 'learning_rate': 7.998059372799409e-05, 'epoch': 0.3}
{'loss': 0.5135, 'learning_rate': 7.996124436336192e-05, 'epoch': 0.3}
{'loss': 0.5281, 'learning_rate': 7.994188799547105e-05, 'epoch': 0.3}
{'loss': 0.5275, 'learning_rate': 7.99225246288459e-05, 'epoch': 0.3}
{'loss': 0.5426, 'learning_rate': 7.990315426801255e-05, 'epoch': 0.3}
 30%|       | 1921/6500 [3:38:21<9:01:35,  7.10s/it]                                                        30%|       | 1921/6500 [3:38:21<9:01:35,  7.10s/it] 30%|       | 1922/6500 [3:38:27<8:48:51,  6.93s/it]                                                        30%|       | 1922/6500 [3:38:27<8:48:51,  6.93s/it] 30%|       | 1923/6500 [3:38:34<8:39:52,  6.81s/it]                                                        30%|       | 1923/6500 [3:38:34<8:39:52,  6.81s/it] 30%|       | 1924/6500 [3:38:41<8:53:36,  7.00s/it]                                                        30%|       | 1924/6500 [3:38:41<8:53:36,  7.00s/it] 30%|       | 1925/6500 [3:38:48<8:43:26,  6.86s/it]                                                        30%|       | 1925/6500 [3:38:48<8:43:26,  6.86s/it] 30%|       | 1926/6500 [3:38:54<8:36:17,  6.77s/it]                                                       {'loss': 0.5318, 'learning_rate': 7.988377691749871e-05, 'epoch': 0.3}
{'loss': 0.5123, 'learning_rate': 7.986439258183372e-05, 'epoch': 0.3}
{'loss': 0.5239, 'learning_rate': 7.984500126554853e-05, 'epoch': 0.3}
{'loss': 0.5158, 'learning_rate': 7.982560297317575e-05, 'epoch': 0.3}
{'loss': 0.5573, 'learning_rate': 7.980619770924962e-05, 'epoch': 0.3}
 30%|       | 1926/6500 [3:38:54<8:36:17,  6.77s/it] 30%|       | 1927/6500 [3:39:01<8:30:57,  6.70s/it]                                                        30%|       | 1927/6500 [3:39:01<8:30:57,  6.70s/it] 30%|       | 1928/6500 [3:39:07<8:27:04,  6.65s/it]                                                        30%|       | 1928/6500 [3:39:07<8:27:04,  6.65s/it] 30%|       | 1929/6500 [3:39:14<8:24:31,  6.62s/it]                                                        30%|       | 1929/6500 [3:39:14<8:24:31,  6.62s/it] 30%|       | 1930/6500 [3:39:20<8:22:38,  6.60s/it]                                                        30%|       | 1930/6500 [3:39:20<8:22:38,  6.60s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8422235250473022, 'eval_runtime': 1.4855, 'eval_samples_per_second': 8.078, 'eval_steps_per_second': 2.019, 'epoch': 0.3}
                                                        30%|       | 1930/6500 [3:39:22<8:22:38,  6.60s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-1930
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1930I AM HERE AND I AM SAVING THE MODEL

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1930
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1930
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1930
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1930/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1930/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1930/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1930/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.5151, 'learning_rate': 7.9786785478306e-05, 'epoch': 0.3}
{'loss': 0.7969, 'learning_rate': 7.97673662848824e-05, 'epoch': 0.3}
{'loss': 0.5427, 'learning_rate': 7.974794013351789e-05, 'epoch': 0.3}
{'loss': 0.508, 'learning_rate': 7.972850702875327e-05, 'epoch': 0.3}
{'loss': 0.5363, 'learning_rate': 7.970906697513088e-05, 'epoch': 0.3}
 30%|       | 1931/6500 [3:39:29<9:03:32,  7.14s/it]                                                        30%|       | 1931/6500 [3:39:29<9:03:32,  7.14s/it] 30%|       | 1932/6500 [3:39:35<8:49:37,  6.96s/it]                                                        30%|       | 1932/6500 [3:39:35<8:49:37,  6.96s/it] 30%|       | 1933/6500 [3:39:42<8:40:10,  6.83s/it]                                                        30%|       | 1933/6500 [3:39:42<8:40:10,  6.83s/it] 30%|       | 1934/6500 [3:39:48<8:33:08,  6.74s/it]                                                        30%|       | 1934/6500 [3:39:48<8:33:08,  6.74s/it] 30%|       | 1935/6500 [3:39:55<8:28:22,  6.68s/it]                                                        30%|       | 1935/6500 [3:39:55<8:28:22,  6.68s/it] 30%|       | 1936/6500 [3:40:01<8:25:00,  6.64s/it]                                                       {'loss': 0.5005, 'learning_rate': 7.96896199771947e-05, 'epoch': 0.3}
{'loss': 0.5117, 'learning_rate': 7.96701660394904e-05, 'epoch': 0.3}
{'loss': 0.5215, 'learning_rate': 7.965070516656517e-05, 'epoch': 0.3}
{'loss': 0.5257, 'learning_rate': 7.963123736296787e-05, 'epoch': 0.3}
{'loss': 0.5312, 'learning_rate': 7.961176263324901e-05, 'epoch': 0.3}
 30%|       | 1936/6500 [3:40:01<8:25:00,  6.64s/it] 30%|       | 1937/6500 [3:40:08<8:22:35,  6.61s/it]                                                        30%|       | 1937/6500 [3:40:08<8:22:35,  6.61s/it] 30%|       | 1938/6500 [3:40:15<8:20:49,  6.59s/it]                                                        30%|       | 1938/6500 [3:40:15<8:20:49,  6.59s/it] 30%|       | 1939/6500 [3:40:21<8:19:32,  6.57s/it]                                                        30%|       | 1939/6500 [3:40:21<8:19:32,  6.57s/it] 30%|       | 1940/6500 [3:40:29<8:41:39,  6.86s/it]                                                        30%|       | 1940/6500 [3:40:29<8:41:39,  6.86s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.834977924823761, 'eval_runtime': 1.4842, 'eval_samples_per_second': 8.085, 'eval_steps_per_second': 2.021, 'epoch': 0.3}
                                                        30%|       | 1940/6500 [3:40:30<8:41:39,  6.86s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-1940
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1940
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1940
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1940
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1940
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1940/pytorch_model.bin
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1940/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1940/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1940/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.5343, 'learning_rate': 7.959228098196067e-05, 'epoch': 0.3}
{'loss': 0.5229, 'learning_rate': 7.957279241365659e-05, 'epoch': 0.3}
{'loss': 0.5179, 'learning_rate': 7.955329693289207e-05, 'epoch': 0.3}
{'loss': 0.5093, 'learning_rate': 7.953379454422409e-05, 'epoch': 0.3}
{'loss': 0.5558, 'learning_rate': 7.951428525221121e-05, 'epoch': 0.3}
 30%|       | 1941/6500 [3:40:37<9:17:07,  7.33s/it]                                                        30%|       | 1941/6500 [3:40:37<9:17:07,  7.33s/it] 30%|       | 1942/6500 [3:40:44<8:59:03,  7.10s/it]                                                        30%|       | 1942/6500 [3:40:44<8:59:03,  7.10s/it] 30%|       | 1943/6500 [3:40:50<8:46:19,  6.93s/it]                                                        30%|       | 1943/6500 [3:40:50<8:46:19,  6.93s/it] 30%|       | 1944/6500 [3:40:57<8:37:16,  6.81s/it]                                                        30%|       | 1944/6500 [3:40:57<8:37:16,  6.81s/it] 30%|       | 1945/6500 [3:41:03<8:31:10,  6.73s/it]                                                        30%|       | 1945/6500 [3:41:03<8:31:10,  6.73s/it] 30%|       | 1946/6500 [3:41:10<8:26:42,  6.68s/it]                                                       {'loss': 0.5235, 'learning_rate': 7.94947690614136e-05, 'epoch': 0.3}
{'loss': 0.8004, 'learning_rate': 7.947524597639304e-05, 'epoch': 0.3}
{'loss': 0.5369, 'learning_rate': 7.945571600171295e-05, 'epoch': 0.3}
{'loss': 0.5215, 'learning_rate': 7.943617914193833e-05, 'epoch': 0.3}
{'loss': 0.5163, 'learning_rate': 7.941663540163582e-05, 'epoch': 0.3}
 30%|       | 1946/6500 [3:41:10<8:26:42,  6.68s/it] 30%|       | 1947/6500 [3:41:16<8:23:19,  6.63s/it]                                                        30%|       | 1947/6500 [3:41:16<8:23:19,  6.63s/it] 30%|       | 1948/6500 [3:41:23<8:21:27,  6.61s/it]                                                        30%|       | 1948/6500 [3:41:23<8:21:27,  6.61s/it] 30%|       | 1949/6500 [3:41:29<8:19:43,  6.59s/it]                                                        30%|       | 1949/6500 [3:41:29<8:19:43,  6.59s/it] 30%|       | 1950/6500 [3:41:36<8:18:28,  6.57s/it]                                                        30%|       | 1950/6500 [3:41:36<8:18:28,  6.57s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8426684737205505, 'eval_runtime': 1.4815, 'eval_samples_per_second': 8.1, 'eval_steps_per_second': 2.025, 'epoch': 0.3}
                                                        30%|       | 1950/6500 [3:41:37<8:18:28,  6.57s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-1950
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1950
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1950
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1950
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1950
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1950/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1950/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1950/pytorch_model.binthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1950/pytorch_model.bin

/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.5095, 'learning_rate': 7.939708478537364e-05, 'epoch': 0.3}
{'loss': 0.5206, 'learning_rate': 7.937752729772164e-05, 'epoch': 0.3}
{'loss': 0.5198, 'learning_rate': 7.935796294325124e-05, 'epoch': 0.3}
{'loss': 0.539, 'learning_rate': 7.93383917265355e-05, 'epoch': 0.3}
{'loss': 0.5248, 'learning_rate': 7.931881365214908e-05, 'epoch': 0.3}
 30%|       | 1951/6500 [3:41:44<8:57:39,  7.09s/it]                                                        30%|       | 1951/6500 [3:41:44<8:57:39,  7.09s/it] 30%|       | 1952/6500 [3:41:51<8:44:53,  6.92s/it]                                                        30%|       | 1952/6500 [3:41:51<8:44:53,  6.92s/it] 30%|       | 1953/6500 [3:41:57<8:35:54,  6.81s/it]                                                        30%|       | 1953/6500 [3:41:57<8:35:54,  6.81s/it] 30%|       | 1954/6500 [3:42:04<8:29:28,  6.72s/it]                                                        30%|       | 1954/6500 [3:42:04<8:29:28,  6.72s/it] 30%|       | 1955/6500 [3:42:10<8:24:48,  6.66s/it]                                                        30%|       | 1955/6500 [3:42:10<8:24:48,  6.66s/it] 30%|       | 1956/6500 [3:42:17<8:36:01,  6.81s/it]                                                       {'loss': 0.5254, 'learning_rate': 7.929922872466823e-05, 'epoch': 0.3}
{'loss': 0.5223, 'learning_rate': 7.92796369486708e-05, 'epoch': 0.3}
{'loss': 0.5176, 'learning_rate': 7.926003832873627e-05, 'epoch': 0.3}
{'loss': 0.5085, 'learning_rate': 7.924043286944569e-05, 'epoch': 0.3}
{'loss': 0.549, 'learning_rate': 7.92208205753817e-05, 'epoch': 0.3}
 30%|       | 1956/6500 [3:42:17<8:36:01,  6.81s/it] 30%|       | 1957/6500 [3:42:24<8:29:14,  6.73s/it]                                                        30%|       | 1957/6500 [3:42:24<8:29:14,  6.73s/it] 30%|       | 1958/6500 [3:42:31<8:24:55,  6.67s/it]                                                        30%|       | 1958/6500 [3:42:31<8:24:55,  6.67s/it] 30%|       | 1959/6500 [3:42:37<8:22:00,  6.63s/it]                                                        30%|       | 1959/6500 [3:42:37<8:22:00,  6.63s/it] 30%|       | 1960/6500 [3:42:44<8:19:48,  6.61s/it]                                                        30%|       | 1960/6500 [3:42:44<8:19:48,  6.61s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8403835296630859, 'eval_runtime': 1.7397, 'eval_samples_per_second': 6.898, 'eval_steps_per_second': 1.724, 'epoch': 0.3}
                                                        30%|       | 1960/6500 [3:42:45<8:19:48,  6.61s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-1960
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1960I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1960

I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1960
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1960
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1960/pytorch_model.bin
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1960/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1960/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1960/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.5234, 'learning_rate': 7.920120145112855e-05, 'epoch': 0.3}
{'loss': 0.8028, 'learning_rate': 7.918157550127213e-05, 'epoch': 0.3}
{'loss': 0.5264, 'learning_rate': 7.916194273039986e-05, 'epoch': 0.3}
{'loss': 0.5276, 'learning_rate': 7.914230314310079e-05, 'epoch': 0.3}
{'loss': 0.5121, 'learning_rate': 7.912265674396553e-05, 'epoch': 0.3}
 30%|       | 1961/6500 [3:42:52<9:06:46,  7.23s/it]                                                        30%|       | 1961/6500 [3:42:52<9:06:46,  7.23s/it] 30%|       | 1962/6500 [3:42:59<8:50:45,  7.02s/it]                                                        30%|       | 1962/6500 [3:42:59<8:50:45,  7.02s/it] 30%|       | 1963/6500 [3:43:05<8:39:37,  6.87s/it]                                                        30%|       | 1963/6500 [3:43:05<8:39:37,  6.87s/it] 30%|       | 1964/6500 [3:43:12<8:31:36,  6.77s/it]                                                        30%|       | 1964/6500 [3:43:12<8:31:36,  6.77s/it] 30%|       | 1965/6500 [3:43:18<8:26:25,  6.70s/it]                                                        30%|       | 1965/6500 [3:43:18<8:26:25,  6.70s/it] 30%|       | 1966/6500 [3:43:25<8:22:17,  6.65s/it]                                                       {'loss': 0.509, 'learning_rate': 7.910300353758633e-05, 'epoch': 0.3}
{'loss': 0.5109, 'learning_rate': 7.9083343528557e-05, 'epoch': 0.3}
{'loss': 0.5297, 'learning_rate': 7.906367672147297e-05, 'epoch': 0.3}
{'loss': 0.5387, 'learning_rate': 7.904400312093119e-05, 'epoch': 0.3}
{'loss': 0.5287, 'learning_rate': 7.902432273153028e-05, 'epoch': 0.3}
 30%|       | 1966/6500 [3:43:25<8:22:17,  6.65s/it] 30%|       | 1967/6500 [3:43:31<8:19:35,  6.61s/it]                                                        30%|       | 1967/6500 [3:43:31<8:19:35,  6.61s/it] 30%|       | 1968/6500 [3:43:38<8:18:17,  6.60s/it]                                                        30%|       | 1968/6500 [3:43:38<8:18:17,  6.60s/it] 30%|       | 1969/6500 [3:43:45<8:17:11,  6.58s/it]                                                        30%|       | 1969/6500 [3:43:45<8:17:11,  6.58s/it] 30%|       | 1970/6500 [3:43:51<8:16:41,  6.58s/it]                                                        30%|       | 1970/6500 [3:43:51<8:16:41,  6.58s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8348073363304138, 'eval_runtime': 1.4817, 'eval_samples_per_second': 8.099, 'eval_steps_per_second': 2.025, 'epoch': 0.3}
                                                        30%|       | 1970/6500 [3:43:53<8:16:41,  6.58s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-1970
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in I AM HERE AND I AM SAVING THE MODEL 
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1970
/projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1970
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1970
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1970
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1970/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1970/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1970/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1970/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.5208, 'learning_rate': 7.900463555787042e-05, 'epoch': 0.3}
{'loss': 0.5291, 'learning_rate': 7.898494160455334e-05, 'epoch': 0.3}
{'loss': 0.5052, 'learning_rate': 7.896524087618238e-05, 'epoch': 0.3}
{'loss': 0.5153, 'learning_rate': 7.894553337736248e-05, 'epoch': 0.3}
{'loss': 0.5442, 'learning_rate': 7.892581911270013e-05, 'epoch': 0.3}
 30%|       | 1971/6500 [3:44:00<8:57:58,  7.13s/it]                                                        30%|       | 1971/6500 [3:44:00<8:57:58,  7.13s/it] 30%|       | 1972/6500 [3:44:07<8:58:56,  7.14s/it]                                                        30%|       | 1972/6500 [3:44:07<8:58:56,  7.14s/it] 30%|       | 1973/6500 [3:44:13<8:45:20,  6.96s/it]                                                        30%|       | 1973/6500 [3:44:13<8:45:20,  6.96s/it] 30%|       | 1974/6500 [3:44:20<8:36:00,  6.84s/it]                                                        30%|       | 1974/6500 [3:44:20<8:36:00,  6.84s/it] 30%|       | 1975/6500 [3:44:26<8:29:18,  6.75s/it]                                                        30%|       | 1975/6500 [3:44:26<8:29:18,  6.75s/it] 30%|       | 1976/6500 [3:44:33<8:24:54,  6.70s/it]                                                       {'loss': 0.5219, 'learning_rate': 7.890609808680347e-05, 'epoch': 0.3}
{'loss': 0.8091, 'learning_rate': 7.888637030428211e-05, 'epoch': 0.3}
{'loss': 0.5088, 'learning_rate': 7.886663576974733e-05, 'epoch': 0.3}
{'loss': 0.5398, 'learning_rate': 7.884689448781196e-05, 'epoch': 0.3}
{'loss': 0.5045, 'learning_rate': 7.882714646309038e-05, 'epoch': 0.3}
 30%|       | 1976/6500 [3:44:33<8:24:54,  6.70s/it] 30%|       | 1977/6500 [3:44:40<8:21:05,  6.65s/it]                                                        30%|       | 1977/6500 [3:44:40<8:21:05,  6.65s/it] 30%|       | 1978/6500 [3:44:46<8:18:46,  6.62s/it]                                                        30%|       | 1978/6500 [3:44:46<8:18:46,  6.62s/it] 30%|       | 1979/6500 [3:44:53<8:17:30,  6.60s/it]                                                        30%|       | 1979/6500 [3:44:53<8:17:30,  6.60s/it] 30%|       | 1980/6500 [3:44:59<8:16:55,  6.60s/it]                                                        30%|       | 1980/6500 [3:44:59<8:16:55,  6.60s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8443752527236938, 'eval_runtime': 1.4799, 'eval_samples_per_second': 8.108, 'eval_steps_per_second': 2.027, 'epoch': 0.3}
                                                        30%|       | 1980/6500 [3:45:01<8:16:55,  6.60s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-1980
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1980I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1980

I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1980
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1980
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1980/pytorch_model.bin
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1980/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1980/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1980/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.4973, 'learning_rate': 7.88073917001986e-05, 'epoch': 0.3}
{'loss': 0.5112, 'learning_rate': 7.878763020375415e-05, 'epoch': 0.3}
{'loss': 0.515, 'learning_rate': 7.876786197837617e-05, 'epoch': 0.31}
{'loss': 0.5354, 'learning_rate': 7.874808702868536e-05, 'epoch': 0.31}
{'loss': 0.5172, 'learning_rate': 7.872830535930401e-05, 'epoch': 0.31}
 30%|       | 1981/6500 [3:45:08<8:58:56,  7.16s/it]                                                        30%|       | 1981/6500 [3:45:08<8:58:56,  7.16s/it] 30%|       | 1982/6500 [3:45:14<8:45:15,  6.98s/it]                                                        30%|       | 1982/6500 [3:45:14<8:45:15,  6.98s/it] 31%|       | 1983/6500 [3:45:21<8:35:55,  6.85s/it]                                                        31%|       | 1983/6500 [3:45:21<8:35:55,  6.85s/it] 31%|       | 1984/6500 [3:45:27<8:29:12,  6.77s/it]                                                        31%|       | 1984/6500 [3:45:27<8:29:12,  6.77s/it] 31%|       | 1985/6500 [3:45:34<8:24:32,  6.70s/it]                                                        31%|       | 1985/6500 [3:45:34<8:24:32,  6.70s/it] 31%|       | 1986/6500 [3:45:40<8:21:18,  6.66s/it]                                                       {'loss': 0.5096, 'learning_rate': 7.870851697485596e-05, 'epoch': 0.31}
{'loss': 0.5281, 'learning_rate': 7.868872187996659e-05, 'epoch': 0.31}
{'loss': 0.5162, 'learning_rate': 7.866892007926294e-05, 'epoch': 0.31}
{'loss': 0.5263, 'learning_rate': 7.864911157737352e-05, 'epoch': 0.31}
{'loss': 0.5253, 'learning_rate': 7.862929637892845e-05, 'epoch': 0.31}
 31%|       | 1986/6500 [3:45:40<8:21:18,  6.66s/it] 31%|       | 1987/6500 [3:45:47<8:19:16,  6.64s/it]                                                        31%|       | 1987/6500 [3:45:47<8:19:16,  6.64s/it] 31%|       | 1988/6500 [3:45:55<8:37:43,  6.88s/it]                                                        31%|       | 1988/6500 [3:45:55<8:37:43,  6.88s/it] 31%|       | 1989/6500 [3:46:01<8:30:27,  6.79s/it]                                                        31%|       | 1989/6500 [3:46:01<8:30:27,  6.79s/it] 31%|       | 1990/6500 [3:46:08<8:25:20,  6.72s/it]                                                        31%|       | 1990/6500 [3:46:08<8:25:20,  6.72s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8404334783554077, 'eval_runtime': 1.5058, 'eval_samples_per_second': 7.969, 'eval_steps_per_second': 1.992, 'epoch': 0.31}
                                                        31%|       | 1990/6500 [3:46:09<8:25:20,  6.72s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-1990
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1990I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1990

I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1990
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1990
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1990/pytorch_model.bin
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1990/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1990/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-1990/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.6194, 'learning_rate': 7.860947448855944e-05, 'epoch': 0.31}
{'loss': 0.7185, 'learning_rate': 7.85896459108997e-05, 'epoch': 0.31}
{'loss': 0.4977, 'learning_rate': 7.856981065058407e-05, 'epoch': 0.31}
{'loss': 0.5332, 'learning_rate': 7.85499687122489e-05, 'epoch': 0.31}
{'loss': 0.4996, 'learning_rate': 7.853012010053212e-05, 'epoch': 0.31}
 31%|       | 1991/6500 [3:46:16<9:02:46,  7.22s/it]                                                        31%|       | 1991/6500 [3:46:16<9:02:46,  7.22s/it] 31%|       | 1992/6500 [3:46:23<8:47:21,  7.02s/it]                                                        31%|       | 1992/6500 [3:46:23<8:47:21,  7.02s/it] 31%|       | 1993/6500 [3:46:29<8:36:16,  6.87s/it]                                                        31%|       | 1993/6500 [3:46:29<8:36:16,  6.87s/it] 31%|       | 1994/6500 [3:46:36<8:28:56,  6.78s/it]                                                        31%|       | 1994/6500 [3:46:36<8:28:56,  6.78s/it] 31%|       | 1995/6500 [3:46:42<8:23:35,  6.71s/it]                                                        31%|       | 1995/6500 [3:46:42<8:23:35,  6.71s/it] 31%|       | 1996/6500 [3:46:49<8:19:35,  6.66s/it]                                                       {'loss': 0.5015, 'learning_rate': 7.851026482007324e-05, 'epoch': 0.31}
{'loss': 0.5152, 'learning_rate': 7.849040287551331e-05, 'epoch': 0.31}
{'loss': 0.5185, 'learning_rate': 7.847053427149494e-05, 'epoch': 0.31}
{'loss': 0.5375, 'learning_rate': 7.845065901266227e-05, 'epoch': 0.31}
{'loss': 0.5322, 'learning_rate': 7.843077710366105e-05, 'epoch': 0.31}
 31%|       | 1996/6500 [3:46:49<8:19:35,  6.66s/it] 31%|       | 1997/6500 [3:46:55<8:16:57,  6.62s/it]                                                        31%|       | 1997/6500 [3:46:55<8:16:57,  6.62s/it] 31%|       | 1998/6500 [3:47:02<8:15:21,  6.60s/it]                                                        31%|       | 1998/6500 [3:47:02<8:15:21,  6.60s/it] 31%|       | 1999/6500 [3:47:08<8:14:53,  6.60s/it]                                                        31%|       | 1999/6500 [3:47:08<8:14:53,  6.60s/it] 31%|       | 2000/6500 [3:47:15<8:13:29,  6.58s/it]                                                        31%|       | 2000/6500 [3:47:15<8:13:29,  6.58s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8379544615745544, 'eval_runtime': 1.4731, 'eval_samples_per_second': 8.146, 'eval_steps_per_second': 2.036, 'epoch': 0.31}
                                                        31%|       | 2000/6500 [3:47:16<8:13:29,  6.58s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-2000
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2000
/projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2000
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2000
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2000
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2000/pytorch_model.bin
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2000/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2000/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2000/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.5073, 'learning_rate': 7.841088854913853e-05, 'epoch': 0.31}
{'loss': 0.521, 'learning_rate': 7.839099335374355e-05, 'epoch': 0.31}
{'loss': 0.4987, 'learning_rate': 7.837109152212652e-05, 'epoch': 0.31}
{'loss': 0.5479, 'learning_rate': 7.835118305893933e-05, 'epoch': 0.31}
{'loss': 0.5041, 'learning_rate': 7.833126796883548e-05, 'epoch': 0.31}
 31%|       | 2001/6500 [3:47:23<8:51:43,  7.09s/it]                                                        31%|       | 2001/6500 [3:47:23<8:51:43,  7.09s/it] 31%|       | 2002/6500 [3:47:30<8:39:29,  6.93s/it]                                                        31%|       | 2002/6500 [3:47:30<8:39:29,  6.93s/it] 31%|       | 2003/6500 [3:47:36<8:30:23,  6.81s/it]                                                        31%|       | 2003/6500 [3:47:36<8:30:23,  6.81s/it] 31%|       | 2004/6500 [3:47:44<8:44:11,  7.00s/it]                                                        31%|       | 2004/6500 [3:47:44<8:44:11,  7.00s/it] 31%|       | 2005/6500 [3:47:50<8:34:33,  6.87s/it]                                                        31%|       | 2005/6500 [3:47:50<8:34:33,  6.87s/it] 31%|       | 2006/6500 [3:47:57<8:27:08,  6.77s/it]                                                       {'loss': 0.7936, 'learning_rate': 7.831134625646999e-05, 'epoch': 0.31}
{'loss': 0.5366, 'learning_rate': 7.829141792649946e-05, 'epoch': 0.31}
{'loss': 0.5013, 'learning_rate': 7.8271482983582e-05, 'epoch': 0.31}
{'loss': 0.5239, 'learning_rate': 7.825154143237729e-05, 'epoch': 0.31}
{'loss': 0.4925, 'learning_rate': 7.823159327754655e-05, 'epoch': 0.31}
 31%|       | 2006/6500 [3:47:57<8:27:08,  6.77s/it] 31%|       | 2007/6500 [3:48:03<8:21:58,  6.70s/it]                                                        31%|       | 2007/6500 [3:48:03<8:21:58,  6.70s/it] 31%|       | 2008/6500 [3:48:10<8:17:57,  6.65s/it]                                                        31%|       | 2008/6500 [3:48:10<8:17:57,  6.65s/it] 31%|       | 2009/6500 [3:48:17<8:15:45,  6.62s/it]                                                        31%|       | 2009/6500 [3:48:17<8:15:45,  6.62s/it] 31%|       | 2010/6500 [3:48:23<8:13:47,  6.60s/it]                                                        31%|       | 2010/6500 [3:48:23<8:13:47,  6.60s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.845282256603241, 'eval_runtime': 1.4875, 'eval_samples_per_second': 8.067, 'eval_steps_per_second': 2.017, 'epoch': 0.31}
                                                        31%|       | 2010/6500 [3:48:25<8:13:47,  6.60s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-2010
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2010I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2010

I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2010
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2010
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2010/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2010/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2010/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2010/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.504, 'learning_rate': 7.821163852375251e-05, 'epoch': 0.31}
{'loss': 0.5223, 'learning_rate': 7.819167717565951e-05, 'epoch': 0.31}
{'loss': 0.5209, 'learning_rate': 7.817170923793338e-05, 'epoch': 0.31}
{'loss': 0.524, 'learning_rate': 7.815173471524149e-05, 'epoch': 0.31}
{'loss': 0.5257, 'learning_rate': 7.813175361225278e-05, 'epoch': 0.31}
 31%|       | 2011/6500 [3:48:31<8:52:33,  7.12s/it]                                                        31%|       | 2011/6500 [3:48:31<8:52:33,  7.12s/it] 31%|       | 2012/6500 [3:48:38<8:39:08,  6.94s/it]                                                        31%|       | 2012/6500 [3:48:38<8:39:08,  6.94s/it] 31%|       | 2013/6500 [3:48:44<8:30:06,  6.82s/it]                                                        31%|       | 2013/6500 [3:48:44<8:30:06,  6.82s/it] 31%|       | 2014/6500 [3:48:51<8:23:34,  6.74s/it]                                                        31%|       | 2014/6500 [3:48:51<8:23:34,  6.74s/it] 31%|       | 2015/6500 [3:48:58<8:19:15,  6.68s/it]                                                        31%|       | 2015/6500 [3:48:58<8:19:15,  6.68s/it] 31%|       | 2016/6500 [3:49:04<8:15:52,  6.64s/it]                                                       {'loss': 0.5013, 'learning_rate': 7.811176593363772e-05, 'epoch': 0.31}
{'loss': 0.5111, 'learning_rate': 7.809177168406827e-05, 'epoch': 0.31}
{'loss': 0.5024, 'learning_rate': 7.807177086821802e-05, 'epoch': 0.31}
{'loss': 0.5488, 'learning_rate': 7.805176349076199e-05, 'epoch': 0.31}
{'loss': 0.5138, 'learning_rate': 7.80317495563768e-05, 'epoch': 0.31}
 31%|       | 2016/6500 [3:49:04<8:15:52,  6.64s/it] 31%|       | 2017/6500 [3:49:11<8:13:50,  6.61s/it]                                                        31%|       | 2017/6500 [3:49:11<8:13:50,  6.61s/it] 31%|       | 2018/6500 [3:49:17<8:12:03,  6.59s/it]                                                        31%|       | 2018/6500 [3:49:17<8:12:03,  6.59s/it] 31%|       | 2019/6500 [3:49:24<8:11:02,  6.58s/it]                                                        31%|       | 2019/6500 [3:49:24<8:11:02,  6.58s/it] 31%|       | 2020/6500 [3:49:31<8:29:36,  6.83s/it]                                                        31%|       | 2020/6500 [3:49:31<8:29:36,  6.83s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.839603841304779, 'eval_runtime': 1.4903, 'eval_samples_per_second': 8.052, 'eval_steps_per_second': 2.013, 'epoch': 0.31}
                                                        31%|       | 2020/6500 [3:49:33<8:29:36,  6.83s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-2020
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2020I AM HERE AND I AM SAVING THE MODEL

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2020
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2020
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2020
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2020/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2020/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2020/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2020/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.7893, 'learning_rate': 7.80117290697406e-05, 'epoch': 0.31}
{'loss': 0.5325, 'learning_rate': 7.799170203553304e-05, 'epoch': 0.31}
{'loss': 0.5056, 'learning_rate': 7.797166845843531e-05, 'epoch': 0.31}
{'loss': 0.509, 'learning_rate': 7.795162834313014e-05, 'epoch': 0.31}
{'loss': 0.4935, 'learning_rate': 7.793158169430181e-05, 'epoch': 0.31}
 31%|       | 2021/6500 [3:49:40<9:05:28,  7.31s/it]                                                        31%|       | 2021/6500 [3:49:40<9:05:28,  7.31s/it] 31%|       | 2022/6500 [3:49:46<8:47:52,  7.07s/it]                                                        31%|       | 2022/6500 [3:49:46<8:47:52,  7.07s/it] 31%|       | 2023/6500 [3:49:53<8:35:33,  6.91s/it]                                                        31%|       | 2023/6500 [3:49:53<8:35:33,  6.91s/it] 31%|       | 2024/6500 [3:49:59<8:27:10,  6.80s/it]                                                        31%|       | 2024/6500 [3:49:59<8:27:10,  6.80s/it] 31%|       | 2025/6500 [3:50:06<8:21:43,  6.73s/it]                                                        31%|       | 2025/6500 [3:50:06<8:21:43,  6.73s/it] 31%|       | 2026/6500 [3:50:12<8:17:20,  6.67s/it]                                                       {'loss': 0.5, 'learning_rate': 7.79115285166361e-05, 'epoch': 0.31}
{'loss': 0.5126, 'learning_rate': 7.789146881482027e-05, 'epoch': 0.31}
{'loss': 0.5256, 'learning_rate': 7.787140259354322e-05, 'epoch': 0.31}
{'loss': 0.5201, 'learning_rate': 7.785132985749526e-05, 'epoch': 0.31}
{'loss': 0.5144, 'learning_rate': 7.78312506113683e-05, 'epoch': 0.31}
 31%|       | 2026/6500 [3:50:12<8:17:20,  6.67s/it] 31%|       | 2027/6500 [3:50:19<8:13:58,  6.63s/it]                                                        31%|       | 2027/6500 [3:50:19<8:13:58,  6.63s/it] 31%|       | 2028/6500 [3:50:25<8:11:47,  6.60s/it]                                                        31%|       | 2028/6500 [3:50:25<8:11:47,  6.60s/it] 31%|       | 2029/6500 [3:50:32<8:09:56,  6.57s/it]                                                        31%|       | 2029/6500 [3:50:32<8:09:56,  6.57s/it] 31%|       | 2030/6500 [3:50:38<8:09:08,  6.57s/it]                                                        31%|       | 2030/6500 [3:50:38<8:09:08,  6.57s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.837563157081604, 'eval_runtime': 1.4773, 'eval_samples_per_second': 8.123, 'eval_steps_per_second': 2.031, 'epoch': 0.31}
                                                        31%|       | 2030/6500 [3:50:40<8:09:08,  6.57s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-2030
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2030
/projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2030
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2030
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2030
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2030/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2030/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2030/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2030/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.5096, 'learning_rate': 7.78111648598557e-05, 'epoch': 0.31}
{'loss': 0.5094, 'learning_rate': 7.779107260765242e-05, 'epoch': 0.31}
{'loss': 0.4961, 'learning_rate': 7.777097385945489e-05, 'epoch': 0.31}
{'loss': 0.5424, 'learning_rate': 7.775086861996108e-05, 'epoch': 0.31}
{'loss': 0.5079, 'learning_rate': 7.773075689387043e-05, 'epoch': 0.31}
 31%|       | 2031/6500 [3:50:47<8:48:52,  7.10s/it]                                                        31%|       | 2031/6500 [3:50:47<8:48:52,  7.10s/it] 31%|      | 2032/6500 [3:50:53<8:36:03,  6.93s/it]                                                        31%|      | 2032/6500 [3:50:53<8:36:03,  6.93s/it] 31%|      | 2033/6500 [3:51:00<8:27:02,  6.81s/it]                                                        31%|      | 2033/6500 [3:51:00<8:27:02,  6.81s/it] 31%|      | 2034/6500 [3:51:06<8:20:49,  6.73s/it]                                                        31%|      | 2034/6500 [3:51:06<8:20:49,  6.73s/it] 31%|      | 2035/6500 [3:51:13<8:16:47,  6.68s/it]                                                        31%|      | 2035/6500 [3:51:13<8:16:47,  6.68s/it] 31%|      | 2036/6500 [3:51:19<8:13:35,  6.63s/it]                                      {'loss': 0.7967, 'learning_rate': 7.771063868588399e-05, 'epoch': 0.31}
{'loss': 0.5225, 'learning_rate': 7.769051400070425e-05, 'epoch': 0.31}
{'loss': 0.5118, 'learning_rate': 7.767038284303521e-05, 'epoch': 0.31}
{'loss': 0.5057, 'learning_rate': 7.76502452175824e-05, 'epoch': 0.31}
{'loss': 0.4957, 'learning_rate': 7.763010112905291e-05, 'epoch': 0.31}
                  31%|      | 2036/6500 [3:51:19<8:13:35,  6.63s/it] 31%|      | 2037/6500 [3:51:27<8:31:43,  6.88s/it]                                                        31%|      | 2037/6500 [3:51:27<8:31:43,  6.88s/it] 31%|      | 2038/6500 [3:51:33<8:23:49,  6.77s/it]                                                        31%|      | 2038/6500 [3:51:33<8:23:49,  6.77s/it] 31%|      | 2039/6500 [3:51:40<8:18:37,  6.71s/it]                                                        31%|      | 2039/6500 [3:51:40<8:18:37,  6.71s/it] 31%|      | 2040/6500 [3:51:46<8:14:21,  6.65s/it]                                                        31%|      | 2040/6500 [3:51:46<8:14:21,  6.65s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8448516130447388, 'eval_runtime': 1.4832, 'eval_samples_per_second': 8.091, 'eval_steps_per_second': 2.023, 'epoch': 0.31}
                                                        31%|      | 2040/6500 [3:51:48<8:14:21,  6.65s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-2040
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  I AM HERE AND I AM SAVING THE MODEL
/projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2040the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2040

I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2040
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2040
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2040/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2040/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2040/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2040/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.5044, 'learning_rate': 7.760995058215525e-05, 'epoch': 0.31}
{'loss': 0.513, 'learning_rate': 7.758979358159954e-05, 'epoch': 0.31}
{'loss': 0.5302, 'learning_rate': 7.756963013209733e-05, 'epoch': 0.31}
{'loss': 0.5218, 'learning_rate': 7.754946023836168e-05, 'epoch': 0.31}
{'loss': 0.5123, 'learning_rate': 7.752928390510722e-05, 'epoch': 0.31}
 31%|      | 2041/6500 [3:51:55<8:54:13,  7.19s/it]                                                        31%|      | 2041/6500 [3:51:55<8:54:13,  7.19s/it] 31%|      | 2042/6500 [3:52:01<8:40:31,  7.01s/it]                                                        31%|      | 2042/6500 [3:52:01<8:40:31,  7.01s/it] 31%|      | 2043/6500 [3:52:08<8:31:06,  6.88s/it]                                                        31%|      | 2043/6500 [3:52:08<8:31:06,  6.88s/it] 31%|      | 2044/6500 [3:52:15<8:24:06,  6.79s/it]                                                        31%|      | 2044/6500 [3:52:15<8:24:06,  6.79s/it] 31%|      | 2045/6500 [3:52:21<8:19:05,  6.72s/it]                                                        31%|      | 2045/6500 [3:52:21<8:19:05,  6.72s/it] 31%|      | 2046/6500 [3:52:28<8:15:39,  6.68s/it]                                  {'loss': 0.5152, 'learning_rate': 7.750910113705001e-05, 'epoch': 0.31}
{'loss': 0.4992, 'learning_rate': 7.748891193890768e-05, 'epoch': 0.31}
{'loss': 0.5063, 'learning_rate': 7.746871631539934e-05, 'epoch': 0.32}
{'loss': 0.5372, 'learning_rate': 7.744851427124554e-05, 'epoch': 0.32}
{'loss': 0.5103, 'learning_rate': 7.742830581116843e-05, 'epoch': 0.32}
                      31%|      | 2046/6500 [3:52:28<8:15:39,  6.68s/it] 31%|      | 2047/6500 [3:52:34<8:13:18,  6.65s/it]                                                        31%|      | 2047/6500 [3:52:34<8:13:18,  6.65s/it] 32%|      | 2048/6500 [3:52:41<8:11:44,  6.63s/it]                                                        32%|      | 2048/6500 [3:52:41<8:11:44,  6.63s/it] 32%|      | 2049/6500 [3:52:48<8:10:35,  6.61s/it]                                                        32%|      | 2049/6500 [3:52:48<8:10:35,  6.61s/it] 32%|      | 2050/6500 [3:52:54<8:09:25,  6.60s/it]                                                        32%|      | 2050/6500 [3:52:54<8:09:25,  6.60s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8387547135353088, 'eval_runtime': 1.4844, 'eval_samples_per_second': 8.084, 'eval_steps_per_second': 2.021, 'epoch': 0.32}
                                                        32%|      | 2050/6500 [3:52:56<8:09:25,  6.60s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-2050
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2050
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2050
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2050
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2050
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2050/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2050/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2050/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2050/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.7939, 'learning_rate': 7.74080909398916e-05, 'epoch': 0.32}
{'loss': 0.5019, 'learning_rate': 7.738786966214016e-05, 'epoch': 0.32}
{'loss': 0.5223, 'learning_rate': 7.736764198264072e-05, 'epoch': 0.32}
{'loss': 0.4974, 'learning_rate': 7.734740790612136e-05, 'epoch': 0.32}
{'loss': 0.4899, 'learning_rate': 7.732716743731166e-05, 'epoch': 0.32}
 32%|      | 2051/6500 [3:53:03<8:50:49,  7.16s/it]                                                        32%|      | 2051/6500 [3:53:03<8:50:49,  7.16s/it] 32%|      | 2052/6500 [3:53:09<8:38:10,  6.99s/it]                                                        32%|      | 2052/6500 [3:53:09<8:38:10,  6.99s/it] 32%|      | 2053/6500 [3:53:16<8:42:51,  7.05s/it]                                                        32%|      | 2053/6500 [3:53:16<8:42:51,  7.05s/it] 32%|      | 2054/6500 [3:53:23<8:32:18,  6.91s/it]                                                        32%|      | 2054/6500 [3:53:23<8:32:18,  6.91s/it] 32%|      | 2055/6500 [3:53:29<8:24:17,  6.81s/it]                                                        32%|      | 2055/6500 [3:53:29<8:24:17,  6.81s/it] 32%|      | 2056/6500 [3:53:36<8:18:49,  6.73s/it]                                  {'loss': 0.4955, 'learning_rate': 7.730692058094273e-05, 'epoch': 0.32}
{'loss': 0.5112, 'learning_rate': 7.728666734174715e-05, 'epoch': 0.32}
{'loss': 0.5272, 'learning_rate': 7.726640772445899e-05, 'epoch': 0.32}
{'loss': 0.5147, 'learning_rate': 7.72461417338138e-05, 'epoch': 0.32}
{'loss': 0.5113, 'learning_rate': 7.722586937454864e-05, 'epoch': 0.32}
                      32%|      | 2056/6500 [3:53:36<8:18:49,  6.73s/it] 32%|      | 2057/6500 [3:53:43<8:15:07,  6.69s/it]                                                        32%|      | 2057/6500 [3:53:43<8:15:07,  6.69s/it] 32%|      | 2058/6500 [3:53:49<8:12:44,  6.66s/it]                                                        32%|      | 2058/6500 [3:53:49<8:12:44,  6.66s/it] 32%|      | 2059/6500 [3:53:56<8:11:01,  6.63s/it]                                                        32%|      | 2059/6500 [3:53:56<8:11:01,  6.63s/it] 32%|      | 2060/6500 [3:54:02<8:09:42,  6.62s/it]                                                        32%|      | 2060/6500 [3:54:02<8:09:42,  6.62s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8367992639541626, 'eval_runtime': 1.4954, 'eval_samples_per_second': 8.025, 'eval_steps_per_second': 2.006, 'epoch': 0.32}
                                                        32%|      | 2060/6500 [3:54:04<8:09:42,  6.62s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-2060
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2060
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2060
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2060
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2060
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2060/pytorch_model.bin
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2060/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2060/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2060/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.5212, 'learning_rate': 7.720559065140203e-05, 'epoch': 0.32}
{'loss': 0.4944, 'learning_rate': 7.718530556911403e-05, 'epoch': 0.32}
{'loss': 0.5106, 'learning_rate': 7.716501413242616e-05, 'epoch': 0.32}
{'loss': 0.5257, 'learning_rate': 7.714471634608138e-05, 'epoch': 0.32}
{'loss': 0.5157, 'learning_rate': 7.712441221482421e-05, 'epoch': 0.32}
 32%|      | 2061/6500 [3:54:11<8:49:24,  7.16s/it]                                                        32%|      | 2061/6500 [3:54:11<8:49:24,  7.16s/it] 32%|      | 2062/6500 [3:54:17<8:35:55,  6.98s/it]                                                        32%|      | 2062/6500 [3:54:17<8:35:55,  6.98s/it] 32%|      | 2063/6500 [3:54:24<8:26:47,  6.85s/it]                                                        32%|      | 2063/6500 [3:54:24<8:26:47,  6.85s/it] 32%|      | 2064/6500 [3:54:30<8:20:08,  6.76s/it]                                                        32%|      | 2064/6500 [3:54:30<8:20:08,  6.76s/it] 32%|      | 2065/6500 [3:54:37<8:15:36,  6.70s/it]                                                        32%|      | 2065/6500 [3:54:37<8:15:36,  6.70s/it] 32%|      | 2066/6500 [3:54:44<8:12:22,  6.66s/it]                                  {'loss': 0.7968, 'learning_rate': 7.710410174340061e-05, 'epoch': 0.32}
{'loss': 0.5036, 'learning_rate': 7.7083784936558e-05, 'epoch': 0.32}
{'loss': 0.5257, 'learning_rate': 7.706346179904535e-05, 'epoch': 0.32}
{'loss': 0.481, 'learning_rate': 7.704313233561305e-05, 'epoch': 0.32}
{'loss': 0.5003, 'learning_rate': 7.7022796551013e-05, 'epoch': 0.32}
                      32%|      | 2066/6500 [3:54:44<8:12:22,  6.66s/it] 32%|      | 2067/6500 [3:54:50<8:10:02,  6.63s/it]                                                        32%|      | 2067/6500 [3:54:50<8:10:02,  6.63s/it] 32%|      | 2068/6500 [3:54:57<8:08:00,  6.61s/it]                                                        32%|      | 2068/6500 [3:54:57<8:08:00,  6.61s/it] 32%|      | 2069/6500 [3:55:04<8:26:28,  6.86s/it]                                                        32%|      | 2069/6500 [3:55:04<8:26:28,  6.86s/it] 32%|      | 2070/6500 [3:55:11<8:20:12,  6.77s/it]                                                        32%|      | 2070/6500 [3:55:11<8:20:12,  6.77s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8458943367004395, 'eval_runtime': 1.4816, 'eval_samples_per_second': 8.099, 'eval_steps_per_second': 2.025, 'epoch': 0.32}
                                                        32%|      | 2070/6500 [3:55:12<8:20:12,  6.77s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-2070
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2070
/projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2070
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2070
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2070
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2070/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2070/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2070/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2070/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.5058, 'learning_rate': 7.700245444999854e-05, 'epoch': 0.32}
{'loss': 0.5004, 'learning_rate': 7.698210603732454e-05, 'epoch': 0.32}
{'loss': 0.5367, 'learning_rate': 7.69617513177473e-05, 'epoch': 0.32}
{'loss': 0.531, 'learning_rate': 7.694139029602465e-05, 'epoch': 0.32}
{'loss': 0.5011, 'learning_rate': 7.692102297691578e-05, 'epoch': 0.32}
 32%|      | 2071/6500 [3:55:19<8:57:14,  7.28s/it]                                                        32%|      | 2071/6500 [3:55:19<8:57:14,  7.28s/it] 32%|      | 2072/6500 [3:55:26<8:41:26,  7.07s/it]                                                        32%|      | 2072/6500 [3:55:26<8:41:26,  7.07s/it] 32%|      | 2073/6500 [3:55:32<8:30:02,  6.91s/it]                                                        32%|      | 2073/6500 [3:55:32<8:30:02,  6.91s/it] 32%|      | 2074/6500 [3:55:39<8:22:05,  6.81s/it]                                                        32%|      | 2074/6500 [3:55:39<8:22:05,  6.81s/it] 32%|      | 2075/6500 [3:55:45<8:16:40,  6.73s/it]                                                        32%|      | 2075/6500 [3:55:45<8:16:40,  6.73s/it] 32%|      | 2076/6500 [3:55:52<8:12:36,  6.68s/it]                                  {'loss': 0.5148, 'learning_rate': 7.690064936518151e-05, 'epoch': 0.32}
{'loss': 0.4913, 'learning_rate': 7.688026946558397e-05, 'epoch': 0.32}
{'loss': 0.5462, 'learning_rate': 7.685988328288691e-05, 'epoch': 0.32}
{'loss': 0.4943, 'learning_rate': 7.683949082185544e-05, 'epoch': 0.32}
{'loss': 0.7842, 'learning_rate': 7.681909208725617e-05, 'epoch': 0.32}
                      32%|      | 2076/6500 [3:55:52<8:12:36,  6.68s/it] 32%|      | 2077/6500 [3:55:59<8:09:24,  6.64s/it]                                                        32%|      | 2077/6500 [3:55:59<8:09:24,  6.64s/it] 32%|      | 2078/6500 [3:56:05<8:07:29,  6.61s/it]                                                        32%|      | 2078/6500 [3:56:05<8:07:29,  6.61s/it] 32%|      | 2079/6500 [3:56:12<8:06:39,  6.60s/it]                                                        32%|      | 2079/6500 [3:56:12<8:06:39,  6.60s/it] 32%|      | 2080/6500 [3:56:18<8:05:26,  6.59s/it]                                                        32%|      | 2080/6500 [3:56:18<8:05:26,  6.59s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8395066261291504, 'eval_runtime': 1.4755, 'eval_samples_per_second': 8.133, 'eval_steps_per_second': 2.033, 'epoch': 0.32}
                                                        32%|      | 2080/6500 [3:56:20<8:05:26,  6.59s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-2080
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in I AM HERE AND I AM SAVING THE MODEL
 the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2080
/projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2080
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2080
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2080
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2080/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2080/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2080/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2080/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.5221, 'learning_rate': 7.679868708385718e-05, 'epoch': 0.32}
{'loss': 0.5, 'learning_rate': 7.677827581642804e-05, 'epoch': 0.32}
{'loss': 0.5204, 'learning_rate': 7.675785828973972e-05, 'epoch': 0.32}
{'loss': 0.4904, 'learning_rate': 7.673743450856473e-05, 'epoch': 0.32}
{'loss': 0.5027, 'learning_rate': 7.6717004477677e-05, 'epoch': 0.32}
 32%|      | 2081/6500 [3:56:27<8:42:50,  7.10s/it]                                                        32%|      | 2081/6500 [3:56:27<8:42:50,  7.10s/it] 32%|      | 2082/6500 [3:56:33<8:30:39,  6.94s/it]                                                        32%|      | 2082/6500 [3:56:33<8:30:39,  6.94s/it] 32%|      | 2083/6500 [3:56:40<8:21:51,  6.82s/it]                                                        32%|      | 2083/6500 [3:56:40<8:21:51,  6.82s/it] 32%|      | 2084/6500 [3:56:46<8:15:50,  6.74s/it]                                                        32%|      | 2084/6500 [3:56:46<8:15:50,  6.74s/it] 32%|      | 2085/6500 [3:56:54<8:30:43,  6.94s/it]                                                        32%|      | 2085/6500 [3:56:54<8:30:43,  6.94s/it] 32%|      | 2086/6500 [3:57:00<8:22:25,  6.83s/it]                                  {'loss': 0.5082, 'learning_rate': 7.669656820185189e-05, 'epoch': 0.32}
{'loss': 0.5156, 'learning_rate': 7.66761256858663e-05, 'epoch': 0.32}
{'loss': 0.5219, 'learning_rate': 7.66556769344985e-05, 'epoch': 0.32}
{'loss': 0.5222, 'learning_rate': 7.663522195252832e-05, 'epoch': 0.32}
{'loss': 0.4974, 'learning_rate': 7.661476074473695e-05, 'epoch': 0.32}
                      32%|      | 2086/6500 [3:57:00<8:22:25,  6.83s/it] 32%|      | 2087/6500 [3:57:07<8:16:18,  6.75s/it]                                                        32%|      | 2087/6500 [3:57:07<8:16:18,  6.75s/it] 32%|      | 2088/6500 [3:57:13<8:12:04,  6.69s/it]                                                        32%|      | 2088/6500 [3:57:13<8:12:04,  6.69s/it] 32%|      | 2089/6500 [3:57:20<8:09:08,  6.65s/it]                                                        32%|      | 2089/6500 [3:57:20<8:09:08,  6.65s/it] 32%|      | 2090/6500 [3:57:26<8:06:50,  6.62s/it]                                                        32%|      | 2090/6500 [3:57:26<8:06:50,  6.62s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.836530327796936, 'eval_runtime': 1.4854, 'eval_samples_per_second': 8.079, 'eval_steps_per_second': 2.02, 'epoch': 0.32}
                                                        32%|      | 2090/6500 [3:57:28<8:06:50,  6.62s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-2090
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2090
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2090
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2090
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2090
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2090/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2090/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2090/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2090/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.5089, 'learning_rate': 7.659429331590706e-05, 'epoch': 0.32}
{'loss': 0.4914, 'learning_rate': 7.65738196708228e-05, 'epoch': 0.32}
{'loss': 0.5519, 'learning_rate': 7.655333981426978e-05, 'epoch': 0.32}
{'loss': 0.5009, 'learning_rate': 7.6532853751035e-05, 'epoch': 0.32}
{'loss': 0.7827, 'learning_rate': 7.651236148590699e-05, 'epoch': 0.32}
 32%|      | 2091/6500 [3:57:35<8:44:26,  7.14s/it]                                                        32%|      | 2091/6500 [3:57:35<8:44:26,  7.14s/it] 32%|      | 2092/6500 [3:57:41<8:31:28,  6.96s/it]                                                        32%|      | 2092/6500 [3:57:41<8:31:28,  6.96s/it] 32%|      | 2093/6500 [3:57:48<8:22:31,  6.84s/it]                                                        32%|      | 2093/6500 [3:57:48<8:22:31,  6.84s/it] 32%|      | 2094/6500 [3:57:54<8:16:13,  6.76s/it]                                                        32%|      | 2094/6500 [3:57:54<8:16:13,  6.76s/it] 32%|      | 2095/6500 [3:58:01<8:11:33,  6.70s/it]                                                        32%|      | 2095/6500 [3:58:01<8:11:33,  6.70s/it] 32%|      | 2096/6500 [3:58:07<8:08:21,  6.65s/it]                                  {'loss': 0.52, 'learning_rate': 7.64918630236757e-05, 'epoch': 0.32}
{'loss': 0.4929, 'learning_rate': 7.647135836913249e-05, 'epoch': 0.32}
{'loss': 0.5162, 'learning_rate': 7.645084752707019e-05, 'epoch': 0.32}
{'loss': 0.4845, 'learning_rate': 7.643033050228312e-05, 'epoch': 0.32}
{'loss': 0.4958, 'learning_rate': 7.640980729956699e-05, 'epoch': 0.32}
                      32%|      | 2096/6500 [3:58:07<8:08:21,  6.65s/it] 32%|      | 2097/6500 [3:58:14<8:05:50,  6.62s/it]                                                        32%|      | 2097/6500 [3:58:14<8:05:50,  6.62s/it] 32%|      | 2098/6500 [3:58:21<8:04:04,  6.60s/it]                                                        32%|      | 2098/6500 [3:58:21<8:04:04,  6.60s/it] 32%|      | 2099/6500 [3:58:27<8:03:00,  6.59s/it]                                                        32%|      | 2099/6500 [3:58:27<8:03:00,  6.59s/it] 32%|      | 2100/6500 [3:58:34<8:02:10,  6.58s/it]                                                        32%|      | 2100/6500 [3:58:34<8:02:10,  6.58s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.846937358379364, 'eval_runtime': 1.4798, 'eval_samples_per_second': 8.109, 'eval_steps_per_second': 2.027, 'epoch': 0.32}
                                                        32%|      | 2100/6500 [3:58:35<8:02:10,  6.58s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-2100
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2100
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2100
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2100
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2100
the pytorch model path isthe pytorch model path is  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2100/pytorch_model.bin/projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2100/pytorch_model.bin

the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2100/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2100/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.5068, 'learning_rate': 7.6389277923719e-05, 'epoch': 0.32}
{'loss': 0.5183, 'learning_rate': 7.636874237953771e-05, 'epoch': 0.32}
{'loss': 0.5077, 'learning_rate': 7.634820067182323e-05, 'epoch': 0.32}
{'loss': 0.5135, 'learning_rate': 7.632765280537703e-05, 'epoch': 0.32}
{'loss': 0.497, 'learning_rate': 7.630709878500208e-05, 'epoch': 0.32}
 32%|      | 2101/6500 [3:58:43<9:01:00,  7.38s/it]                                                        32%|      | 2101/6500 [3:58:43<9:01:00,  7.38s/it] 32%|      | 2102/6500 [3:58:50<8:42:54,  7.13s/it]                                                        32%|      | 2102/6500 [3:58:50<8:42:54,  7.13s/it] 32%|      | 2103/6500 [3:58:56<8:30:27,  6.97s/it]                                                        32%|      | 2103/6500 [3:58:56<8:30:27,  6.97s/it] 32%|      | 2104/6500 [3:59:03<8:21:24,  6.84s/it]                                                        32%|      | 2104/6500 [3:59:03<8:21:24,  6.84s/it] 32%|      | 2105/6500 [3:59:09<8:14:45,  6.75s/it]                                                        32%|      | 2105/6500 [3:59:09<8:14:45,  6.75s/it] 32%|      | 2106/6500 [3:59:16<8:10:26,  6.70s/it]                                  {'loss': 0.4997, 'learning_rate': 7.628653861550275e-05, 'epoch': 0.32}
{'loss': 0.4878, 'learning_rate': 7.626597230168482e-05, 'epoch': 0.32}
{'loss': 0.5395, 'learning_rate': 7.624539984835557e-05, 'epoch': 0.32}
{'loss': 0.5023, 'learning_rate': 7.622482126032368e-05, 'epoch': 0.32}
{'loss': 0.7788, 'learning_rate': 7.620423654239928e-05, 'epoch': 0.32}
                      32%|      | 2106/6500 [3:59:16<8:10:26,  6.70s/it] 32%|      | 2107/6500 [3:59:22<8:07:04,  6.65s/it]                                                        32%|      | 2107/6500 [3:59:22<8:07:04,  6.65s/it] 32%|      | 2108/6500 [3:59:29<8:04:53,  6.62s/it]                                                        32%|      | 2108/6500 [3:59:29<8:04:53,  6.62s/it] 32%|      | 2109/6500 [3:59:35<8:03:10,  6.60s/it]                                                        32%|      | 2109/6500 [3:59:35<8:03:10,  6.60s/it] 32%|      | 2110/6500 [3:59:42<8:02:05,  6.59s/it]                                                        32%|      | 2110/6500 [3:59:42<8:02:05,  6.59s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8370670080184937, 'eval_runtime': 1.4798, 'eval_samples_per_second': 8.109, 'eval_steps_per_second': 2.027, 'epoch': 0.32}
                                                        32%|      | 2110/6500 [3:59:43<8:02:05,  6.59s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-2110
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2110
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2110
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2110
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2110
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2110/pytorch_model.bin 
/projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2110/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2110/pytorch_model.binthe pytorch model path is
 /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2110/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.5242, 'learning_rate': 7.618364569939391e-05, 'epoch': 0.32}
{'loss': 0.4985, 'learning_rate': 7.616304873612055e-05, 'epoch': 0.32}
{'loss': 0.4961, 'learning_rate': 7.614244565739361e-05, 'epoch': 0.33}
{'loss': 0.4839, 'learning_rate': 7.612183646802895e-05, 'epoch': 0.33}
{'loss': 0.4961, 'learning_rate': 7.610122117284386e-05, 'epoch': 0.33}
 32%|      | 2111/6500 [3:59:50<8:40:17,  7.11s/it]                                                        32%|      | 2111/6500 [3:59:50<8:40:17,  7.11s/it] 32%|      | 2112/6500 [3:59:57<8:27:40,  6.94s/it]                                                        32%|      | 2112/6500 [3:59:57<8:27:40,  6.94s/it] 33%|      | 2113/6500 [4:00:03<8:18:59,  6.82s/it]                                                        33%|      | 2113/6500 [4:00:03<8:18:59,  6.82s/it] 33%|      | 2114/6500 [4:00:10<8:13:05,  6.75s/it]                                                        33%|      | 2114/6500 [4:00:10<8:13:05,  6.75s/it] 33%|      | 2115/6500 [4:00:17<8:08:54,  6.69s/it]                                                        33%|      | 2115/6500 [4:00:17<8:08:54,  6.69s/it] 33%|      | 2116/6500 [4:00:23<8:05:54,  6.65s/it]                                  {'loss': 0.4952, 'learning_rate': 7.6080599776657e-05, 'epoch': 0.33}
{'loss': 0.516, 'learning_rate': 7.605997228428853e-05, 'epoch': 0.33}
{'loss': 0.5134, 'learning_rate': 7.603933870055997e-05, 'epoch': 0.33}
{'loss': 0.5076, 'learning_rate': 7.601869903029432e-05, 'epoch': 0.33}
{'loss': 0.5098, 'learning_rate': 7.599805327831596e-05, 'epoch': 0.33}
                      33%|      | 2116/6500 [4:00:23<8:05:54,  6.65s/it] 33%|      | 2117/6500 [4:00:31<8:25:57,  6.93s/it]                                                        33%|      | 2117/6500 [4:00:31<8:25:57,  6.93s/it] 33%|      | 2118/6500 [4:00:37<8:17:58,  6.82s/it]                                                        33%|      | 2118/6500 [4:00:37<8:17:58,  6.82s/it] 33%|      | 2119/6500 [4:00:44<8:12:13,  6.74s/it]                                                        33%|      | 2119/6500 [4:00:44<8:12:13,  6.74s/it] 33%|      | 2120/6500 [4:00:50<8:08:05,  6.69s/it]                                                        33%|      | 2120/6500 [4:00:50<8:08:05,  6.69s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8357075452804565, 'eval_runtime': 1.4831, 'eval_samples_per_second': 8.091, 'eval_steps_per_second': 2.023, 'epoch': 0.33}
                                                        33%|      | 2120/6500 [4:00:52<8:08:05,  6.69s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-2120
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2120
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2120
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2120
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2120
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2120/pytorch_model.bin
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2120/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2120/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2120/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.5018, 'learning_rate': 7.597740144945073e-05, 'epoch': 0.33}
{'loss': 0.4942, 'learning_rate': 7.595674354852584e-05, 'epoch': 0.33}
{'loss': 0.5322, 'learning_rate': 7.593607958036998e-05, 'epoch': 0.33}
{'loss': 0.4998, 'learning_rate': 7.59154095498132e-05, 'epoch': 0.33}
{'loss': 0.7907, 'learning_rate': 7.589473346168702e-05, 'epoch': 0.33}
 33%|      | 2121/6500 [4:00:59<8:44:15,  7.18s/it]                                                        33%|      | 2121/6500 [4:00:59<8:44:15,  7.18s/it] 33%|      | 2122/6500 [4:01:05<8:30:28,  7.00s/it]                                                        33%|      | 2122/6500 [4:01:05<8:30:28,  7.00s/it] 33%|      | 2123/6500 [4:01:12<8:20:39,  6.86s/it]                                                        33%|      | 2123/6500 [4:01:12<8:20:39,  6.86s/it] 33%|      | 2124/6500 [4:01:18<8:14:06,  6.77s/it]                                                        33%|      | 2124/6500 [4:01:18<8:14:06,  6.77s/it] 33%|      | 2125/6500 [4:01:25<8:09:06,  6.71s/it]                                                        33%|      | 2125/6500 [4:01:25<8:09:06,  6.71s/it] 33%|      | 2126/6500 [4:01:31<8:05:43,  6.66s/it]                                  {'loss': 0.5078, 'learning_rate': 7.587405132082433e-05, 'epoch': 0.33}
{'loss': 0.5169, 'learning_rate': 7.585336313205944e-05, 'epoch': 0.33}
{'loss': 0.489, 'learning_rate': 7.583266890022814e-05, 'epoch': 0.33}
{'loss': 0.4866, 'learning_rate': 7.581196863016754e-05, 'epoch': 0.33}
{'loss': 0.4956, 'learning_rate': 7.579126232671621e-05, 'epoch': 0.33}
                      33%|      | 2126/6500 [4:01:31<8:05:43,  6.66s/it] 33%|      | 2127/6500 [4:01:38<8:03:09,  6.63s/it]                                                        33%|      | 2127/6500 [4:01:38<8:03:09,  6.63s/it] 33%|      | 2128/6500 [4:01:45<8:01:41,  6.61s/it]                                                        33%|      | 2128/6500 [4:01:45<8:01:41,  6.61s/it] 33%|      | 2129/6500 [4:01:51<8:00:35,  6.60s/it]                                                        33%|      | 2129/6500 [4:01:51<8:00:35,  6.60s/it] 33%|      | 2130/6500 [4:01:58<7:59:38,  6.59s/it]                                                        33%|      | 2130/6500 [4:01:58<7:59:38,  6.59s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8510231375694275, 'eval_runtime': 1.4791, 'eval_samples_per_second': 8.113, 'eval_steps_per_second': 2.028, 'epoch': 0.33}
                                                        33%|      | 2130/6500 [4:01:59<7:59:38,  6.59s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-2130
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2130
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2130
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2130
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2130
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2130/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2130/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2130/pytorch_model.binthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2130/pytorch_model.bin

/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.5034, 'learning_rate': 7.577054999471411e-05, 'epoch': 0.33}
{'loss': 0.5276, 'learning_rate': 7.574983163900265e-05, 'epoch': 0.33}
{'loss': 0.5046, 'learning_rate': 7.572910726442462e-05, 'epoch': 0.33}
{'loss': 0.4973, 'learning_rate': 7.57083768758242e-05, 'epoch': 0.33}
{'loss': 0.5215, 'learning_rate': 7.568764047804698e-05, 'epoch': 0.33}
 33%|      | 2131/6500 [4:02:06<8:38:53,  7.13s/it]                                                        33%|      | 2131/6500 [4:02:06<8:38:53,  7.13s/it] 33%|      | 2132/6500 [4:02:13<8:26:23,  6.96s/it]                                                        33%|      | 2132/6500 [4:02:13<8:26:23,  6.96s/it] 33%|      | 2133/6500 [4:02:19<8:17:39,  6.84s/it]                                                        33%|      | 2133/6500 [4:02:19<8:17:39,  6.84s/it] 33%|      | 2134/6500 [4:02:27<8:31:11,  7.03s/it]                                                        33%|      | 2134/6500 [4:02:27<8:31:11,  7.03s/it] 33%|      | 2135/6500 [4:02:33<8:21:03,  6.89s/it]                                                        33%|      | 2135/6500 [4:02:33<8:21:03,  6.89s/it] 33%|      | 2136/6500 [4:02:40<8:13:41,  6.79s/it]                                  {'loss': 0.4924, 'learning_rate': 7.566689807593998e-05, 'epoch': 0.33}
{'loss': 0.4971, 'learning_rate': 7.56461496743516e-05, 'epoch': 0.33}
{'loss': 0.5217, 'learning_rate': 7.562539527813169e-05, 'epoch': 0.33}
{'loss': 0.5023, 'learning_rate': 7.560463489213143e-05, 'epoch': 0.33}
{'loss': 0.7914, 'learning_rate': 7.558386852120344e-05, 'epoch': 0.33}
                      33%|      | 2136/6500 [4:02:40<8:13:41,  6.79s/it] 33%|      | 2137/6500 [4:02:46<8:08:50,  6.72s/it]                                                        33%|      | 2137/6500 [4:02:46<8:08:50,  6.72s/it] 33%|      | 2138/6500 [4:02:53<8:05:14,  6.67s/it]                                                        33%|      | 2138/6500 [4:02:53<8:05:14,  6.67s/it] 33%|      | 2139/6500 [4:02:59<8:02:27,  6.64s/it]                                                        33%|      | 2139/6500 [4:02:59<8:02:27,  6.64s/it] 33%|      | 2140/6500 [4:03:06<8:00:28,  6.61s/it]                                                        33%|      | 2140/6500 [4:03:06<8:00:28,  6.61s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8403095602989197, 'eval_runtime': 1.4815, 'eval_samples_per_second': 8.1, 'eval_steps_per_second': 2.025, 'epoch': 0.33}
                                                        33%|      | 2140/6500 [4:03:08<8:00:28,  6.61s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-2140
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2140
/projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2140
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2140
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2140
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2140/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2140/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2140/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2140/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.4967, 'learning_rate': 7.556309617020175e-05, 'epoch': 0.33}
{'loss': 0.5233, 'learning_rate': 7.554231784398173e-05, 'epoch': 0.33}
{'loss': 0.4933, 'learning_rate': 7.552153354740023e-05, 'epoch': 0.33}
{'loss': 0.4861, 'learning_rate': 7.550074328531545e-05, 'epoch': 0.33}
{'loss': 0.4923, 'learning_rate': 7.547994706258694e-05, 'epoch': 0.33}
 33%|      | 2141/6500 [4:03:14<8:38:13,  7.13s/it]                                                        33%|      | 2141/6500 [4:03:14<8:38:13,  7.13s/it] 33%|      | 2142/6500 [4:03:21<8:25:31,  6.96s/it]                                                        33%|      | 2142/6500 [4:03:21<8:25:31,  6.96s/it] 33%|      | 2143/6500 [4:03:27<8:16:30,  6.84s/it]                                                        33%|      | 2143/6500 [4:03:27<8:16:30,  6.84s/it] 33%|      | 2144/6500 [4:03:34<8:10:21,  6.75s/it]                                                        33%|      | 2144/6500 [4:03:34<8:10:21,  6.75s/it] 33%|      | 2145/6500 [4:03:41<8:06:11,  6.70s/it]                                                        33%|      | 2145/6500 [4:03:41<8:06:11,  6.70s/it] 33%|      | 2146/6500 [4:03:47<8:02:46,  6.65s/it]                                  {'loss': 0.5042, 'learning_rate': 7.545914488407575e-05, 'epoch': 0.33}
{'loss': 0.53, 'learning_rate': 7.543833675464422e-05, 'epoch': 0.33}
{'loss': 0.515, 'learning_rate': 7.541752267915615e-05, 'epoch': 0.33}
{'loss': 0.5014, 'learning_rate': 7.539670266247671e-05, 'epoch': 0.33}
{'loss': 0.5047, 'learning_rate': 7.537587670947244e-05, 'epoch': 0.33}
                      33%|      | 2146/6500 [4:03:47<8:02:46,  6.65s/it] 33%|      | 2147/6500 [4:03:54<8:00:50,  6.63s/it]                                                        33%|      | 2147/6500 [4:03:54<8:00:50,  6.63s/it] 33%|      | 2148/6500 [4:04:00<7:59:08,  6.61s/it]                                                        33%|      | 2148/6500 [4:04:00<7:59:08,  6.61s/it] 33%|      | 2149/6500 [4:04:07<7:57:54,  6.59s/it]                                                        33%|      | 2149/6500 [4:04:07<7:57:54,  6.59s/it] 33%|      | 2150/6500 [4:04:14<8:10:38,  6.77s/it]                                                        33%|      | 2150/6500 [4:04:14<8:10:38,  6.77s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8309826850891113, 'eval_runtime': 1.5066, 'eval_samples_per_second': 7.965, 'eval_steps_per_second': 1.991, 'epoch': 0.33}
                                                        33%|      | 2150/6500 [4:04:16<8:10:38,  6.77s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-2150
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2150
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2150
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2150
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2150
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2150/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2150/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2150/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2150/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.4881, 'learning_rate': 7.535504482501126e-05, 'epoch': 0.33}
{'loss': 0.5286, 'learning_rate': 7.53342070139625e-05, 'epoch': 0.33}
{'loss': 0.4901, 'learning_rate': 7.53133632811969e-05, 'epoch': 0.33}
{'loss': 0.7696, 'learning_rate': 7.529251363158653e-05, 'epoch': 0.33}
{'loss': 0.5293, 'learning_rate': 7.527165807000492e-05, 'epoch': 0.33}
 33%|      | 2151/6500 [4:04:22<8:45:58,  7.26s/it]                                                        33%|      | 2151/6500 [4:04:22<8:45:58,  7.26s/it] 33%|      | 2152/6500 [4:04:29<8:30:35,  7.05s/it]                                                        33%|      | 2152/6500 [4:04:29<8:30:35,  7.05s/it] 33%|      | 2153/6500 [4:04:36<8:19:53,  6.90s/it]                                                        33%|      | 2153/6500 [4:04:36<8:19:53,  6.90s/it] 33%|      | 2154/6500 [4:04:42<8:12:20,  6.80s/it]                                                        33%|      | 2154/6500 [4:04:42<8:12:20,  6.80s/it] 33%|      | 2155/6500 [4:04:49<8:07:41,  6.73s/it]                                                        33%|      | 2155/6500 [4:04:49<8:07:41,  6.73s/it] 33%|      | 2156/6500 [4:04:55<8:03:56,  6.68s/it]                                  {'loss': 0.4883, 'learning_rate': 7.525079660132685e-05, 'epoch': 0.33}
{'loss': 0.5215, 'learning_rate': 7.522992923042861e-05, 'epoch': 0.33}
{'loss': 0.4776, 'learning_rate': 7.520905596218781e-05, 'epoch': 0.33}
{'loss': 0.4869, 'learning_rate': 7.518817680148347e-05, 'epoch': 0.33}
{'loss': 0.4903, 'learning_rate': 7.516729175319592e-05, 'epoch': 0.33}
                      33%|      | 2156/6500 [4:04:55<8:03:56,  6.68s/it] 33%|      | 2157/6500 [4:05:02<8:01:01,  6.65s/it]                                                        33%|      | 2157/6500 [4:05:02<8:01:01,  6.65s/it] 33%|      | 2158/6500 [4:05:08<7:58:52,  6.62s/it]                                                        33%|      | 2158/6500 [4:05:08<7:58:52,  6.62s/it] 33%|      | 2159/6500 [4:05:15<7:57:36,  6.60s/it]                                                        33%|      | 2159/6500 [4:05:15<7:57:36,  6.60s/it] 33%|      | 2160/6500 [4:05:21<7:56:36,  6.59s/it]                                                        33%|      | 2160/6500 [4:05:21<7:56:36,  6.59s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8487064242362976, 'eval_runtime': 1.4834, 'eval_samples_per_second': 8.09, 'eval_steps_per_second': 2.022, 'epoch': 0.33}
                                                        33%|      | 2160/6500 [4:05:23<7:56:36,  6.59s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-2160
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  I AM HERE AND I AM SAVING THE MODEL/projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2160
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2160

I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2160
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2160
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2160/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2160/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2160/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2160/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.4926, 'learning_rate': 7.514640082220697e-05, 'epoch': 0.33}
{'loss': 0.5213, 'learning_rate': 7.512550401339971e-05, 'epoch': 0.33}
{'loss': 0.51, 'learning_rate': 7.510460133165867e-05, 'epoch': 0.33}
{'loss': 0.4903, 'learning_rate': 7.508369278186967e-05, 'epoch': 0.33}
{'loss': 0.5099, 'learning_rate': 7.506277836892001e-05, 'epoch': 0.33}
 33%|      | 2161/6500 [4:05:30<8:35:33,  7.13s/it]                                                        33%|      | 2161/6500 [4:05:30<8:35:33,  7.13s/it] 33%|      | 2162/6500 [4:05:36<8:23:12,  6.96s/it]                                                        33%|      | 2162/6500 [4:05:36<8:23:12,  6.96s/it] 33%|      | 2163/6500 [4:05:43<8:14:32,  6.84s/it]                                                        33%|      | 2163/6500 [4:05:43<8:14:32,  6.84s/it] 33%|      | 2164/6500 [4:05:50<8:08:36,  6.76s/it]                                                        33%|      | 2164/6500 [4:05:50<8:08:36,  6.76s/it] 33%|      | 2165/6500 [4:05:56<8:04:12,  6.70s/it]                                                        33%|      | 2165/6500 [4:05:56<8:04:12,  6.70s/it] 33%|      | 2166/6500 [4:06:04<8:19:53,  6.92s/it]                                  {'loss': 0.494, 'learning_rate': 7.50418580976983e-05, 'epoch': 0.33}
{'loss': 0.535, 'learning_rate': 7.502093197309452e-05, 'epoch': 0.33}
{'loss': 0.4897, 'learning_rate': 7.500000000000001e-05, 'epoch': 0.33}
{'loss': 0.7716, 'learning_rate': 7.49790621833075e-05, 'epoch': 0.33}
{'loss': 0.5215, 'learning_rate': 7.495811852791107e-05, 'epoch': 0.33}
                      33%|      | 2166/6500 [4:06:04<8:19:53,  6.92s/it] 33%|      | 2167/6500 [4:06:10<8:12:07,  6.81s/it]                                                        33%|      | 2167/6500 [4:06:10<8:12:07,  6.81s/it] 33%|      | 2168/6500 [4:06:17<8:06:42,  6.74s/it]                                                        33%|      | 2168/6500 [4:06:17<8:06:42,  6.74s/it] 33%|      | 2169/6500 [4:06:23<8:02:52,  6.69s/it]                                                        33%|      | 2169/6500 [4:06:23<8:02:52,  6.69s/it] 33%|      | 2170/6500 [4:06:30<8:00:01,  6.65s/it]                                                        33%|      | 2170/6500 [4:06:30<8:00:01,  6.65s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.851326048374176, 'eval_runtime': 1.477, 'eval_samples_per_second': 8.125, 'eval_steps_per_second': 2.031, 'epoch': 0.33}
                                                        33%|      | 2170/6500 [4:06:31<8:00:01,  6.65s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-2170
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2170
/projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2170
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2170
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2170
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2170/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2170/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2170/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2170/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.4856, 'learning_rate': 7.493716903870621e-05, 'epoch': 0.33}
{'loss': 0.5092, 'learning_rate': 7.49162137205897e-05, 'epoch': 0.33}
{'loss': 0.4849, 'learning_rate': 7.489525257845971e-05, 'epoch': 0.33}
{'loss': 0.4965, 'learning_rate': 7.48742856172158e-05, 'epoch': 0.33}
{'loss': 0.5066, 'learning_rate': 7.485331284175887e-05, 'epoch': 0.33}
 33%|      | 2171/6500 [4:06:38<8:36:11,  7.15s/it]                                                        33%|      | 2171/6500 [4:06:38<8:36:11,  7.15s/it] 33%|      | 2172/6500 [4:06:45<8:23:03,  6.97s/it]                                                        33%|      | 2172/6500 [4:06:45<8:23:03,  6.97s/it] 33%|      | 2173/6500 [4:06:51<8:14:07,  6.85s/it]                                                        33%|      | 2173/6500 [4:06:51<8:14:07,  6.85s/it] 33%|      | 2174/6500 [4:06:58<8:07:56,  6.77s/it]                                                        33%|      | 2174/6500 [4:06:58<8:07:56,  6.77s/it] 33%|      | 2175/6500 [4:07:04<8:03:22,  6.71s/it]                                                        33%|      | 2175/6500 [4:07:04<8:03:22,  6.71s/it] 33%|      | 2176/6500 [4:07:11<8:00:04,  6.66s/it]                                  {'loss': 0.5092, 'learning_rate': 7.483233425699119e-05, 'epoch': 0.33}
{'loss': 0.5, 'learning_rate': 7.481134986781634e-05, 'epoch': 0.33}
{'loss': 0.5122, 'learning_rate': 7.47903596791393e-05, 'epoch': 0.34}
{'loss': 0.4918, 'learning_rate': 7.476936369586645e-05, 'epoch': 0.34}
{'loss': 0.4962, 'learning_rate': 7.47483619229054e-05, 'epoch': 0.34}
                      33%|      | 2176/6500 [4:07:11<8:00:04,  6.66s/it] 33%|      | 2177/6500 [4:07:18<7:57:55,  6.63s/it]                                                        33%|      | 2177/6500 [4:07:18<7:57:55,  6.63s/it] 34%|      | 2178/6500 [4:07:24<7:56:31,  6.62s/it]                                                        34%|      | 2178/6500 [4:07:24<7:56:31,  6.62s/it] 34%|      | 2179/6500 [4:07:31<7:55:23,  6.60s/it]                                                        34%|      | 2179/6500 [4:07:31<7:55:23,  6.60s/it] 34%|      | 2180/6500 [4:07:37<7:54:28,  6.59s/it]                                                        34%|      | 2180/6500 [4:07:37<7:54:28,  6.59s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8344358205795288, 'eval_runtime': 1.4855, 'eval_samples_per_second': 8.078, 'eval_steps_per_second': 2.02, 'epoch': 0.34}
                                                        34%|      | 2180/6500 [4:07:39<7:54:28,  6.59s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-2180
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2180 /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2180

I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2180
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2180
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2180/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2180/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2180/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2180/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.4825, 'learning_rate': 7.472735436516524e-05, 'epoch': 0.34}
{'loss': 0.5291, 'learning_rate': 7.470634102755636e-05, 'epoch': 0.34}
{'loss': 0.4948, 'learning_rate': 7.468532191499045e-05, 'epoch': 0.34}
{'loss': 0.7727, 'learning_rate': 7.466429703238065e-05, 'epoch': 0.34}
{'loss': 0.5122, 'learning_rate': 7.464326638464138e-05, 'epoch': 0.34}
 34%|      | 2181/6500 [4:07:46<8:34:08,  7.14s/it]                                                        34%|      | 2181/6500 [4:07:46<8:34:08,  7.14s/it] 34%|      | 2182/6500 [4:07:53<8:40:15,  7.23s/it]                                                        34%|      | 2182/6500 [4:07:53<8:40:15,  7.23s/it] 34%|      | 2183/6500 [4:08:00<8:25:39,  7.03s/it]                                                        34%|      | 2183/6500 [4:08:00<8:25:39,  7.03s/it] 34%|      | 2184/6500 [4:08:06<8:15:20,  6.89s/it]                                                        34%|      | 2184/6500 [4:08:06<8:15:20,  6.89s/it] 34%|      | 2185/6500 [4:08:13<8:08:16,  6.79s/it]                                                        34%|      | 2185/6500 [4:08:13<8:08:16,  6.79s/it] 34%|      | 2186/6500 [4:08:19<8:03:01,  6.72s/it]                                  {'loss': 0.4914, 'learning_rate': 7.462222997668841e-05, 'epoch': 0.34}
{'loss': 0.4829, 'learning_rate': 7.460118781343893e-05, 'epoch': 0.34}
{'loss': 0.4827, 'learning_rate': 7.458013989981133e-05, 'epoch': 0.34}
{'loss': 0.4868, 'learning_rate': 7.45590862407255e-05, 'epoch': 0.34}
{'loss': 0.4956, 'learning_rate': 7.453802684110257e-05, 'epoch': 0.34}
                      34%|      | 2186/6500 [4:08:19<8:03:01,  6.72s/it] 34%|      | 2187/6500 [4:08:26<7:59:14,  6.67s/it]                                                        34%|      | 2187/6500 [4:08:26<7:59:14,  6.67s/it] 34%|      | 2188/6500 [4:08:32<7:56:53,  6.64s/it]                                                        34%|      | 2188/6500 [4:08:32<7:56:53,  6.64s/it] 34%|      | 2189/6500 [4:08:39<7:55:21,  6.62s/it]                                                        34%|      | 2189/6500 [4:08:39<7:55:21,  6.62s/it] 34%|      | 2190/6500 [4:08:46<7:54:03,  6.60s/it]                                                        34%|      | 2190/6500 [4:08:46<7:54:03,  6.60s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8401399850845337, 'eval_runtime': 1.4786, 'eval_samples_per_second': 8.116, 'eval_steps_per_second': 2.029, 'epoch': 0.34}
                                                        34%|      | 2190/6500 [4:08:47<7:54:03,  6.60s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-2190
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2190
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2190
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2190
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2190
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2190/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2190/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2190/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2190/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.5091, 'learning_rate': 7.451696170586507e-05, 'epoch': 0.34}
{'loss': 0.5096, 'learning_rate': 7.449589083993684e-05, 'epoch': 0.34}
{'loss': 0.4944, 'learning_rate': 7.447481424824305e-05, 'epoch': 0.34}
{'loss': 0.4991, 'learning_rate': 7.445373193571026e-05, 'epoch': 0.34}
{'loss': 0.4857, 'learning_rate': 7.443264390726629e-05, 'epoch': 0.34}
 34%|      | 2191/6500 [4:08:54<8:31:20,  7.12s/it]                                                        34%|      | 2191/6500 [4:08:54<8:31:20,  7.12s/it] 34%|      | 2192/6500 [4:09:00<8:19:05,  6.95s/it]                                                        34%|      | 2192/6500 [4:09:00<8:19:05,  6.95s/it] 34%|      | 2193/6500 [4:09:07<8:10:43,  6.84s/it]                                                        34%|      | 2193/6500 [4:09:07<8:10:43,  6.84s/it] 34%|      | 2194/6500 [4:09:14<8:04:45,  6.75s/it]                                                        34%|      | 2194/6500 [4:09:14<8:04:45,  6.75s/it] 34%|      | 2195/6500 [4:09:20<8:00:37,  6.70s/it]                                                        34%|      | 2195/6500 [4:09:20<8:00:37,  6.70s/it] 34%|      | 2196/6500 [4:09:27<7:57:46,  6.66s/it]                                  {'loss': 0.4812, 'learning_rate': 7.441155016784037e-05, 'epoch': 0.34}
{'loss': 0.5239, 'learning_rate': 7.439045072236301e-05, 'epoch': 0.34}
{'loss': 0.4947, 'learning_rate': 7.436934557576612e-05, 'epoch': 0.34}
{'loss': 0.7804, 'learning_rate': 7.434823473298283e-05, 'epoch': 0.34}
{'loss': 0.504, 'learning_rate': 7.432711819894775e-05, 'epoch': 0.34}
                      34%|      | 2196/6500 [4:09:27<7:57:46,  6.66s/it] 34%|      | 2197/6500 [4:09:33<7:55:43,  6.63s/it]                                                        34%|      | 2197/6500 [4:09:33<7:55:43,  6.63s/it] 34%|      | 2198/6500 [4:09:41<8:13:31,  6.88s/it]                                                        34%|      | 2198/6500 [4:09:41<8:13:31,  6.88s/it] 34%|      | 2199/6500 [4:09:47<8:06:11,  6.78s/it]                                                        34%|      | 2199/6500 [4:09:47<8:06:11,  6.78s/it] 34%|      | 2200/6500 [4:09:54<8:01:45,  6.72s/it]                                                        34%|      | 2200/6500 [4:09:54<8:01:45,  6.72s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8465731143951416, 'eval_runtime': 1.4832, 'eval_samples_per_second': 8.091, 'eval_steps_per_second': 2.023, 'epoch': 0.34}
                                                        34%|      | 2200/6500 [4:09:55<8:01:45,  6.72s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-2200
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2200I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2200

I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2200
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2200
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2200/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2200/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2200/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2200/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.4947, 'learning_rate': 7.430599597859666e-05, 'epoch': 0.34}
{'loss': 0.4838, 'learning_rate': 7.428486807686683e-05, 'epoch': 0.34}
{'loss': 0.4752, 'learning_rate': 7.426373449869673e-05, 'epoch': 0.34}
{'loss': 0.4782, 'learning_rate': 7.424259524902622e-05, 'epoch': 0.34}
{'loss': 0.4944, 'learning_rate': 7.422145033279646e-05, 'epoch': 0.34}
 34%|      | 2201/6500 [4:10:02<8:37:54,  7.23s/it]                                                        34%|      | 2201/6500 [4:10:02<8:37:54,  7.23s/it] 34%|      | 2202/6500 [4:10:09<8:23:40,  7.03s/it]                                                        34%|      | 2202/6500 [4:10:09<8:23:40,  7.03s/it] 34%|      | 2203/6500 [4:10:15<8:13:52,  6.90s/it]                                                        34%|      | 2203/6500 [4:10:15<8:13:52,  6.90s/it] 34%|      | 2204/6500 [4:10:22<8:06:36,  6.80s/it]                                                        34%|      | 2204/6500 [4:10:22<8:06:36,  6.80s/it] 34%|      | 2205/6500 [4:10:29<8:01:36,  6.73s/it]                                                        34%|      | 2205/6500 [4:10:29<8:01:36,  6.73s/it] 34%|      | 2206/6500 [4:10:35<7:57:52,  6.68s/it]                                  {'loss': 0.5079, 'learning_rate': 7.420029975494995e-05, 'epoch': 0.34}
{'loss': 0.5032, 'learning_rate': 7.417914352043052e-05, 'epoch': 0.34}
{'loss': 0.495, 'learning_rate': 7.41579816341833e-05, 'epoch': 0.34}
{'loss': 0.4985, 'learning_rate': 7.413681410115474e-05, 'epoch': 0.34}
{'loss': 0.4856, 'learning_rate': 7.411564092629267e-05, 'epoch': 0.34}
                      34%|      | 2206/6500 [4:10:35<7:57:52,  6.68s/it] 34%|      | 2207/6500 [4:10:42<7:55:22,  6.64s/it]                                                        34%|      | 2207/6500 [4:10:42<7:55:22,  6.64s/it] 34%|      | 2208/6500 [4:10:48<7:53:27,  6.62s/it]                                                        34%|      | 2208/6500 [4:10:48<7:53:27,  6.62s/it] 34%|      | 2209/6500 [4:10:55<7:52:11,  6.60s/it]                                                        34%|      | 2209/6500 [4:10:55<7:52:11,  6.60s/it] 34%|      | 2210/6500 [4:11:01<7:51:05,  6.59s/it]                                                        34%|      | 2210/6500 [4:11:01<7:51:05,  6.59s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.836142897605896, 'eval_runtime': 1.4795, 'eval_samples_per_second': 8.111, 'eval_steps_per_second': 2.028, 'epoch': 0.34}
                                                        34%|      | 2210/6500 [4:11:03<7:51:05,  6.59s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-2210
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2210
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2210
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2210
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2210
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2210/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2210/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2210/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2210/pytorch_model.bin
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.4914, 'learning_rate': 7.409446211454615e-05, 'epoch': 0.34}
{'loss': 0.5149, 'learning_rate': 7.407327767086561e-05, 'epoch': 0.34}
{'loss': 0.4875, 'learning_rate': 7.405208760020276e-05, 'epoch': 0.34}
{'loss': 0.7831, 'learning_rate': 7.403089190751069e-05, 'epoch': 0.34}
{'loss': 0.4837, 'learning_rate': 7.400969059774375e-05, 'epoch': 0.34}
 34%|      | 2211/6500 [4:11:10<8:30:26,  7.14s/it]                                                        34%|      | 2211/6500 [4:11:10<8:30:26,  7.14s/it] 34%|      | 2212/6500 [4:11:16<8:17:50,  6.97s/it]                                                        34%|      | 2212/6500 [4:11:16<8:17:50,  6.97s/it] 34%|      | 2213/6500 [4:11:23<8:08:51,  6.84s/it]                                                        34%|      | 2213/6500 [4:11:23<8:08:51,  6.84s/it] 34%|      | 2214/6500 [4:11:30<8:16:05,  6.94s/it]                                                        34%|      | 2214/6500 [4:11:30<8:16:05,  6.94s/it] 34%|      | 2215/6500 [4:11:37<8:07:46,  6.83s/it]                                                        34%|      | 2215/6500 [4:11:37<8:07:46,  6.83s/it] 34%|      | 2216/6500 [4:11:43<8:01:44,  6.75s/it]                                  {'loss': 0.5089, 'learning_rate': 7.39884836758576e-05, 'epoch': 0.34}
{'loss': 0.4834, 'learning_rate': 7.396727114680925e-05, 'epoch': 0.34}
{'loss': 0.4812, 'learning_rate': 7.394605301555699e-05, 'epoch': 0.34}
{'loss': 0.4845, 'learning_rate': 7.392482928706044e-05, 'epoch': 0.34}
{'loss': 0.4984, 'learning_rate': 7.390359996628051e-05, 'epoch': 0.34}
                      34%|      | 2216/6500 [4:11:43<8:01:44,  6.75s/it] 34%|      | 2217/6500 [4:11:50<7:57:43,  6.69s/it]                                                        34%|      | 2217/6500 [4:11:50<7:57:43,  6.69s/it] 34%|      | 2218/6500 [4:11:56<7:54:46,  6.65s/it]                                                        34%|      | 2218/6500 [4:11:56<7:54:46,  6.65s/it] 34%|      | 2219/6500 [4:12:03<7:52:52,  6.63s/it]                                                        34%|      | 2219/6500 [4:12:03<7:52:52,  6.63s/it] 34%|      | 2220/6500 [4:12:10<7:51:50,  6.61s/it]                                                        34%|      | 2220/6500 [4:12:10<7:51:50,  6.61s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8435304760932922, 'eval_runtime': 1.7126, 'eval_samples_per_second': 7.007, 'eval_steps_per_second': 1.752, 'epoch': 0.34}
                                                        34%|      | 2220/6500 [4:12:11<7:51:50,  6.61s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-2220
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2220
/projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2220
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2220
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2220
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2220/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2220/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2220/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2220/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.5109, 'learning_rate': 7.388236505817943e-05, 'epoch': 0.34}
{'loss': 0.4957, 'learning_rate': 7.386112456772071e-05, 'epoch': 0.34}
{'loss': 0.4926, 'learning_rate': 7.38398784998692e-05, 'epoch': 0.34}
{'loss': 0.496, 'learning_rate': 7.381862685959104e-05, 'epoch': 0.34}
{'loss': 0.4809, 'learning_rate': 7.379736965185368e-05, 'epoch': 0.34}
 34%|      | 2221/6500 [4:12:18<8:35:35,  7.23s/it]                                                        34%|      | 2221/6500 [4:12:18<8:35:35,  7.23s/it] 34%|      | 2222/6500 [4:12:25<8:21:05,  7.03s/it]                                                        34%|      | 2222/6500 [4:12:25<8:21:05,  7.03s/it] 34%|      | 2223/6500 [4:12:31<8:10:59,  6.89s/it]                                                        34%|      | 2223/6500 [4:12:31<8:10:59,  6.89s/it] 34%|      | 2224/6500 [4:12:38<8:03:42,  6.79s/it]                                                        34%|      | 2224/6500 [4:12:38<8:03:42,  6.79s/it] 34%|      | 2225/6500 [4:12:44<7:58:45,  6.72s/it]                                                        34%|      | 2225/6500 [4:12:44<7:58:45,  6.72s/it] 34%|      | 2226/6500 [4:12:51<7:55:09,  6.67s/it]                                  {'loss': 0.4956, 'learning_rate': 7.377610688162586e-05, 'epoch': 0.34}
{'loss': 0.5033, 'learning_rate': 7.375483855387761e-05, 'epoch': 0.34}
{'loss': 0.4979, 'learning_rate': 7.373356467358027e-05, 'epoch': 0.34}
{'loss': 0.7818, 'learning_rate': 7.371228524570649e-05, 'epoch': 0.34}
{'loss': 0.4794, 'learning_rate': 7.369100027523022e-05, 'epoch': 0.34}
                      34%|      | 2226/6500 [4:12:51<7:55:09,  6.67s/it] 34%|      | 2227/6500 [4:12:58<7:52:43,  6.64s/it]                                                        34%|      | 2227/6500 [4:12:58<7:52:43,  6.64s/it] 34%|      | 2228/6500 [4:13:04<7:50:56,  6.61s/it]                                                        34%|      | 2228/6500 [4:13:04<7:50:56,  6.61s/it] 34%|      | 2229/6500 [4:13:11<7:51:05,  6.62s/it]                                                        34%|      | 2229/6500 [4:13:11<7:51:05,  6.62s/it] 34%|      | 2230/6500 [4:13:17<7:49:53,  6.60s/it]                                                        34%|      | 2230/6500 [4:13:17<7:49:53,  6.60s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8489333987236023, 'eval_runtime': 1.484, 'eval_samples_per_second': 8.086, 'eval_steps_per_second': 2.022, 'epoch': 0.34}
                                                        34%|      | 2230/6500 [4:13:19<7:49:53,  6.60s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-2230
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2230
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2230
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2230
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2230
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2230/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2230/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2230/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2230/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.5086, 'learning_rate': 7.366970976712668e-05, 'epoch': 0.34}
{'loss': 0.4683, 'learning_rate': 7.364841372637239e-05, 'epoch': 0.34}
{'loss': 0.4713, 'learning_rate': 7.362711215794517e-05, 'epoch': 0.34}
{'loss': 0.4866, 'learning_rate': 7.360580506682414e-05, 'epoch': 0.34}
{'loss': 0.4825, 'learning_rate': 7.35844924579897e-05, 'epoch': 0.34}
 34%|      | 2231/6500 [4:13:26<8:42:21,  7.34s/it]                                                        34%|      | 2231/6500 [4:13:26<8:42:21,  7.34s/it] 34%|      | 2232/6500 [4:13:33<8:25:41,  7.11s/it]                                                        34%|      | 2232/6500 [4:13:33<8:25:41,  7.11s/it] 34%|      | 2233/6500 [4:13:39<8:14:13,  6.95s/it]                                                        34%|      | 2233/6500 [4:13:39<8:14:13,  6.95s/it] 34%|      | 2234/6500 [4:13:46<8:05:58,  6.84s/it]                                                        34%|      | 2234/6500 [4:13:46<8:05:58,  6.84s/it] 34%|      | 2235/6500 [4:13:53<8:00:31,  6.76s/it]                                                        34%|      | 2235/6500 [4:13:53<8:00:31,  6.76s/it] 34%|      | 2236/6500 [4:13:59<7:56:13,  6.70s/it]                                  {'loss': 0.5092, 'learning_rate': 7.356317433642357e-05, 'epoch': 0.34}
{'loss': 0.5033, 'learning_rate': 7.354185070710869e-05, 'epoch': 0.34}
{'loss': 0.4839, 'learning_rate': 7.352052157502932e-05, 'epoch': 0.34}
{'loss': 0.4966, 'learning_rate': 7.349918694517106e-05, 'epoch': 0.34}
{'loss': 0.4762, 'learning_rate': 7.347784682252072e-05, 'epoch': 0.34}
                      34%|      | 2236/6500 [4:13:59<7:56:13,  6.70s/it] 34%|      | 2237/6500 [4:14:06<7:53:21,  6.66s/it]                                                        34%|      | 2237/6500 [4:14:06<7:53:21,  6.66s/it] 34%|      | 2238/6500 [4:14:12<7:51:04,  6.63s/it]                                                        34%|      | 2238/6500 [4:14:12<7:51:04,  6.63s/it] 34%|      | 2239/6500 [4:14:19<7:49:59,  6.62s/it]                                                        34%|      | 2239/6500 [4:14:19<7:49:59,  6.62s/it] 34%|      | 2240/6500 [4:14:26<7:49:10,  6.61s/it]                                                        34%|      | 2240/6500 [4:14:26<7:49:10,  6.61s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8364694714546204, 'eval_runtime': 1.4946, 'eval_samples_per_second': 8.029, 'eval_steps_per_second': 2.007, 'epoch': 0.34}
                                                        34%|      | 2240/6500 [4:14:27<7:49:10,  6.61s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-2240
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2240I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2240

I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2240
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2240
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2240/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2240/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2240/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2240/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.5267, 'learning_rate': 7.345650121206645e-05, 'epoch': 0.34}
{'loss': 0.4765, 'learning_rate': 7.343515011879763e-05, 'epoch': 0.34}
{'loss': 0.767, 'learning_rate': 7.341379354770496e-05, 'epoch': 0.35}
{'loss': 0.4979, 'learning_rate': 7.33924315037804e-05, 'epoch': 0.35}
{'loss': 0.4875, 'learning_rate': 7.337106399201721e-05, 'epoch': 0.35}
 34%|      | 2241/6500 [4:14:34<8:29:14,  7.17s/it]                                                        34%|      | 2241/6500 [4:14:34<8:29:14,  7.17s/it] 34%|      | 2242/6500 [4:14:41<8:16:23,  6.99s/it]                                                        34%|      | 2242/6500 [4:14:41<8:16:23,  6.99s/it] 35%|      | 2243/6500 [4:14:47<8:07:07,  6.87s/it]                                                        35%|      | 2243/6500 [4:14:47<8:07:07,  6.87s/it] 35%|      | 2244/6500 [4:14:54<8:00:59,  6.78s/it]                                                        35%|      | 2244/6500 [4:14:54<8:00:59,  6.78s/it] 35%|      | 2245/6500 [4:15:00<7:56:37,  6.72s/it]                                                        35%|      | 2245/6500 [4:15:00<7:56:37,  6.72s/it] 35%|      | 2246/6500 [4:15:07<7:53:44,  6.68s/it]                                  {'loss': 0.5018, 'learning_rate': 7.334969101740991e-05, 'epoch': 0.35}
{'loss': 0.4691, 'learning_rate': 7.33283125849543e-05, 'epoch': 0.35}
{'loss': 0.4841, 'learning_rate': 7.330692869964746e-05, 'epoch': 0.35}
{'loss': 0.4925, 'learning_rate': 7.328553936648774e-05, 'epoch': 0.35}
{'loss': 0.4985, 'learning_rate': 7.326414459047477e-05, 'epoch': 0.35}
                      35%|      | 2246/6500 [4:15:07<7:53:44,  6.68s/it] 35%|      | 2247/6500 [4:15:14<8:10:37,  6.92s/it]                                                        35%|      | 2247/6500 [4:15:14<8:10:37,  6.92s/it] 35%|      | 2248/6500 [4:15:21<8:03:21,  6.82s/it]                                                        35%|      | 2248/6500 [4:15:21<8:03:21,  6.82s/it] 35%|      | 2249/6500 [4:15:28<7:58:12,  6.75s/it]                                                        35%|      | 2249/6500 [4:15:28<7:58:12,  6.75s/it] 35%|      | 2250/6500 [4:15:34<7:54:17,  6.70s/it]                                                        35%|      | 2250/6500 [4:15:34<7:54:17,  6.70s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.841743528842926, 'eval_runtime': 1.4853, 'eval_samples_per_second': 8.079, 'eval_steps_per_second': 2.02, 'epoch': 0.35}
                                                        35%|      | 2250/6500 [4:15:36<7:54:17,  6.70s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-2250
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2250
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2250
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2250
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2250
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2250/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2250/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2250/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2250/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.4901, 'learning_rate': 7.324274437660947e-05, 'epoch': 0.35}
{'loss': 0.4995, 'learning_rate': 7.322133872989398e-05, 'epoch': 0.35}
{'loss': 0.4886, 'learning_rate': 7.319992765533174e-05, 'epoch': 0.35}
{'loss': 0.4875, 'learning_rate': 7.317851115792749e-05, 'epoch': 0.35}
{'loss': 0.4768, 'learning_rate': 7.315708924268716e-05, 'epoch': 0.35}
 35%|      | 2251/6500 [4:15:43<8:31:54,  7.23s/it]                                                        35%|      | 2251/6500 [4:15:43<8:31:54,  7.23s/it] 35%|      | 2252/6500 [4:15:49<8:17:37,  7.03s/it]                                                        35%|      | 2252/6500 [4:15:49<8:17:37,  7.03s/it] 35%|      | 2253/6500 [4:15:56<8:07:27,  6.89s/it]                                                        35%|      | 2253/6500 [4:15:56<8:07:27,  6.89s/it] 35%|      | 2254/6500 [4:16:02<8:00:09,  6.79s/it]                                                        35%|      | 2254/6500 [4:16:02<8:00:09,  6.79s/it] 35%|      | 2255/6500 [4:16:09<7:55:23,  6.72s/it]                                                        35%|      | 2255/6500 [4:16:09<7:55:23,  6.72s/it] 35%|      | 2256/6500 [4:16:15<7:52:54,  6.69s/it]                                  {'loss': 0.5282, 'learning_rate': 7.313566191461804e-05, 'epoch': 0.35}
{'loss': 0.4923, 'learning_rate': 7.311422917872861e-05, 'epoch': 0.35}
{'loss': 0.7549, 'learning_rate': 7.309279104002864e-05, 'epoch': 0.35}
{'loss': 0.5087, 'learning_rate': 7.307134750352916e-05, 'epoch': 0.35}
{'loss': 0.4744, 'learning_rate': 7.304989857424249e-05, 'epoch': 0.35}
                      35%|      | 2256/6500 [4:16:15<7:52:54,  6.69s/it] 35%|      | 2257/6500 [4:16:22<7:49:55,  6.65s/it]                                                        35%|      | 2257/6500 [4:16:22<7:49:55,  6.65s/it] 35%|      | 2258/6500 [4:16:29<7:48:13,  6.62s/it]                                                        35%|      | 2258/6500 [4:16:29<7:48:13,  6.62s/it] 35%|      | 2259/6500 [4:16:35<7:46:49,  6.60s/it]                                                        35%|      | 2259/6500 [4:16:35<7:46:49,  6.60s/it] 35%|      | 2260/6500 [4:16:42<7:45:37,  6.59s/it]                                                        35%|      | 2260/6500 [4:16:42<7:45:37,  6.59s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8455104827880859, 'eval_runtime': 1.4846, 'eval_samples_per_second': 8.083, 'eval_steps_per_second': 2.021, 'epoch': 0.35}
                                                        35%|      | 2260/6500 [4:16:43<7:45:37,  6.59s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-2260
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2260
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2260
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2260
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2260
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2260/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2260/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2260/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2260/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.496, 'learning_rate': 7.302844425718218e-05, 'epoch': 0.35}
{'loss': 0.4774, 'learning_rate': 7.300698455736302e-05, 'epoch': 0.35}
{'loss': 0.4823, 'learning_rate': 7.298551947980113e-05, 'epoch': 0.35}
{'loss': 0.4909, 'learning_rate': 7.296404902951381e-05, 'epoch': 0.35}
{'loss': 0.5086, 'learning_rate': 7.294257321151964e-05, 'epoch': 0.35}
 35%|      | 2261/6500 [4:16:50<8:24:30,  7.14s/it]                                                        35%|      | 2261/6500 [4:16:50<8:24:30,  7.14s/it] 35%|      | 2262/6500 [4:16:57<8:12:01,  6.97s/it]                                                        35%|      | 2262/6500 [4:16:57<8:12:01,  6.97s/it] 35%|      | 2263/6500 [4:17:04<8:21:25,  7.10s/it]                                                        35%|      | 2263/6500 [4:17:04<8:21:25,  7.10s/it] 35%|      | 2264/6500 [4:17:11<8:10:38,  6.95s/it]                                                        35%|      | 2264/6500 [4:17:11<8:10:38,  6.95s/it] 35%|      | 2265/6500 [4:17:17<8:02:18,  6.83s/it]                                                        35%|      | 2265/6500 [4:17:17<8:02:18,  6.83s/it] 35%|      | 2266/6500 [4:17:24<7:56:15,  6.75s/it]                                  {'loss': 0.4936, 'learning_rate': 7.292109203083848e-05, 'epoch': 0.35}
{'loss': 0.4977, 'learning_rate': 7.289960549249141e-05, 'epoch': 0.35}
{'loss': 0.4882, 'learning_rate': 7.287811360150081e-05, 'epoch': 0.35}
{'loss': 0.4834, 'learning_rate': 7.285661636289025e-05, 'epoch': 0.35}
{'loss': 0.4769, 'learning_rate': 7.283511378168458e-05, 'epoch': 0.35}
                      35%|      | 2266/6500 [4:17:24<7:56:15,  6.75s/it] 35%|      | 2267/6500 [4:17:30<7:52:13,  6.69s/it]                                                        35%|      | 2267/6500 [4:17:30<7:52:13,  6.69s/it] 35%|      | 2268/6500 [4:17:37<7:49:07,  6.65s/it]                                                        35%|      | 2268/6500 [4:17:37<7:49:07,  6.65s/it] 35%|      | 2269/6500 [4:17:43<7:46:51,  6.62s/it]                                                        35%|      | 2269/6500 [4:17:43<7:46:51,  6.62s/it] 35%|      | 2270/6500 [4:17:50<7:45:03,  6.60s/it]                                                        35%|      | 2270/6500 [4:17:50<7:45:03,  6.60s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8375691771507263, 'eval_runtime': 1.4844, 'eval_samples_per_second': 8.084, 'eval_steps_per_second': 2.021, 'epoch': 0.35}
                                                        35%|      | 2270/6500 [4:17:51<7:45:03,  6.60s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-2270
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2270
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2270
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2270
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2270
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2270/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2270/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2270/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2270/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.5217, 'learning_rate': 7.28136058629099e-05, 'epoch': 0.35}
{'loss': 0.4839, 'learning_rate': 7.279209261159357e-05, 'epoch': 0.35}
{'loss': 0.7689, 'learning_rate': 7.277057403276416e-05, 'epoch': 0.35}
{'loss': 0.4991, 'learning_rate': 7.274905013145153e-05, 'epoch': 0.35}
{'loss': 0.4858, 'learning_rate': 7.272752091268673e-05, 'epoch': 0.35}
 35%|      | 2271/6500 [4:17:58<8:24:28,  7.16s/it]                                                        35%|      | 2271/6500 [4:17:58<8:24:28,  7.16s/it] 35%|      | 2272/6500 [4:18:05<8:11:38,  6.98s/it]                                                        35%|      | 2272/6500 [4:18:05<8:11:38,  6.98s/it] 35%|      | 2273/6500 [4:18:12<8:02:21,  6.85s/it]                                                        35%|      | 2273/6500 [4:18:12<8:02:21,  6.85s/it] 35%|      | 2274/6500 [4:18:18<7:55:53,  6.76s/it]                                                        35%|      | 2274/6500 [4:18:18<7:55:53,  6.76s/it] 35%|      | 2275/6500 [4:18:25<7:51:16,  6.69s/it]                                                        35%|      | 2275/6500 [4:18:25<7:51:16,  6.69s/it] 35%|      | 2276/6500 [4:18:31<7:47:54,  6.65s/it]                                  {'loss': 0.4796, 'learning_rate': 7.270598638150211e-05, 'epoch': 0.35}
{'loss': 0.4647, 'learning_rate': 7.268444654293122e-05, 'epoch': 0.35}
{'loss': 0.4773, 'learning_rate': 7.266290140200889e-05, 'epoch': 0.35}
{'loss': 0.4839, 'learning_rate': 7.264135096377115e-05, 'epoch': 0.35}
{'loss': 0.5055, 'learning_rate': 7.261979523325528e-05, 'epoch': 0.35}
                      35%|      | 2276/6500 [4:18:31<7:47:54,  6.65s/it] 35%|      | 2277/6500 [4:18:38<7:45:58,  6.62s/it]                                                        35%|      | 2277/6500 [4:18:38<7:45:58,  6.62s/it] 35%|      | 2278/6500 [4:18:44<7:44:24,  6.60s/it]                                                        35%|      | 2278/6500 [4:18:44<7:44:24,  6.60s/it] 35%|      | 2279/6500 [4:18:52<8:01:31,  6.84s/it]                                                        35%|      | 2279/6500 [4:18:52<8:01:31,  6.84s/it] 35%|      | 2280/6500 [4:19:02<9:20:55,  7.98s/it]                                                        35%|      | 2280/6500 [4:19:02<9:20:55,  7.98s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.842242419719696, 'eval_runtime': 1.8035, 'eval_samples_per_second': 6.654, 'eval_steps_per_second': 1.663, 'epoch': 0.35}
                                                        35%|      | 2280/6500 [4:19:04<9:20:55,  7.98s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-2280
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2280I AM HERE AND I AM SAVING THE MODEL

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2280
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2280
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2280
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2280/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2280/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2280/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2280/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.492, 'learning_rate': 7.25982342154998e-05, 'epoch': 0.35}
{'loss': 0.4835, 'learning_rate': 7.257666791554448e-05, 'epoch': 0.35}
{'loss': 0.4873, 'learning_rate': 7.25550963384303e-05, 'epoch': 0.35}
{'loss': 0.4795, 'learning_rate': 7.253351948919948e-05, 'epoch': 0.35}
{'loss': 0.4811, 'learning_rate': 7.25119373728955e-05, 'epoch': 0.35}
 35%|      | 2281/6500 [4:19:11<9:37:57,  8.22s/it]                                                        35%|      | 2281/6500 [4:19:11<9:37:57,  8.22s/it] 35%|      | 2282/6500 [4:19:18<9:02:44,  7.72s/it]                                                        35%|      | 2282/6500 [4:19:18<9:02:44,  7.72s/it] 35%|      | 2283/6500 [4:19:24<8:37:42,  7.37s/it]                                                        35%|      | 2283/6500 [4:19:24<8:37:42,  7.37s/it] 35%|      | 2284/6500 [4:19:31<8:20:18,  7.12s/it]                                                        35%|      | 2284/6500 [4:19:31<8:20:18,  7.12s/it] 35%|      | 2285/6500 [4:19:37<8:08:11,  6.95s/it]                                                        35%|      | 2285/6500 [4:19:37<8:08:11,  6.95s/it] 35%|      | 2286/6500 [4:19:44<7:59:41,  6.83s/it]                                  {'loss': 0.5106, 'learning_rate': 7.249034999456301e-05, 'epoch': 0.35}
{'loss': 0.4839, 'learning_rate': 7.246875735924797e-05, 'epoch': 0.35}
{'loss': 0.7638, 'learning_rate': 7.244715947199749e-05, 'epoch': 0.35}
{'loss': 0.4799, 'learning_rate': 7.242555633785999e-05, 'epoch': 0.35}
{'loss': 0.4976, 'learning_rate': 7.240394796188505e-05, 'epoch': 0.35}
                      35%|      | 2286/6500 [4:19:44<7:59:41,  6.83s/it] 35%|      | 2287/6500 [4:19:50<7:53:47,  6.75s/it]                                                        35%|      | 2287/6500 [4:19:50<7:53:47,  6.75s/it] 35%|      | 2288/6500 [4:19:57<7:50:21,  6.70s/it]                                                        35%|      | 2288/6500 [4:19:57<7:50:21,  6.70s/it] 35%|      | 2289/6500 [4:20:04<7:47:22,  6.66s/it]                                                        35%|      | 2289/6500 [4:20:04<7:47:22,  6.66s/it] 35%|      | 2290/6500 [4:20:10<7:44:53,  6.63s/it]                                                        35%|      | 2290/6500 [4:20:10<7:44:53,  6.63s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8451879024505615, 'eval_runtime': 1.4837, 'eval_samples_per_second': 8.088, 'eval_steps_per_second': 2.022, 'epoch': 0.35}
                                                        35%|      | 2290/6500 [4:20:12<7:44:53,  6.63s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-2290
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2290
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2290
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2290
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2290
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2290/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2290/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2290/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2290/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.4741, 'learning_rate': 7.238233434912347e-05, 'epoch': 0.35}
{'loss': 0.4617, 'learning_rate': 7.236071550462733e-05, 'epoch': 0.35}
{'loss': 0.4708, 'learning_rate': 7.23390914334499e-05, 'epoch': 0.35}
{'loss': 0.4829, 'learning_rate': 7.231746214064566e-05, 'epoch': 0.35}
{'loss': 0.4994, 'learning_rate': 7.229582763127036e-05, 'epoch': 0.35}
 35%|      | 2291/6500 [4:20:19<8:22:04,  7.16s/it]                                                        35%|      | 2291/6500 [4:20:19<8:22:04,  7.16s/it] 35%|      | 2292/6500 [4:20:25<8:09:15,  6.98s/it]                                                        35%|      | 2292/6500 [4:20:25<8:09:15,  6.98s/it] 35%|      | 2293/6500 [4:20:32<8:00:26,  6.85s/it]                                                        35%|      | 2293/6500 [4:20:32<8:00:26,  6.85s/it] 35%|      | 2294/6500 [4:20:38<7:53:56,  6.76s/it]                                                        35%|      | 2294/6500 [4:20:38<7:53:56,  6.76s/it] 35%|      | 2295/6500 [4:20:46<8:14:20,  7.05s/it]                                                        35%|      | 2295/6500 [4:20:46<8:14:20,  7.05s/it] 35%|      | 2296/6500 [4:20:52<8:04:16,  6.91s/it]                                  {'loss': 0.4909, 'learning_rate': 7.227418791038089e-05, 'epoch': 0.35}
{'loss': 0.4844, 'learning_rate': 7.225254298303543e-05, 'epoch': 0.35}
{'loss': 0.5008, 'learning_rate': 7.223089285429335e-05, 'epoch': 0.35}
{'loss': 0.4741, 'learning_rate': 7.220923752921524e-05, 'epoch': 0.35}
{'loss': 0.4877, 'learning_rate': 7.218757701286287e-05, 'epoch': 0.35}
                      35%|      | 2296/6500 [4:20:52<8:04:16,  6.91s/it] 35%|      | 2297/6500 [4:20:59<7:56:51,  6.81s/it]                                                        35%|      | 2297/6500 [4:20:59<7:56:51,  6.81s/it] 35%|      | 2298/6500 [4:21:06<7:52:34,  6.75s/it]                                                        35%|      | 2298/6500 [4:21:06<7:52:34,  6.75s/it] 35%|      | 2299/6500 [4:21:12<7:48:31,  6.69s/it]                                                        35%|      | 2299/6500 [4:21:12<7:48:31,  6.69s/it] 35%|      | 2300/6500 [4:21:19<7:45:39,  6.65s/it]                                                        35%|      | 2300/6500 [4:21:19<7:45:39,  6.65s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8372125029563904, 'eval_runtime': 1.5238, 'eval_samples_per_second': 7.875, 'eval_steps_per_second': 1.969, 'epoch': 0.35}
                                                        35%|      | 2300/6500 [4:21:20<7:45:39,  6.65s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-2300
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2300
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2300
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2300
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2300
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2300/pytorch_model.bin
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2300/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2300/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2300/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.5, 'learning_rate': 7.21659113102993e-05, 'epoch': 0.35}
{'loss': 0.4884, 'learning_rate': 7.214424042658872e-05, 'epoch': 0.35}
{'loss': 0.7741, 'learning_rate': 7.212256436679658e-05, 'epoch': 0.35}
{'loss': 0.4772, 'learning_rate': 7.210088313598953e-05, 'epoch': 0.35}
{'loss': 0.5092, 'learning_rate': 7.207919673923542e-05, 'epoch': 0.35}
 35%|      | 2301/6500 [4:21:27<8:25:00,  7.22s/it]                                                        35%|      | 2301/6500 [4:21:27<8:25:00,  7.22s/it] 35%|      | 2302/6500 [4:21:34<8:10:50,  7.02s/it]                                                        35%|      | 2302/6500 [4:21:34<8:10:50,  7.02s/it] 35%|      | 2303/6500 [4:21:40<8:02:01,  6.89s/it]                                                        35%|      | 2303/6500 [4:21:40<8:02:01,  6.89s/it] 35%|      | 2304/6500 [4:21:48<8:20:22,  7.15s/it]                                                        35%|      | 2304/6500 [4:21:48<8:20:22,  7.15s/it] 35%|      | 2305/6500 [4:21:55<8:08:18,  6.98s/it]                                                        35%|      | 2305/6500 [4:21:55<8:08:18,  6.98s/it] 35%|      | 2306/6500 [4:22:01<7:59:17,  6.86s/it]                                  {'loss': 0.4584, 'learning_rate': 7.20575051816033e-05, 'epoch': 0.35}
{'loss': 0.4731, 'learning_rate': 7.203580846816348e-05, 'epoch': 0.35}
{'loss': 0.4811, 'learning_rate': 7.20141066039874e-05, 'epoch': 0.36}
{'loss': 0.4732, 'learning_rate': 7.199239959414775e-05, 'epoch': 0.36}
{'loss': 0.4979, 'learning_rate': 7.197068744371841e-05, 'epoch': 0.36}
                      35%|      | 2306/6500 [4:22:01<7:59:17,  6.86s/it] 35%|      | 2307/6500 [4:22:08<7:53:08,  6.77s/it]                                                        35%|      | 2307/6500 [4:22:08<7:53:08,  6.77s/it] 36%|      | 2308/6500 [4:22:15<7:48:32,  6.71s/it]                                                        36%|      | 2308/6500 [4:22:15<7:48:32,  6.71s/it] 36%|      | 2309/6500 [4:22:21<7:45:25,  6.66s/it]                                                        36%|      | 2309/6500 [4:22:21<7:45:25,  6.66s/it] 36%|      | 2310/6500 [4:22:28<7:43:20,  6.63s/it]                                                        36%|      | 2310/6500 [4:22:28<7:43:20,  6.63s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8410887718200684, 'eval_runtime': 1.6106, 'eval_samples_per_second': 7.451, 'eval_steps_per_second': 1.863, 'epoch': 0.36}
                                                        36%|      | 2310/6500 [4:22:29<7:43:20,  6.63s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-2310
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2310
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2310
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2310
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2310
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2310/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2310/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2310/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2310/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.4999, 'learning_rate': 7.194897015777447e-05, 'epoch': 0.36}
{'loss': 0.4674, 'learning_rate': 7.19272477413922e-05, 'epoch': 0.36}
{'loss': 0.493, 'learning_rate': 7.190552019964909e-05, 'epoch': 0.36}
{'loss': 0.4725, 'learning_rate': 7.188378753762382e-05, 'epoch': 0.36}
{'loss': 0.5167, 'learning_rate': 7.186204976039628e-05, 'epoch': 0.36}
 36%|      | 2311/6500 [4:22:37<8:38:20,  7.42s/it]                                                        36%|      | 2311/6500 [4:22:37<8:38:20,  7.42s/it] 36%|      | 2312/6500 [4:22:43<8:20:05,  7.16s/it]                                                        36%|      | 2312/6500 [4:22:43<8:20:05,  7.16s/it] 36%|      | 2313/6500 [4:22:50<8:07:25,  6.98s/it]                                                        36%|      | 2313/6500 [4:22:50<8:07:25,  6.98s/it] 36%|      | 2314/6500 [4:22:57<7:58:29,  6.86s/it]                                                        36%|      | 2314/6500 [4:22:57<7:58:29,  6.86s/it] 36%|      | 2315/6500 [4:23:03<7:52:40,  6.78s/it]                                                        36%|      | 2315/6500 [4:23:03<7:52:40,  6.78s/it] 36%|      | 2316/6500 [4:23:10<7:48:20,  6.72s/it]                                  {'loss': 0.4659, 'learning_rate': 7.184030687304752e-05, 'epoch': 0.36}
{'loss': 0.7582, 'learning_rate': 7.181855888065982e-05, 'epoch': 0.36}
{'loss': 0.4993, 'learning_rate': 7.179680578831666e-05, 'epoch': 0.36}
{'loss': 0.4767, 'learning_rate': 7.177504760110265e-05, 'epoch': 0.36}
{'loss': 0.4927, 'learning_rate': 7.175328432410366e-05, 'epoch': 0.36}
                      36%|      | 2316/6500 [4:23:10<7:48:20,  6.72s/it] 36%|      | 2317/6500 [4:23:16<7:45:09,  6.67s/it]                                                        36%|      | 2317/6500 [4:23:16<7:45:09,  6.67s/it] 36%|      | 2318/6500 [4:23:23<7:42:52,  6.64s/it]                                                        36%|      | 2318/6500 [4:23:23<7:42:52,  6.64s/it] 36%|      | 2319/6500 [4:23:29<7:41:19,  6.62s/it]                                                        36%|      | 2319/6500 [4:23:29<7:41:19,  6.62s/it] 36%|      | 2320/6500 [4:23:36<7:40:37,  6.61s/it]                                                        36%|      | 2320/6500 [4:23:36<7:40:37,  6.61s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8456717729568481, 'eval_runtime': 3.059, 'eval_samples_per_second': 3.923, 'eval_steps_per_second': 0.981, 'epoch': 0.36}
                                                        36%|      | 2320/6500 [4:23:39<7:40:37,  6.61s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-2320
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2320
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2320
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2320
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2320
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2320/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2320/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2320/pytorch_model.binthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2320/pytorch_model.bin

/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.4681, 'learning_rate': 7.173151596240673e-05, 'epoch': 0.36}
{'loss': 0.4786, 'learning_rate': 7.170974252110008e-05, 'epoch': 0.36}
{'loss': 0.4899, 'learning_rate': 7.168796400527312e-05, 'epoch': 0.36}
{'loss': 0.4844, 'learning_rate': 7.16661804200164e-05, 'epoch': 0.36}
{'loss': 0.4932, 'learning_rate': 7.164439177042178e-05, 'epoch': 0.36}
 36%|      | 2321/6500 [4:23:46<8:50:45,  7.62s/it]                                                        36%|      | 2321/6500 [4:23:46<8:50:45,  7.62s/it] 36%|      | 2322/6500 [4:23:53<8:28:27,  7.30s/it]                                                        36%|      | 2322/6500 [4:23:53<8:28:27,  7.30s/it] 36%|      | 2323/6500 [4:23:59<8:13:12,  7.08s/it]                                                        36%|      | 2323/6500 [4:23:59<8:13:12,  7.08s/it] 36%|      | 2324/6500 [4:24:06<8:02:06,  6.93s/it]                                                        36%|      | 2324/6500 [4:24:06<8:02:06,  6.93s/it] 36%|      | 2325/6500 [4:24:12<7:54:11,  6.81s/it]                                                        36%|      | 2325/6500 [4:24:12<7:54:11,  6.81s/it] 36%|      | 2326/6500 [4:24:19<7:48:57,  6.74s/it]                                  {'loss': 0.4918, 'learning_rate': 7.162259806158215e-05, 'epoch': 0.36}
{'loss': 0.473, 'learning_rate': 7.16007992985917e-05, 'epoch': 0.36}
{'loss': 0.4871, 'learning_rate': 7.157899548654576e-05, 'epoch': 0.36}
{'loss': 0.4664, 'learning_rate': 7.155718663054083e-05, 'epoch': 0.36}
{'loss': 0.516, 'learning_rate': 7.153537273567459e-05, 'epoch': 0.36}
                      36%|      | 2326/6500 [4:24:19<7:48:57,  6.74s/it] 36%|      | 2327/6500 [4:24:26<7:48:34,  6.74s/it]                                                        36%|      | 2327/6500 [4:24:26<7:48:34,  6.74s/it] 36%|      | 2328/6500 [4:24:33<8:04:03,  6.96s/it]                                                        36%|      | 2328/6500 [4:24:33<8:04:03,  6.96s/it] 36%|      | 2329/6500 [4:24:40<7:55:40,  6.84s/it]                                                        36%|      | 2329/6500 [4:24:40<7:55:40,  6.84s/it] 36%|      | 2330/6500 [4:24:46<7:50:00,  6.76s/it]                                                        36%|      | 2330/6500 [4:24:46<7:50:00,  6.76s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8435717225074768, 'eval_runtime': 1.5066, 'eval_samples_per_second': 7.965, 'eval_steps_per_second': 1.991, 'epoch': 0.36}
                                                        36%|      | 2330/6500 [4:24:48<7:50:00,  6.76s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-2330
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2330
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2330
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2330
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2330
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2330/pytorch_model.bin
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2330/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2330/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2330/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.4736, 'learning_rate': 7.15135538070459e-05, 'epoch': 0.36}
{'loss': 0.755, 'learning_rate': 7.149172984975482e-05, 'epoch': 0.36}
{'loss': 0.4964, 'learning_rate': 7.146990086890258e-05, 'epoch': 0.36}
{'loss': 0.4669, 'learning_rate': 7.144806686959151e-05, 'epoch': 0.36}
{'loss': 0.4951, 'learning_rate': 7.142622785692524e-05, 'epoch': 0.36}
 36%|      | 2331/6500 [4:24:55<8:25:38,  7.28s/it]                                                        36%|      | 2331/6500 [4:24:55<8:25:38,  7.28s/it] 36%|      | 2332/6500 [4:25:01<8:11:00,  7.07s/it]                                                        36%|      | 2332/6500 [4:25:01<8:11:00,  7.07s/it] 36%|      | 2333/6500 [4:25:08<8:00:47,  6.92s/it]                                                        36%|      | 2333/6500 [4:25:08<8:00:47,  6.92s/it] 36%|      | 2334/6500 [4:25:14<7:53:36,  6.82s/it]                                                        36%|      | 2334/6500 [4:25:14<7:53:36,  6.82s/it] 36%|      | 2335/6500 [4:25:21<7:48:20,  6.75s/it]                                                        36%|      | 2335/6500 [4:25:21<7:48:20,  6.75s/it] 36%|      | 2336/6500 [4:25:28<7:44:39,  6.70s/it]                                  {'loss': 0.4637, 'learning_rate': 7.140438383600848e-05, 'epoch': 0.36}
{'loss': 0.4746, 'learning_rate': 7.138253481194714e-05, 'epoch': 0.36}
{'loss': 0.4806, 'learning_rate': 7.136068078984829e-05, 'epoch': 0.36}
{'loss': 0.4818, 'learning_rate': 7.133882177482019e-05, 'epoch': 0.36}
{'loss': 0.4796, 'learning_rate': 7.131695777197224e-05, 'epoch': 0.36}
                      36%|      | 2336/6500 [4:25:28<7:44:39,  6.70s/it] 36%|      | 2337/6500 [4:25:34<7:41:59,  6.66s/it]                                                        36%|      | 2337/6500 [4:25:34<7:41:59,  6.66s/it] 36%|      | 2338/6500 [4:25:41<7:40:16,  6.64s/it]                                                        36%|      | 2338/6500 [4:25:41<7:40:16,  6.64s/it] 36%|      | 2339/6500 [4:25:47<7:38:50,  6.62s/it]                                                        36%|      | 2339/6500 [4:25:47<7:38:50,  6.62s/it] 36%|      | 2340/6500 [4:25:54<7:38:01,  6.61s/it]                                                        36%|      | 2340/6500 [4:25:54<7:38:01,  6.61s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8406776189804077, 'eval_runtime': 1.4798, 'eval_samples_per_second': 8.109, 'eval_steps_per_second': 2.027, 'epoch': 0.36}
                                                        36%|      | 2340/6500 [4:25:55<7:38:01,  6.61s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-2340
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in I AM HERE AND I AM SAVING THE MODEL
 the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2340
/projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2340
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2340
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2340
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2340/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2340/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2340/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2340/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.4845, 'learning_rate': 7.129508878641502e-05, 'epoch': 0.36}
{'loss': 0.4788, 'learning_rate': 7.127321482326026e-05, 'epoch': 0.36}
{'loss': 0.4808, 'learning_rate': 7.125133588762088e-05, 'epoch': 0.36}
{'loss': 0.4751, 'learning_rate': 7.122945198461097e-05, 'epoch': 0.36}
{'loss': 0.5184, 'learning_rate': 7.120756311934571e-05, 'epoch': 0.36}
 36%|      | 2341/6500 [4:26:02<8:17:07,  7.17s/it]                                                        36%|      | 2341/6500 [4:26:02<8:17:07,  7.17s/it] 36%|      | 2342/6500 [4:26:09<8:04:44,  6.99s/it]                                                        36%|      | 2342/6500 [4:26:09<8:04:44,  6.99s/it] 36%|      | 2343/6500 [4:26:16<7:55:55,  6.87s/it]                                                        36%|      | 2343/6500 [4:26:16<7:55:55,  6.87s/it] 36%|      | 2344/6500 [4:26:23<8:10:21,  7.08s/it]                                                        36%|      | 2344/6500 [4:26:23<8:10:21,  7.08s/it] 36%|      | 2345/6500 [4:26:30<7:59:49,  6.93s/it]                                                        36%|      | 2345/6500 [4:26:30<7:59:49,  6.93s/it] 36%|      | 2346/6500 [4:26:36<7:51:57,  6.82s/it]                                  {'loss': 0.4772, 'learning_rate': 7.118566929694152e-05, 'epoch': 0.36}
{'loss': 0.7553, 'learning_rate': 7.116377052251595e-05, 'epoch': 0.36}
{'loss': 0.4949, 'learning_rate': 7.11418668011877e-05, 'epoch': 0.36}
{'loss': 0.4689, 'learning_rate': 7.111995813807662e-05, 'epoch': 0.36}
{'loss': 0.4766, 'learning_rate': 7.109804453830375e-05, 'epoch': 0.36}
                      36%|      | 2346/6500 [4:26:36<7:51:57,  6.82s/it] 36%|      | 2347/6500 [4:26:43<7:46:32,  6.74s/it]                                                        36%|      | 2347/6500 [4:26:43<7:46:32,  6.74s/it] 36%|      | 2348/6500 [4:26:49<7:42:38,  6.69s/it]                                                        36%|      | 2348/6500 [4:26:49<7:42:38,  6.69s/it] 36%|      | 2349/6500 [4:26:56<7:39:53,  6.65s/it]                                                        36%|      | 2349/6500 [4:26:56<7:39:53,  6.65s/it] 36%|      | 2350/6500 [4:27:02<7:37:55,  6.62s/it]                                                        36%|      | 2350/6500 [4:27:02<7:37:55,  6.62s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8442344069480896, 'eval_runtime': 1.4793, 'eval_samples_per_second': 8.112, 'eval_steps_per_second': 2.028, 'epoch': 0.36}
                                                        36%|      | 2350/6500 [4:27:04<7:37:55,  6.62s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-2350
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  I AM HERE AND I AM SAVING THE MODEL
/projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2350the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2350

I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2350
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2350
the pytorch model path isthe pytorch model path is  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2350/pytorch_model.bin/projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2350/pytorch_model.bin

the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2350/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2350/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.4681, 'learning_rate': 7.107612600699124e-05, 'epoch': 0.36}
{'loss': 0.477, 'learning_rate': 7.105420254926241e-05, 'epoch': 0.36}
{'loss': 0.4846, 'learning_rate': 7.103227417024176e-05, 'epoch': 0.36}
{'loss': 0.5007, 'learning_rate': 7.10103408750549e-05, 'epoch': 0.36}
{'loss': 0.4865, 'learning_rate': 7.09884026688286e-05, 'epoch': 0.36}
 36%|      | 2351/6500 [4:27:11<8:13:49,  7.14s/it]                                                        36%|      | 2351/6500 [4:27:11<8:13:49,  7.14s/it] 36%|      | 2352/6500 [4:27:17<8:01:28,  6.96s/it]                                                        36%|      | 2352/6500 [4:27:17<8:01:28,  6.96s/it] 36%|      | 2353/6500 [4:27:24<7:52:47,  6.84s/it]                                                        36%|      | 2353/6500 [4:27:24<7:52:47,  6.84s/it] 36%|      | 2354/6500 [4:27:30<7:46:55,  6.76s/it]                                                        36%|      | 2354/6500 [4:27:30<7:46:55,  6.76s/it] 36%|      | 2355/6500 [4:27:37<7:42:50,  6.70s/it]                                                        36%|      | 2355/6500 [4:27:37<7:42:50,  6.70s/it] 36%|      | 2356/6500 [4:27:44<7:39:48,  6.66s/it]                                  {'loss': 0.4822, 'learning_rate': 7.09664595566908e-05, 'epoch': 0.36}
{'loss': 0.4822, 'learning_rate': 7.094451154377054e-05, 'epoch': 0.36}
{'loss': 0.4752, 'learning_rate': 7.092255863519806e-05, 'epoch': 0.36}
{'loss': 0.4775, 'learning_rate': 7.090060083610471e-05, 'epoch': 0.36}
{'loss': 0.4997, 'learning_rate': 7.087863815162298e-05, 'epoch': 0.36}
                      36%|      | 2356/6500 [4:27:44<7:39:48,  6.66s/it] 36%|      | 2357/6500 [4:27:50<7:37:52,  6.63s/it]                                                        36%|      | 2357/6500 [4:27:50<7:37:52,  6.63s/it] 36%|      | 2358/6500 [4:27:57<7:36:01,  6.61s/it]                                                        36%|      | 2358/6500 [4:27:57<7:36:01,  6.61s/it] 36%|      | 2359/6500 [4:28:03<7:34:27,  6.58s/it]                                                        36%|      | 2359/6500 [4:28:03<7:34:27,  6.58s/it] 36%|      | 2360/6500 [4:28:11<7:52:19,  6.85s/it]                                                        36%|      | 2360/6500 [4:28:11<7:52:19,  6.85s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8437113761901855, 'eval_runtime': 1.4783, 'eval_samples_per_second': 8.118, 'eval_steps_per_second': 2.029, 'epoch': 0.36}
                                                        36%|      | 2360/6500 [4:28:12<7:52:19,  6.85s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-2360
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in I AM HERE AND I AM SAVING THE MODEL
 the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2360
/projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2360
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2360
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2360
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2360/pytorch_model.bin
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2360/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2360/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2360/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.4816, 'learning_rate': 7.085667058688656e-05, 'epoch': 0.36}
{'loss': 0.7612, 'learning_rate': 7.083469814703017e-05, 'epoch': 0.36}
{'loss': 0.4814, 'learning_rate': 7.081272083718977e-05, 'epoch': 0.36}
{'loss': 0.4755, 'learning_rate': 7.079073866250241e-05, 'epoch': 0.36}
{'loss': 0.4646, 'learning_rate': 7.07687516281063e-05, 'epoch': 0.36}
 36%|      | 2361/6500 [4:28:19<8:23:23,  7.30s/it]                                                        36%|      | 2361/6500 [4:28:19<8:23:23,  7.30s/it] 36%|      | 2362/6500 [4:28:26<8:07:57,  7.08s/it]                                                        36%|      | 2362/6500 [4:28:26<8:07:57,  7.08s/it] 36%|      | 2363/6500 [4:28:32<7:56:52,  6.92s/it]                                                        36%|      | 2363/6500 [4:28:32<7:56:52,  6.92s/it] 36%|      | 2364/6500 [4:28:39<7:49:17,  6.81s/it]                                                        36%|      | 2364/6500 [4:28:39<7:49:17,  6.81s/it] 36%|      | 2365/6500 [4:28:45<7:43:38,  6.73s/it]                                                        36%|      | 2365/6500 [4:28:45<7:43:38,  6.73s/it] 36%|      | 2366/6500 [4:28:52<7:39:44,  6.67s/it]                                  {'loss': 0.4618, 'learning_rate': 7.07467597391408e-05, 'epoch': 0.36}
{'loss': 0.4676, 'learning_rate': 7.072476300074633e-05, 'epoch': 0.36}
{'loss': 0.4756, 'learning_rate': 7.070276141806452e-05, 'epoch': 0.36}
{'loss': 0.4953, 'learning_rate': 7.06807549962381e-05, 'epoch': 0.36}
{'loss': 0.4884, 'learning_rate': 7.065874374041095e-05, 'epoch': 0.36}
                      36%|      | 2366/6500 [4:28:52<7:39:44,  6.67s/it] 36%|      | 2367/6500 [4:28:58<7:37:14,  6.64s/it]                                                        36%|      | 2367/6500 [4:28:58<7:37:14,  6.64s/it] 36%|      | 2368/6500 [4:29:05<7:38:00,  6.65s/it]                                                        36%|      | 2368/6500 [4:29:05<7:38:00,  6.65s/it] 36%|      | 2369/6500 [4:29:12<7:36:14,  6.63s/it]                                                        36%|      | 2369/6500 [4:29:12<7:36:14,  6.63s/it] 36%|      | 2370/6500 [4:29:18<7:34:50,  6.61s/it]                                                        36%|      | 2370/6500 [4:29:18<7:34:50,  6.61s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8398532867431641, 'eval_runtime': 2.1255, 'eval_samples_per_second': 5.646, 'eval_steps_per_second': 1.411, 'epoch': 0.36}
                                                        36%|      | 2370/6500 [4:29:20<7:34:50,  6.61s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-2370
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2370
/projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2370
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2370
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2370
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2370/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2370/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2370/pytorch_model.binthe pytorch model path is
 /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2370/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.4714, 'learning_rate': 7.063672765572806e-05, 'epoch': 0.36}
{'loss': 0.4855, 'learning_rate': 7.061470674733554e-05, 'epoch': 0.36}
{'loss': 0.4654, 'learning_rate': 7.059268102038066e-05, 'epoch': 0.37}
{'loss': 0.4764, 'learning_rate': 7.057065048001181e-05, 'epoch': 0.37}
{'loss': 0.4933, 'learning_rate': 7.054861513137847e-05, 'epoch': 0.37}
 36%|      | 2371/6500 [4:29:27<8:25:50,  7.35s/it]                                                        36%|      | 2371/6500 [4:29:27<8:25:50,  7.35s/it] 36%|      | 2372/6500 [4:29:34<8:09:34,  7.12s/it]                                                        36%|      | 2372/6500 [4:29:34<8:09:34,  7.12s/it] 37%|      | 2373/6500 [4:29:40<7:58:00,  6.95s/it]                                                        37%|      | 2373/6500 [4:29:40<7:58:00,  6.95s/it] 37%|      | 2374/6500 [4:29:47<7:49:50,  6.83s/it]                                                        37%|      | 2374/6500 [4:29:47<7:49:50,  6.83s/it] 37%|      | 2375/6500 [4:29:54<7:44:19,  6.75s/it]                                                        37%|      | 2375/6500 [4:29:54<7:44:19,  6.75s/it] 37%|      | 2376/6500 [4:30:01<8:00:38,  6.99s/it]                                  {'loss': 0.4798, 'learning_rate': 7.052657497963129e-05, 'epoch': 0.37}
{'loss': 0.7611, 'learning_rate': 7.050453002992201e-05, 'epoch': 0.37}
{'loss': 0.4699, 'learning_rate': 7.04824802874035e-05, 'epoch': 0.37}
{'loss': 0.4869, 'learning_rate': 7.046042575722976e-05, 'epoch': 0.37}
{'loss': 0.457, 'learning_rate': 7.04383664445559e-05, 'epoch': 0.37}
                      37%|      | 2376/6500 [4:30:01<8:00:38,  6.99s/it] 37%|      | 2377/6500 [4:30:08<7:51:58,  6.87s/it]                                                        37%|      | 2377/6500 [4:30:08<7:51:58,  6.87s/it] 37%|      | 2378/6500 [4:30:14<7:45:32,  6.78s/it]                                                        37%|      | 2378/6500 [4:30:14<7:45:32,  6.78s/it] 37%|      | 2379/6500 [4:30:21<7:41:17,  6.72s/it]                                                        37%|      | 2379/6500 [4:30:21<7:41:17,  6.72s/it] 37%|      | 2380/6500 [4:30:27<7:38:16,  6.67s/it]                                                        37%|      | 2380/6500 [4:30:27<7:38:16,  6.67s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.844632625579834, 'eval_runtime': 1.4769, 'eval_samples_per_second': 8.125, 'eval_steps_per_second': 2.031, 'epoch': 0.37}
                                                        37%|      | 2380/6500 [4:30:29<7:38:16,  6.67s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-2380
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2380
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2380
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2380
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2380
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2380/pytorch_model.binthe pytorch model path is
 /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2380/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2380/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2380/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.459, 'learning_rate': 7.041630235453816e-05, 'epoch': 0.37}
{'loss': 0.467, 'learning_rate': 7.039423349233387e-05, 'epoch': 0.37}
{'loss': 0.4667, 'learning_rate': 7.03721598631015e-05, 'epoch': 0.37}
{'loss': 0.4976, 'learning_rate': 7.035008147200062e-05, 'epoch': 0.37}
{'loss': 0.4822, 'learning_rate': 7.032799832419193e-05, 'epoch': 0.37}
 37%|      | 2381/6500 [4:30:36<8:14:17,  7.20s/it]                                                        37%|      | 2381/6500 [4:30:36<8:14:17,  7.20s/it] 37%|      | 2382/6500 [4:30:42<8:00:37,  7.00s/it]                                                        37%|      | 2382/6500 [4:30:42<8:00:37,  7.00s/it] 37%|      | 2383/6500 [4:30:49<7:51:56,  6.88s/it]                                                        37%|      | 2383/6500 [4:30:49<7:51:56,  6.88s/it] 37%|      | 2384/6500 [4:30:56<7:45:35,  6.79s/it]                                                        37%|      | 2384/6500 [4:30:56<7:45:35,  6.79s/it] 37%|      | 2385/6500 [4:31:02<7:40:31,  6.71s/it]                                                        37%|      | 2385/6500 [4:31:02<7:40:31,  6.71s/it] 37%|      | 2386/6500 [4:31:09<7:37:21,  6.67s/it]                                  {'loss': 0.4725, 'learning_rate': 7.030591042483723e-05, 'epoch': 0.37}
{'loss': 0.4798, 'learning_rate': 7.028381777909943e-05, 'epoch': 0.37}
{'loss': 0.4643, 'learning_rate': 7.026172039214256e-05, 'epoch': 0.37}
{'loss': 0.5023, 'learning_rate': 7.023961826913174e-05, 'epoch': 0.37}
{'loss': 0.4602, 'learning_rate': 7.02175114152332e-05, 'epoch': 0.37}
                      37%|      | 2386/6500 [4:31:09<7:37:21,  6.67s/it] 37%|      | 2387/6500 [4:31:15<7:34:25,  6.63s/it]                                                        37%|      | 2387/6500 [4:31:15<7:34:25,  6.63s/it] 37%|      | 2388/6500 [4:31:22<7:32:40,  6.61s/it]                                                        37%|      | 2388/6500 [4:31:22<7:32:40,  6.61s/it] 37%|      | 2389/6500 [4:31:28<7:31:25,  6.59s/it]                                                        37%|      | 2389/6500 [4:31:28<7:31:25,  6.59s/it] 37%|      | 2390/6500 [4:31:35<7:30:48,  6.58s/it]                                                        37%|      | 2390/6500 [4:31:35<7:30:48,  6.58s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8448836207389832, 'eval_runtime': 1.6339, 'eval_samples_per_second': 7.344, 'eval_steps_per_second': 1.836, 'epoch': 0.37}
                                                        37%|      | 2390/6500 [4:31:36<7:30:48,  6.58s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-2390
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  I AM HERE AND I AM SAVING THE MODEL
/projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2390the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2390

I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2390
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2390
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2390/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2390/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2390/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2390/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.733, 'learning_rate': 7.01953998356143e-05, 'epoch': 0.37}
{'loss': 0.5089, 'learning_rate': 7.017328353544346e-05, 'epoch': 0.37}
{'loss': 0.4599, 'learning_rate': 7.015116251989027e-05, 'epoch': 0.37}
{'loss': 0.4977, 'learning_rate': 7.012903679412536e-05, 'epoch': 0.37}
{'loss': 0.456, 'learning_rate': 7.010690636332047e-05, 'epoch': 0.37}
 37%|      | 2391/6500 [4:31:43<8:11:20,  7.17s/it]                                                        37%|      | 2391/6500 [4:31:43<8:11:20,  7.17s/it] 37%|      | 2392/6500 [4:31:51<8:17:04,  7.26s/it]                                                        37%|      | 2392/6500 [4:31:51<8:17:04,  7.26s/it] 37%|      | 2393/6500 [4:31:57<8:02:37,  7.05s/it]                                                        37%|      | 2393/6500 [4:31:57<8:02:37,  7.05s/it] 37%|      | 2394/6500 [4:32:04<7:52:23,  6.90s/it]                                                        37%|      | 2394/6500 [4:32:04<7:52:23,  6.90s/it] 37%|      | 2395/6500 [4:32:11<7:45:06,  6.80s/it]                                                        37%|      | 2395/6500 [4:32:11<7:45:06,  6.80s/it] 37%|      | 2396/6500 [4:32:17<7:39:54,  6.72s/it]                                  {'loss': 0.4706, 'learning_rate': 7.008477123264848e-05, 'epoch': 0.37}
{'loss': 0.4758, 'learning_rate': 7.006263140728333e-05, 'epoch': 0.37}
{'loss': 0.4753, 'learning_rate': 7.004048689240007e-05, 'epoch': 0.37}
{'loss': 0.496, 'learning_rate': 7.001833769317486e-05, 'epoch': 0.37}
{'loss': 0.4813, 'learning_rate': 6.999618381478492e-05, 'epoch': 0.37}
                      37%|      | 2396/6500 [4:32:17<7:39:54,  6.72s/it] 37%|      | 2397/6500 [4:32:24<7:36:35,  6.68s/it]                                                        37%|      | 2397/6500 [4:32:24<7:36:35,  6.68s/it] 37%|      | 2398/6500 [4:32:30<7:34:01,  6.64s/it]                                                        37%|      | 2398/6500 [4:32:30<7:34:01,  6.64s/it] 37%|      | 2399/6500 [4:32:37<7:32:10,  6.62s/it]                                                        37%|      | 2399/6500 [4:32:37<7:32:10,  6.62s/it] 37%|      | 2400/6500 [4:32:43<7:30:51,  6.60s/it]                                                        37%|      | 2400/6500 [4:32:43<7:30:51,  6.60s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8420872688293457, 'eval_runtime': 1.4909, 'eval_samples_per_second': 8.049, 'eval_steps_per_second': 2.012, 'epoch': 0.37}
                                                        37%|      | 2400/6500 [4:32:45<7:30:51,  6.60s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-2400
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2400
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2400
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2400
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2400
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2400/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2400/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2400/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2400/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.4663, 'learning_rate': 6.997402526240857e-05, 'epoch': 0.37}
{'loss': 0.4794, 'learning_rate': 6.995186204122528e-05, 'epoch': 0.37}
{'loss': 0.4575, 'learning_rate': 6.992969415641555e-05, 'epoch': 0.37}
{'loss': 0.5091, 'learning_rate': 6.990752161316098e-05, 'epoch': 0.37}
{'loss': 0.4629, 'learning_rate': 6.988534441664427e-05, 'epoch': 0.37}
 37%|      | 2401/6500 [4:32:52<8:08:59,  7.16s/it]                                                        37%|      | 2401/6500 [4:32:52<8:08:59,  7.16s/it] 37%|      | 2402/6500 [4:32:58<7:56:11,  6.97s/it]                                                        37%|      | 2402/6500 [4:32:58<7:56:11,  6.97s/it] 37%|      | 2403/6500 [4:33:05<7:47:19,  6.84s/it]                                                        37%|      | 2403/6500 [4:33:05<7:47:19,  6.84s/it] 37%|      | 2404/6500 [4:33:11<7:41:03,  6.75s/it]                                                        37%|      | 2404/6500 [4:33:11<7:41:03,  6.75s/it] 37%|      | 2405/6500 [4:33:18<7:36:33,  6.69s/it]                                                        37%|      | 2405/6500 [4:33:18<7:36:33,  6.69s/it] 37%|      | 2406/6500 [4:33:24<7:33:24,  6.65s/it]                                  {'loss': 0.75, 'learning_rate': 6.986316257204921e-05, 'epoch': 0.37}
{'loss': 0.4862, 'learning_rate': 6.984097608456067e-05, 'epoch': 0.37}
{'loss': 0.4604, 'learning_rate': 6.98187849593646e-05, 'epoch': 0.37}
{'loss': 0.4863, 'learning_rate': 6.979658920164806e-05, 'epoch': 0.37}
{'loss': 0.4537, 'learning_rate': 6.977438881659916e-05, 'epoch': 0.37}
                      37%|      | 2406/6500 [4:33:24<7:33:24,  6.65s/it] 37%|      | 2407/6500 [4:33:31<7:31:34,  6.62s/it]                                                        37%|      | 2407/6500 [4:33:31<7:31:34,  6.62s/it] 37%|      | 2408/6500 [4:33:38<7:42:43,  6.78s/it]                                                        37%|      | 2408/6500 [4:33:38<7:42:43,  6.78s/it] 37%|      | 2409/6500 [4:33:45<7:37:37,  6.71s/it]                                                        37%|      | 2409/6500 [4:33:45<7:37:37,  6.71s/it] 37%|      | 2410/6500 [4:33:51<7:34:22,  6.67s/it]                                                        37%|      | 2410/6500 [4:33:51<7:34:22,  6.67s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8476728200912476, 'eval_runtime': 1.7486, 'eval_samples_per_second': 6.863, 'eval_steps_per_second': 1.716, 'epoch': 0.37}
                                                        37%|      | 2410/6500 [4:33:53<7:34:22,  6.67s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-2410
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  I AM HERE AND I AM SAVING THE MODEL
/projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2410the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2410

I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2410
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2410
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2410/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2410/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2410/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2410/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.4615, 'learning_rate': 6.975218380940709e-05, 'epoch': 0.37}
{'loss': 0.4729, 'learning_rate': 6.972997418526215e-05, 'epoch': 0.37}
{'loss': 0.4783, 'learning_rate': 6.970775994935571e-05, 'epoch': 0.37}
{'loss': 0.4734, 'learning_rate': 6.96855411068802e-05, 'epoch': 0.37}
{'loss': 0.4836, 'learning_rate': 6.966331766302916e-05, 'epoch': 0.37}
 37%|      | 2411/6500 [4:34:00<8:15:42,  7.27s/it]                                                        37%|      | 2411/6500 [4:34:00<8:15:42,  7.27s/it] 37%|      | 2412/6500 [4:34:07<8:00:58,  7.06s/it]                                                        37%|      | 2412/6500 [4:34:07<8:00:58,  7.06s/it] 37%|      | 2413/6500 [4:34:13<7:50:30,  6.91s/it]                                                        37%|      | 2413/6500 [4:34:13<7:50:30,  6.91s/it] 37%|      | 2414/6500 [4:34:20<7:43:19,  6.80s/it]                                                        37%|      | 2414/6500 [4:34:20<7:43:19,  6.80s/it] 37%|      | 2415/6500 [4:34:26<7:38:04,  6.73s/it]                                                        37%|      | 2415/6500 [4:34:26<7:38:04,  6.73s/it] 37%|      | 2416/6500 [4:34:33<7:34:22,  6.68s/it]                                  {'loss': 0.4713, 'learning_rate': 6.964108962299717e-05, 'epoch': 0.37}
{'loss': 0.4776, 'learning_rate': 6.961885699197989e-05, 'epoch': 0.37}
{'loss': 0.4571, 'learning_rate': 6.959661977517408e-05, 'epoch': 0.37}
{'loss': 0.5101, 'learning_rate': 6.957437797777754e-05, 'epoch': 0.37}
{'loss': 0.4755, 'learning_rate': 6.955213160498917e-05, 'epoch': 0.37}
                      37%|      | 2416/6500 [4:34:33<7:34:22,  6.68s/it] 37%|      | 2417/6500 [4:34:39<7:31:50,  6.64s/it]                                                        37%|      | 2417/6500 [4:34:39<7:31:50,  6.64s/it] 37%|      | 2418/6500 [4:34:46<7:29:54,  6.61s/it]                                                        37%|      | 2418/6500 [4:34:46<7:29:54,  6.61s/it] 37%|      | 2419/6500 [4:34:52<7:28:32,  6.59s/it]                                                        37%|      | 2419/6500 [4:34:52<7:28:32,  6.59s/it] 37%|      | 2420/6500 [4:34:59<7:27:45,  6.58s/it]                                                        37%|      | 2420/6500 [4:34:59<7:27:45,  6.58s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8463501930236816, 'eval_runtime': 1.4873, 'eval_samples_per_second': 8.069, 'eval_steps_per_second': 2.017, 'epoch': 0.37}
                                                        37%|      | 2420/6500 [4:35:00<7:27:45,  6.58s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-2420
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2420
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2420
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2420
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2420
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2420/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2420/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2420/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2420/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.7425, 'learning_rate': 6.952988066200891e-05, 'epoch': 0.37}
{'loss': 0.4884, 'learning_rate': 6.950762515403779e-05, 'epoch': 0.37}
{'loss': 0.4684, 'learning_rate': 6.94853650862779e-05, 'epoch': 0.37}
{'loss': 0.4646, 'learning_rate': 6.946310046393239e-05, 'epoch': 0.37}
{'loss': 0.4582, 'learning_rate': 6.944083129220548e-05, 'epoch': 0.37}
 37%|      | 2421/6500 [4:35:07<8:04:24,  7.13s/it]                                                        37%|      | 2421/6500 [4:35:07<8:04:24,  7.13s/it] 37%|      | 2422/6500 [4:35:14<7:52:46,  6.96s/it]                                                        37%|      | 2422/6500 [4:35:14<7:52:46,  6.96s/it] 37%|      | 2423/6500 [4:35:20<7:44:07,  6.83s/it]                                                        37%|      | 2423/6500 [4:35:20<7:44:07,  6.83s/it] 37%|      | 2424/6500 [4:35:27<7:38:19,  6.75s/it]                                                        37%|      | 2424/6500 [4:35:27<7:38:19,  6.75s/it] 37%|      | 2425/6500 [4:35:34<7:46:55,  6.87s/it]                                                        37%|      | 2425/6500 [4:35:34<7:46:55,  6.87s/it] 37%|      | 2426/6500 [4:35:41<7:40:22,  6.78s/it]                                  {'loss': 0.4723, 'learning_rate': 6.941855757630248e-05, 'epoch': 0.37}
{'loss': 0.4644, 'learning_rate': 6.939627932142969e-05, 'epoch': 0.37}
{'loss': 0.4903, 'learning_rate': 6.937399653279454e-05, 'epoch': 0.37}
{'loss': 0.481, 'learning_rate': 6.935170921560552e-05, 'epoch': 0.37}
{'loss': 0.4756, 'learning_rate': 6.932941737507211e-05, 'epoch': 0.37}
                      37%|      | 2426/6500 [4:35:41<7:40:22,  6.78s/it] 37%|      | 2427/6500 [4:35:47<7:35:32,  6.71s/it]                                                        37%|      | 2427/6500 [4:35:47<7:35:32,  6.71s/it] 37%|      | 2428/6500 [4:35:54<7:32:36,  6.67s/it]                                                        37%|      | 2428/6500 [4:35:54<7:32:36,  6.67s/it] 37%|      | 2429/6500 [4:36:00<7:30:09,  6.63s/it]                                                        37%|      | 2429/6500 [4:36:00<7:30:09,  6.63s/it] 37%|      | 2430/6500 [4:36:07<7:28:55,  6.62s/it]                                                        37%|      | 2430/6500 [4:36:07<7:28:55,  6.62s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8381478786468506, 'eval_runtime': 1.5029, 'eval_samples_per_second': 7.985, 'eval_steps_per_second': 1.996, 'epoch': 0.37}
                                                        37%|      | 2430/6500 [4:36:09<7:28:55,  6.62s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-2430
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2430
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2430
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2430
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2430
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2430/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2430/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2430/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2430/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.4816, 'learning_rate': 6.930712101640492e-05, 'epoch': 0.37}
{'loss': 0.4674, 'learning_rate': 6.928482014481558e-05, 'epoch': 0.37}
{'loss': 0.4601, 'learning_rate': 6.92625147655168e-05, 'epoch': 0.37}
{'loss': 0.5109, 'learning_rate': 6.924020488372229e-05, 'epoch': 0.37}
{'loss': 0.468, 'learning_rate': 6.921789050464688e-05, 'epoch': 0.37}
 37%|      | 2431/6500 [4:36:15<8:05:11,  7.15s/it]                                                        37%|      | 2431/6500 [4:36:15<8:05:11,  7.15s/it] 37%|      | 2432/6500 [4:36:22<7:52:56,  6.98s/it]                                                        37%|      | 2432/6500 [4:36:22<7:52:56,  6.98s/it] 37%|      | 2433/6500 [4:36:29<7:44:13,  6.85s/it]                                                        37%|      | 2433/6500 [4:36:29<7:44:13,  6.85s/it] 37%|      | 2434/6500 [4:36:35<7:38:09,  6.76s/it]                                                        37%|      | 2434/6500 [4:36:35<7:38:09,  6.76s/it] 37%|      | 2435/6500 [4:36:42<7:33:55,  6.70s/it]                                                        37%|      | 2435/6500 [4:36:42<7:33:55,  6.70s/it] 37%|      | 2436/6500 [4:36:48<7:31:01,  6.66s/it]                                  {'loss': 0.7529, 'learning_rate': 6.91955716335064e-05, 'epoch': 0.37}
{'loss': 0.4827, 'learning_rate': 6.917324827551778e-05, 'epoch': 0.37}
{'loss': 0.4753, 'learning_rate': 6.915092043589895e-05, 'epoch': 0.38}
{'loss': 0.4636, 'learning_rate': 6.912858811986888e-05, 'epoch': 0.38}
{'loss': 0.4592, 'learning_rate': 6.910625133264766e-05, 'epoch': 0.38}
                      37%|      | 2436/6500 [4:36:48<7:31:01,  6.66s/it] 37%|      | 2437/6500 [4:36:55<7:28:49,  6.63s/it]                                                        37%|      | 2437/6500 [4:36:55<7:28:49,  6.63s/it] 38%|      | 2438/6500 [4:37:01<7:27:12,  6.61s/it]                                                        38%|      | 2438/6500 [4:37:01<7:27:12,  6.61s/it] 38%|      | 2439/6500 [4:37:08<7:26:02,  6.59s/it]                                                        38%|      | 2439/6500 [4:37:08<7:26:02,  6.59s/it] 38%|      | 2440/6500 [4:37:14<7:25:24,  6.58s/it]                                                        38%|      | 2440/6500 [4:37:14<7:25:24,  6.58s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8458508849143982, 'eval_runtime': 1.7313, 'eval_samples_per_second': 6.931, 'eval_steps_per_second': 1.733, 'epoch': 0.38}
                                                        38%|      | 2440/6500 [4:37:16<7:25:24,  6.58s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-2440
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2440
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2440
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2440
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2440
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2440/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2440/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2440/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2440/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.4594, 'learning_rate': 6.908391007945636e-05, 'epoch': 0.38}
{'loss': 0.472, 'learning_rate': 6.906156436551712e-05, 'epoch': 0.38}
{'loss': 0.4926, 'learning_rate': 6.903921419605308e-05, 'epoch': 0.38}
{'loss': 0.4828, 'learning_rate': 6.90168595762885e-05, 'epoch': 0.38}
{'loss': 0.476, 'learning_rate': 6.899450051144862e-05, 'epoch': 0.38}
 38%|      | 2441/6500 [4:37:24<8:19:14,  7.38s/it]                                                        38%|      | 2441/6500 [4:37:24<8:19:14,  7.38s/it] 38%|      | 2442/6500 [4:37:30<8:02:19,  7.13s/it]                                                        38%|      | 2442/6500 [4:37:30<8:02:19,  7.13s/it] 38%|      | 2443/6500 [4:37:37<7:50:33,  6.96s/it]                                                        38%|      | 2443/6500 [4:37:37<7:50:33,  6.96s/it] 38%|      | 2444/6500 [4:37:43<7:42:24,  6.84s/it]                                                        38%|      | 2444/6500 [4:37:43<7:42:24,  6.84s/it] 38%|      | 2445/6500 [4:37:50<7:36:48,  6.76s/it]                                                        38%|      | 2445/6500 [4:37:50<7:36:48,  6.76s/it] 38%|      | 2446/6500 [4:37:56<7:32:35,  6.70s/it]                                  {'loss': 0.4746, 'learning_rate': 6.897213700675973e-05, 'epoch': 0.38}
{'loss': 0.4644, 'learning_rate': 6.894976906744916e-05, 'epoch': 0.38}
{'loss': 0.4714, 'learning_rate': 6.89273966987453e-05, 'epoch': 0.38}
{'loss': 0.4971, 'learning_rate': 6.890501990587754e-05, 'epoch': 0.38}
{'loss': 0.4748, 'learning_rate': 6.888263869407631e-05, 'epoch': 0.38}
                      38%|      | 2446/6500 [4:37:56<7:32:35,  6.70s/it] 38%|      | 2447/6500 [4:38:03<7:29:19,  6.65s/it]                                                        38%|      | 2447/6500 [4:38:03<7:29:19,  6.65s/it] 38%|      | 2448/6500 [4:38:10<7:27:04,  6.62s/it]                                                        38%|      | 2448/6500 [4:38:10<7:27:04,  6.62s/it] 38%|      | 2449/6500 [4:38:16<7:25:46,  6.60s/it]                                                        38%|      | 2449/6500 [4:38:16<7:25:46,  6.60s/it] 38%|      | 2450/6500 [4:38:23<7:24:50,  6.59s/it]                                                        38%|      | 2450/6500 [4:38:23<7:24:50,  6.59s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.845443069934845, 'eval_runtime': 1.4902, 'eval_samples_per_second': 8.052, 'eval_steps_per_second': 2.013, 'epoch': 0.38}
                                                        38%|      | 2450/6500 [4:38:24<7:24:50,  6.59s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-2450
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2450I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2450

I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2450
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2450
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2450/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2450/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2450/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2450/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.7558, 'learning_rate': 6.886025306857311e-05, 'epoch': 0.38}
{'loss': 0.4602, 'learning_rate': 6.883786303460041e-05, 'epoch': 0.38}
{'loss': 0.4825, 'learning_rate': 6.881546859739179e-05, 'epoch': 0.38}
{'loss': 0.4623, 'learning_rate': 6.879306976218177e-05, 'epoch': 0.38}
{'loss': 0.45, 'learning_rate': 6.877066653420594e-05, 'epoch': 0.38}
 38%|      | 2451/6500 [4:38:31<8:00:29,  7.12s/it]                                                        38%|      | 2451/6500 [4:38:31<8:00:29,  7.12s/it] 38%|      | 2452/6500 [4:38:38<7:48:49,  6.95s/it]                                                        38%|      | 2452/6500 [4:38:38<7:48:49,  6.95s/it] 38%|      | 2453/6500 [4:38:44<7:40:45,  6.83s/it]                                                        38%|      | 2453/6500 [4:38:44<7:40:45,  6.83s/it] 38%|      | 2454/6500 [4:38:51<7:35:01,  6.75s/it]                                                        38%|      | 2454/6500 [4:38:51<7:35:01,  6.75s/it] 38%|      | 2455/6500 [4:38:57<7:31:08,  6.69s/it]                                                        38%|      | 2455/6500 [4:38:57<7:31:08,  6.69s/it] 38%|      | 2456/6500 [4:39:04<7:28:27,  6.65s/it]                                  {'loss': 0.4603, 'learning_rate': 6.874825891870093e-05, 'epoch': 0.38}
{'loss': 0.4775, 'learning_rate': 6.872584692090442e-05, 'epoch': 0.38}
{'loss': 0.4889, 'learning_rate': 6.870343054605503e-05, 'epoch': 0.38}
{'loss': 0.4684, 'learning_rate': 6.868100979939249e-05, 'epoch': 0.38}
{'loss': 0.4677, 'learning_rate': 6.865858468615747e-05, 'epoch': 0.38}
                      38%|      | 2456/6500 [4:39:04<7:28:27,  6.65s/it] 38%|      | 2457/6500 [4:39:11<7:43:50,  6.88s/it]                                                        38%|      | 2457/6500 [4:39:11<7:43:50,  6.88s/it] 38%|      | 2458/6500 [4:39:18<7:37:32,  6.79s/it]                                                        38%|      | 2458/6500 [4:39:18<7:37:32,  6.79s/it] 38%|      | 2459/6500 [4:39:24<7:32:55,  6.73s/it]                                                        38%|      | 2459/6500 [4:39:24<7:32:55,  6.73s/it] 38%|      | 2460/6500 [4:39:31<7:29:37,  6.68s/it]                                                        38%|      | 2460/6500 [4:39:31<7:29:37,  6.68s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8359386324882507, 'eval_runtime': 1.4946, 'eval_samples_per_second': 8.029, 'eval_steps_per_second': 2.007, 'epoch': 0.38}
                                                        38%|      | 2460/6500 [4:39:32<7:29:37,  6.68s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-2460
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2460
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2460
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2460
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2460
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2460/pytorch_model.bin
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2460/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2460/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2460/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.4733, 'learning_rate': 6.863615521159172e-05, 'epoch': 0.38}
{'loss': 0.4569, 'learning_rate': 6.8613721380938e-05, 'epoch': 0.38}
{'loss': 0.4721, 'learning_rate': 6.85912831994401e-05, 'epoch': 0.38}
{'loss': 0.4785, 'learning_rate': 6.856884067234279e-05, 'epoch': 0.38}
{'loss': 0.4722, 'learning_rate': 6.854639380489184e-05, 'epoch': 0.38}
 38%|      | 2461/6500 [4:39:39<8:04:42,  7.20s/it]                                                        38%|      | 2461/6500 [4:39:39<8:04:42,  7.20s/it] 38%|      | 2462/6500 [4:39:46<7:51:22,  7.00s/it]                                                        38%|      | 2462/6500 [4:39:46<7:51:22,  7.00s/it] 38%|      | 2463/6500 [4:39:52<7:42:02,  6.87s/it]                                                        38%|      | 2463/6500 [4:39:52<7:42:02,  6.87s/it] 38%|      | 2464/6500 [4:39:59<7:35:48,  6.78s/it]                                                        38%|      | 2464/6500 [4:39:59<7:35:48,  6.78s/it] 38%|      | 2465/6500 [4:40:06<7:31:25,  6.71s/it]                                                        38%|      | 2465/6500 [4:40:06<7:31:25,  6.71s/it] 38%|      | 2466/6500 [4:40:12<7:28:22,  6.67s/it]                                  {'loss': 0.7503, 'learning_rate': 6.852394260233414e-05, 'epoch': 0.38}
{'loss': 0.4575, 'learning_rate': 6.850148706991745e-05, 'epoch': 0.38}
{'loss': 0.4873, 'learning_rate': 6.847902721289068e-05, 'epoch': 0.38}
{'loss': 0.4454, 'learning_rate': 6.845656303650365e-05, 'epoch': 0.38}
{'loss': 0.4515, 'learning_rate': 6.843409454600722e-05, 'epoch': 0.38}
                      38%|      | 2466/6500 [4:40:12<7:28:22,  6.67s/it] 38%|      | 2467/6500 [4:40:19<7:26:03,  6.64s/it]                                                        38%|      | 2467/6500 [4:40:19<7:26:03,  6.64s/it] 38%|      | 2468/6500 [4:40:25<7:24:12,  6.61s/it]                                                        38%|      | 2468/6500 [4:40:25<7:24:12,  6.61s/it] 38%|      | 2469/6500 [4:40:32<7:22:59,  6.59s/it]                                                        38%|      | 2469/6500 [4:40:32<7:22:59,  6.59s/it] 38%|      | 2470/6500 [4:40:38<7:22:18,  6.59s/it]                                                        38%|      | 2470/6500 [4:40:38<7:22:18,  6.59s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8479211926460266, 'eval_runtime': 1.484, 'eval_samples_per_second': 8.086, 'eval_steps_per_second': 2.022, 'epoch': 0.38}
                                                        38%|      | 2470/6500 [4:40:40<7:22:18,  6.59s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-2470
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2470I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in 
 /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2470
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2470
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2470
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2470/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2470/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2470/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2470/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.4698, 'learning_rate': 6.841162174665326e-05, 'epoch': 0.38}
{'loss': 0.4518, 'learning_rate': 6.838914464369467e-05, 'epoch': 0.38}
{'loss': 0.4843, 'learning_rate': 6.836666324238532e-05, 'epoch': 0.38}
{'loss': 0.4853, 'learning_rate': 6.834417754798011e-05, 'epoch': 0.38}
{'loss': 0.4636, 'learning_rate': 6.832168756573496e-05, 'epoch': 0.38}
 38%|      | 2471/6500 [4:40:47<7:58:15,  7.12s/it]                                                        38%|      | 2471/6500 [4:40:47<7:58:15,  7.12s/it] 38%|      | 2472/6500 [4:40:53<7:47:06,  6.96s/it]                                                        38%|      | 2472/6500 [4:40:53<7:47:06,  6.96s/it] 38%|      | 2473/6500 [4:41:01<7:59:29,  7.14s/it]                                                        38%|      | 2473/6500 [4:41:01<7:59:29,  7.14s/it] 38%|      | 2474/6500 [4:41:07<7:47:45,  6.97s/it]                                                        38%|      | 2474/6500 [4:41:07<7:47:45,  6.97s/it] 38%|      | 2475/6500 [4:41:14<7:39:27,  6.85s/it]                                                        38%|      | 2475/6500 [4:41:14<7:39:27,  6.85s/it] 38%|      | 2476/6500 [4:41:21<7:33:24,  6.76s/it]                                  {'loss': 0.4738, 'learning_rate': 6.82991933009067e-05, 'epoch': 0.38}
{'loss': 0.4562, 'learning_rate': 6.827669475875328e-05, 'epoch': 0.38}
{'loss': 0.5054, 'learning_rate': 6.825419194453359e-05, 'epoch': 0.38}
{'loss': 0.457, 'learning_rate': 6.823168486350753e-05, 'epoch': 0.38}
{'loss': 0.7482, 'learning_rate': 6.820917352093597e-05, 'epoch': 0.38}
                      38%|      | 2476/6500 [4:41:21<7:33:24,  6.76s/it] 38%|      | 2477/6500 [4:41:27<7:29:09,  6.70s/it]                                                        38%|      | 2477/6500 [4:41:27<7:29:09,  6.70s/it] 38%|      | 2478/6500 [4:41:34<7:26:15,  6.66s/it]                                                        38%|      | 2478/6500 [4:41:34<7:26:15,  6.66s/it] 38%|      | 2479/6500 [4:41:40<7:24:20,  6.63s/it]                                                        38%|      | 2479/6500 [4:41:40<7:24:20,  6.63s/it] 38%|      | 2480/6500 [4:41:47<7:22:42,  6.61s/it]                                                        38%|      | 2480/6500 [4:41:47<7:22:42,  6.61s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8460890054702759, 'eval_runtime': 1.4888, 'eval_samples_per_second': 8.06, 'eval_steps_per_second': 2.015, 'epoch': 0.38}
                                                        38%|      | 2480/6500 [4:41:48<7:22:42,  6.61s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-2480
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2480I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2480

I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2480
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2480
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2480/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2480/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2480/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2480/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.4811, 'learning_rate': 6.818665792208082e-05, 'epoch': 0.38}
{'loss': 0.4618, 'learning_rate': 6.816413807220495e-05, 'epoch': 0.38}
{'loss': 0.4812, 'learning_rate': 6.814161397657224e-05, 'epoch': 0.38}
{'loss': 0.45, 'learning_rate': 6.811908564044756e-05, 'epoch': 0.38}
{'loss': 0.4595, 'learning_rate': 6.809655306909681e-05, 'epoch': 0.38}
 38%|      | 2481/6500 [4:41:55<7:59:11,  7.15s/it]                                                        38%|      | 2481/6500 [4:41:55<7:59:11,  7.15s/it] 38%|      | 2482/6500 [4:42:02<7:47:00,  6.97s/it]                                                        38%|      | 2482/6500 [4:42:02<7:47:00,  6.97s/it] 38%|      | 2483/6500 [4:42:08<7:38:34,  6.85s/it]                                                        38%|      | 2483/6500 [4:42:08<7:38:34,  6.85s/it] 38%|      | 2484/6500 [4:42:15<7:32:52,  6.77s/it]                                                        38%|      | 2484/6500 [4:42:15<7:32:52,  6.77s/it] 38%|      | 2485/6500 [4:42:22<7:28:50,  6.71s/it]                                                        38%|      | 2485/6500 [4:42:22<7:28:50,  6.71s/it] 38%|      | 2486/6500 [4:42:28<7:26:05,  6.67s/it]                                  {'loss': 0.468, 'learning_rate': 6.807401626778679e-05, 'epoch': 0.38}
{'loss': 0.4724, 'learning_rate': 6.805147524178535e-05, 'epoch': 0.38}
{'loss': 0.4755, 'learning_rate': 6.802892999636134e-05, 'epoch': 0.38}
{'loss': 0.4707, 'learning_rate': 6.800638053678455e-05, 'epoch': 0.38}
{'loss': 0.4633, 'learning_rate': 6.79838268683258e-05, 'epoch': 0.38}
                      38%|      | 2486/6500 [4:42:28<7:26:05,  6.67s/it] 38%|      | 2487/6500 [4:42:35<7:23:42,  6.63s/it]                                                        38%|      | 2487/6500 [4:42:35<7:23:42,  6.63s/it] 38%|      | 2488/6500 [4:42:41<7:22:04,  6.61s/it]                                                        38%|      | 2488/6500 [4:42:41<7:22:04,  6.61s/it] 38%|      | 2489/6500 [4:42:49<7:39:03,  6.87s/it]                                                        38%|      | 2489/6500 [4:42:49<7:39:03,  6.87s/it] 38%|      | 2490/6500 [4:42:55<7:33:30,  6.79s/it]                                                        38%|      | 2490/6500 [4:42:55<7:33:30,  6.79s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8385931849479675, 'eval_runtime': 1.4876, 'eval_samples_per_second': 8.066, 'eval_steps_per_second': 2.017, 'epoch': 0.38}
                                                        38%|      | 2490/6500 [4:42:57<7:33:30,  6.79s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-2490
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2490I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in 
 /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2490
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2490
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2490
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2490/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2490/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2490/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2490/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.4658, 'learning_rate': 6.796126899625688e-05, 'epoch': 0.38}
{'loss': 0.4516, 'learning_rate': 6.793870692585051e-05, 'epoch': 0.38}
{'loss': 0.5058, 'learning_rate': 6.791614066238047e-05, 'epoch': 0.38}
{'loss': 0.4524, 'learning_rate': 6.789357021112148e-05, 'epoch': 0.38}
{'loss': 0.742, 'learning_rate': 6.787099557734927e-05, 'epoch': 0.38}
 38%|      | 2491/6500 [4:43:04<8:06:16,  7.28s/it]                                                        38%|      | 2491/6500 [4:43:04<8:06:16,  7.28s/it] 38%|      | 2492/6500 [4:43:10<7:51:38,  7.06s/it]                                                        38%|      | 2492/6500 [4:43:10<7:51:38,  7.06s/it] 38%|      | 2493/6500 [4:43:17<7:41:28,  6.91s/it]                                                        38%|      | 2493/6500 [4:43:17<7:41:28,  6.91s/it] 38%|      | 2494/6500 [4:43:23<7:34:15,  6.80s/it]                                                        38%|      | 2494/6500 [4:43:23<7:34:15,  6.80s/it] 38%|      | 2495/6500 [4:43:30<7:28:59,  6.73s/it]                                                        38%|      | 2495/6500 [4:43:30<7:28:59,  6.73s/it] 38%|      | 2496/6500 [4:43:36<7:25:38,  6.68s/it]                                  {'loss': 0.4875, 'learning_rate': 6.784841676634048e-05, 'epoch': 0.38}
{'loss': 0.4524, 'learning_rate': 6.78258337833728e-05, 'epoch': 0.38}
{'loss': 0.477, 'learning_rate': 6.780324663372485e-05, 'epoch': 0.38}
{'loss': 0.4537, 'learning_rate': 6.778065532267624e-05, 'epoch': 0.38}
{'loss': 0.4597, 'learning_rate': 6.775805985550756e-05, 'epoch': 0.38}
                      38%|      | 2496/6500 [4:43:36<7:25:38,  6.68s/it] 38%|      | 2497/6500 [4:43:43<7:22:55,  6.64s/it]                                                        38%|      | 2497/6500 [4:43:43<7:22:55,  6.64s/it] 38%|      | 2498/6500 [4:43:50<7:21:14,  6.62s/it]                                                        38%|      | 2498/6500 [4:43:50<7:21:14,  6.62s/it] 38%|      | 2499/6500 [4:43:56<7:20:09,  6.60s/it]                                                        38%|      | 2499/6500 [4:43:56<7:20:09,  6.60s/it] 38%|      | 2500/6500 [4:44:03<7:19:04,  6.59s/it]                                                        38%|      | 2500/6500 [4:44:03<7:19:04,  6.59s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.849704384803772, 'eval_runtime': 1.4837, 'eval_samples_per_second': 8.088, 'eval_steps_per_second': 2.022, 'epoch': 0.38}
                                                        38%|      | 2500/6500 [4:44:04<7:19:04,  6.59s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-2500
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2500
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2500
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2500
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2500
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2500/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2500/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2500/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2500/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.4735, 'learning_rate': 6.773546023750037e-05, 'epoch': 0.38}
{'loss': 0.48, 'learning_rate': 6.771285647393719e-05, 'epoch': 0.38}
{'loss': 0.469, 'learning_rate': 6.769024857010148e-05, 'epoch': 0.39}
{'loss': 0.4728, 'learning_rate': 6.766763653127773e-05, 'epoch': 0.39}
{'loss': 0.4684, 'learning_rate': 6.764502036275138e-05, 'epoch': 0.39}
 38%|      | 2501/6500 [4:44:11<7:54:55,  7.13s/it]                                                        38%|      | 2501/6500 [4:44:11<7:54:55,  7.13s/it] 38%|      | 2502/6500 [4:44:18<7:43:30,  6.96s/it]                                                        38%|      | 2502/6500 [4:44:18<7:43:30,  6.96s/it] 39%|      | 2503/6500 [4:44:24<7:35:32,  6.84s/it]                                                        39%|      | 2503/6500 [4:44:24<7:35:32,  6.84s/it] 39%|      | 2504/6500 [4:44:31<7:29:50,  6.75s/it]                                                        39%|      | 2504/6500 [4:44:31<7:29:50,  6.75s/it] 39%|      | 2505/6500 [4:44:38<7:43:56,  6.97s/it]                                                        39%|      | 2505/6500 [4:44:38<7:43:56,  6.97s/it] 39%|      | 2506/6500 [4:44:45<7:35:34,  6.84s/it]                                  {'loss': 0.4605, 'learning_rate': 6.762240006980878e-05, 'epoch': 0.39}
{'loss': 0.452, 'learning_rate': 6.759977565773734e-05, 'epoch': 0.39}
{'loss': 0.5, 'learning_rate': 6.757714713182533e-05, 'epoch': 0.39}
{'loss': 0.4676, 'learning_rate': 6.755451449736204e-05, 'epoch': 0.39}
{'loss': 0.7456, 'learning_rate': 6.753187775963773e-05, 'epoch': 0.39}
                      39%|      | 2506/6500 [4:44:45<7:35:34,  6.84s/it] 39%|      | 2507/6500 [4:44:51<7:30:06,  6.76s/it]                                                        39%|      | 2507/6500 [4:44:51<7:30:06,  6.76s/it] 39%|      | 2508/6500 [4:44:58<7:25:55,  6.70s/it]                                                        39%|      | 2508/6500 [4:44:58<7:25:55,  6.70s/it] 39%|      | 2509/6500 [4:45:04<7:22:51,  6.66s/it]                                                        39%|      | 2509/6500 [4:45:04<7:22:51,  6.66s/it] 39%|      | 2510/6500 [4:45:11<7:20:42,  6.63s/it]                                                        39%|      | 2510/6500 [4:45:11<7:20:42,  6.63s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8477936387062073, 'eval_runtime': 1.4785, 'eval_samples_per_second': 8.116, 'eval_steps_per_second': 2.029, 'epoch': 0.39}
                                                        39%|      | 2510/6500 [4:45:13<7:20:42,  6.63s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-2510
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2510
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2510
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2510
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2510
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2510/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2510/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2510/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2510/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.4783, 'learning_rate': 6.750923692394359e-05, 'epoch': 0.39}
{'loss': 0.4581, 'learning_rate': 6.748659199557177e-05, 'epoch': 0.39}
{'loss': 0.4602, 'learning_rate': 6.74639429798154e-05, 'epoch': 0.39}
{'loss': 0.4486, 'learning_rate': 6.744128988196853e-05, 'epoch': 0.39}
{'loss': 0.4601, 'learning_rate': 6.741863270732619e-05, 'epoch': 0.39}
 39%|      | 2511/6500 [4:45:20<7:57:17,  7.18s/it]                                                        39%|      | 2511/6500 [4:45:20<7:57:17,  7.18s/it] 39%|      | 2512/6500 [4:45:26<7:44:46,  6.99s/it]                                                        39%|      | 2512/6500 [4:45:26<7:44:46,  6.99s/it] 39%|      | 2513/6500 [4:45:33<7:36:26,  6.87s/it]                                                        39%|      | 2513/6500 [4:45:33<7:36:26,  6.87s/it] 39%|      | 2514/6500 [4:45:39<7:29:58,  6.77s/it]                                                        39%|      | 2514/6500 [4:45:39<7:29:58,  6.77s/it] 39%|      | 2515/6500 [4:45:46<7:25:30,  6.71s/it]                                                        39%|      | 2515/6500 [4:45:46<7:25:30,  6.71s/it] 39%|      | 2516/6500 [4:45:52<7:22:40,  6.67s/it]                                  {'loss': 0.4598, 'learning_rate': 6.739597146118436e-05, 'epoch': 0.39}
{'loss': 0.4781, 'learning_rate': 6.737330614884001e-05, 'epoch': 0.39}
{'loss': 0.4742, 'learning_rate': 6.735063677559095e-05, 'epoch': 0.39}
{'loss': 0.4587, 'learning_rate': 6.732796334673603e-05, 'epoch': 0.39}
{'loss': 0.4755, 'learning_rate': 6.730528586757505e-05, 'epoch': 0.39}
                      39%|      | 2516/6500 [4:45:52<7:22:40,  6.67s/it] 39%|      | 2517/6500 [4:45:59<7:20:39,  6.64s/it]                                                        39%|      | 2517/6500 [4:45:59<7:20:39,  6.64s/it] 39%|      | 2518/6500 [4:46:05<7:19:05,  6.62s/it]                                                        39%|      | 2518/6500 [4:46:05<7:19:05,  6.62s/it] 39%|      | 2519/6500 [4:46:12<7:17:49,  6.60s/it]                                                        39%|      | 2519/6500 [4:46:12<7:17:49,  6.60s/it] 39%|      | 2520/6500 [4:46:19<7:16:44,  6.58s/it]                                                        39%|      | 2520/6500 [4:46:19<7:16:44,  6.58s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.835338830947876, 'eval_runtime': 1.4824, 'eval_samples_per_second': 8.095, 'eval_steps_per_second': 2.024, 'epoch': 0.39}
                                                        39%|      | 2520/6500 [4:46:20<7:16:44,  6.58s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-2520
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2520
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2520
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2520
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2520
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2520/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2520/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2520/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2520/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.4674, 'learning_rate': 6.72826043434087e-05, 'epoch': 0.39}
{'loss': 0.4683, 'learning_rate': 6.725991877953868e-05, 'epoch': 0.39}
{'loss': 0.4909, 'learning_rate': 6.723722918126758e-05, 'epoch': 0.39}
{'loss': 0.4656, 'learning_rate': 6.721453555389897e-05, 'epoch': 0.39}
{'loss': 0.7422, 'learning_rate': 6.719183790273733e-05, 'epoch': 0.39}
 39%|      | 2521/6500 [4:46:28<8:04:46,  7.31s/it]                                                        39%|      | 2521/6500 [4:46:28<8:04:46,  7.31s/it] 39%|      | 2522/6500 [4:46:34<7:49:56,  7.09s/it]                                                        39%|      | 2522/6500 [4:46:34<7:49:56,  7.09s/it] 39%|      | 2523/6500 [4:46:41<7:39:13,  6.93s/it]                                                        39%|      | 2523/6500 [4:46:41<7:39:13,  6.93s/it] 39%|      | 2524/6500 [4:46:47<7:32:01,  6.82s/it]                                                        39%|      | 2524/6500 [4:46:47<7:32:01,  6.82s/it] 39%|      | 2525/6500 [4:46:54<7:26:59,  6.75s/it]                                                        39%|      | 2525/6500 [4:46:54<7:26:59,  6.75s/it] 39%|      | 2526/6500 [4:47:00<7:23:18,  6.69s/it]                                  {'loss': 0.4619, 'learning_rate': 6.716913623308812e-05, 'epoch': 0.39}
{'loss': 0.4691, 'learning_rate': 6.714643055025769e-05, 'epoch': 0.39}
{'loss': 0.4565, 'learning_rate': 6.712372085955339e-05, 'epoch': 0.39}
{'loss': 0.4502, 'learning_rate': 6.710100716628344e-05, 'epoch': 0.39}
{'loss': 0.46, 'learning_rate': 6.707828947575706e-05, 'epoch': 0.39}
                      39%|      | 2526/6500 [4:47:00<7:23:18,  6.69s/it] 39%|      | 2527/6500 [4:47:07<7:20:13,  6.65s/it]                                                        39%|      | 2527/6500 [4:47:07<7:20:13,  6.65s/it] 39%|      | 2528/6500 [4:47:14<7:18:21,  6.62s/it]                                                        39%|      | 2528/6500 [4:47:14<7:18:21,  6.62s/it] 39%|      | 2529/6500 [4:47:20<7:17:27,  6.61s/it]                                                        39%|      | 2529/6500 [4:47:20<7:17:27,  6.61s/it] 39%|      | 2530/6500 [4:47:27<7:16:30,  6.60s/it]                                                        39%|      | 2530/6500 [4:47:27<7:16:30,  6.60s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8477882742881775, 'eval_runtime': 1.496, 'eval_samples_per_second': 8.021, 'eval_steps_per_second': 2.005, 'epoch': 0.39}
                                                        39%|      | 2530/6500 [4:47:28<7:16:30,  6.60s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-2530
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  I AM HERE AND I AM SAVING THE MODEL
/projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2530the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2530

I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2530
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2530
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2530/pytorch_model.bin
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2530/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2530/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2530/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.4738, 'learning_rate': 6.705556779328433e-05, 'epoch': 0.39}
{'loss': 0.4857, 'learning_rate': 6.703284212417632e-05, 'epoch': 0.39}
{'loss': 0.4654, 'learning_rate': 6.701011247374505e-05, 'epoch': 0.39}
{'loss': 0.4614, 'learning_rate': 6.69873788473034e-05, 'epoch': 0.39}
{'loss': 0.4783, 'learning_rate': 6.696464125016522e-05, 'epoch': 0.39}
 39%|      | 2531/6500 [4:47:35<7:51:14,  7.12s/it]                                                        39%|      | 2531/6500 [4:47:35<7:51:14,  7.12s/it] 39%|      | 2532/6500 [4:47:42<7:40:11,  6.96s/it]                                                        39%|      | 2532/6500 [4:47:42<7:40:11,  6.96s/it] 39%|      | 2533/6500 [4:47:48<7:32:25,  6.84s/it]                                                        39%|      | 2533/6500 [4:47:48<7:32:25,  6.84s/it] 39%|      | 2534/6500 [4:47:55<7:27:02,  6.76s/it]                                                        39%|      | 2534/6500 [4:47:55<7:27:02,  6.76s/it] 39%|      | 2535/6500 [4:48:01<7:22:49,  6.70s/it]                                                        39%|      | 2535/6500 [4:48:01<7:22:49,  6.70s/it] 39%|      | 2536/6500 [4:48:08<7:19:54,  6.66s/it]                                  {'loss': 0.4542, 'learning_rate': 6.694189968764532e-05, 'epoch': 0.39}
{'loss': 0.4624, 'learning_rate': 6.691915416505935e-05, 'epoch': 0.39}
{'loss': 0.474, 'learning_rate': 6.689640468772398e-05, 'epoch': 0.39}
{'loss': 0.4675, 'learning_rate': 6.687365126095674e-05, 'epoch': 0.39}
{'loss': 0.742, 'learning_rate': 6.685089389007612e-05, 'epoch': 0.39}
                      39%|      | 2536/6500 [4:48:08<7:19:54,  6.66s/it] 39%|      | 2537/6500 [4:48:14<7:18:10,  6.63s/it]                                                        39%|      | 2537/6500 [4:48:14<7:18:10,  6.63s/it] 39%|      | 2538/6500 [4:48:22<7:34:38,  6.88s/it]                                                        39%|      | 2538/6500 [4:48:22<7:34:38,  6.88s/it] 39%|      | 2539/6500 [4:48:28<7:28:26,  6.79s/it]                                                        39%|      | 2539/6500 [4:48:28<7:28:26,  6.79s/it] 39%|      | 2540/6500 [4:48:35<7:24:02,  6.73s/it]                                                        39%|      | 2540/6500 [4:48:35<7:24:02,  6.73s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8522427678108215, 'eval_runtime': 1.4839, 'eval_samples_per_second': 8.087, 'eval_steps_per_second': 2.022, 'epoch': 0.39}
                                                        39%|      | 2540/6500 [4:48:37<7:24:02,  6.73s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-2540
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  I AM HERE AND I AM SAVING THE MODEL
/projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2540the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2540

I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2540
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2540
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2540/pytorch_model.bin
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2540/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2540/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2540/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.4582, 'learning_rate': 6.682813258040151e-05, 'epoch': 0.39}
{'loss': 0.4796, 'learning_rate': 6.68053673372532e-05, 'epoch': 0.39}
{'loss': 0.4402, 'learning_rate': 6.678259816595246e-05, 'epoch': 0.39}
{'loss': 0.4486, 'learning_rate': 6.675982507182145e-05, 'epoch': 0.39}
{'loss': 0.4616, 'learning_rate': 6.673704806018326e-05, 'epoch': 0.39}
 39%|      | 2541/6500 [4:48:43<7:56:25,  7.22s/it]                                                        39%|      | 2541/6500 [4:48:43<7:56:25,  7.22s/it] 39%|      | 2542/6500 [4:48:50<7:43:00,  7.02s/it]                                                        39%|      | 2542/6500 [4:48:50<7:43:00,  7.02s/it] 39%|      | 2543/6500 [4:48:57<7:33:51,  6.88s/it]                                                        39%|      | 2543/6500 [4:48:57<7:33:51,  6.88s/it] 39%|      | 2544/6500 [4:49:03<7:27:36,  6.79s/it]                                                        39%|      | 2544/6500 [4:49:03<7:27:36,  6.79s/it] 39%|      | 2545/6500 [4:49:10<7:23:07,  6.72s/it]                                                        39%|      | 2545/6500 [4:49:10<7:23:07,  6.72s/it] 39%|      | 2546/6500 [4:49:16<7:20:07,  6.68s/it]                                  {'loss': 0.4498, 'learning_rate': 6.67142671363618e-05, 'epoch': 0.39}
{'loss': 0.4777, 'learning_rate': 6.669148230568205e-05, 'epoch': 0.39}
{'loss': 0.484, 'learning_rate': 6.666869357346978e-05, 'epoch': 0.39}
{'loss': 0.4492, 'learning_rate': 6.664590094505174e-05, 'epoch': 0.39}
{'loss': 0.4662, 'learning_rate': 6.662310442575556e-05, 'epoch': 0.39}
                      39%|      | 2546/6500 [4:49:16<7:20:07,  6.68s/it] 39%|      | 2547/6500 [4:49:23<7:17:56,  6.65s/it]                                                        39%|      | 2547/6500 [4:49:23<7:17:56,  6.65s/it] 39%|      | 2548/6500 [4:49:29<7:16:23,  6.63s/it]                                                        39%|      | 2548/6500 [4:49:29<7:16:23,  6.63s/it] 39%|      | 2549/6500 [4:49:36<7:15:36,  6.62s/it]                                                        39%|      | 2549/6500 [4:49:36<7:15:36,  6.62s/it] 39%|      | 2550/6500 [4:49:43<7:14:24,  6.60s/it]                                                        39%|      | 2550/6500 [4:49:43<7:14:24,  6.60s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8421687483787537, 'eval_runtime': 1.481, 'eval_samples_per_second': 8.103, 'eval_steps_per_second': 2.026, 'epoch': 0.39}
                                                        39%|      | 2550/6500 [4:49:44<7:14:24,  6.60s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-2550
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2550
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2550
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2550
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2550
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2550/pytorch_model.bin
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2550/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2550/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2550/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.4477, 'learning_rate': 6.660030402090981e-05, 'epoch': 0.39}
{'loss': 0.4996, 'learning_rate': 6.65774997358439e-05, 'epoch': 0.39}
{'loss': 0.4442, 'learning_rate': 6.655469157588823e-05, 'epoch': 0.39}
{'loss': 0.7385, 'learning_rate': 6.653187954637405e-05, 'epoch': 0.39}
{'loss': 0.4816, 'learning_rate': 6.650906365263356e-05, 'epoch': 0.39}
 39%|      | 2551/6500 [4:49:51<7:50:42,  7.15s/it]                                                        39%|      | 2551/6500 [4:49:51<7:50:42,  7.15s/it] 39%|      | 2552/6500 [4:49:58<7:38:59,  6.98s/it]                                                        39%|      | 2552/6500 [4:49:58<7:38:59,  6.98s/it] 39%|      | 2553/6500 [4:50:04<7:30:56,  6.86s/it]                                                        39%|      | 2553/6500 [4:50:04<7:30:56,  6.86s/it] 39%|      | 2554/6500 [4:50:12<7:42:44,  7.04s/it]                                                        39%|      | 2554/6500 [4:50:12<7:42:44,  7.04s/it] 39%|      | 2555/6500 [4:50:18<7:33:51,  6.90s/it]                                                        39%|      | 2555/6500 [4:50:18<7:33:51,  6.90s/it] 39%|      | 2556/6500 [4:50:25<7:27:37,  6.81s/it]                                  {'loss': 0.4484, 'learning_rate': 6.64862438999998e-05, 'epoch': 0.39}
{'loss': 0.4714, 'learning_rate': 6.646342029380679e-05, 'epoch': 0.39}
{'loss': 0.4417, 'learning_rate': 6.644059283938938e-05, 'epoch': 0.39}
{'loss': 0.4494, 'learning_rate': 6.641776154208334e-05, 'epoch': 0.39}
{'loss': 0.4552, 'learning_rate': 6.639492640722536e-05, 'epoch': 0.39}
                      39%|      | 2556/6500 [4:50:25<7:27:37,  6.81s/it] 39%|      | 2557/6500 [4:50:31<7:22:51,  6.74s/it]                                                        39%|      | 2557/6500 [4:50:31<7:22:51,  6.74s/it] 39%|      | 2558/6500 [4:50:38<7:19:27,  6.69s/it]                                                        39%|      | 2558/6500 [4:50:38<7:19:27,  6.69s/it] 39%|      | 2559/6500 [4:50:45<7:17:21,  6.66s/it]                                                        39%|      | 2559/6500 [4:50:45<7:17:21,  6.66s/it] 39%|      | 2560/6500 [4:50:51<7:15:36,  6.63s/it]                                                        39%|      | 2560/6500 [4:50:51<7:15:36,  6.63s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8497570157051086, 'eval_runtime': 1.4826, 'eval_samples_per_second': 8.094, 'eval_steps_per_second': 2.023, 'epoch': 0.39}
                                                        39%|      | 2560/6500 [4:50:53<7:15:36,  6.63s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-2560
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2560I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2560

I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2560
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2560
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2560/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2560/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2560/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2560/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.4523, 'learning_rate': 6.637208744015303e-05, 'epoch': 0.39}
{'loss': 0.4758, 'learning_rate': 6.63492446462048e-05, 'epoch': 0.39}
{'loss': 0.466, 'learning_rate': 6.632639803072003e-05, 'epoch': 0.39}
{'loss': 0.4485, 'learning_rate': 6.630354759903898e-05, 'epoch': 0.39}
{'loss': 0.4709, 'learning_rate': 6.628069335650282e-05, 'epoch': 0.39}
 39%|      | 2561/6500 [4:50:59<7:49:24,  7.15s/it]                                                        39%|      | 2561/6500 [4:50:59<7:49:24,  7.15s/it] 39%|      | 2562/6500 [4:51:06<7:37:44,  6.97s/it]                                                        39%|      | 2562/6500 [4:51:06<7:37:44,  6.97s/it] 39%|      | 2563/6500 [4:51:13<7:29:35,  6.85s/it]                                                        39%|      | 2563/6500 [4:51:13<7:29:35,  6.85s/it] 39%|      | 2564/6500 [4:51:19<7:24:23,  6.77s/it]                                                        39%|      | 2564/6500 [4:51:19<7:24:23,  6.77s/it] 39%|      | 2565/6500 [4:51:26<7:19:49,  6.71s/it]                                                        39%|      | 2565/6500 [4:51:26<7:19:49,  6.71s/it] 39%|      | 2566/6500 [4:51:32<7:16:29,  6.66s/it]                                  {'loss': 0.4484, 'learning_rate': 6.625783530845359e-05, 'epoch': 0.39}
{'loss': 0.4923, 'learning_rate': 6.623497346023418e-05, 'epoch': 0.39}
{'loss': 0.4492, 'learning_rate': 6.621210781718844e-05, 'epoch': 0.4}
{'loss': 0.732, 'learning_rate': 6.618923838466108e-05, 'epoch': 0.4}
{'loss': 0.4758, 'learning_rate': 6.616636516799766e-05, 'epoch': 0.4}
                      39%|      | 2566/6500 [4:51:32<7:16:29,  6.66s/it] 39%|      | 2567/6500 [4:51:39<7:14:21,  6.63s/it]                                                        39%|      | 2567/6500 [4:51:39<7:14:21,  6.63s/it] 40%|      | 2568/6500 [4:51:45<7:12:35,  6.60s/it]                                                        40%|      | 2568/6500 [4:51:45<7:12:35,  6.60s/it] 40%|      | 2569/6500 [4:51:52<7:11:17,  6.58s/it]                                                        40%|      | 2569/6500 [4:51:52<7:11:17,  6.58s/it] 40%|      | 2570/6500 [4:51:59<7:28:47,  6.85s/it]                                                        40%|      | 2570/6500 [4:51:59<7:28:47,  6.85s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8462579250335693, 'eval_runtime': 1.4962, 'eval_samples_per_second': 8.02, 'eval_steps_per_second': 2.005, 'epoch': 0.4}
                                                        40%|      | 2570/6500 [4:52:01<7:28:47,  6.85s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-2570
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2570I AM HERE AND I AM SAVING THE MODEL

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2570
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2570
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2570
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2570/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2570/pytorch_model.bin
the pytorch model path is the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2570/pytorch_model.bin
/projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2570/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.4475, 'learning_rate': 6.61434881725447e-05, 'epoch': 0.4}
{'loss': 0.4712, 'learning_rate': 6.612060740364952e-05, 'epoch': 0.4}
{'loss': 0.4441, 'learning_rate': 6.609772286666037e-05, 'epoch': 0.4}
{'loss': 0.46, 'learning_rate': 6.607483456692638e-05, 'epoch': 0.4}
{'loss': 0.4667, 'learning_rate': 6.605194250979755e-05, 'epoch': 0.4}
 40%|      | 2571/6500 [4:52:08<7:58:46,  7.31s/it]                                                        40%|      | 2571/6500 [4:52:08<7:58:46,  7.31s/it] 40%|      | 2572/6500 [4:52:14<7:43:41,  7.08s/it]                                                        40%|      | 2572/6500 [4:52:14<7:43:41,  7.08s/it] 40%|      | 2573/6500 [4:52:21<7:33:21,  6.93s/it]                                                        40%|      | 2573/6500 [4:52:21<7:33:21,  6.93s/it] 40%|      | 2574/6500 [4:52:27<7:25:59,  6.82s/it]                                                        40%|      | 2574/6500 [4:52:27<7:25:59,  6.82s/it] 40%|      | 2575/6500 [4:52:34<7:20:34,  6.73s/it]                                                        40%|      | 2575/6500 [4:52:34<7:20:34,  6.73s/it] 40%|      | 2576/6500 [4:52:41<7:16:48,  6.68s/it]                                  {'loss': 0.4668, 'learning_rate': 6.602904670062476e-05, 'epoch': 0.4}
{'loss': 0.4617, 'learning_rate': 6.600614714475975e-05, 'epoch': 0.4}
{'loss': 0.4694, 'learning_rate': 6.598324384755518e-05, 'epoch': 0.4}
{'loss': 0.4589, 'learning_rate': 6.596033681436452e-05, 'epoch': 0.4}
{'loss': 0.4576, 'learning_rate': 6.593742605054218e-05, 'epoch': 0.4}
                      40%|      | 2576/6500 [4:52:41<7:16:48,  6.68s/it] 40%|      | 2577/6500 [4:52:47<7:14:20,  6.64s/it]                                                        40%|      | 2577/6500 [4:52:47<7:14:20,  6.64s/it] 40%|      | 2578/6500 [4:52:54<7:12:25,  6.62s/it]                                                        40%|      | 2578/6500 [4:52:54<7:12:25,  6.62s/it] 40%|      | 2579/6500 [4:53:00<7:11:13,  6.60s/it]                                                        40%|      | 2579/6500 [4:53:00<7:11:13,  6.60s/it] 40%|      | 2580/6500 [4:53:07<7:09:59,  6.58s/it]                                                        40%|      | 2580/6500 [4:53:07<7:09:59,  6.58s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8414601683616638, 'eval_runtime': 1.486, 'eval_samples_per_second': 8.076, 'eval_steps_per_second': 2.019, 'epoch': 0.4}
                                                        40%|      | 2580/6500 [4:53:08<7:09:59,  6.58s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-2580
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2580
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2580
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2580
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2580
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2580/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2580/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2580/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2580/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.4457, 'learning_rate': 6.59145115614434e-05, 'epoch': 0.4}
{'loss': 0.4948, 'learning_rate': 6.58915933524243e-05, 'epoch': 0.4}
{'loss': 0.4592, 'learning_rate': 6.58686714288419e-05, 'epoch': 0.4}
{'loss': 0.733, 'learning_rate': 6.584574579605401e-05, 'epoch': 0.4}
{'loss': 0.4721, 'learning_rate': 6.58228164594194e-05, 'epoch': 0.4}
 40%|      | 2581/6500 [4:53:15<7:44:37,  7.11s/it]                                                        40%|      | 2581/6500 [4:53:15<7:44:37,  7.11s/it] 40%|      | 2582/6500 [4:53:22<7:33:35,  6.95s/it]                                                        40%|      | 2582/6500 [4:53:22<7:33:35,  6.95s/it] 40%|      | 2583/6500 [4:53:28<7:25:36,  6.83s/it]                                                        40%|      | 2583/6500 [4:53:28<7:25:36,  6.83s/it] 40%|      | 2584/6500 [4:53:35<7:19:58,  6.74s/it]                                                        40%|      | 2584/6500 [4:53:35<7:19:58,  6.74s/it] 40%|      | 2585/6500 [4:53:41<7:16:18,  6.69s/it]                                                        40%|      | 2585/6500 [4:53:41<7:16:18,  6.69s/it] 40%|      | 2586/6500 [4:53:48<7:25:16,  6.83s/it]                                  {'loss': 0.4515, 'learning_rate': 6.579988342429763e-05, 'epoch': 0.4}
{'loss': 0.4474, 'learning_rate': 6.577694669604919e-05, 'epoch': 0.4}
{'loss': 0.4444, 'learning_rate': 6.575400628003538e-05, 'epoch': 0.4}
{'loss': 0.4521, 'learning_rate': 6.57310621816184e-05, 'epoch': 0.4}
{'loss': 0.456, 'learning_rate': 6.570811440616125e-05, 'epoch': 0.4}
                      40%|      | 2586/6500 [4:53:48<7:25:16,  6.83s/it] 40%|      | 2587/6500 [4:53:55<7:19:44,  6.74s/it]                                                        40%|      | 2587/6500 [4:53:55<7:19:44,  6.74s/it] 40%|      | 2588/6500 [4:54:02<7:15:48,  6.68s/it]                                                        40%|      | 2588/6500 [4:54:02<7:15:48,  6.68s/it] 40%|      | 2589/6500 [4:54:08<7:13:35,  6.65s/it]                                                        40%|      | 2589/6500 [4:54:08<7:13:35,  6.65s/it] 40%|      | 2590/6500 [4:54:15<7:11:54,  6.63s/it]                                                        40%|      | 2590/6500 [4:54:15<7:11:54,  6.63s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8528368473052979, 'eval_runtime': 1.495, 'eval_samples_per_second': 8.027, 'eval_steps_per_second': 2.007, 'epoch': 0.4}
                                                        40%|      | 2590/6500 [4:54:16<7:11:54,  6.63s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-2590
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2590I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2590

I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2590
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2590
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2590/pytorch_model.bin
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2590/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2590/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2590/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.4695, 'learning_rate': 6.568516295902788e-05, 'epoch': 0.4}
{'loss': 0.4592, 'learning_rate': 6.566220784558304e-05, 'epoch': 0.4}
{'loss': 0.4659, 'learning_rate': 6.563924907119234e-05, 'epoch': 0.4}
{'loss': 0.4653, 'learning_rate': 6.561628664122226e-05, 'epoch': 0.4}
{'loss': 0.4555, 'learning_rate': 6.559332056104012e-05, 'epoch': 0.4}
 40%|      | 2591/6500 [4:54:23<7:45:30,  7.15s/it]                                                        40%|      | 2591/6500 [4:54:23<7:45:30,  7.15s/it] 40%|      | 2592/6500 [4:54:30<7:33:53,  6.97s/it]                                                        40%|      | 2592/6500 [4:54:30<7:33:53,  6.97s/it] 40%|      | 2593/6500 [4:54:36<7:25:55,  6.85s/it]                                                        40%|      | 2593/6500 [4:54:36<7:25:55,  6.85s/it] 40%|      | 2594/6500 [4:54:43<7:20:11,  6.76s/it]                                                        40%|      | 2594/6500 [4:54:43<7:20:11,  6.76s/it] 40%|      | 2595/6500 [4:54:49<7:16:07,  6.70s/it]                                                        40%|      | 2595/6500 [4:54:49<7:16:07,  6.70s/it] 40%|      | 2596/6500 [4:54:56<7:13:03,  6.66s/it]                                  {'loss': 0.455, 'learning_rate': 6.557035083601413e-05, 'epoch': 0.4}
{'loss': 0.4824, 'learning_rate': 6.554737747151328e-05, 'epoch': 0.4}
{'loss': 0.4635, 'learning_rate': 6.552440047290747e-05, 'epoch': 0.4}
{'loss': 0.7412, 'learning_rate': 6.550141984556747e-05, 'epoch': 0.4}
{'loss': 0.4631, 'learning_rate': 6.547843559486481e-05, 'epoch': 0.4}
                      40%|      | 2596/6500 [4:54:56<7:13:03,  6.66s/it] 40%|      | 2597/6500 [4:55:02<7:10:59,  6.63s/it]                                                        40%|      | 2597/6500 [4:55:02<7:10:59,  6.63s/it] 40%|      | 2598/6500 [4:55:09<7:09:28,  6.60s/it]                                                        40%|      | 2598/6500 [4:55:09<7:09:28,  6.60s/it] 40%|      | 2599/6500 [4:55:15<7:08:20,  6.59s/it]                                                        40%|      | 2599/6500 [4:55:15<7:08:20,  6.59s/it] 40%|      | 2600/6500 [4:55:22<7:07:31,  6.58s/it]                                                        40%|      | 2600/6500 [4:55:22<7:07:31,  6.58s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8480536341667175, 'eval_runtime': 1.4856, 'eval_samples_per_second': 8.077, 'eval_steps_per_second': 2.019, 'epoch': 0.4}
                                                        40%|      | 2600/6500 [4:55:24<7:07:31,  6.58s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-2600
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2600I AM HERE AND I AM SAVING THE MODEL

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2600
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2600
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2600
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2600/pytorch_model.bin
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2600/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2600/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2600/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.4606, 'learning_rate': 6.545544772617194e-05, 'epoch': 0.4}
{'loss': 0.4524, 'learning_rate': 6.543245624486214e-05, 'epoch': 0.4}
{'loss': 0.4432, 'learning_rate': 6.540946115630952e-05, 'epoch': 0.4}
{'loss': 0.4507, 'learning_rate': 6.538646246588906e-05, 'epoch': 0.4}
{'loss': 0.4606, 'learning_rate': 6.536346017897653e-05, 'epoch': 0.4}
 40%|      | 2601/6500 [4:55:30<7:42:04,  7.11s/it]                                                        40%|      | 2601/6500 [4:55:30<7:42:04,  7.11s/it] 40%|      | 2602/6500 [4:55:38<7:48:09,  7.21s/it]                                                        40%|      | 2602/6500 [4:55:38<7:48:09,  7.21s/it] 40%|      | 2603/6500 [4:55:44<7:35:40,  7.02s/it]                                                        40%|      | 2603/6500 [4:55:44<7:35:40,  7.02s/it] 40%|      | 2604/6500 [4:55:51<7:26:44,  6.88s/it]                                                        40%|      | 2604/6500 [4:55:51<7:26:44,  6.88s/it] 40%|      | 2605/6500 [4:55:58<7:20:11,  6.78s/it]                                                        40%|      | 2605/6500 [4:55:58<7:20:11,  6.78s/it] 40%|      | 2606/6500 [4:56:04<7:15:39,  6.71s/it]                                  {'loss': 0.4726, 'learning_rate': 6.53404543009486e-05, 'epoch': 0.4}
{'loss': 0.4654, 'learning_rate': 6.531744483718274e-05, 'epoch': 0.4}
{'loss': 0.4567, 'learning_rate': 6.529443179305728e-05, 'epoch': 0.4}
{'loss': 0.4723, 'learning_rate': 6.52714151739514e-05, 'epoch': 0.4}
{'loss': 0.4491, 'learning_rate': 6.524839498524508e-05, 'epoch': 0.4}
                      40%|      | 2606/6500 [4:56:04<7:15:39,  6.71s/it] 40%|      | 2607/6500 [4:56:11<7:12:49,  6.67s/it]                                                        40%|      | 2607/6500 [4:56:11<7:12:49,  6.67s/it] 40%|      | 2608/6500 [4:56:17<7:10:43,  6.64s/it]                                                        40%|      | 2608/6500 [4:56:17<7:10:43,  6.64s/it] 40%|      | 2609/6500 [4:56:24<7:08:52,  6.61s/it]                                                        40%|      | 2609/6500 [4:56:24<7:08:52,  6.61s/it] 40%|      | 2610/6500 [4:56:30<7:07:44,  6.60s/it]                                                        40%|      | 2610/6500 [4:56:30<7:07:44,  6.60s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8389614224433899, 'eval_runtime': 1.4892, 'eval_samples_per_second': 8.058, 'eval_steps_per_second': 2.014, 'epoch': 0.4}
                                                        40%|      | 2610/6500 [4:56:32<7:07:44,  6.60s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-2610
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2610
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2610
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2610
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2610
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2610/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2610/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2610/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2610/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.4545, 'learning_rate': 6.522537123231912e-05, 'epoch': 0.4}
{'loss': 0.48, 'learning_rate': 6.520234392055522e-05, 'epoch': 0.4}
{'loss': 0.4576, 'learning_rate': 6.517931305533584e-05, 'epoch': 0.4}
{'loss': 0.7435, 'learning_rate': 6.515627864204434e-05, 'epoch': 0.4}
{'loss': 0.4508, 'learning_rate': 6.513324068606488e-05, 'epoch': 0.4}
 40%|      | 2611/6500 [4:56:39<7:42:01,  7.13s/it]                                                        40%|      | 2611/6500 [4:56:39<7:42:01,  7.13s/it] 40%|      | 2612/6500 [4:56:45<7:30:40,  6.95s/it]                                                        40%|      | 2612/6500 [4:56:45<7:30:40,  6.95s/it] 40%|      | 2613/6500 [4:56:52<7:22:48,  6.84s/it]                                                        40%|      | 2613/6500 [4:56:52<7:22:48,  6.84s/it] 40%|      | 2614/6500 [4:56:58<7:17:03,  6.75s/it]                                                        40%|      | 2614/6500 [4:56:58<7:17:03,  6.75s/it] 40%|      | 2615/6500 [4:57:05<7:13:09,  6.69s/it]                                                        40%|      | 2615/6500 [4:57:05<7:13:09,  6.69s/it] 40%|      | 2616/6500 [4:57:11<7:10:13,  6.65s/it]                                  {'loss': 0.4759, 'learning_rate': 6.511019919278239e-05, 'epoch': 0.4}
{'loss': 0.4414, 'learning_rate': 6.508715416758273e-05, 'epoch': 0.4}
{'loss': 0.4489, 'learning_rate': 6.50641056158525e-05, 'epoch': 0.4}
{'loss': 0.4462, 'learning_rate': 6.504105354297918e-05, 'epoch': 0.4}
{'loss': 0.4667, 'learning_rate': 6.501799795435104e-05, 'epoch': 0.4}
                      40%|      | 2616/6500 [4:57:11<7:10:13,  6.65s/it] 40%|      | 2617/6500 [4:57:18<7:08:24,  6.62s/it]                                                        40%|      | 2617/6500 [4:57:18<7:08:24,  6.62s/it] 40%|      | 2618/6500 [4:57:25<7:23:41,  6.86s/it]                                                        40%|      | 2618/6500 [4:57:25<7:23:41,  6.86s/it] 40%|      | 2619/6500 [4:57:32<7:17:50,  6.77s/it]                                                        40%|      | 2619/6500 [4:57:32<7:17:50,  6.77s/it] 40%|      | 2620/6500 [4:57:39<7:13:32,  6.70s/it]                                                        40%|      | 2620/6500 [4:57:39<7:13:32,  6.70s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8508906960487366, 'eval_runtime': 1.488, 'eval_samples_per_second': 8.064, 'eval_steps_per_second': 2.016, 'epoch': 0.4}
                                                        40%|      | 2620/6500 [4:57:40<7:13:32,  6.70s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-2620
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  I AM HERE AND I AM SAVING THE MODEL/projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2620
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2620

I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2620
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2620
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2620/pytorch_model.binthe pytorch model path is
 /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2620/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2620/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2620/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.4842, 'learning_rate': 6.499493885535721e-05, 'epoch': 0.4}
{'loss': 0.4687, 'learning_rate': 6.497187625138757e-05, 'epoch': 0.4}
{'loss': 0.4526, 'learning_rate': 6.49488101478329e-05, 'epoch': 0.4}
{'loss': 0.4632, 'learning_rate': 6.492574055008473e-05, 'epoch': 0.4}
{'loss': 0.4438, 'learning_rate': 6.490266746353547e-05, 'epoch': 0.4}
 40%|      | 2621/6500 [4:57:47<7:45:46,  7.20s/it]                                                        40%|      | 2621/6500 [4:57:47<7:45:46,  7.20s/it] 40%|      | 2622/6500 [4:57:53<7:33:07,  7.01s/it]                                                        40%|      | 2622/6500 [4:57:53<7:33:07,  7.01s/it] 40%|      | 2623/6500 [4:58:00<7:23:53,  6.87s/it]                                                        40%|      | 2623/6500 [4:58:00<7:23:53,  6.87s/it] 40%|      | 2624/6500 [4:58:07<7:17:38,  6.77s/it]                                                        40%|      | 2624/6500 [4:58:07<7:17:38,  6.77s/it] 40%|      | 2625/6500 [4:58:13<7:13:08,  6.71s/it]                                                        40%|      | 2625/6500 [4:58:13<7:13:08,  6.71s/it] 40%|      | 2626/6500 [4:58:20<7:10:16,  6.66s/it]                                  {'loss': 0.4881, 'learning_rate': 6.48795908935783e-05, 'epoch': 0.4}
{'loss': 0.4471, 'learning_rate': 6.485651084560723e-05, 'epoch': 0.4}
{'loss': 0.5881, 'learning_rate': 6.483342732501707e-05, 'epoch': 0.4}
{'loss': 0.6232, 'learning_rate': 6.481034033720347e-05, 'epoch': 0.4}
{'loss': 0.4467, 'learning_rate': 6.478724988756285e-05, 'epoch': 0.4}
                      40%|      | 2626/6500 [4:58:20<7:10:16,  6.66s/it] 40%|      | 2627/6500 [4:58:26<7:08:00,  6.63s/it]                                                        40%|      | 2627/6500 [4:58:26<7:08:00,  6.63s/it] 40%|      | 2628/6500 [4:58:33<7:06:23,  6.61s/it]                                                        40%|      | 2628/6500 [4:58:33<7:06:23,  6.61s/it] 40%|      | 2629/6500 [4:58:39<7:05:08,  6.59s/it]                                                        40%|      | 2629/6500 [4:58:39<7:05:08,  6.59s/it] 40%|      | 2630/6500 [4:58:46<7:04:10,  6.58s/it]                                                        40%|      | 2630/6500 [4:58:46<7:04:10,  6.58s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8513821959495544, 'eval_runtime': 1.488, 'eval_samples_per_second': 8.064, 'eval_steps_per_second': 2.016, 'epoch': 0.4}
                                                        40%|      | 2630/6500 [4:58:47<7:04:10,  6.58s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-2630
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2630
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2630
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2630
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2630
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2630/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2630/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2630/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2630/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.4692, 'learning_rate': 6.47641559814925e-05, 'epoch': 0.4}
{'loss': 0.4325, 'learning_rate': 6.474105862439045e-05, 'epoch': 0.4}
{'loss': 0.4399, 'learning_rate': 6.471795782165556e-05, 'epoch': 0.41}
{'loss': 0.4552, 'learning_rate': 6.469485357868754e-05, 'epoch': 0.41}
{'loss': 0.4529, 'learning_rate': 6.467174590088681e-05, 'epoch': 0.41}
 40%|      | 2631/6500 [4:58:54<7:39:47,  7.13s/it]                                                        40%|      | 2631/6500 [4:58:54<7:39:47,  7.13s/it] 40%|      | 2632/6500 [4:59:01<7:28:41,  6.96s/it]                                                        40%|      | 2632/6500 [4:59:01<7:28:41,  6.96s/it] 41%|      | 2633/6500 [4:59:07<7:20:40,  6.84s/it]                                                        41%|      | 2633/6500 [4:59:07<7:20:40,  6.84s/it] 41%|      | 2634/6500 [4:59:14<7:14:51,  6.75s/it]                                                        41%|      | 2634/6500 [4:59:14<7:14:51,  6.75s/it] 41%|      | 2635/6500 [4:59:21<7:27:51,  6.95s/it]                                                        41%|      | 2635/6500 [4:59:21<7:27:51,  6.95s/it] 41%|      | 2636/6500 [4:59:28<7:20:13,  6.84s/it]                                  {'loss': 0.4716, 'learning_rate': 6.46486347936547e-05, 'epoch': 0.41}
{'loss': 0.4621, 'learning_rate': 6.462552026239328e-05, 'epoch': 0.41}
{'loss': 0.443, 'learning_rate': 6.46024023125054e-05, 'epoch': 0.41}
{'loss': 0.4579, 'learning_rate': 6.457928094939478e-05, 'epoch': 0.41}
{'loss': 0.4397, 'learning_rate': 6.455615617846588e-05, 'epoch': 0.41}
                      41%|      | 2636/6500 [4:59:28<7:20:13,  6.84s/it] 41%|      | 2637/6500 [4:59:34<7:14:28,  6.75s/it]                                                        41%|      | 2637/6500 [4:59:34<7:14:28,  6.75s/it] 41%|      | 2638/6500 [4:59:41<7:10:42,  6.69s/it]                                                        41%|      | 2638/6500 [4:59:41<7:10:42,  6.69s/it] 41%|      | 2639/6500 [4:59:48<7:07:48,  6.65s/it]                                                        41%|      | 2639/6500 [4:59:48<7:07:48,  6.65s/it] 41%|      | 2640/6500 [4:59:54<7:05:46,  6.62s/it]                                                        41%|      | 2640/6500 [4:59:54<7:05:46,  6.62s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.843051552772522, 'eval_runtime': 1.484, 'eval_samples_per_second': 8.086, 'eval_steps_per_second': 2.022, 'epoch': 0.41}
                                                        41%|      | 2640/6500 [4:59:56<7:05:46,  6.62s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-2640
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2640
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2640
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2640
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2640
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2640/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2640/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2640/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2640/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.4928, 'learning_rate': 6.453302800512398e-05, 'epoch': 0.41}
{'loss': 0.4432, 'learning_rate': 6.450989643477515e-05, 'epoch': 0.41}
{'loss': 0.7272, 'learning_rate': 6.448676147282625e-05, 'epoch': 0.41}
{'loss': 0.4675, 'learning_rate': 6.446362312468492e-05, 'epoch': 0.41}
{'loss': 0.4427, 'learning_rate': 6.444048139575963e-05, 'epoch': 0.41}
 41%|      | 2641/6500 [5:00:03<7:39:41,  7.15s/it]                                                        41%|      | 2641/6500 [5:00:03<7:39:41,  7.15s/it] 41%|      | 2642/6500 [5:00:09<7:28:14,  6.97s/it]                                                        41%|      | 2642/6500 [5:00:09<7:28:14,  6.97s/it] 41%|      | 2643/6500 [5:00:16<7:19:49,  6.84s/it]                                                        41%|      | 2643/6500 [5:00:16<7:19:49,  6.84s/it] 41%|      | 2644/6500 [5:00:22<7:14:06,  6.75s/it]                                                        41%|      | 2644/6500 [5:00:22<7:14:06,  6.75s/it] 41%|      | 2645/6500 [5:00:29<7:10:02,  6.69s/it]                                                        41%|      | 2645/6500 [5:00:29<7:10:02,  6.69s/it] 41%|      | 2646/6500 [5:00:35<7:07:07,  6.65s/it]                                  {'loss': 0.4629, 'learning_rate': 6.441733629145961e-05, 'epoch': 0.41}
{'loss': 0.4323, 'learning_rate': 6.43941878171949e-05, 'epoch': 0.41}
{'loss': 0.4461, 'learning_rate': 6.437103597837631e-05, 'epoch': 0.41}
{'loss': 0.4527, 'learning_rate': 6.434788078041543e-05, 'epoch': 0.41}
{'loss': 0.454, 'learning_rate': 6.432472222872465e-05, 'epoch': 0.41}
                      41%|      | 2646/6500 [5:00:35<7:07:07,  6.65s/it] 41%|      | 2647/6500 [5:00:42<7:05:12,  6.62s/it]                                                        41%|      | 2647/6500 [5:00:42<7:05:12,  6.62s/it] 41%|      | 2648/6500 [5:00:48<7:03:43,  6.60s/it]                                                        41%|      | 2648/6500 [5:00:48<7:03:43,  6.60s/it] 41%|      | 2649/6500 [5:00:55<7:02:40,  6.59s/it]                                                        41%|      | 2649/6500 [5:00:55<7:02:40,  6.59s/it] 41%|      | 2650/6500 [5:01:01<7:02:07,  6.58s/it]                                                        41%|      | 2650/6500 [5:01:01<7:02:07,  6.58s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8480048775672913, 'eval_runtime': 1.486, 'eval_samples_per_second': 8.075, 'eval_steps_per_second': 2.019, 'epoch': 0.41}
                                                        41%|      | 2650/6500 [5:01:03<7:02:07,  6.58s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-2650
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2650I AM HERE AND I AM SAVING THE MODEL

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2650
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2650
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2650
the pytorch model path isthe pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2650/pytorch_model.bin 
/projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2650/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2650/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2650/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.4565, 'learning_rate': 6.430156032871715e-05, 'epoch': 0.41}
{'loss': 0.4595, 'learning_rate': 6.427839508580687e-05, 'epoch': 0.41}
{'loss': 0.4496, 'learning_rate': 6.425522650540857e-05, 'epoch': 0.41}
{'loss': 0.4539, 'learning_rate': 6.423205459293773e-05, 'epoch': 0.41}
{'loss': 0.4378, 'learning_rate': 6.420887935381067e-05, 'epoch': 0.41}
 41%|      | 2651/6500 [5:01:11<7:56:09,  7.42s/it]                                                        41%|      | 2651/6500 [5:01:11<7:56:09,  7.42s/it] 41%|      | 2652/6500 [5:01:17<7:39:19,  7.16s/it]                                                        41%|      | 2652/6500 [5:01:17<7:39:19,  7.16s/it] 41%|      | 2653/6500 [5:01:24<7:27:20,  6.98s/it]                                                        41%|      | 2653/6500 [5:01:24<7:27:20,  6.98s/it] 41%|      | 2654/6500 [5:01:31<7:18:57,  6.85s/it]                                                        41%|      | 2654/6500 [5:01:31<7:18:57,  6.85s/it] 41%|      | 2655/6500 [5:01:37<7:13:02,  6.76s/it]                                                        41%|      | 2655/6500 [5:01:37<7:13:02,  6.76s/it] 41%|      | 2656/6500 [5:01:44<7:08:52,  6.69s/it]                                  {'loss': 0.4909, 'learning_rate': 6.418570079344444e-05, 'epoch': 0.41}
{'loss': 0.4591, 'learning_rate': 6.416251891725692e-05, 'epoch': 0.41}
{'loss': 0.7251, 'learning_rate': 6.413933373066671e-05, 'epoch': 0.41}
{'loss': 0.4698, 'learning_rate': 6.411614523909321e-05, 'epoch': 0.41}
{'loss': 0.4433, 'learning_rate': 6.409295344795657e-05, 'epoch': 0.41}
                      41%|      | 2656/6500 [5:01:44<7:08:52,  6.69s/it] 41%|      | 2657/6500 [5:01:50<7:05:54,  6.65s/it]                                                        41%|      | 2657/6500 [5:01:50<7:05:54,  6.65s/it] 41%|      | 2658/6500 [5:01:57<7:03:56,  6.62s/it]                                                        41%|      | 2658/6500 [5:01:57<7:03:56,  6.62s/it] 41%|      | 2659/6500 [5:02:03<7:02:25,  6.60s/it]                                                        41%|      | 2659/6500 [5:02:03<7:02:25,  6.60s/it] 41%|      | 2660/6500 [5:02:10<7:01:14,  6.58s/it]                                                        41%|      | 2660/6500 [5:02:10<7:01:14,  6.58s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8496055006980896, 'eval_runtime': 1.4755, 'eval_samples_per_second': 8.133, 'eval_steps_per_second': 2.033, 'epoch': 0.41}
                                                        41%|      | 2660/6500 [5:02:11<7:01:14,  6.58s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-2660
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2660
/projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2660
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2660
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2660
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2660/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2660/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2660/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2660/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.4572, 'learning_rate': 6.406975836267776e-05, 'epoch': 0.41}
{'loss': 0.439, 'learning_rate': 6.404655998867848e-05, 'epoch': 0.41}
{'loss': 0.4521, 'learning_rate': 6.40233583313812e-05, 'epoch': 0.41}
{'loss': 0.4488, 'learning_rate': 6.400015339620917e-05, 'epoch': 0.41}
{'loss': 0.4698, 'learning_rate': 6.397694518858643e-05, 'epoch': 0.41}
 41%|      | 2661/6500 [5:02:18<7:34:57,  7.11s/it]                                                        41%|      | 2661/6500 [5:02:18<7:34:57,  7.11s/it] 41%|      | 2662/6500 [5:02:25<7:24:10,  6.94s/it]                                                        41%|      | 2662/6500 [5:02:25<7:24:10,  6.94s/it] 41%|      | 2663/6500 [5:02:31<7:16:31,  6.83s/it]                                                        41%|      | 2663/6500 [5:02:31<7:16:31,  6.83s/it] 41%|      | 2664/6500 [5:02:38<7:11:14,  6.75s/it]                                                        41%|      | 2664/6500 [5:02:38<7:11:14,  6.75s/it] 41%|      | 2665/6500 [5:02:44<7:07:44,  6.69s/it]                                                        41%|      | 2665/6500 [5:02:44<7:07:44,  6.69s/it] 41%|      | 2666/6500 [5:02:51<7:04:53,  6.65s/it]                                  {'loss': 0.4517, 'learning_rate': 6.39537337139377e-05, 'epoch': 0.41}
{'loss': 0.4588, 'learning_rate': 6.393051897768858e-05, 'epoch': 0.41}
{'loss': 0.4498, 'learning_rate': 6.390730098526533e-05, 'epoch': 0.41}
{'loss': 0.4517, 'learning_rate': 6.388407974209505e-05, 'epoch': 0.41}
{'loss': 0.4374, 'learning_rate': 6.386085525360553e-05, 'epoch': 0.41}
                      41%|      | 2666/6500 [5:02:51<7:04:53,  6.65s/it] 41%|      | 2667/6500 [5:02:58<7:14:45,  6.81s/it]                                                        41%|      | 2667/6500 [5:02:58<7:14:45,  6.81s/it] 41%|      | 2668/6500 [5:03:05<7:09:46,  6.73s/it]                                                        41%|      | 2668/6500 [5:03:05<7:09:46,  6.73s/it] 41%|      | 2669/6500 [5:03:11<7:06:24,  6.68s/it]                                                        41%|      | 2669/6500 [5:03:11<7:06:24,  6.68s/it] 41%|      | 2670/6500 [5:03:18<7:04:06,  6.64s/it]                                                        41%|      | 2670/6500 [5:03:18<7:04:06,  6.64s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8429707884788513, 'eval_runtime': 1.4923, 'eval_samples_per_second': 8.041, 'eval_steps_per_second': 2.01, 'epoch': 0.41}
                                                        41%|      | 2670/6500 [5:03:19<7:04:06,  6.64s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-2670
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2670
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2670
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2670
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2670
the pytorch model path isthe pytorch model path isthe pytorch model path is  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2670/pytorch_model.bin/projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2670/pytorch_model.bin

 /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2670/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2670/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.4865, 'learning_rate': 6.383762752522539e-05, 'epoch': 0.41}
{'loss': 0.4488, 'learning_rate': 6.381439656238393e-05, 'epoch': 0.41}
{'loss': 0.7314, 'learning_rate': 6.379116237051127e-05, 'epoch': 0.41}
{'loss': 0.4624, 'learning_rate': 6.376792495503828e-05, 'epoch': 0.41}
{'loss': 0.4552, 'learning_rate': 6.374468432139652e-05, 'epoch': 0.41}
 41%|      | 2671/6500 [5:03:26<7:37:29,  7.17s/it]                                                        41%|      | 2671/6500 [5:03:26<7:37:29,  7.17s/it] 41%|      | 2672/6500 [5:03:33<7:25:26,  6.98s/it]                                                        41%|      | 2672/6500 [5:03:33<7:25:26,  6.98s/it] 41%|      | 2673/6500 [5:03:39<7:16:54,  6.85s/it]                                                        41%|      | 2673/6500 [5:03:39<7:16:54,  6.85s/it] 41%|      | 2674/6500 [5:03:46<7:11:08,  6.76s/it]                                                        41%|      | 2674/6500 [5:03:46<7:11:08,  6.76s/it] 41%|      | 2675/6500 [5:03:52<7:06:38,  6.69s/it]                                                        41%|      | 2675/6500 [5:03:52<7:06:38,  6.69s/it] 41%|      | 2676/6500 [5:03:59<7:03:56,  6.65s/it]                                  {'loss': 0.4428, 'learning_rate': 6.372144047501837e-05, 'epoch': 0.41}
{'loss': 0.4377, 'learning_rate': 6.369819342133694e-05, 'epoch': 0.41}
{'loss': 0.4489, 'learning_rate': 6.367494316578609e-05, 'epoch': 0.41}
{'loss': 0.4506, 'learning_rate': 6.36516897138004e-05, 'epoch': 0.41}
{'loss': 0.4697, 'learning_rate': 6.362843307081527e-05, 'epoch': 0.41}
                      41%|      | 2676/6500 [5:03:59<7:03:56,  6.65s/it] 41%|      | 2677/6500 [5:04:05<7:01:54,  6.62s/it]                                                        41%|      | 2677/6500 [5:04:05<7:01:54,  6.62s/it] 41%|      | 2678/6500 [5:04:12<7:00:33,  6.60s/it]                                                        41%|      | 2678/6500 [5:04:12<7:00:33,  6.60s/it] 41%|      | 2679/6500 [5:04:19<6:59:28,  6.59s/it]                                                        41%|      | 2679/6500 [5:04:19<6:59:28,  6.59s/it] 41%|      | 2680/6500 [5:04:25<6:58:49,  6.58s/it]                                                        41%|      | 2680/6500 [5:04:25<6:58:49,  6.58s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8483714461326599, 'eval_runtime': 1.7264, 'eval_samples_per_second': 6.951, 'eval_steps_per_second': 1.738, 'epoch': 0.41}
                                                        41%|      | 2680/6500 [5:04:27<6:58:49,  6.58s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-2680
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2680
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2680
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2680
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2680
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2680/pytorch_model.bin
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2680/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2680/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2680/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.4611, 'learning_rate': 6.360517324226676e-05, 'epoch': 0.41}
{'loss': 0.4511, 'learning_rate': 6.358191023359172e-05, 'epoch': 0.41}
{'loss': 0.4603, 'learning_rate': 6.355864405022774e-05, 'epoch': 0.41}
{'loss': 0.4421, 'learning_rate': 6.353537469761315e-05, 'epoch': 0.41}
{'loss': 0.4444, 'learning_rate': 6.351210218118704e-05, 'epoch': 0.41}
 41%|      | 2681/6500 [5:04:34<7:39:29,  7.22s/it]                                                        41%|      | 2681/6500 [5:04:34<7:39:29,  7.22s/it] 41%|     | 2682/6500 [5:04:40<7:26:41,  7.02s/it]                                                        41%|     | 2682/6500 [5:04:40<7:26:41,  7.02s/it] 41%|     | 2683/6500 [5:04:48<7:29:34,  7.07s/it]                                                        41%|     | 2683/6500 [5:04:48<7:29:34,  7.07s/it] 41%|     | 2684/6500 [5:04:54<7:19:42,  6.91s/it]                                                        41%|     | 2684/6500 [5:04:54<7:19:42,  6.91s/it] 41%|     | 2685/6500 [5:05:01<7:12:52,  6.81s/it]                                                        41%|     | 2685/6500 [5:05:01<7:12:52,  6.81s/it] 41%|     | 2686/6500 [5:05:07<7:08:01,  6.73s/it]                {'loss': 0.4757, 'learning_rate': 6.34888265063892e-05, 'epoch': 0.41}
{'loss': 0.4525, 'learning_rate': 6.346554767866017e-05, 'epoch': 0.41}
{'loss': 0.7371, 'learning_rate': 6.344226570344123e-05, 'epoch': 0.41}
{'loss': 0.4432, 'learning_rate': 6.341898058617442e-05, 'epoch': 0.41}
{'loss': 0.4638, 'learning_rate': 6.339569233230249e-05, 'epoch': 0.41}
                                        41%|     | 2686/6500 [5:05:07<7:08:01,  6.73s/it] 41%|     | 2687/6500 [5:05:14<7:04:35,  6.68s/it]                                                        41%|     | 2687/6500 [5:05:14<7:04:35,  6.68s/it] 41%|     | 2688/6500 [5:05:20<7:01:55,  6.64s/it]                                                        41%|     | 2688/6500 [5:05:20<7:01:55,  6.64s/it] 41%|     | 2689/6500 [5:05:27<7:00:09,  6.62s/it]                                                        41%|     | 2689/6500 [5:05:27<7:00:09,  6.62s/it] 41%|     | 2690/6500 [5:05:33<6:58:55,  6.60s/it]                                                        41%|     | 2690/6500 [5:05:33<6:58:55,  6.60s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8493188619613647, 'eval_runtime': 1.4978, 'eval_samples_per_second': 8.012, 'eval_steps_per_second': 2.003, 'epoch': 0.41}
                                                        41%|     | 2690/6500 [5:05:35<6:58:55,  6.60s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-2690
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2690
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2690
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2690
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2690
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2690/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2690/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2690/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2690/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.4394, 'learning_rate': 6.337240094726893e-05, 'epoch': 0.41}
{'loss': 0.4363, 'learning_rate': 6.334910643651794e-05, 'epoch': 0.41}
{'loss': 0.4393, 'learning_rate': 6.33258088054945e-05, 'epoch': 0.41}
{'loss': 0.4514, 'learning_rate': 6.330250805964425e-05, 'epoch': 0.41}
{'loss': 0.4647, 'learning_rate': 6.327920420441365e-05, 'epoch': 0.41}
 41%|     | 2691/6500 [5:05:42<7:32:25,  7.13s/it]                                                        41%|     | 2691/6500 [5:05:42<7:32:25,  7.13s/it] 41%|     | 2692/6500 [5:05:48<7:21:41,  6.96s/it]                                                        41%|     | 2692/6500 [5:05:48<7:21:41,  6.96s/it] 41%|     | 2693/6500 [5:05:55<7:13:47,  6.84s/it]                                                        41%|     | 2693/6500 [5:05:55<7:13:47,  6.84s/it] 41%|     | 2694/6500 [5:06:01<7:08:12,  6.75s/it]                                                        41%|     | 2694/6500 [5:06:01<7:08:12,  6.75s/it] 41%|     | 2695/6500 [5:06:08<7:04:36,  6.70s/it]                                                        41%|     | 2695/6500 [5:06:08<7:04:36,  6.70s/it] 41%|     | 2696/6500 [5:06:15<7:01:57,  6.66s/it]            {'loss': 0.453, 'learning_rate': 6.325589724524978e-05, 'epoch': 0.41}
{'loss': 0.444, 'learning_rate': 6.323258718760055e-05, 'epoch': 0.41}
{'loss': 0.4649, 'learning_rate': 6.32092740369145e-05, 'epoch': 0.42}
{'loss': 0.4475, 'learning_rate': 6.318595779864098e-05, 'epoch': 0.42}
{'loss': 0.4558, 'learning_rate': 6.316263847822997e-05, 'epoch': 0.42}
                                            41%|     | 2696/6500 [5:06:15<7:01:57,  6.66s/it] 41%|     | 2697/6500 [5:06:21<6:59:47,  6.62s/it]                                                        41%|     | 2697/6500 [5:06:21<6:59:47,  6.62s/it] 42%|     | 2698/6500 [5:06:28<6:58:19,  6.60s/it]                                                        42%|     | 2698/6500 [5:06:28<6:58:19,  6.60s/it] 42%|     | 2699/6500 [5:06:35<7:13:54,  6.85s/it]                                                        42%|     | 2699/6500 [5:06:35<7:13:54,  6.85s/it] 42%|     | 2700/6500 [5:06:42<7:08:26,  6.76s/it]                                                        42%|     | 2700/6500 [5:06:42<7:08:26,  6.76s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8436151742935181, 'eval_runtime': 1.4802, 'eval_samples_per_second': 8.107, 'eval_steps_per_second': 2.027, 'epoch': 0.42}
                                                        42%|     | 2700/6500 [5:06:43<7:08:26,  6.76s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-2700
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2700
 /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2700
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2700
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2700
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2700/pytorch_model.bin
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2700/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2700/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2700/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.4683, 'learning_rate': 6.313931608113226e-05, 'epoch': 0.42}
{'loss': 0.4578, 'learning_rate': 6.311599061279932e-05, 'epoch': 0.42}
{'loss': 0.736, 'learning_rate': 6.30926620786833e-05, 'epoch': 0.42}
{'loss': 0.4401, 'learning_rate': 6.30693304842371e-05, 'epoch': 0.42}
{'loss': 0.4626, 'learning_rate': 6.30459958349144e-05, 'epoch': 0.42}
 42%|     | 2701/6500 [5:06:50<7:38:43,  7.24s/it]                                                        42%|     | 2701/6500 [5:06:50<7:38:43,  7.24s/it] 42%|     | 2702/6500 [5:06:57<7:25:33,  7.04s/it]                                                        42%|     | 2702/6500 [5:06:57<7:25:33,  7.04s/it] 42%|     | 2703/6500 [5:07:03<7:16:13,  6.89s/it]                                                        42%|     | 2703/6500 [5:07:03<7:16:13,  6.89s/it] 42%|     | 2704/6500 [5:07:10<7:09:20,  6.79s/it]                                                        42%|     | 2704/6500 [5:07:10<7:09:20,  6.79s/it] 42%|     | 2705/6500 [5:07:16<7:04:46,  6.72s/it]                                                        42%|     | 2705/6500 [5:07:16<7:04:46,  6.72s/it] 42%|     | 2706/6500 [5:07:23<7:01:31,  6.67s/it]            {'loss': 0.433, 'learning_rate': 6.302265813616947e-05, 'epoch': 0.42}
{'loss': 0.4457, 'learning_rate': 6.299931739345741e-05, 'epoch': 0.42}
{'loss': 0.45, 'learning_rate': 6.297597361223392e-05, 'epoch': 0.42}
{'loss': 0.4459, 'learning_rate': 6.29526267979555e-05, 'epoch': 0.42}
{'loss': 0.4667, 'learning_rate': 6.292927695607933e-05, 'epoch': 0.42}
                                            42%|     | 2706/6500 [5:07:23<7:01:31,  6.67s/it] 42%|     | 2707/6500 [5:07:29<6:59:08,  6.63s/it]                                                        42%|     | 2707/6500 [5:07:29<6:59:08,  6.63s/it] 42%|     | 2708/6500 [5:07:36<6:57:31,  6.61s/it]                                                        42%|     | 2708/6500 [5:07:36<6:57:31,  6.61s/it] 42%|     | 2709/6500 [5:07:42<6:56:39,  6.59s/it]                                                        42%|     | 2709/6500 [5:07:42<6:56:39,  6.59s/it] 42%|     | 2710/6500 [5:07:49<6:55:42,  6.58s/it]                                                        42%|     | 2710/6500 [5:07:49<6:55:42,  6.58s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8507663607597351, 'eval_runtime': 1.4847, 'eval_samples_per_second': 8.082, 'eval_steps_per_second': 2.021, 'epoch': 0.42}
                                                        42%|     | 2710/6500 [5:07:51<6:55:42,  6.58s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-2710
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2710I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2710

I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2710
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2710
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2710/pytorch_model.bin
the pytorch model path isthe pytorch model path is  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2710/pytorch_model.bin/projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2710/pytorch_model.bin

the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2710/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.468, 'learning_rate': 6.290592409206327e-05, 'epoch': 0.42}
{'loss': 0.444, 'learning_rate': 6.288256821136594e-05, 'epoch': 0.42}
{'loss': 0.4589, 'learning_rate': 6.285920931944661e-05, 'epoch': 0.42}
{'loss': 0.4335, 'learning_rate': 6.283584742176528e-05, 'epoch': 0.42}
{'loss': 0.481, 'learning_rate': 6.281248252378267e-05, 'epoch': 0.42}
 42%|     | 2711/6500 [5:07:57<7:28:55,  7.11s/it]                                                        42%|     | 2711/6500 [5:07:57<7:28:55,  7.11s/it] 42%|     | 2712/6500 [5:08:04<7:18:15,  6.94s/it]                                                        42%|     | 2712/6500 [5:08:04<7:18:15,  6.94s/it] 42%|     | 2713/6500 [5:08:10<7:10:43,  6.82s/it]                                                        42%|     | 2713/6500 [5:08:10<7:10:43,  6.82s/it] 42%|     | 2714/6500 [5:08:17<7:05:25,  6.74s/it]                                                        42%|     | 2714/6500 [5:08:17<7:05:25,  6.74s/it] 42%|     | 2715/6500 [5:08:24<7:18:09,  6.95s/it]                                                        42%|     | 2715/6500 [5:08:24<7:18:09,  6.95s/it] 42%|     | 2716/6500 [5:08:31<7:10:37,  6.83s/it]            {'loss': 0.4343, 'learning_rate': 6.278911463096016e-05, 'epoch': 0.42}
{'loss': 0.7299, 'learning_rate': 6.276574374875986e-05, 'epoch': 0.42}
{'loss': 0.4627, 'learning_rate': 6.274236988264459e-05, 'epoch': 0.42}
{'loss': 0.4409, 'learning_rate': 6.271899303807783e-05, 'epoch': 0.42}
{'loss': 0.4559, 'learning_rate': 6.269561322052378e-05, 'epoch': 0.42}
                                            42%|     | 2716/6500 [5:08:31<7:10:37,  6.83s/it] 42%|     | 2717/6500 [5:08:38<7:05:20,  6.75s/it]                                                        42%|     | 2717/6500 [5:08:38<7:05:20,  6.75s/it] 42%|     | 2718/6500 [5:08:44<7:01:47,  6.69s/it]                                                        42%|     | 2718/6500 [5:08:44<7:01:47,  6.69s/it] 42%|     | 2719/6500 [5:08:51<6:59:13,  6.65s/it]                                                        42%|     | 2719/6500 [5:08:51<6:59:13,  6.65s/it] 42%|     | 2720/6500 [5:08:57<6:57:29,  6.63s/it]                                                        42%|     | 2720/6500 [5:08:57<6:57:29,  6.63s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.852817177772522, 'eval_runtime': 1.4823, 'eval_samples_per_second': 8.095, 'eval_steps_per_second': 2.024, 'epoch': 0.42}
                                                        42%|     | 2720/6500 [5:08:59<6:57:29,  6.63s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-2720
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  I AM HERE AND I AM SAVING THE MODEL
/projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2720the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2720

I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2720
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2720
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2720/pytorch_model.bin
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2720/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2720/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2720/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.4304, 'learning_rate': 6.267223043544732e-05, 'epoch': 0.42}
{'loss': 0.4346, 'learning_rate': 6.264884468831405e-05, 'epoch': 0.42}
{'loss': 0.4506, 'learning_rate': 6.262545598459025e-05, 'epoch': 0.42}
{'loss': 0.4468, 'learning_rate': 6.260206432974288e-05, 'epoch': 0.42}
{'loss': 0.4576, 'learning_rate': 6.257866972923956e-05, 'epoch': 0.42}
 42%|     | 2721/6500 [5:09:06<7:29:31,  7.14s/it]                                                        42%|     | 2721/6500 [5:09:06<7:29:31,  7.14s/it] 42%|     | 2722/6500 [5:09:12<7:18:08,  6.96s/it]                                                        42%|     | 2722/6500 [5:09:12<7:18:08,  6.96s/it] 42%|     | 2723/6500 [5:09:19<7:10:09,  6.83s/it]                                                        42%|     | 2723/6500 [5:09:19<7:10:09,  6.83s/it] 42%|     | 2724/6500 [5:09:25<7:04:44,  6.75s/it]                                                        42%|     | 2724/6500 [5:09:25<7:04:44,  6.75s/it] 42%|     | 2725/6500 [5:09:32<7:00:43,  6.69s/it]                                                        42%|     | 2725/6500 [5:09:32<7:00:43,  6.69s/it] 42%|     | 2726/6500 [5:09:38<6:58:01,  6.65s/it]            {'loss': 0.4549, 'learning_rate': 6.25552721885487e-05, 'epoch': 0.42}
{'loss': 0.4384, 'learning_rate': 6.25318717131393e-05, 'epoch': 0.42}
{'loss': 0.4501, 'learning_rate': 6.250846830848108e-05, 'epoch': 0.42}
{'loss': 0.431, 'learning_rate': 6.248506198004445e-05, 'epoch': 0.42}
{'loss': 0.4836, 'learning_rate': 6.246165273330049e-05, 'epoch': 0.42}
                                            42%|     | 2726/6500 [5:09:38<6:58:01,  6.65s/it] 42%|     | 2727/6500 [5:09:45<6:55:58,  6.61s/it]                                                        42%|     | 2727/6500 [5:09:45<6:55:58,  6.61s/it] 42%|     | 2728/6500 [5:09:51<6:54:43,  6.60s/it]                                                        42%|     | 2728/6500 [5:09:51<6:54:43,  6.60s/it] 42%|     | 2729/6500 [5:09:58<6:53:23,  6.58s/it]                                                        42%|     | 2729/6500 [5:09:58<6:53:23,  6.58s/it] 42%|     | 2730/6500 [5:10:04<6:52:41,  6.57s/it]                                                        42%|     | 2730/6500 [5:10:04<6:52:41,  6.57s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8470001816749573, 'eval_runtime': 1.4808, 'eval_samples_per_second': 8.104, 'eval_steps_per_second': 2.026, 'epoch': 0.42}
                                                        42%|     | 2730/6500 [5:10:06<6:52:41,  6.57s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-2730
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2730
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2730
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2730
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2730
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2730/pytorch_model.bin
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2730/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2730/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2730/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.4365, 'learning_rate': 6.243824057372098e-05, 'epoch': 0.42}
{'loss': 0.7297, 'learning_rate': 6.241482550677833e-05, 'epoch': 0.42}
{'loss': 0.4665, 'learning_rate': 6.239140753794574e-05, 'epoch': 0.42}
{'loss': 0.4281, 'learning_rate': 6.236798667269695e-05, 'epoch': 0.42}
{'loss': 0.4536, 'learning_rate': 6.23445629165065e-05, 'epoch': 0.42}
 42%|     | 2731/6500 [5:10:13<7:26:08,  7.10s/it]                                                        42%|     | 2731/6500 [5:10:13<7:26:08,  7.10s/it] 42%|     | 2732/6500 [5:10:20<7:32:17,  7.20s/it]                                                        42%|     | 2732/6500 [5:10:20<7:32:17,  7.20s/it] 42%|     | 2733/6500 [5:10:27<7:20:11,  7.01s/it]                                                        42%|     | 2733/6500 [5:10:27<7:20:11,  7.01s/it] 42%|     | 2734/6500 [5:10:33<7:11:19,  6.87s/it]                                                        42%|     | 2734/6500 [5:10:33<7:11:19,  6.87s/it] 42%|     | 2735/6500 [5:10:40<7:05:13,  6.78s/it]                                                        42%|     | 2735/6500 [5:10:40<7:05:13,  6.78s/it] 42%|     | 2736/6500 [5:10:46<7:00:42,  6.71s/it]            {'loss': 0.426, 'learning_rate': 6.23211362748495e-05, 'epoch': 0.42}
{'loss': 0.4376, 'learning_rate': 6.229770675320184e-05, 'epoch': 0.42}
{'loss': 0.4502, 'learning_rate': 6.227427435703997e-05, 'epoch': 0.42}
{'loss': 0.454, 'learning_rate': 6.225083909184109e-05, 'epoch': 0.42}
{'loss': 0.4516, 'learning_rate': 6.222740096308309e-05, 'epoch': 0.42}
                                            42%|     | 2736/6500 [5:10:46<7:00:42,  6.71s/it] 42%|     | 2737/6500 [5:10:53<6:57:16,  6.65s/it]                                                        42%|     | 2737/6500 [5:10:53<6:57:16,  6.65s/it] 42%|     | 2738/6500 [5:11:00<6:55:10,  6.62s/it]                                                        42%|     | 2738/6500 [5:11:00<6:55:10,  6.62s/it] 42%|     | 2739/6500 [5:11:06<6:53:48,  6.60s/it]                                                        42%|     | 2739/6500 [5:11:06<6:53:48,  6.60s/it] 42%|     | 2740/6500 [5:11:13<6:52:33,  6.58s/it]                                                        42%|     | 2740/6500 [5:11:13<6:52:33,  6.58s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8486589193344116, 'eval_runtime': 1.4802, 'eval_samples_per_second': 8.107, 'eval_steps_per_second': 2.027, 'epoch': 0.42}
                                                        42%|     | 2740/6500 [5:11:14<6:52:33,  6.58s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-2740
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2740
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2740
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2740
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2740
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2740/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2740/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2740/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2740/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.455, 'learning_rate': 6.220395997624443e-05, 'epoch': 0.42}
{'loss': 0.4432, 'learning_rate': 6.218051613680435e-05, 'epoch': 0.42}
{'loss': 0.4449, 'learning_rate': 6.215706945024267e-05, 'epoch': 0.42}
{'loss': 0.4337, 'learning_rate': 6.213361992203991e-05, 'epoch': 0.42}
{'loss': 0.4796, 'learning_rate': 6.211016755767729e-05, 'epoch': 0.42}
 42%|     | 2741/6500 [5:11:21<7:26:19,  7.12s/it]                                                        42%|     | 2741/6500 [5:11:21<7:26:19,  7.12s/it] 42%|     | 2742/6500 [5:11:28<7:15:27,  6.95s/it]                                                        42%|     | 2742/6500 [5:11:28<7:15:27,  6.95s/it] 42%|     | 2743/6500 [5:11:34<7:08:04,  6.84s/it]                                                        42%|     | 2743/6500 [5:11:34<7:08:04,  6.84s/it] 42%|     | 2744/6500 [5:11:41<7:02:44,  6.75s/it]                                                        42%|     | 2744/6500 [5:11:41<7:02:44,  6.75s/it] 42%|     | 2745/6500 [5:11:47<6:58:54,  6.69s/it]                                                        42%|     | 2745/6500 [5:11:47<6:58:54,  6.69s/it] 42%|     | 2746/6500 [5:11:54<6:55:52,  6.65s/it]            {'loss': 0.4447, 'learning_rate': 6.208671236263663e-05, 'epoch': 0.42}
{'loss': 0.7241, 'learning_rate': 6.206325434240043e-05, 'epoch': 0.42}
{'loss': 0.4617, 'learning_rate': 6.203979350245188e-05, 'epoch': 0.42}
{'loss': 0.4399, 'learning_rate': 6.20163298482748e-05, 'epoch': 0.42}
{'loss': 0.4423, 'learning_rate': 6.199286338535369e-05, 'epoch': 0.42}
                                            42%|     | 2746/6500 [5:11:54<6:55:52,  6.65s/it] 42%|     | 2747/6500 [5:12:00<6:53:50,  6.62s/it]                                                        42%|     | 2747/6500 [5:12:00<6:53:50,  6.62s/it] 42%|     | 2748/6500 [5:12:08<7:04:36,  6.79s/it]                                                        42%|     | 2748/6500 [5:12:08<7:04:36,  6.79s/it] 42%|     | 2749/6500 [5:12:14<6:59:42,  6.71s/it]                                                        42%|     | 2749/6500 [5:12:14<6:59:42,  6.71s/it] 42%|     | 2750/6500 [5:12:21<6:56:39,  6.67s/it]                                                        42%|     | 2750/6500 [5:12:21<6:56:39,  6.67s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8544127345085144, 'eval_runtime': 1.5091, 'eval_samples_per_second': 7.952, 'eval_steps_per_second': 1.988, 'epoch': 0.42}
                                                        42%|     | 2750/6500 [5:12:22<6:56:39,  6.67s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-2750
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2750
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2750
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2750
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2750
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2750/pytorch_model.bin
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2750/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2750/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2750/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.436, 'learning_rate': 6.196939411917369e-05, 'epoch': 0.42}
{'loss': 0.4436, 'learning_rate': 6.194592205522056e-05, 'epoch': 0.42}
{'loss': 0.4508, 'learning_rate': 6.192244719898079e-05, 'epoch': 0.42}
{'loss': 0.4622, 'learning_rate': 6.189896955594148e-05, 'epoch': 0.42}
{'loss': 0.4539, 'learning_rate': 6.187548913159039e-05, 'epoch': 0.42}
 42%|     | 2751/6500 [5:12:29<7:28:54,  7.18s/it]                                                        42%|     | 2751/6500 [5:12:29<7:28:54,  7.18s/it] 42%|     | 2752/6500 [5:12:36<7:16:47,  6.99s/it]                                                        42%|     | 2752/6500 [5:12:36<7:16:47,  6.99s/it] 42%|     | 2753/6500 [5:12:42<7:08:13,  6.86s/it]                                                        42%|     | 2753/6500 [5:12:42<7:08:13,  6.86s/it] 42%|     | 2754/6500 [5:12:49<7:02:36,  6.77s/it]                                                        42%|     | 2754/6500 [5:12:49<7:02:36,  6.77s/it] 42%|     | 2755/6500 [5:12:55<6:58:13,  6.70s/it]                                                        42%|     | 2755/6500 [5:12:55<6:58:13,  6.70s/it] 42%|     | 2756/6500 [5:13:02<6:55:26,  6.66s/it]            {'loss': 0.4436, 'learning_rate': 6.185200593141593e-05, 'epoch': 0.42}
{'loss': 0.4504, 'learning_rate': 6.182851996090713e-05, 'epoch': 0.42}
{'loss': 0.442, 'learning_rate': 6.18050312255537e-05, 'epoch': 0.42}
{'loss': 0.435, 'learning_rate': 6.1781539730846e-05, 'epoch': 0.42}
{'loss': 0.4736, 'learning_rate': 6.175804548227502e-05, 'epoch': 0.42}
                                            42%|     | 2756/6500 [5:13:02<6:55:26,  6.66s/it] 42%|     | 2757/6500 [5:13:08<6:53:20,  6.63s/it]                                                        42%|     | 2757/6500 [5:13:08<6:53:20,  6.63s/it] 42%|     | 2758/6500 [5:13:15<6:52:01,  6.61s/it]                                                        42%|     | 2758/6500 [5:13:15<6:52:01,  6.61s/it] 42%|     | 2759/6500 [5:13:21<6:51:07,  6.59s/it]                                                        42%|     | 2759/6500 [5:13:21<6:51:07,  6.59s/it] 42%|     | 2760/6500 [5:13:28<6:50:18,  6.58s/it]                                                        42%|     | 2760/6500 [5:13:28<6:50:18,  6.58s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8489920496940613, 'eval_runtime': 1.7281, 'eval_samples_per_second': 6.944, 'eval_steps_per_second': 1.736, 'epoch': 0.42}
                                                        42%|     | 2760/6500 [5:13:30<6:50:18,  6.58s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-2760
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2760
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2760
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2760
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2760
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2760/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2760/pytorch_model.binthe pytorch model path is
 /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2760/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2760/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.445, 'learning_rate': 6.173454848533242e-05, 'epoch': 0.42}
{'loss': 0.7256, 'learning_rate': 6.171104874551044e-05, 'epoch': 0.42}
{'loss': 0.4517, 'learning_rate': 6.168754626830202e-05, 'epoch': 0.43}
{'loss': 0.4491, 'learning_rate': 6.166404105920073e-05, 'epoch': 0.43}
{'loss': 0.4271, 'learning_rate': 6.164053312370074e-05, 'epoch': 0.43}
 42%|     | 2761/6500 [5:13:37<7:27:39,  7.18s/it]                                                        42%|     | 2761/6500 [5:13:37<7:27:39,  7.18s/it] 42%|     | 2762/6500 [5:13:43<7:15:34,  6.99s/it]                                                        42%|     | 2762/6500 [5:13:43<7:15:34,  6.99s/it] 43%|     | 2763/6500 [5:13:50<7:07:29,  6.86s/it]                                                        43%|     | 2763/6500 [5:13:50<7:07:29,  6.86s/it] 43%|     | 2764/6500 [5:13:57<7:13:04,  6.96s/it]                                                        43%|     | 2764/6500 [5:13:57<7:13:04,  6.96s/it] 43%|     | 2765/6500 [5:14:03<7:05:43,  6.84s/it]                                                        43%|     | 2765/6500 [5:14:03<7:05:43,  6.84s/it] 43%|     | 2766/6500 [5:14:10<7:00:12,  6.75s/it]            {'loss': 0.4304, 'learning_rate': 6.161702246729692e-05, 'epoch': 0.43}
{'loss': 0.4341, 'learning_rate': 6.159350909548475e-05, 'epoch': 0.43}
{'loss': 0.4424, 'learning_rate': 6.156999301376031e-05, 'epoch': 0.43}
{'loss': 0.4612, 'learning_rate': 6.154647422762033e-05, 'epoch': 0.43}
{'loss': 0.4514, 'learning_rate': 6.152295274256222e-05, 'epoch': 0.43}
                                            43%|     | 2766/6500 [5:14:10<7:00:12,  6.75s/it] 43%|     | 2767/6500 [5:14:17<6:56:27,  6.69s/it]                                                        43%|     | 2767/6500 [5:14:17<6:56:27,  6.69s/it] 43%|     | 2768/6500 [5:14:23<6:54:00,  6.66s/it]                                                        43%|     | 2768/6500 [5:14:23<6:54:00,  6.66s/it] 43%|     | 2769/6500 [5:14:30<6:51:59,  6.63s/it]                                                        43%|     | 2769/6500 [5:14:30<6:51:59,  6.63s/it] 43%|     | 2770/6500 [5:14:36<6:50:44,  6.61s/it]                                                        43%|     | 2770/6500 [5:14:36<6:50:44,  6.61s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8496518135070801, 'eval_runtime': 1.4994, 'eval_samples_per_second': 8.003, 'eval_steps_per_second': 2.001, 'epoch': 0.43}
                                                        43%|     | 2770/6500 [5:14:38<6:50:44,  6.61s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-2770
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2770
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2770
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2770
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2770
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2770/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2770/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2770/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2770/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.4441, 'learning_rate': 6.149942856408396e-05, 'epoch': 0.43}
{'loss': 0.4573, 'learning_rate': 6.147590169768419e-05, 'epoch': 0.43}
{'loss': 0.4374, 'learning_rate': 6.145237214886219e-05, 'epoch': 0.43}
{'loss': 0.4445, 'learning_rate': 6.142883992311781e-05, 'epoch': 0.43}
{'loss': 0.4671, 'learning_rate': 6.14053050259516e-05, 'epoch': 0.43}
 43%|     | 2771/6500 [5:14:45<7:24:08,  7.15s/it]                                                        43%|     | 2771/6500 [5:14:45<7:24:08,  7.15s/it] 43%|     | 2772/6500 [5:14:51<7:12:58,  6.97s/it]                                                        43%|     | 2772/6500 [5:14:51<7:12:58,  6.97s/it] 43%|     | 2773/6500 [5:14:58<7:05:02,  6.84s/it]                                                        43%|     | 2773/6500 [5:14:58<7:05:02,  6.84s/it] 43%|     | 2774/6500 [5:15:04<6:59:36,  6.76s/it]                                                        43%|     | 2774/6500 [5:15:04<6:59:36,  6.76s/it] 43%|     | 2775/6500 [5:15:11<6:55:44,  6.70s/it]                                                        43%|     | 2775/6500 [5:15:11<6:55:44,  6.70s/it] 43%|     | 2776/6500 [5:15:17<6:53:07,  6.66s/it]            {'loss': 0.4439, 'learning_rate': 6.138176746286468e-05, 'epoch': 0.43}
{'loss': 0.7339, 'learning_rate': 6.135822723935882e-05, 'epoch': 0.43}
{'loss': 0.4353, 'learning_rate': 6.13346843609364e-05, 'epoch': 0.43}
{'loss': 0.4593, 'learning_rate': 6.131113883310041e-05, 'epoch': 0.43}
{'loss': 0.4246, 'learning_rate': 6.128759066135451e-05, 'epoch': 0.43}
                                            43%|     | 2776/6500 [5:15:17<6:53:07,  6.66s/it] 43%|     | 2777/6500 [5:15:24<6:50:57,  6.62s/it]                                                        43%|     | 2777/6500 [5:15:24<6:50:57,  6.62s/it] 43%|     | 2778/6500 [5:15:31<6:49:25,  6.60s/it]                                                        43%|     | 2778/6500 [5:15:31<6:49:25,  6.60s/it] 43%|     | 2779/6500 [5:15:37<6:48:26,  6.59s/it]                                                        43%|     | 2779/6500 [5:15:37<6:48:26,  6.59s/it] 43%|     | 2780/6500 [5:15:44<7:03:55,  6.84s/it]                                                        43%|     | 2780/6500 [5:15:44<7:03:55,  6.84s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8566515445709229, 'eval_runtime': 1.5009, 'eval_samples_per_second': 7.995, 'eval_steps_per_second': 1.999, 'epoch': 0.43}
                                                        43%|     | 2780/6500 [5:15:46<7:03:55,  6.84s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-2780
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2780
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2780
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2780
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2780
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2780/pytorch_model.bin
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2780/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2780/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2780/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.437, 'learning_rate': 6.126403985120292e-05, 'epoch': 0.43}
{'loss': 0.4411, 'learning_rate': 6.12404864081505e-05, 'epoch': 0.43}
{'loss': 0.4424, 'learning_rate': 6.121693033770274e-05, 'epoch': 0.43}
{'loss': 0.4621, 'learning_rate': 6.11933716453657e-05, 'epoch': 0.43}
{'loss': 0.4613, 'learning_rate': 6.116981033664609e-05, 'epoch': 0.43}
 43%|     | 2781/6500 [5:15:53<7:32:24,  7.30s/it]                                                        43%|     | 2781/6500 [5:15:53<7:32:24,  7.30s/it] 43%|     | 2782/6500 [5:15:59<7:18:11,  7.07s/it]                                                        43%|     | 2782/6500 [5:15:59<7:18:11,  7.07s/it] 43%|     | 2783/6500 [5:16:06<7:08:06,  6.91s/it]                                                        43%|     | 2783/6500 [5:16:06<7:08:06,  6.91s/it] 43%|     | 2784/6500 [5:16:12<7:01:13,  6.80s/it]                                                        43%|     | 2784/6500 [5:16:12<7:01:13,  6.80s/it] 43%|     | 2785/6500 [5:16:19<6:56:15,  6.72s/it]                                                        43%|     | 2785/6500 [5:16:19<6:56:15,  6.72s/it] 43%|     | 2786/6500 [5:16:26<6:52:46,  6.67s/it]            {'loss': 0.448, 'learning_rate': 6.114624641705122e-05, 'epoch': 0.43}
{'loss': 0.4499, 'learning_rate': 6.112267989208904e-05, 'epoch': 0.43}
{'loss': 0.4336, 'learning_rate': 6.109911076726806e-05, 'epoch': 0.43}
{'loss': 0.4758, 'learning_rate': 6.107553904809741e-05, 'epoch': 0.43}
{'loss': 0.4373, 'learning_rate': 6.105196474008686e-05, 'epoch': 0.43}
                                            43%|     | 2786/6500 [5:16:26<6:52:46,  6.67s/it] 43%|     | 2787/6500 [5:16:32<6:50:24,  6.63s/it]                                                        43%|     | 2787/6500 [5:16:32<6:50:24,  6.63s/it] 43%|     | 2788/6500 [5:16:39<6:48:39,  6.61s/it]                                                        43%|     | 2788/6500 [5:16:39<6:48:39,  6.61s/it] 43%|     | 2789/6500 [5:16:45<6:47:19,  6.59s/it]                                                        43%|     | 2789/6500 [5:16:45<6:47:19,  6.59s/it] 43%|     | 2790/6500 [5:16:52<6:46:26,  6.57s/it]                                                        43%|     | 2790/6500 [5:16:52<6:46:26,  6.57s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8462232351303101, 'eval_runtime': 1.4786, 'eval_samples_per_second': 8.116, 'eval_steps_per_second': 2.029, 'epoch': 0.43}
                                                        43%|     | 2790/6500 [5:16:53<6:46:26,  6.57s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-2790
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2790I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2790

I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2790
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2790
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2790/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2790/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2790/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2790/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.7078, 'learning_rate': 6.1028387848746737e-05, 'epoch': 0.43}
{'loss': 0.4733, 'learning_rate': 6.100480837958802e-05, 'epoch': 0.43}
{'loss': 0.4377, 'learning_rate': 6.098122633812225e-05, 'epoch': 0.43}
{'loss': 0.459, 'learning_rate': 6.095764172986159e-05, 'epoch': 0.43}
{'loss': 0.43, 'learning_rate': 6.09340545603188e-05, 'epoch': 0.43}
 43%|     | 2791/6500 [5:17:00<7:19:26,  7.11s/it]                                                        43%|     | 2791/6500 [5:17:00<7:19:26,  7.11s/it] 43%|     | 2792/6500 [5:17:07<7:08:47,  6.94s/it]                                                        43%|     | 2792/6500 [5:17:07<7:08:47,  6.94s/it] 43%|     | 2793/6500 [5:17:13<7:01:13,  6.82s/it]                                                        43%|     | 2793/6500 [5:17:13<7:01:13,  6.82s/it] 43%|     | 2794/6500 [5:17:20<6:55:59,  6.74s/it]                                                        43%|     | 2794/6500 [5:17:20<6:55:59,  6.74s/it] 43%|     | 2795/6500 [5:17:26<6:52:12,  6.68s/it]                                                        43%|     | 2795/6500 [5:17:26<6:52:12,  6.68s/it] 43%|     | 2796/6500 [5:17:34<7:05:30,  6.89s/it]            {'loss': 0.4354, 'learning_rate': 6.091046483500723e-05, 'epoch': 0.43}
{'loss': 0.4404, 'learning_rate': 6.0886872559440845e-05, 'epoch': 0.43}
{'loss': 0.4493, 'learning_rate': 6.086327773913419e-05, 'epoch': 0.43}
{'loss': 0.465, 'learning_rate': 6.083968037960243e-05, 'epoch': 0.43}
{'loss': 0.4519, 'learning_rate': 6.081608048636127e-05, 'epoch': 0.43}
                                            43%|     | 2796/6500 [5:17:34<7:05:30,  6.89s/it] 43%|     | 2797/6500 [5:17:40<6:59:10,  6.79s/it]                                                        43%|     | 2797/6500 [5:17:40<6:59:10,  6.79s/it] 43%|     | 2798/6500 [5:17:47<6:54:50,  6.72s/it]                                                        43%|     | 2798/6500 [5:17:47<6:54:50,  6.72s/it] 43%|     | 2799/6500 [5:17:53<6:51:19,  6.67s/it]                                                        43%|     | 2799/6500 [5:17:53<6:51:19,  6.67s/it] 43%|     | 2800/6500 [5:18:00<6:49:07,  6.63s/it]                                                        43%|     | 2800/6500 [5:18:00<6:49:07,  6.63s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8473614454269409, 'eval_runtime': 1.4857, 'eval_samples_per_second': 8.077, 'eval_steps_per_second': 2.019, 'epoch': 0.43}
                                                        43%|     | 2800/6500 [5:18:01<6:49:07,  6.63s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-2800
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2800
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2800
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2800
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2800
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2800/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2800/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2800/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2800/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.4361, 'learning_rate': 6.079247806492707e-05, 'epoch': 0.43}
{'loss': 0.452, 'learning_rate': 6.076887312081673e-05, 'epoch': 0.43}
{'loss': 0.4343, 'learning_rate': 6.074526565954778e-05, 'epoch': 0.43}
{'loss': 0.4787, 'learning_rate': 6.072165568663831e-05, 'epoch': 0.43}
{'loss': 0.4396, 'learning_rate': 6.069804320760703e-05, 'epoch': 0.43}
 43%|     | 2801/6500 [5:18:08<7:21:42,  7.16s/it]                                                        43%|     | 2801/6500 [5:18:08<7:21:42,  7.16s/it] 43%|     | 2802/6500 [5:18:15<7:10:33,  6.99s/it]                                                        43%|     | 2802/6500 [5:18:15<7:10:33,  6.99s/it] 43%|     | 2803/6500 [5:18:21<7:02:31,  6.86s/it]                                                        43%|     | 2803/6500 [5:18:21<7:02:31,  6.86s/it] 43%|     | 2804/6500 [5:18:28<6:56:48,  6.77s/it]                                                        43%|     | 2804/6500 [5:18:28<6:56:48,  6.77s/it] 43%|     | 2805/6500 [5:18:34<6:52:39,  6.70s/it]                                                        43%|     | 2805/6500 [5:18:34<6:52:39,  6.70s/it] 43%|     | 2806/6500 [5:18:41<6:49:27,  6.65s/it]            {'loss': 0.719, 'learning_rate': 6.067442822797318e-05, 'epoch': 0.43}
{'loss': 0.4587, 'learning_rate': 6.065081075325663e-05, 'epoch': 0.43}
{'loss': 0.4309, 'learning_rate': 6.0627190788977825e-05, 'epoch': 0.43}
{'loss': 0.4476, 'learning_rate': 6.060356834065779e-05, 'epoch': 0.43}
{'loss': 0.4263, 'learning_rate': 6.057994341381813e-05, 'epoch': 0.43}
                                            43%|     | 2806/6500 [5:18:41<6:49:27,  6.65s/it] 43%|     | 2807/6500 [5:18:48<6:47:23,  6.62s/it]                                                        43%|     | 2807/6500 [5:18:48<6:47:23,  6.62s/it] 43%|     | 2808/6500 [5:18:54<6:45:48,  6.59s/it]                                                        43%|     | 2808/6500 [5:18:54<6:45:48,  6.59s/it] 43%|     | 2809/6500 [5:19:01<6:44:42,  6.58s/it]                                                        43%|     | 2809/6500 [5:19:01<6:44:42,  6.58s/it] 43%|     | 2810/6500 [5:19:07<6:44:00,  6.57s/it]                                                        43%|     | 2810/6500 [5:19:07<6:44:00,  6.57s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8563656806945801, 'eval_runtime': 1.4852, 'eval_samples_per_second': 8.079, 'eval_steps_per_second': 2.02, 'epoch': 0.43}
                                                        43%|     | 2810/6500 [5:19:09<6:44:00,  6.57s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-2810
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2810
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2810
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2810
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2810
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2810/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2810/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2810/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2810/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.4323, 'learning_rate': 6.055631601398103e-05, 'epoch': 0.43}
{'loss': 0.4496, 'learning_rate': 6.0532686146669236e-05, 'epoch': 0.43}
{'loss': 0.4485, 'learning_rate': 6.050905381740609e-05, 'epoch': 0.43}
{'loss': 0.4411, 'learning_rate': 6.0485419031715516e-05, 'epoch': 0.43}
{'loss': 0.4475, 'learning_rate': 6.0461781795122e-05, 'epoch': 0.43}
 43%|     | 2811/6500 [5:19:16<7:17:58,  7.12s/it]                                                        43%|     | 2811/6500 [5:19:16<7:17:58,  7.12s/it] 43%|     | 2812/6500 [5:19:23<7:23:23,  7.21s/it]                                                        43%|     | 2812/6500 [5:19:23<7:23:23,  7.21s/it] 43%|     | 2813/6500 [5:19:30<7:11:24,  7.02s/it]                                                        43%|     | 2813/6500 [5:19:30<7:11:24,  7.02s/it] 43%|     | 2814/6500 [5:19:36<7:02:57,  6.88s/it]                                                        43%|     | 2814/6500 [5:19:36<7:02:57,  6.88s/it] 43%|     | 2815/6500 [5:19:43<6:56:46,  6.79s/it]                                                        43%|     | 2815/6500 [5:19:43<6:56:46,  6.79s/it] 43%|     | 2816/6500 [5:19:49<6:52:10,  6.71s/it]            {'loss': 0.4395, 'learning_rate': 6.04381421131506e-05, 'epoch': 0.43}
{'loss': 0.4407, 'learning_rate': 6.0414499991326934e-05, 'epoch': 0.43}
{'loss': 0.4259, 'learning_rate': 6.039085543517722e-05, 'epoch': 0.43}
{'loss': 0.4758, 'learning_rate': 6.036720845022823e-05, 'epoch': 0.43}
{'loss': 0.4424, 'learning_rate': 6.034355904200729e-05, 'epoch': 0.43}
                                            43%|     | 2816/6500 [5:19:49<6:52:10,  6.71s/it] 43%|     | 2817/6500 [5:19:56<6:49:22,  6.67s/it]                                                        43%|     | 2817/6500 [5:19:56<6:49:22,  6.67s/it] 43%|     | 2818/6500 [5:20:02<6:47:08,  6.63s/it]                                                        43%|     | 2818/6500 [5:20:02<6:47:08,  6.63s/it] 43%|     | 2819/6500 [5:20:09<6:45:37,  6.61s/it]                                                        43%|     | 2819/6500 [5:20:09<6:45:37,  6.61s/it] 43%|     | 2820/6500 [5:20:15<6:44:18,  6.59s/it]                                                        43%|     | 2820/6500 [5:20:15<6:44:18,  6.59s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8470821976661682, 'eval_runtime': 1.4856, 'eval_samples_per_second': 8.078, 'eval_steps_per_second': 2.019, 'epoch': 0.43}
                                                        43%|     | 2820/6500 [5:20:17<6:44:18,  6.59s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-2820
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2820
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2820
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2820
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2820
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2820/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2820/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2820/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2820/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.7143, 'learning_rate': 6.0319907216042324e-05, 'epoch': 0.43}
{'loss': 0.4527, 'learning_rate': 6.029625297786179e-05, 'epoch': 0.43}
{'loss': 0.4342, 'learning_rate': 6.0272596332994725e-05, 'epoch': 0.43}
{'loss': 0.4363, 'learning_rate': 6.024893728697072e-05, 'epoch': 0.43}
{'loss': 0.4213, 'learning_rate': 6.022527584531994e-05, 'epoch': 0.43}
 43%|     | 2821/6500 [5:20:24<7:17:31,  7.14s/it]                                                        43%|     | 2821/6500 [5:20:24<7:17:31,  7.14s/it] 43%|     | 2822/6500 [5:20:30<7:06:45,  6.96s/it]                                                        43%|     | 2822/6500 [5:20:30<7:06:45,  6.96s/it] 43%|     | 2823/6500 [5:20:37<6:59:06,  6.84s/it]                                                        43%|     | 2823/6500 [5:20:37<6:59:06,  6.84s/it] 43%|     | 2824/6500 [5:20:44<6:53:38,  6.75s/it]                                                        43%|     | 2824/6500 [5:20:44<6:53:38,  6.75s/it] 43%|     | 2825/6500 [5:20:50<6:49:59,  6.69s/it]                                                        43%|     | 2825/6500 [5:20:50<6:49:59,  6.69s/it] 43%|     | 2826/6500 [5:20:57<6:47:18,  6.65s/it]            {'loss': 0.4363, 'learning_rate': 6.0201612013573116e-05, 'epoch': 0.43}
{'loss': 0.4341, 'learning_rate': 6.017794579726149e-05, 'epoch': 0.43}
{'loss': 0.4499, 'learning_rate': 6.015427720191693e-05, 'epoch': 0.44}
{'loss': 0.4465, 'learning_rate': 6.013060623307181e-05, 'epoch': 0.44}
{'loss': 0.4463, 'learning_rate': 6.010693289625907e-05, 'epoch': 0.44}
                                            43%|     | 2826/6500 [5:20:57<6:47:18,  6.65s/it] 43%|     | 2827/6500 [5:21:03<6:45:18,  6.62s/it]                                                        43%|     | 2827/6500 [5:21:03<6:45:18,  6.62s/it] 44%|     | 2828/6500 [5:21:10<6:43:51,  6.60s/it]                                                        44%|     | 2828/6500 [5:21:10<6:43:51,  6.60s/it] 44%|     | 2829/6500 [5:21:17<7:01:53,  6.90s/it]                                                        44%|     | 2829/6500 [5:21:17<7:01:53,  6.90s/it] 44%|     | 2830/6500 [5:21:24<6:55:30,  6.79s/it]                                                        44%|     | 2830/6500 [5:21:24<6:55:30,  6.79s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8461706638336182, 'eval_runtime': 1.488, 'eval_samples_per_second': 8.065, 'eval_steps_per_second': 2.016, 'epoch': 0.44}
                                                        44%|     | 2830/6500 [5:21:25<6:55:30,  6.79s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-2830
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2830I AM HERE AND I AM SAVING THE MODEL

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2830
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2830
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2830
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2830/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2830/pytorch_model.binthe pytorch model path is
 /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2830/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2830/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.4474, 'learning_rate': 6.0083257197012224e-05, 'epoch': 0.44}
{'loss': 0.4363, 'learning_rate': 6.005957914086533e-05, 'epoch': 0.44}
{'loss': 0.4279, 'learning_rate': 6.003589873335296e-05, 'epoch': 0.44}
{'loss': 0.4764, 'learning_rate': 6.001221598001028e-05, 'epoch': 0.44}
{'loss': 0.4429, 'learning_rate': 5.9988530886372985e-05, 'epoch': 0.44}
 44%|     | 2831/6500 [5:21:32<7:24:17,  7.27s/it]                                                        44%|     | 2831/6500 [5:21:32<7:24:17,  7.27s/it] 44%|     | 2832/6500 [5:21:39<7:11:13,  7.05s/it]                                                        44%|     | 2832/6500 [5:21:39<7:11:13,  7.05s/it] 44%|     | 2833/6500 [5:21:45<7:02:11,  6.91s/it]                                                        44%|     | 2833/6500 [5:21:45<7:02:11,  6.91s/it] 44%|     | 2834/6500 [5:21:52<6:55:39,  6.80s/it]                                                        44%|     | 2834/6500 [5:21:52<6:55:39,  6.80s/it] 44%|     | 2835/6500 [5:21:59<6:50:51,  6.73s/it]                                                        44%|     | 2835/6500 [5:21:59<6:50:51,  6.73s/it] 44%|     | 2836/6500 [5:22:05<6:47:44,  6.68s/it]            {'loss': 0.7214, 'learning_rate': 5.996484345797733e-05, 'epoch': 0.44}
{'loss': 0.4487, 'learning_rate': 5.994115370036011e-05, 'epoch': 0.44}
{'loss': 0.4477, 'learning_rate': 5.991746161905865e-05, 'epoch': 0.44}
{'loss': 0.4315, 'learning_rate': 5.9893767219610844e-05, 'epoch': 0.44}
{'loss': 0.4266, 'learning_rate': 5.9870070507555084e-05, 'epoch': 0.44}
                                            44%|     | 2836/6500 [5:22:05<6:47:44,  6.68s/it] 44%|     | 2837/6500 [5:22:12<6:45:41,  6.65s/it]                                                        44%|     | 2837/6500 [5:22:12<6:45:41,  6.65s/it] 44%|     | 2838/6500 [5:22:18<6:43:43,  6.61s/it]                                                        44%|     | 2838/6500 [5:22:18<6:43:43,  6.61s/it] 44%|     | 2839/6500 [5:22:25<6:42:38,  6.60s/it]                                                        44%|     | 2839/6500 [5:22:25<6:42:38,  6.60s/it] 44%|     | 2840/6500 [5:22:31<6:41:54,  6.59s/it]                                                        44%|     | 2840/6500 [5:22:31<6:41:54,  6.59s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.853161633014679, 'eval_runtime': 1.4845, 'eval_samples_per_second': 8.084, 'eval_steps_per_second': 2.021, 'epoch': 0.44}
                                                        44%|     | 2840/6500 [5:22:33<6:41:54,  6.59s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-2840
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2840
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2840
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2840
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2840
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2840/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2840/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2840/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2840/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.4345, 'learning_rate': 5.984637148843037e-05, 'epoch': 0.44}
{'loss': 0.4432, 'learning_rate': 5.9822670167776183e-05, 'epoch': 0.44}
{'loss': 0.4547, 'learning_rate': 5.979896655113259e-05, 'epoch': 0.44}
{'loss': 0.4439, 'learning_rate': 5.977526064404012e-05, 'epoch': 0.44}
{'loss': 0.4395, 'learning_rate': 5.9751552452039916e-05, 'epoch': 0.44}
 44%|     | 2841/6500 [5:22:40<7:15:18,  7.14s/it]                                                        44%|     | 2841/6500 [5:22:40<7:15:18,  7.14s/it] 44%|     | 2842/6500 [5:22:46<7:04:23,  6.96s/it]                                                        44%|     | 2842/6500 [5:22:46<7:04:23,  6.96s/it] 44%|     | 2843/6500 [5:22:53<6:56:43,  6.84s/it]                                                        44%|     | 2843/6500 [5:22:53<6:56:43,  6.84s/it] 44%|     | 2844/6500 [5:22:59<6:51:26,  6.75s/it]                                                        44%|     | 2844/6500 [5:22:59<6:51:26,  6.75s/it] 44%|     | 2845/6500 [5:23:07<6:58:54,  6.88s/it]                                                        44%|     | 2845/6500 [5:23:07<6:58:54,  6.88s/it] 44%|     | 2846/6500 [5:23:13<6:52:46,  6.78s/it]            {'loss': 0.4459, 'learning_rate': 5.9727841980673604e-05, 'epoch': 0.44}
{'loss': 0.4308, 'learning_rate': 5.970412923548339e-05, 'epoch': 0.44}
{'loss': 0.4384, 'learning_rate': 5.9680414222011974e-05, 'epoch': 0.44}
{'loss': 0.4631, 'learning_rate': 5.965669694580258e-05, 'epoch': 0.44}
{'loss': 0.4394, 'learning_rate': 5.9632977412399e-05, 'epoch': 0.44}
                                            44%|     | 2846/6500 [5:23:13<6:52:46,  6.78s/it] 44%|     | 2847/6500 [5:23:20<6:48:32,  6.71s/it]                                                        44%|     | 2847/6500 [5:23:20<6:48:32,  6.71s/it] 44%|     | 2848/6500 [5:23:26<6:45:38,  6.66s/it]                                                        44%|     | 2848/6500 [5:23:26<6:45:38,  6.66s/it] 44%|     | 2849/6500 [5:23:33<6:43:34,  6.63s/it]                                                        44%|     | 2849/6500 [5:23:33<6:43:34,  6.63s/it] 44%|     | 2850/6500 [5:23:39<6:41:54,  6.61s/it]                                                        44%|     | 2850/6500 [5:23:39<6:41:54,  6.61s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8480337262153625, 'eval_runtime': 1.4957, 'eval_samples_per_second': 8.023, 'eval_steps_per_second': 2.006, 'epoch': 0.44}
                                                        44%|     | 2850/6500 [5:23:41<6:41:54,  6.61s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-2850
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2850
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2850
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2850
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2850
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2850/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2850/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2850/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2850/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.7247, 'learning_rate': 5.9609255627345495e-05, 'epoch': 0.44}
{'loss': 0.4351, 'learning_rate': 5.958553159618693e-05, 'epoch': 0.44}
{'loss': 0.4562, 'learning_rate': 5.956180532446863e-05, 'epoch': 0.44}
{'loss': 0.4321, 'learning_rate': 5.953807681773649e-05, 'epoch': 0.44}
{'loss': 0.431, 'learning_rate': 5.9514346081536855e-05, 'epoch': 0.44}
 44%|     | 2851/6500 [5:23:48<7:14:33,  7.15s/it]                                                        44%|     | 2851/6500 [5:23:48<7:14:33,  7.15s/it] 44%|     | 2852/6500 [5:23:54<7:03:19,  6.96s/it]                                                        44%|     | 2852/6500 [5:23:54<7:03:19,  6.96s/it] 44%|     | 2853/6500 [5:24:02<7:14:41,  7.15s/it]                                                        44%|     | 2853/6500 [5:24:02<7:14:41,  7.15s/it] 44%|     | 2854/6500 [5:24:08<7:04:33,  6.99s/it]                                                        44%|     | 2854/6500 [5:24:08<7:04:33,  6.99s/it] 44%|     | 2855/6500 [5:24:15<6:56:24,  6.85s/it]                                                        44%|     | 2855/6500 [5:24:15<6:56:24,  6.85s/it] 44%|     | 2856/6500 [5:24:22<6:54:22,  6.82s/it]            {'loss': 0.4335, 'learning_rate': 5.949061312141668e-05, 'epoch': 0.44}
{'loss': 0.4415, 'learning_rate': 5.946687794292341e-05, 'epoch': 0.44}
{'loss': 0.4566, 'learning_rate': 5.944314055160497e-05, 'epoch': 0.44}
{'loss': 0.4454, 'learning_rate': 5.941940095300984e-05, 'epoch': 0.44}
{'loss': 0.439, 'learning_rate': 5.939565915268701e-05, 'epoch': 0.44}
                                            44%|     | 2856/6500 [5:24:22<6:54:22,  6.82s/it] 44%|     | 2857/6500 [5:24:28<6:49:11,  6.74s/it]                                                        44%|     | 2857/6500 [5:24:28<6:49:11,  6.74s/it] 44%|     | 2858/6500 [5:24:35<6:45:41,  6.68s/it]                                                        44%|     | 2858/6500 [5:24:35<6:45:41,  6.68s/it] 44%|     | 2859/6500 [5:24:41<6:43:06,  6.64s/it]                                                        44%|     | 2859/6500 [5:24:41<6:43:06,  6.64s/it] 44%|     | 2860/6500 [5:24:48<6:41:12,  6.61s/it]                                                        44%|     | 2860/6500 [5:24:48<6:41:12,  6.61s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8499655723571777, 'eval_runtime': 1.9385, 'eval_samples_per_second': 6.19, 'eval_steps_per_second': 1.548, 'epoch': 0.44}
                                                        44%|     | 2860/6500 [5:24:50<6:41:12,  6.61s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-2860
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2860
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2860
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2860
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2860
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2860/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2860/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2860/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2860/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.4448, 'learning_rate': 5.937191515618598e-05, 'epoch': 0.44}
{'loss': 0.4257, 'learning_rate': 5.934816896905676e-05, 'epoch': 0.44}
{'loss': 0.4433, 'learning_rate': 5.9324420596849886e-05, 'epoch': 0.44}
{'loss': 0.4524, 'learning_rate': 5.93006700451164e-05, 'epoch': 0.44}
{'loss': 0.4435, 'learning_rate': 5.927691731940783e-05, 'epoch': 0.44}
 44%|     | 2861/6500 [5:24:57<7:34:43,  7.50s/it]                                                        44%|     | 2861/6500 [5:24:57<7:34:43,  7.50s/it] 44%|     | 2862/6500 [5:25:04<7:17:28,  7.22s/it]                                                        44%|     | 2862/6500 [5:25:04<7:17:28,  7.22s/it] 44%|     | 2863/6500 [5:25:11<7:05:45,  7.02s/it]                                                        44%|     | 2863/6500 [5:25:11<7:05:45,  7.02s/it] 44%|     | 2864/6500 [5:25:17<6:56:50,  6.88s/it]                                                        44%|     | 2864/6500 [5:25:17<6:56:50,  6.88s/it] 44%|     | 2865/6500 [5:25:24<6:50:51,  6.78s/it]                                                        44%|     | 2865/6500 [5:25:24<6:50:51,  6.78s/it] 44%|     | 2866/6500 [5:25:30<6:46:09,  6.71s/it]            {'loss': 0.7258, 'learning_rate': 5.925316242527623e-05, 'epoch': 0.44}
{'loss': 0.4306, 'learning_rate': 5.922940536827419e-05, 'epoch': 0.44}
{'loss': 0.4564, 'learning_rate': 5.920564615395475e-05, 'epoch': 0.44}
{'loss': 0.4174, 'learning_rate': 5.91818847878715e-05, 'epoch': 0.44}
{'loss': 0.4239, 'learning_rate': 5.915812127557851e-05, 'epoch': 0.44}
                                            44%|     | 2866/6500 [5:25:30<6:46:09,  6.71s/it] 44%|     | 2867/6500 [5:25:37<6:43:01,  6.66s/it]                                                        44%|     | 2867/6500 [5:25:37<6:43:01,  6.66s/it] 44%|     | 2868/6500 [5:25:43<6:41:05,  6.63s/it]                                                        44%|     | 2868/6500 [5:25:43<6:41:05,  6.63s/it] 44%|     | 2869/6500 [5:25:50<6:39:35,  6.60s/it]                                                        44%|     | 2869/6500 [5:25:50<6:39:35,  6.60s/it] 44%|     | 2870/6500 [5:25:56<6:38:32,  6.59s/it]                                                        44%|     | 2870/6500 [5:25:56<6:38:32,  6.59s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8561603426933289, 'eval_runtime': 1.4924, 'eval_samples_per_second': 8.041, 'eval_steps_per_second': 2.01, 'epoch': 0.44}
                                                        44%|     | 2870/6500 [5:25:58<6:38:32,  6.59s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-2870
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2870
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2870
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2870
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2870
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2870/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2870/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2870/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2870/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.439, 'learning_rate': 5.9134355622630356e-05, 'epoch': 0.44}
{'loss': 0.4271, 'learning_rate': 5.91105878345821e-05, 'epoch': 0.44}
{'loss': 0.4502, 'learning_rate': 5.9086817916989335e-05, 'epoch': 0.44}
{'loss': 0.4488, 'learning_rate': 5.906304587540813e-05, 'epoch': 0.44}
{'loss': 0.4305, 'learning_rate': 5.903927171539507e-05, 'epoch': 0.44}
 44%|     | 2871/6500 [5:26:05<7:11:11,  7.13s/it]                                                        44%|     | 2871/6500 [5:26:05<7:11:11,  7.13s/it] 44%|     | 2872/6500 [5:26:11<7:00:33,  6.96s/it]                                                        44%|     | 2872/6500 [5:26:11<7:00:33,  6.96s/it] 44%|     | 2873/6500 [5:26:18<6:53:04,  6.83s/it]                                                        44%|     | 2873/6500 [5:26:18<6:53:04,  6.83s/it] 44%|     | 2874/6500 [5:26:24<6:47:34,  6.74s/it]                                                        44%|     | 2874/6500 [5:26:24<6:47:34,  6.74s/it] 44%|     | 2875/6500 [5:26:31<6:43:41,  6.68s/it]                                                        44%|     | 2875/6500 [5:26:31<6:43:41,  6.68s/it] 44%|     | 2876/6500 [5:26:38<6:40:58,  6.64s/it]            {'loss': 0.4493, 'learning_rate': 5.9015495442507194e-05, 'epoch': 0.44}
{'loss': 0.4333, 'learning_rate': 5.899171706230208e-05, 'epoch': 0.44}
{'loss': 0.478, 'learning_rate': 5.896793658033776e-05, 'epoch': 0.44}
{'loss': 0.4279, 'learning_rate': 5.89441540021728e-05, 'epoch': 0.44}
{'loss': 0.7199, 'learning_rate': 5.892036933336622e-05, 'epoch': 0.44}
                                            44%|     | 2876/6500 [5:26:38<6:40:58,  6.64s/it] 44%|     | 2877/6500 [5:26:45<6:55:25,  6.88s/it]                                                        44%|     | 2877/6500 [5:26:45<6:55:25,  6.88s/it] 44%|     | 2878/6500 [5:26:52<6:54:58,  6.87s/it]                                                        44%|     | 2878/6500 [5:26:52<6:54:58,  6.87s/it] 44%|     | 2879/6500 [5:26:58<6:48:46,  6.77s/it]                                                        44%|     | 2879/6500 [5:26:58<6:48:46,  6.77s/it] 44%|     | 2880/6500 [5:27:05<6:44:28,  6.70s/it]                                                        44%|     | 2880/6500 [5:27:05<6:44:28,  6.70s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8483405709266663, 'eval_runtime': 1.4905, 'eval_samples_per_second': 8.051, 'eval_steps_per_second': 2.013, 'epoch': 0.44}
                                                        44%|     | 2880/6500 [5:27:06<6:44:28,  6.70s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-2880
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2880
/projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2880
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2880
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2880
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2880/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2880/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2880/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2880/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.4501, 'learning_rate': 5.889658257947755e-05, 'epoch': 0.44}
{'loss': 0.4326, 'learning_rate': 5.887279374606679e-05, 'epoch': 0.44}
{'loss': 0.4491, 'learning_rate': 5.884900283869445e-05, 'epoch': 0.44}
{'loss': 0.4234, 'learning_rate': 5.882520986292148e-05, 'epoch': 0.44}
{'loss': 0.4385, 'learning_rate': 5.8801414824309365e-05, 'epoch': 0.44}
 44%|     | 2881/6500 [5:27:13<7:13:43,  7.19s/it]                                                        44%|     | 2881/6500 [5:27:13<7:13:43,  7.19s/it] 44%|     | 2882/6500 [5:27:20<7:01:55,  7.00s/it]                                                        44%|     | 2882/6500 [5:27:20<7:01:55,  7.00s/it] 44%|     | 2883/6500 [5:27:26<6:53:41,  6.86s/it]                                                        44%|     | 2883/6500 [5:27:26<6:53:41,  6.86s/it] 44%|     | 2884/6500 [5:27:33<6:47:57,  6.77s/it]                                                        44%|     | 2884/6500 [5:27:33<6:47:57,  6.77s/it] 44%|     | 2885/6500 [5:27:39<6:43:57,  6.70s/it]                                                        44%|     | 2885/6500 [5:27:39<6:43:57,  6.70s/it] 44%|     | 2886/6500 [5:27:46<6:40:45,  6.65s/it]            {'loss': 0.4413, 'learning_rate': 5.8777617728420075e-05, 'epoch': 0.44}
{'loss': 0.4481, 'learning_rate': 5.875381858081599e-05, 'epoch': 0.44}
{'loss': 0.4381, 'learning_rate': 5.8730017387060035e-05, 'epoch': 0.44}
{'loss': 0.451, 'learning_rate': 5.870621415271559e-05, 'epoch': 0.44}
{'loss': 0.4368, 'learning_rate': 5.868240888334653e-05, 'epoch': 0.44}
                                            44%|     | 2886/6500 [5:27:46<6:40:45,  6.65s/it] 44%|     | 2887/6500 [5:27:53<6:38:38,  6.62s/it]                                                        44%|     | 2887/6500 [5:27:53<6:38:38,  6.62s/it] 44%|     | 2888/6500 [5:27:59<6:37:12,  6.60s/it]                                                        44%|     | 2888/6500 [5:27:59<6:37:12,  6.60s/it] 44%|     | 2889/6500 [5:28:06<6:36:01,  6.58s/it]                                                        44%|     | 2889/6500 [5:28:06<6:36:01,  6.58s/it] 44%|     | 2890/6500 [5:28:12<6:35:21,  6.57s/it]                                                        44%|     | 2890/6500 [5:28:12<6:35:21,  6.57s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8479813933372498, 'eval_runtime': 1.4746, 'eval_samples_per_second': 8.138, 'eval_steps_per_second': 2.034, 'epoch': 0.44}
                                                        44%|     | 2890/6500 [5:28:14<6:35:21,  6.57s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-2890
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2890I AM HERE AND I AM SAVING THE MODEL

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2890
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2890
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2890
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2890/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2890/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2890/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2890/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.4376, 'learning_rate': 5.865860158451717e-05, 'epoch': 0.44}
{'loss': 0.4242, 'learning_rate': 5.863479226179236e-05, 'epoch': 0.44}
{'loss': 0.4702, 'learning_rate': 5.861098092073733e-05, 'epoch': 0.45}
{'loss': 0.442, 'learning_rate': 5.8587167566917874e-05, 'epoch': 0.45}
{'loss': 0.7106, 'learning_rate': 5.856335220590022e-05, 'epoch': 0.45}
 44%|     | 2891/6500 [5:28:21<7:08:04,  7.12s/it]                                                        44%|     | 2891/6500 [5:28:21<7:08:04,  7.12s/it] 44%|     | 2892/6500 [5:28:27<6:57:27,  6.94s/it]                                                        44%|     | 2892/6500 [5:28:27<6:57:27,  6.94s/it] 45%|     | 2893/6500 [5:28:35<7:08:52,  7.13s/it]                                                        45%|     | 2893/6500 [5:28:35<7:08:52,  7.13s/it] 45%|     | 2894/6500 [5:28:41<7:00:22,  6.99s/it]                                                        45%|     | 2894/6500 [5:28:41<7:00:22,  6.99s/it] 45%|     | 2895/6500 [5:28:48<6:52:16,  6.86s/it]                                                        45%|     | 2895/6500 [5:28:48<6:52:16,  6.86s/it] 45%|     | 2896/6500 [5:28:54<6:46:38,  6.77s/it]            {'loss': 0.4562, 'learning_rate': 5.8539534843251064e-05, 'epoch': 0.45}
{'loss': 0.4224, 'learning_rate': 5.8515715484537534e-05, 'epoch': 0.45}
{'loss': 0.4362, 'learning_rate': 5.849189413532731e-05, 'epoch': 0.45}
{'loss': 0.4232, 'learning_rate': 5.846807080118845e-05, 'epoch': 0.45}
{'loss': 0.4298, 'learning_rate': 5.844424548768952e-05, 'epoch': 0.45}
                                            45%|     | 2896/6500 [5:28:54<6:46:38,  6.77s/it] 45%|     | 2897/6500 [5:29:01<6:42:27,  6.70s/it]                                                        45%|     | 2897/6500 [5:29:01<6:42:27,  6.70s/it] 45%|     | 2898/6500 [5:29:08<6:39:32,  6.66s/it]                                                        45%|     | 2898/6500 [5:29:08<6:39:32,  6.66s/it] 45%|     | 2899/6500 [5:29:14<6:39:31,  6.66s/it]                                                        45%|     | 2899/6500 [5:29:14<6:39:31,  6.66s/it] 45%|     | 2900/6500 [5:29:21<6:37:32,  6.63s/it]                                                        45%|     | 2900/6500 [5:29:21<6:37:32,  6.63s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8577499985694885, 'eval_runtime': 1.6292, 'eval_samples_per_second': 7.366, 'eval_steps_per_second': 1.841, 'epoch': 0.45}
                                                        45%|     | 2900/6500 [5:29:22<6:37:32,  6.63s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-2900
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2900
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2900
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2900
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2900
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2900/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2900/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2900/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2900/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.437, 'learning_rate': 5.842041820039956e-05, 'epoch': 0.45}
{'loss': 0.4435, 'learning_rate': 5.8396588944888044e-05, 'epoch': 0.45}
{'loss': 0.442, 'learning_rate': 5.8372757726724914e-05, 'epoch': 0.45}
{'loss': 0.4375, 'learning_rate': 5.8348924551480565e-05, 'epoch': 0.45}
{'loss': 0.4355, 'learning_rate': 5.8325089424725865e-05, 'epoch': 0.45}
 45%|     | 2901/6500 [5:29:29<7:11:12,  7.19s/it]                                                        45%|     | 2901/6500 [5:29:29<7:11:12,  7.19s/it] 45%|     | 2902/6500 [5:29:36<7:00:16,  7.01s/it]                                                        45%|     | 2902/6500 [5:29:36<7:00:16,  7.01s/it] 45%|     | 2903/6500 [5:29:42<6:51:55,  6.87s/it]                                                        45%|     | 2903/6500 [5:29:42<6:51:55,  6.87s/it] 45%|     | 2904/6500 [5:29:49<6:45:48,  6.77s/it]                                                        45%|     | 2904/6500 [5:29:49<6:45:48,  6.77s/it] 45%|     | 2905/6500 [5:29:55<6:41:46,  6.71s/it]                                                        45%|     | 2905/6500 [5:29:55<6:41:46,  6.71s/it] 45%|     | 2906/6500 [5:30:02<6:38:49,  6.66s/it]            {'loss': 0.4331, 'learning_rate': 5.830125235203213e-05, 'epoch': 0.45}
{'loss': 0.423, 'learning_rate': 5.8277413338971135e-05, 'epoch': 0.45}
{'loss': 0.4686, 'learning_rate': 5.825357239111511e-05, 'epoch': 0.45}
{'loss': 0.4325, 'learning_rate': 5.8229729514036705e-05, 'epoch': 0.45}
{'loss': 0.7181, 'learning_rate': 5.820588471330906e-05, 'epoch': 0.45}
                                            45%|     | 2906/6500 [5:30:02<6:38:49,  6.66s/it] 45%|     | 2907/6500 [5:30:09<6:36:46,  6.63s/it]                                                        45%|     | 2907/6500 [5:30:09<6:36:46,  6.63s/it] 45%|     | 2908/6500 [5:30:15<6:35:07,  6.60s/it]                                                        45%|     | 2908/6500 [5:30:15<6:35:07,  6.60s/it] 45%|     | 2909/6500 [5:30:24<7:07:59,  7.15s/it]                                                        45%|     | 2909/6500 [5:30:24<7:07:59,  7.15s/it] 45%|     | 2910/6500 [5:30:30<6:56:58,  6.97s/it]                                                        45%|     | 2910/6500 [5:30:30<6:56:58,  6.97s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8498973250389099, 'eval_runtime': 1.4801, 'eval_samples_per_second': 8.107, 'eval_steps_per_second': 2.027, 'epoch': 0.45}
                                                        45%|     | 2910/6500 [5:30:32<6:56:58,  6.97s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-2910
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2910
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2910
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2910
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2910
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2910/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2910/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2910/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2910/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.4507, 'learning_rate': 5.818203799450577e-05, 'epoch': 0.45}
{'loss': 0.4346, 'learning_rate': 5.8158189363200834e-05, 'epoch': 0.45}
{'loss': 0.425, 'learning_rate': 5.813433882496875e-05, 'epoch': 0.45}
{'loss': 0.4212, 'learning_rate': 5.811048638538441e-05, 'epoch': 0.45}
{'loss': 0.4221, 'learning_rate': 5.808663205002319e-05, 'epoch': 0.45}
 45%|     | 2911/6500 [5:30:38<7:22:24,  7.40s/it]                                                        45%|     | 2911/6500 [5:30:38<7:22:24,  7.40s/it] 45%|     | 2912/6500 [5:30:45<7:06:46,  7.14s/it]                                                        45%|     | 2912/6500 [5:30:45<7:06:46,  7.14s/it] 45%|     | 2913/6500 [5:30:52<6:55:55,  6.96s/it]                                                        45%|     | 2913/6500 [5:30:52<6:55:55,  6.96s/it] 45%|     | 2914/6500 [5:30:58<6:48:45,  6.84s/it]                                                        45%|     | 2914/6500 [5:30:58<6:48:45,  6.84s/it] 45%|     | 2915/6500 [5:31:05<6:43:24,  6.75s/it]                                                        45%|     | 2915/6500 [5:31:05<6:43:24,  6.75s/it] 45%|     | 2916/6500 [5:31:11<6:39:21,  6.69s/it]            {'loss': 0.4284, 'learning_rate': 5.80627758244609e-05, 'epoch': 0.45}
{'loss': 0.4504, 'learning_rate': 5.803891771427379e-05, 'epoch': 0.45}
{'loss': 0.4436, 'learning_rate': 5.8015057725038534e-05, 'epoch': 0.45}
{'loss': 0.4351, 'learning_rate': 5.799119586233228e-05, 'epoch': 0.45}
{'loss': 0.4402, 'learning_rate': 5.796733213173257e-05, 'epoch': 0.45}
                                            45%|     | 2916/6500 [5:31:11<6:39:21,  6.69s/it] 45%|     | 2917/6500 [5:31:18<6:36:40,  6.64s/it]                                                        45%|     | 2917/6500 [5:31:18<6:36:40,  6.64s/it] 45%|     | 2918/6500 [5:31:24<6:34:45,  6.61s/it]                                                        45%|     | 2918/6500 [5:31:24<6:34:45,  6.61s/it] 45%|     | 2919/6500 [5:31:31<6:35:02,  6.62s/it]                                                        45%|     | 2919/6500 [5:31:31<6:35:02,  6.62s/it] 45%|     | 2920/6500 [5:31:37<6:33:41,  6.60s/it]                                                        45%|     | 2920/6500 [5:31:37<6:33:41,  6.60s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8487510085105896, 'eval_runtime': 1.5509, 'eval_samples_per_second': 7.738, 'eval_steps_per_second': 1.934, 'epoch': 0.45}
                                                        45%|     | 2920/6500 [5:31:39<6:33:41,  6.60s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-2920
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2920I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2920

I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2920
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2920
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2920/pytorch_model.bin
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2920/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2920/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2920/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.429, 'learning_rate': 5.7943466538817416e-05, 'epoch': 0.45}
{'loss': 0.4345, 'learning_rate': 5.791959908916526e-05, 'epoch': 0.45}
{'loss': 0.4582, 'learning_rate': 5.789572978835496e-05, 'epoch': 0.45}
{'loss': 0.431, 'learning_rate': 5.787185864196584e-05, 'epoch': 0.45}
{'loss': 0.7215, 'learning_rate': 5.784798565557762e-05, 'epoch': 0.45}
 45%|     | 2921/6500 [5:31:46<7:06:17,  7.15s/it]                                                        45%|     | 2921/6500 [5:31:46<7:06:17,  7.15s/it] 45%|     | 2922/6500 [5:31:52<6:55:25,  6.97s/it]                                                        45%|     | 2922/6500 [5:31:52<6:55:25,  6.97s/it] 45%|     | 2923/6500 [5:31:59<6:47:23,  6.83s/it]                                                        45%|     | 2923/6500 [5:31:59<6:47:23,  6.83s/it] 45%|     | 2924/6500 [5:32:06<6:42:13,  6.75s/it]                                                        45%|     | 2924/6500 [5:32:06<6:42:13,  6.75s/it] 45%|     | 2925/6500 [5:32:13<6:49:32,  6.87s/it]                                                        45%|     | 2925/6500 [5:32:13<6:49:32,  6.87s/it] 45%|     | 2926/6500 [5:32:19<6:43:42,  6.78s/it]            {'loss': 0.4265, 'learning_rate': 5.782411083477046e-05, 'epoch': 0.45}
{'loss': 0.4485, 'learning_rate': 5.780023418512497e-05, 'epoch': 0.45}
{'loss': 0.429, 'learning_rate': 5.7776355712222166e-05, 'epoch': 0.45}
{'loss': 0.4285, 'learning_rate': 5.775247542164349e-05, 'epoch': 0.45}
{'loss': 0.4244, 'learning_rate': 5.7728593318970825e-05, 'epoch': 0.45}
                                            45%|     | 2926/6500 [5:32:19<6:43:42,  6.78s/it] 45%|     | 2927/6500 [5:32:26<6:39:36,  6.71s/it]                                                        45%|     | 2927/6500 [5:32:26<6:39:36,  6.71s/it] 45%|     | 2928/6500 [5:32:32<6:36:52,  6.67s/it]                                                        45%|     | 2928/6500 [5:32:32<6:36:52,  6.67s/it] 45%|     | 2929/6500 [5:32:39<6:35:05,  6.64s/it]                                                        45%|     | 2929/6500 [5:32:39<6:35:05,  6.64s/it] 45%|     | 2930/6500 [5:32:45<6:33:35,  6.62s/it]                                                        45%|     | 2930/6500 [5:32:45<6:33:35,  6.62s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8573964834213257, 'eval_runtime': 1.8368, 'eval_samples_per_second': 6.533, 'eval_steps_per_second': 1.633, 'epoch': 0.45}
                                                        45%|     | 2930/6500 [5:32:47<6:33:35,  6.62s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-2930
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2930I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2930

I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2930
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2930
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2930/pytorch_model.bin
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2930/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2930/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2930/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.4488, 'learning_rate': 5.7704709409786464e-05, 'epoch': 0.45}
{'loss': 0.449, 'learning_rate': 5.768082369967312e-05, 'epoch': 0.45}
{'loss': 0.4332, 'learning_rate': 5.765693619421394e-05, 'epoch': 0.45}
{'loss': 0.4332, 'learning_rate': 5.763304689899249e-05, 'epoch': 0.45}
{'loss': 0.4464, 'learning_rate': 5.760915581959272e-05, 'epoch': 0.45}
 45%|     | 2931/6500 [5:32:54<7:10:28,  7.24s/it]                                                        45%|     | 2931/6500 [5:32:54<7:10:28,  7.24s/it] 45%|     | 2932/6500 [5:33:01<6:58:16,  7.03s/it]                                                        45%|     | 2932/6500 [5:33:01<6:58:16,  7.03s/it] 45%|     | 2933/6500 [5:33:07<6:49:29,  6.89s/it]                                                        45%|     | 2933/6500 [5:33:07<6:49:29,  6.89s/it] 45%|     | 2934/6500 [5:33:14<6:43:39,  6.79s/it]                                                        45%|     | 2934/6500 [5:33:14<6:43:39,  6.79s/it] 45%|     | 2935/6500 [5:33:20<6:39:07,  6.72s/it]                                                        45%|     | 2935/6500 [5:33:20<6:39:07,  6.72s/it] 45%|     | 2936/6500 [5:33:27<6:36:05,  6.67s/it]            {'loss': 0.4205, 'learning_rate': 5.7585262961599054e-05, 'epoch': 0.45}
{'loss': 0.4389, 'learning_rate': 5.7561368330596275e-05, 'epoch': 0.45}
{'loss': 0.452, 'learning_rate': 5.753747193216963e-05, 'epoch': 0.45}
{'loss': 0.4392, 'learning_rate': 5.751357377190475e-05, 'epoch': 0.45}
{'loss': 0.7113, 'learning_rate': 5.748967385538769e-05, 'epoch': 0.45}
                                            45%|     | 2936/6500 [5:33:27<6:36:05,  6.67s/it] 45%|     | 2937/6500 [5:33:34<6:33:50,  6.63s/it]                                                        45%|     | 2937/6500 [5:33:34<6:33:50,  6.63s/it] 45%|     | 2938/6500 [5:33:40<6:32:27,  6.61s/it]                                                        45%|     | 2938/6500 [5:33:40<6:32:27,  6.61s/it] 45%|     | 2939/6500 [5:33:47<6:31:35,  6.60s/it]                                                        45%|     | 2939/6500 [5:33:47<6:31:35,  6.60s/it] 45%|     | 2940/6500 [5:33:53<6:30:42,  6.58s/it]                                                        45%|     | 2940/6500 [5:33:53<6:30:42,  6.58s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8488360047340393, 'eval_runtime': 1.4751, 'eval_samples_per_second': 8.135, 'eval_steps_per_second': 2.034, 'epoch': 0.45}
                                                        45%|     | 2940/6500 [5:33:55<6:30:42,  6.58s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-2940
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2940I AM HERE AND I AM SAVING THE MODEL

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2940
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2940
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2940
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2940/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2940/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2940/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2940/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.4271, 'learning_rate': 5.7465772188204905e-05, 'epoch': 0.45}
{'loss': 0.45, 'learning_rate': 5.744186877594325e-05, 'epoch': 0.45}
{'loss': 0.4087, 'learning_rate': 5.741796362419003e-05, 'epoch': 0.45}
{'loss': 0.4161, 'learning_rate': 5.73940567385329e-05, 'epoch': 0.45}
{'loss': 0.4299, 'learning_rate': 5.737014812455999e-05, 'epoch': 0.45}
 45%|     | 2941/6500 [5:34:02<7:03:20,  7.14s/it]                                                        45%|     | 2941/6500 [5:34:02<7:03:20,  7.14s/it] 45%|     | 2942/6500 [5:34:10<7:31:29,  7.61s/it]                                                        45%|     | 2942/6500 [5:34:10<7:31:29,  7.61s/it] 45%|     | 2943/6500 [5:34:17<7:13:06,  7.31s/it]                                                        45%|     | 2943/6500 [5:34:17<7:13:06,  7.31s/it] 45%|     | 2944/6500 [5:34:24<7:00:08,  7.09s/it]                                                        45%|     | 2944/6500 [5:34:24<7:00:08,  7.09s/it] 45%|     | 2945/6500 [5:34:30<6:50:58,  6.94s/it]                                                        45%|     | 2945/6500 [5:34:30<6:50:58,  6.94s/it] 45%|     | 2946/6500 [5:34:37<6:44:11,  6.82s/it]            {'loss': 0.4232, 'learning_rate': 5.7346237787859745e-05, 'epoch': 0.45}
{'loss': 0.4483, 'learning_rate': 5.7322325734021086e-05, 'epoch': 0.45}
{'loss': 0.4505, 'learning_rate': 5.7298411968633306e-05, 'epoch': 0.45}
{'loss': 0.4237, 'learning_rate': 5.72744964972861e-05, 'epoch': 0.45}
{'loss': 0.4442, 'learning_rate': 5.7250579325569574e-05, 'epoch': 0.45}
                                            45%|     | 2946/6500 [5:34:37<6:44:11,  6.82s/it] 45%|     | 2947/6500 [5:34:43<6:39:34,  6.75s/it]                                                        45%|     | 2947/6500 [5:34:43<6:39:34,  6.75s/it] 45%|     | 2948/6500 [5:34:50<6:36:37,  6.70s/it]                                                        45%|     | 2948/6500 [5:34:50<6:36:37,  6.70s/it] 45%|     | 2949/6500 [5:34:56<6:34:04,  6.66s/it]                                                        45%|     | 2949/6500 [5:34:56<6:34:04,  6.66s/it] 45%|     | 2950/6500 [5:35:03<6:32:28,  6.63s/it]                                                        45%|     | 2950/6500 [5:35:03<6:32:28,  6.63s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8447803854942322, 'eval_runtime': 1.9307, 'eval_samples_per_second': 6.215, 'eval_steps_per_second': 1.554, 'epoch': 0.45}
                                                        45%|     | 2950/6500 [5:35:05<6:32:28,  6.63s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-2950
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2950
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2950
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2950
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2950
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2950/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2950/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2950/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2950/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.4218, 'learning_rate': 5.722666045907422e-05, 'epoch': 0.45}
{'loss': 0.4686, 'learning_rate': 5.720273990339092e-05, 'epoch': 0.45}
{'loss': 0.4211, 'learning_rate': 5.717881766411095e-05, 'epoch': 0.45}
{'loss': 0.7111, 'learning_rate': 5.7154893746826014e-05, 'epoch': 0.45}
{'loss': 0.4462, 'learning_rate': 5.7130968157128154e-05, 'epoch': 0.45}
 45%|     | 2951/6500 [5:35:12<7:11:29,  7.29s/it]                                                        45%|     | 2951/6500 [5:35:12<7:11:29,  7.29s/it] 45%|     | 2952/6500 [5:35:18<6:58:09,  7.07s/it]                                                        45%|     | 2952/6500 [5:35:18<6:58:09,  7.07s/it] 45%|     | 2953/6500 [5:35:25<6:48:27,  6.91s/it]                                                        45%|     | 2953/6500 [5:35:25<6:48:27,  6.91s/it] 45%|     | 2954/6500 [5:35:31<6:41:58,  6.80s/it]                                                        45%|     | 2954/6500 [5:35:31<6:41:58,  6.80s/it] 45%|     | 2955/6500 [5:35:38<6:37:25,  6.73s/it]                                                        45%|     | 2955/6500 [5:35:38<6:37:25,  6.73s/it] 45%|     | 2956/6500 [5:35:45<6:34:13,  6.67s/it]            {'loss': 0.4277, 'learning_rate': 5.710704090060985e-05, 'epoch': 0.45}
{'loss': 0.4437, 'learning_rate': 5.7083111982863956e-05, 'epoch': 0.45}
{'loss': 0.4153, 'learning_rate': 5.7059181409483684e-05, 'epoch': 0.46}
{'loss': 0.4313, 'learning_rate': 5.703524918606269e-05, 'epoch': 0.46}
{'loss': 0.4342, 'learning_rate': 5.701131531819497e-05, 'epoch': 0.46}
                                            45%|     | 2956/6500 [5:35:45<6:34:13,  6.67s/it] 45%|     | 2957/6500 [5:35:51<6:32:00,  6.64s/it]                                                        45%|     | 2957/6500 [5:35:51<6:32:00,  6.64s/it] 46%|     | 2958/6500 [5:35:59<6:47:51,  6.91s/it]                                                        46%|     | 2958/6500 [5:35:59<6:47:51,  6.91s/it] 46%|     | 2959/6500 [5:36:05<6:41:46,  6.81s/it]                                                        46%|     | 2959/6500 [5:36:05<6:41:46,  6.81s/it] 46%|     | 2960/6500 [5:36:12<6:37:05,  6.73s/it]                                                        46%|     | 2960/6500 [5:36:12<6:37:05,  6.73s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8567913770675659, 'eval_runtime': 1.4841, 'eval_samples_per_second': 8.085, 'eval_steps_per_second': 2.021, 'epoch': 0.46}
                                                        46%|     | 2960/6500 [5:36:13<6:37:05,  6.73s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-2960
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2960
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2960
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2960
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2960
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2960/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2960/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2960/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2960/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.4365, 'learning_rate': 5.698737981147493e-05, 'epoch': 0.46}
{'loss': 0.4395, 'learning_rate': 5.696344267149735e-05, 'epoch': 0.46}
{'loss': 0.442, 'learning_rate': 5.693950390385736e-05, 'epoch': 0.46}
{'loss': 0.4282, 'learning_rate': 5.691556351415054e-05, 'epoch': 0.46}
{'loss': 0.4408, 'learning_rate': 5.6891621507972794e-05, 'epoch': 0.46}
 46%|     | 2961/6500 [5:36:20<7:06:21,  7.23s/it]                                                        46%|     | 2961/6500 [5:36:20<7:06:21,  7.23s/it] 46%|     | 2962/6500 [5:36:27<6:54:19,  7.03s/it]                                                        46%|     | 2962/6500 [5:36:27<6:54:19,  7.03s/it] 46%|     | 2963/6500 [5:36:33<6:45:44,  6.88s/it]                                                        46%|     | 2963/6500 [5:36:33<6:45:44,  6.88s/it] 46%|     | 2964/6500 [5:36:40<6:39:42,  6.78s/it]                                                        46%|     | 2964/6500 [5:36:40<6:39:42,  6.78s/it] 46%|     | 2965/6500 [5:36:46<6:35:19,  6.71s/it]                                                        46%|     | 2965/6500 [5:36:46<6:35:19,  6.71s/it] 46%|     | 2966/6500 [5:36:53<6:32:26,  6.66s/it]            {'loss': 0.4211, 'learning_rate': 5.686767789092041e-05, 'epoch': 0.46}
{'loss': 0.4766, 'learning_rate': 5.684373266859009e-05, 'epoch': 0.46}
{'loss': 0.4284, 'learning_rate': 5.681978584657886e-05, 'epoch': 0.46}
{'loss': 0.7074, 'learning_rate': 5.679583743048416e-05, 'epoch': 0.46}
{'loss': 0.4507, 'learning_rate': 5.677188742590378e-05, 'epoch': 0.46}
                                            46%|     | 2966/6500 [5:36:53<6:32:26,  6.66s/it] 46%|     | 2967/6500 [5:36:59<6:30:22,  6.63s/it]                                                        46%|     | 2967/6500 [5:36:59<6:30:22,  6.63s/it] 46%|     | 2968/6500 [5:37:06<6:28:27,  6.60s/it]                                                        46%|     | 2968/6500 [5:37:06<6:28:27,  6.60s/it] 46%|     | 2969/6500 [5:37:12<6:27:11,  6.58s/it]                                                        46%|     | 2969/6500 [5:37:12<6:27:11,  6.58s/it] 46%|     | 2970/6500 [5:37:19<6:26:08,  6.56s/it]                                                        46%|     | 2970/6500 [5:37:19<6:26:08,  6.56s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8496909141540527, 'eval_runtime': 1.6518, 'eval_samples_per_second': 7.265, 'eval_steps_per_second': 1.816, 'epoch': 0.46}
                                                        46%|     | 2970/6500 [5:37:21<6:26:08,  6.56s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-2970
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2970/projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2970

I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2970
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2970
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2970/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2970/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2970/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2970/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.4195, 'learning_rate': 5.674793583843588e-05, 'epoch': 0.46}
{'loss': 0.4457, 'learning_rate': 5.672398267367902e-05, 'epoch': 0.46}
{'loss': 0.4253, 'learning_rate': 5.670002793723209e-05, 'epoch': 0.46}
{'loss': 0.4274, 'learning_rate': 5.667607163469436e-05, 'epoch': 0.46}
{'loss': 0.4361, 'learning_rate': 5.665211377166548e-05, 'epoch': 0.46}
 46%|     | 2971/6500 [5:37:27<6:59:44,  7.14s/it]                                                        46%|     | 2971/6500 [5:37:27<6:59:44,  7.14s/it] 46%|     | 2972/6500 [5:37:34<6:49:08,  6.96s/it]                                                        46%|     | 2972/6500 [5:37:34<6:49:08,  6.96s/it] 46%|     | 2973/6500 [5:37:41<6:41:40,  6.83s/it]                                                        46%|     | 2973/6500 [5:37:41<6:41:40,  6.83s/it] 46%|     | 2974/6500 [5:37:48<6:53:19,  7.03s/it]                                                        46%|     | 2974/6500 [5:37:48<6:53:19,  7.03s/it] 46%|     | 2975/6500 [5:37:55<6:44:32,  6.89s/it]                                                        46%|     | 2975/6500 [5:37:55<6:44:32,  6.89s/it] 46%|     | 2976/6500 [5:38:01<6:38:46,  6.79s/it]            {'loss': 0.4445, 'learning_rate': 5.662815435374544e-05, 'epoch': 0.46}
{'loss': 0.4418, 'learning_rate': 5.660419338653463e-05, 'epoch': 0.46}
{'loss': 0.4424, 'learning_rate': 5.658023087563379e-05, 'epoch': 0.46}
{'loss': 0.4335, 'learning_rate': 5.655626682664397e-05, 'epoch': 0.46}
{'loss': 0.427, 'learning_rate': 5.653230124516663e-05, 'epoch': 0.46}
                                            46%|     | 2976/6500 [5:38:01<6:38:46,  6.79s/it] 46%|     | 2977/6500 [5:38:08<6:34:27,  6.72s/it]                                                        46%|     | 2977/6500 [5:38:08<6:34:27,  6.72s/it] 46%|     | 2978/6500 [5:38:14<6:31:30,  6.67s/it]                                                        46%|     | 2978/6500 [5:38:14<6:31:30,  6.67s/it] 46%|     | 2979/6500 [5:38:21<6:28:56,  6.63s/it]                                                        46%|     | 2979/6500 [5:38:21<6:28:56,  6.63s/it] 46%|     | 2980/6500 [5:38:27<6:27:12,  6.60s/it]                                                        46%|     | 2980/6500 [5:38:27<6:27:12,  6.60s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8477035760879517, 'eval_runtime': 1.4808, 'eval_samples_per_second': 8.104, 'eval_steps_per_second': 2.026, 'epoch': 0.46}
                                                        46%|     | 2980/6500 [5:38:29<6:27:12,  6.60s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-2980
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2980
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2980
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2980
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2980
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2980/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2980/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2980/pytorch_model.binthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2980/pytorch_model.bin

/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.4247, 'learning_rate': 5.650833413680361e-05, 'epoch': 0.46}
{'loss': 0.4671, 'learning_rate': 5.648436550715704e-05, 'epoch': 0.46}
{'loss': 0.4358, 'learning_rate': 5.646039536182949e-05, 'epoch': 0.46}
{'loss': 0.7093, 'learning_rate': 5.643642370642378e-05, 'epoch': 0.46}
{'loss': 0.4433, 'learning_rate': 5.641245054654316e-05, 'epoch': 0.46}
 46%|     | 2981/6500 [5:38:36<6:57:55,  7.13s/it]                                                        46%|     | 2981/6500 [5:38:36<6:57:55,  7.13s/it] 46%|     | 2982/6500 [5:38:42<6:47:35,  6.95s/it]                                                        46%|     | 2982/6500 [5:38:42<6:47:35,  6.95s/it] 46%|     | 2983/6500 [5:38:49<6:40:26,  6.83s/it]                                                        46%|     | 2983/6500 [5:38:49<6:40:26,  6.83s/it] 46%|     | 2984/6500 [5:38:55<6:35:12,  6.74s/it]                                                        46%|     | 2984/6500 [5:38:55<6:35:12,  6.74s/it] 46%|     | 2985/6500 [5:39:02<6:31:32,  6.68s/it]                                                        46%|     | 2985/6500 [5:39:02<6:31:32,  6.68s/it] 46%|     | 2986/6500 [5:39:08<6:29:06,  6.64s/it]            {'loss': 0.4259, 'learning_rate': 5.638847588779121e-05, 'epoch': 0.46}
{'loss': 0.4191, 'learning_rate': 5.636449973577188e-05, 'epoch': 0.46}
{'loss': 0.4192, 'learning_rate': 5.6340522096089424e-05, 'epoch': 0.46}
{'loss': 0.4281, 'learning_rate': 5.631654297434849e-05, 'epoch': 0.46}
{'loss': 0.4304, 'learning_rate': 5.6292562376154037e-05, 'epoch': 0.46}
                                            46%|     | 2986/6500 [5:39:08<6:29:06,  6.64s/it] 46%|     | 2987/6500 [5:39:15<6:27:11,  6.61s/it]                                                        46%|     | 2987/6500 [5:39:15<6:27:11,  6.61s/it] 46%|     | 2988/6500 [5:39:22<6:25:55,  6.59s/it]                                                        46%|     | 2988/6500 [5:39:22<6:25:55,  6.59s/it] 46%|     | 2989/6500 [5:39:28<6:24:58,  6.58s/it]                                                        46%|     | 2989/6500 [5:39:28<6:24:58,  6.58s/it] 46%|     | 2990/6500 [5:39:35<6:39:32,  6.83s/it]                                                        46%|     | 2990/6500 [5:39:35<6:39:32,  6.83s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.85556960105896, 'eval_runtime': 1.4916, 'eval_samples_per_second': 8.045, 'eval_steps_per_second': 2.011, 'epoch': 0.46}
                                                        46%|     | 2990/6500 [5:39:37<6:39:32,  6.83s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-2990
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2990
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2990
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2990
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2990
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2990/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2990/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2990/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-2990/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.4434, 'learning_rate': 5.6268580307111376e-05, 'epoch': 0.46}
{'loss': 0.4367, 'learning_rate': 5.624459677282619e-05, 'epoch': 0.46}
{'loss': 0.4292, 'learning_rate': 5.622061177890447e-05, 'epoch': 0.46}
{'loss': 0.4329, 'learning_rate': 5.619662533095257e-05, 'epoch': 0.46}
{'loss': 0.4235, 'learning_rate': 5.617263743457719e-05, 'epoch': 0.46}
 46%|     | 2991/6500 [5:39:44<7:06:07,  7.29s/it]                                                        46%|     | 2991/6500 [5:39:44<7:06:07,  7.29s/it] 46%|     | 2992/6500 [5:39:50<6:53:00,  7.06s/it]                                                        46%|     | 2992/6500 [5:39:50<6:53:00,  7.06s/it] 46%|     | 2993/6500 [5:39:57<6:43:50,  6.91s/it]                                                        46%|     | 2993/6500 [5:39:57<6:43:50,  6.91s/it] 46%|     | 2994/6500 [5:40:03<6:37:19,  6.80s/it]                                                        46%|     | 2994/6500 [5:40:03<6:37:19,  6.80s/it] 46%|     | 2995/6500 [5:40:10<6:32:47,  6.72s/it]                                                        46%|     | 2995/6500 [5:40:10<6:32:47,  6.72s/it] 46%|     | 2996/6500 [5:40:17<6:29:18,  6.67s/it]            {'loss': 0.4284, 'learning_rate': 5.6148648095385327e-05, 'epoch': 0.46}
{'loss': 0.4587, 'learning_rate': 5.612465731898435e-05, 'epoch': 0.46}
{'loss': 0.4314, 'learning_rate': 5.610066511098198e-05, 'epoch': 0.46}
{'loss': 0.7054, 'learning_rate': 5.607667147698622e-05, 'epoch': 0.46}
{'loss': 0.4356, 'learning_rate': 5.6052676422605467e-05, 'epoch': 0.46}
                                            46%|     | 2996/6500 [5:40:17<6:29:18,  6.67s/it] 46%|     | 2997/6500 [5:40:23<6:26:49,  6.63s/it]                                                        46%|     | 2997/6500 [5:40:23<6:26:49,  6.63s/it] 46%|     | 2998/6500 [5:40:30<6:25:10,  6.60s/it]                                                        46%|     | 2998/6500 [5:40:30<6:25:10,  6.60s/it] 46%|     | 2999/6500 [5:40:36<6:23:51,  6.58s/it]                                                        46%|     | 2999/6500 [5:40:36<6:23:51,  6.58s/it] 46%|     | 3000/6500 [5:40:43<6:22:57,  6.57s/it]                                                        46%|     | 3000/6500 [5:40:43<6:22:57,  6.57s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8541461229324341, 'eval_runtime': 1.4787, 'eval_samples_per_second': 8.115, 'eval_steps_per_second': 2.029, 'epoch': 0.46}
                                                        46%|     | 3000/6500 [5:40:44<6:22:57,  6.57s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-3000
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3000
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3000
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3000
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3000
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3000/pytorch_model.bin
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3000/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3000/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3000/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.4313, 'learning_rate': 5.6028679953448384e-05, 'epoch': 0.46}
{'loss': 0.4235, 'learning_rate': 5.6004682075124016e-05, 'epoch': 0.46}
{'loss': 0.413, 'learning_rate': 5.598068279324172e-05, 'epoch': 0.46}
{'loss': 0.4226, 'learning_rate': 5.595668211341118e-05, 'epoch': 0.46}
{'loss': 0.4242, 'learning_rate': 5.593268004124243e-05, 'epoch': 0.46}
 46%|     | 3001/6500 [5:40:51<6:53:46,  7.10s/it]                                                        46%|     | 3001/6500 [5:40:51<6:53:46,  7.10s/it] 46%|     | 3002/6500 [5:40:58<6:44:01,  6.93s/it]                                                        46%|     | 3002/6500 [5:40:58<6:44:01,  6.93s/it] 46%|     | 3003/6500 [5:41:04<6:37:15,  6.82s/it]                                                        46%|     | 3003/6500 [5:41:04<6:37:15,  6.82s/it] 46%|     | 3004/6500 [5:41:11<6:32:32,  6.74s/it]                                                        46%|     | 3004/6500 [5:41:11<6:32:32,  6.74s/it] 46%|     | 3005/6500 [5:41:17<6:28:49,  6.68s/it]                                                        46%|     | 3005/6500 [5:41:17<6:28:49,  6.68s/it] 46%|     | 3006/6500 [5:41:25<6:44:40,  6.95s/it]            {'loss': 0.4392, 'learning_rate': 5.5908676582345786e-05, 'epoch': 0.46}
{'loss': 0.4381, 'learning_rate': 5.588467174233192e-05, 'epoch': 0.46}
{'loss': 0.4321, 'learning_rate': 5.586066552681179e-05, 'epoch': 0.46}
{'loss': 0.4402, 'learning_rate': 5.583665794139675e-05, 'epoch': 0.46}
{'loss': 0.4215, 'learning_rate': 5.5812648991698415e-05, 'epoch': 0.46}
                                            46%|     | 3006/6500 [5:41:25<6:44:40,  6.95s/it] 46%|     | 3007/6500 [5:41:31<6:37:58,  6.84s/it]                                                        46%|     | 3007/6500 [5:41:31<6:37:58,  6.84s/it] 46%|     | 3008/6500 [5:41:38<6:32:51,  6.75s/it]                                                        46%|     | 3008/6500 [5:41:38<6:32:51,  6.75s/it] 46%|     | 3009/6500 [5:41:44<6:29:02,  6.69s/it]                                                        46%|     | 3009/6500 [5:41:44<6:29:02,  6.69s/it] 46%|     | 3010/6500 [5:41:51<6:26:24,  6.64s/it]                                                        46%|     | 3010/6500 [5:41:51<6:26:24,  6.64s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8471977114677429, 'eval_runtime': 1.4861, 'eval_samples_per_second': 8.075, 'eval_steps_per_second': 2.019, 'epoch': 0.46}
                                                        46%|     | 3010/6500 [5:41:52<6:26:24,  6.64s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-3010
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3010
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3010
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3010
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3010
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3010/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3010/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3010/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3010/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.4282, 'learning_rate': 5.57886386833287e-05, 'epoch': 0.46}
{'loss': 0.4561, 'learning_rate': 5.576462702189989e-05, 'epoch': 0.46}
{'loss': 0.432, 'learning_rate': 5.574061401302456e-05, 'epoch': 0.46}
{'loss': 0.718, 'learning_rate': 5.571659966231562e-05, 'epoch': 0.46}
{'loss': 0.4236, 'learning_rate': 5.569258397538626e-05, 'epoch': 0.46}
 46%|     | 3011/6500 [5:41:59<6:56:18,  7.16s/it]                                                        46%|     | 3011/6500 [5:41:59<6:56:18,  7.16s/it] 46%|     | 3012/6500 [5:42:06<6:45:42,  6.98s/it]                                                        46%|     | 3012/6500 [5:42:06<6:45:42,  6.98s/it] 46%|     | 3013/6500 [5:42:12<6:38:05,  6.85s/it]                                                        46%|     | 3013/6500 [5:42:12<6:38:05,  6.85s/it] 46%|     | 3014/6500 [5:42:19<6:32:43,  6.76s/it]                                                        46%|     | 3014/6500 [5:42:19<6:32:43,  6.76s/it] 46%|     | 3015/6500 [5:42:26<6:28:49,  6.69s/it]                                                        46%|     | 3015/6500 [5:42:26<6:28:49,  6.69s/it] 46%|     | 3016/6500 [5:42:32<6:26:17,  6.65s/it]            {'loss': 0.4479, 'learning_rate': 5.566856695785001e-05, 'epoch': 0.46}
{'loss': 0.4152, 'learning_rate': 5.564454861532069e-05, 'epoch': 0.46}
{'loss': 0.4212, 'learning_rate': 5.5620528953412456e-05, 'epoch': 0.46}
{'loss': 0.4215, 'learning_rate': 5.5596507977739755e-05, 'epoch': 0.46}
{'loss': 0.4298, 'learning_rate': 5.5572485693917345e-05, 'epoch': 0.46}
                                            46%|     | 3016/6500 [5:42:32<6:26:17,  6.65s/it] 46%|     | 3017/6500 [5:42:39<6:24:18,  6.62s/it]                                                        46%|     | 3017/6500 [5:42:39<6:24:18,  6.62s/it] 46%|     | 3018/6500 [5:42:45<6:22:59,  6.60s/it]                                                        46%|     | 3018/6500 [5:42:45<6:22:59,  6.60s/it] 46%|     | 3019/6500 [5:42:52<6:21:54,  6.58s/it]                                                        46%|     | 3019/6500 [5:42:52<6:21:54,  6.58s/it] 46%|     | 3020/6500 [5:42:58<6:21:13,  6.57s/it]                                                        46%|     | 3020/6500 [5:42:58<6:21:13,  6.57s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8562902212142944, 'eval_runtime': 1.4779, 'eval_samples_per_second': 8.119, 'eval_steps_per_second': 2.03, 'epoch': 0.46}
                                                        46%|     | 3020/6500 [5:43:00<6:21:13,  6.57s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-3020
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3020
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3020
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3020
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3020
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3020/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3020/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3020/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3020/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.4481, 'learning_rate': 5.5548462107560284e-05, 'epoch': 0.46}
{'loss': 0.4375, 'learning_rate': 5.552443722428393e-05, 'epoch': 0.46}
{'loss': 0.4257, 'learning_rate': 5.550041104970397e-05, 'epoch': 0.47}
{'loss': 0.4338, 'learning_rate': 5.547638358943637e-05, 'epoch': 0.47}
{'loss': 0.4175, 'learning_rate': 5.5452354849097396e-05, 'epoch': 0.47}
 46%|     | 3021/6500 [5:43:07<6:52:07,  7.11s/it]                                                        46%|     | 3021/6500 [5:43:07<6:52:07,  7.11s/it] 46%|     | 3022/6500 [5:43:14<6:53:14,  7.13s/it]                                                        46%|     | 3022/6500 [5:43:14<6:53:14,  7.13s/it] 47%|     | 3023/6500 [5:43:20<6:42:54,  6.95s/it]                                                        47%|     | 3023/6500 [5:43:20<6:42:54,  6.95s/it] 47%|     | 3024/6500 [5:43:27<6:35:50,  6.83s/it]                                                        47%|     | 3024/6500 [5:43:27<6:35:50,  6.83s/it] 47%|     | 3025/6500 [5:43:33<6:30:39,  6.75s/it]                                                        47%|     | 3025/6500 [5:43:33<6:30:39,  6.75s/it] 47%|     | 3026/6500 [5:43:40<6:27:26,  6.69s/it]            {'loss': 0.4585, 'learning_rate': 5.542832483430363e-05, 'epoch': 0.47}
{'loss': 0.4195, 'learning_rate': 5.540429355067196e-05, 'epoch': 0.47}
{'loss': 0.6859, 'learning_rate': 5.538026100381951e-05, 'epoch': 0.47}
{'loss': 0.4673, 'learning_rate': 5.5356227199363764e-05, 'epoch': 0.47}
{'loss': 0.4246, 'learning_rate': 5.533219214292248e-05, 'epoch': 0.47}
                                            47%|     | 3026/6500 [5:43:40<6:27:26,  6.69s/it] 47%|     | 3027/6500 [5:43:47<6:24:46,  6.65s/it]                                                        47%|     | 3027/6500 [5:43:47<6:24:46,  6.65s/it] 47%|     | 3028/6500 [5:43:53<6:22:54,  6.62s/it]                                                        47%|     | 3028/6500 [5:43:53<6:22:54,  6.62s/it] 47%|     | 3029/6500 [5:44:00<6:21:29,  6.59s/it]                                                        47%|     | 3029/6500 [5:44:00<6:21:29,  6.59s/it] 47%|     | 3030/6500 [5:44:06<6:20:41,  6.58s/it]                                                        47%|     | 3030/6500 [5:44:06<6:20:41,  6.58s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8514503836631775, 'eval_runtime': 1.4955, 'eval_samples_per_second': 8.024, 'eval_steps_per_second': 2.006, 'epoch': 0.47}
                                                        47%|     | 3030/6500 [5:44:08<6:20:41,  6.58s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-3030
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3030I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3030

I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3030
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3030
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3030/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3030/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3030/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3030/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.4433, 'learning_rate': 5.530815584011371e-05, 'epoch': 0.47}
{'loss': 0.4143, 'learning_rate': 5.528411829655579e-05, 'epoch': 0.47}
{'loss': 0.4247, 'learning_rate': 5.5260079517867323e-05, 'epoch': 0.47}
{'loss': 0.4284, 'learning_rate': 5.523603950966726e-05, 'epoch': 0.47}
{'loss': 0.4275, 'learning_rate': 5.5211998277574805e-05, 'epoch': 0.47}
 47%|     | 3031/6500 [5:44:15<6:50:45,  7.10s/it]                                                        47%|     | 3031/6500 [5:44:15<6:50:45,  7.10s/it] 47%|     | 3032/6500 [5:44:21<6:40:59,  6.94s/it]                                                        47%|     | 3032/6500 [5:44:21<6:40:59,  6.94s/it] 47%|     | 3033/6500 [5:44:28<6:33:46,  6.81s/it]                                                        47%|     | 3033/6500 [5:44:28<6:33:46,  6.81s/it] 47%|     | 3034/6500 [5:44:34<6:28:55,  6.73s/it]                                                        47%|     | 3034/6500 [5:44:34<6:28:55,  6.73s/it] 47%|     | 3035/6500 [5:44:41<6:25:35,  6.68s/it]                                                        47%|     | 3035/6500 [5:44:41<6:25:35,  6.68s/it] 47%|     | 3036/6500 [5:44:47<6:23:24,  6.64s/it]            {'loss': 0.4479, 'learning_rate': 5.518795582720944e-05, 'epoch': 0.47}
{'loss': 0.4394, 'learning_rate': 5.5163912164190935e-05, 'epoch': 0.47}
{'loss': 0.4238, 'learning_rate': 5.513986729413937e-05, 'epoch': 0.47}
{'loss': 0.4339, 'learning_rate': 5.511582122267507e-05, 'epoch': 0.47}
{'loss': 0.4119, 'learning_rate': 5.509177395541866e-05, 'epoch': 0.47}
                                            47%|     | 3036/6500 [5:44:47<6:23:24,  6.64s/it] 47%|     | 3037/6500 [5:44:54<6:21:39,  6.61s/it]                                                        47%|     | 3037/6500 [5:44:54<6:21:39,  6.61s/it] 47%|     | 3038/6500 [5:45:00<6:20:39,  6.60s/it]                                                        47%|     | 3038/6500 [5:45:00<6:20:39,  6.60s/it] 47%|     | 3039/6500 [5:45:08<6:35:39,  6.86s/it]                                                        47%|     | 3039/6500 [5:45:08<6:35:39,  6.86s/it] 47%|     | 3040/6500 [5:45:14<6:30:15,  6.77s/it]                                                        47%|     | 3040/6500 [5:45:14<6:30:15,  6.77s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.849726140499115, 'eval_runtime': 1.4758, 'eval_samples_per_second': 8.131, 'eval_steps_per_second': 2.033, 'epoch': 0.47}
                                                        47%|     | 3040/6500 [5:45:16<6:30:15,  6.77s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-3040
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3040
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3040
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3040
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3040
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3040/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3040/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3040/pytorch_model.binthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3040/pytorch_model.bin

/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.4647, 'learning_rate': 5.506772549799105e-05, 'epoch': 0.47}
{'loss': 0.4206, 'learning_rate': 5.504367585601342e-05, 'epoch': 0.47}
{'loss': 0.7087, 'learning_rate': 5.501962503510721e-05, 'epoch': 0.47}
{'loss': 0.4399, 'learning_rate': 5.499557304089419e-05, 'epoch': 0.47}
{'loss': 0.4183, 'learning_rate': 5.497151987899634e-05, 'epoch': 0.47}
 47%|     | 3041/6500 [5:45:23<6:56:47,  7.23s/it]                                                        47%|     | 3041/6500 [5:45:23<6:56:47,  7.23s/it] 47%|     | 3042/6500 [5:45:29<6:44:48,  7.02s/it]                                                        47%|     | 3042/6500 [5:45:29<6:44:48,  7.02s/it] 47%|     | 3043/6500 [5:45:36<6:36:16,  6.88s/it]                                                        47%|     | 3043/6500 [5:45:36<6:36:16,  6.88s/it] 47%|     | 3044/6500 [5:45:42<6:30:15,  6.78s/it]                                                        47%|     | 3044/6500 [5:45:42<6:30:15,  6.78s/it] 47%|     | 3045/6500 [5:45:50<6:45:48,  7.05s/it]                                                        47%|     | 3045/6500 [5:45:50<6:45:48,  7.05s/it] 47%|     | 3046/6500 [5:45:57<6:37:08,  6.90s/it]            {'loss': 0.4419, 'learning_rate': 5.494746555503593e-05, 'epoch': 0.47}
{'loss': 0.4097, 'learning_rate': 5.492341007463554e-05, 'epoch': 0.47}
{'loss': 0.4202, 'learning_rate': 5.489935344341799e-05, 'epoch': 0.47}
{'loss': 0.4339, 'learning_rate': 5.4875295667006346e-05, 'epoch': 0.47}
{'loss': 0.4246, 'learning_rate': 5.4851236751023985e-05, 'epoch': 0.47}
                                            47%|     | 3046/6500 [5:45:57<6:37:08,  6.90s/it] 47%|     | 3047/6500 [5:46:03<6:30:22,  6.78s/it]                                                        47%|     | 3047/6500 [5:46:03<6:30:22,  6.78s/it] 47%|     | 3048/6500 [5:46:10<6:26:30,  6.72s/it]                                                        47%|     | 3048/6500 [5:46:10<6:26:30,  6.72s/it] 47%|     | 3049/6500 [5:46:16<6:23:17,  6.66s/it]                                                        47%|     | 3049/6500 [5:46:16<6:23:17,  6.66s/it] 47%|     | 3050/6500 [5:46:23<6:20:38,  6.62s/it]                                                        47%|     | 3050/6500 [5:46:23<6:20:38,  6.62s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8596779108047485, 'eval_runtime': 1.7101, 'eval_samples_per_second': 7.017, 'eval_steps_per_second': 1.754, 'epoch': 0.47}
                                                        47%|     | 3050/6500 [5:46:24<6:20:38,  6.62s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-3050
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3050
/projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3050
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3050
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3050
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3050/pytorch_model.bin
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3050/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3050/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3050/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.4269, 'learning_rate': 5.482717670109453e-05, 'epoch': 0.47}
{'loss': 0.4319, 'learning_rate': 5.4803115522841866e-05, 'epoch': 0.47}
{'loss': 0.4232, 'learning_rate': 5.477905322189015e-05, 'epoch': 0.47}
{'loss': 0.4274, 'learning_rate': 5.475498980386382e-05, 'epoch': 0.47}
{'loss': 0.4255, 'learning_rate': 5.4730925274387524e-05, 'epoch': 0.47}
 47%|     | 3051/6500 [5:46:31<6:53:46,  7.20s/it]                                                        47%|     | 3051/6500 [5:46:31<6:53:46,  7.20s/it] 47%|     | 3052/6500 [5:46:38<6:42:04,  7.00s/it]                                                        47%|     | 3052/6500 [5:46:38<6:42:04,  7.00s/it] 47%|     | 3053/6500 [5:46:44<6:33:41,  6.85s/it]                                                        47%|     | 3053/6500 [5:46:44<6:33:41,  6.85s/it] 47%|     | 3054/6500 [5:46:51<6:27:44,  6.75s/it]                                                        47%|     | 3054/6500 [5:46:51<6:27:44,  6.75s/it] 47%|     | 3055/6500 [5:46:58<6:40:00,  6.97s/it]                                                        47%|     | 3055/6500 [5:46:58<6:40:00,  6.97s/it] 47%|     | 3056/6500 [5:47:05<6:32:10,  6.83s/it]            {'loss': 0.4638, 'learning_rate': 5.470685963908621e-05, 'epoch': 0.47}
{'loss': 0.4285, 'learning_rate': 5.468279290358507e-05, 'epoch': 0.47}
{'loss': 0.6983, 'learning_rate': 5.465872507350955e-05, 'epoch': 0.47}
{'loss': 0.4453, 'learning_rate': 5.46346561544854e-05, 'epoch': 0.47}
{'loss': 0.421, 'learning_rate': 5.461058615213852e-05, 'epoch': 0.47}
                                            47%|     | 3056/6500 [5:47:05<6:32:10,  6.83s/it] 47%|     | 3057/6500 [5:47:11<6:27:15,  6.75s/it]                                                        47%|     | 3057/6500 [5:47:11<6:27:15,  6.75s/it] 47%|     | 3058/6500 [5:47:18<6:23:15,  6.68s/it]                                                        47%|     | 3058/6500 [5:47:18<6:23:15,  6.68s/it] 47%|     | 3059/6500 [5:47:24<6:20:23,  6.63s/it]                                                        47%|     | 3059/6500 [5:47:24<6:20:23,  6.63s/it] 47%|     | 3060/6500 [5:47:31<6:18:22,  6.60s/it]                                                        47%|     | 3060/6500 [5:47:31<6:18:22,  6.60s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8537673950195312, 'eval_runtime': 1.4779, 'eval_samples_per_second': 8.12, 'eval_steps_per_second': 2.03, 'epoch': 0.47}
                                                        47%|     | 3060/6500 [5:47:32<6:18:22,  6.60s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-3060
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  I AM HERE AND I AM SAVING THE MODEL
/projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3060the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3060

I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3060
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3060
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3060/pytorch_model.bin
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3060/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3060/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3060/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.4225, 'learning_rate': 5.458651507209518e-05, 'epoch': 0.47}
{'loss': 0.4154, 'learning_rate': 5.4562442919981816e-05, 'epoch': 0.47}
{'loss': 0.4285, 'learning_rate': 5.453836970142516e-05, 'epoch': 0.47}
{'loss': 0.4326, 'learning_rate': 5.45142954220522e-05, 'epoch': 0.47}
{'loss': 0.4426, 'learning_rate': 5.449022008749012e-05, 'epoch': 0.47}
 47%|     | 3061/6500 [5:47:39<6:48:12,  7.12s/it]                                                        47%|     | 3061/6500 [5:47:39<6:48:12,  7.12s/it] 47%|     | 3062/6500 [5:47:46<6:37:49,  6.94s/it]                                                        47%|     | 3062/6500 [5:47:46<6:37:49,  6.94s/it] 47%|     | 3063/6500 [5:47:52<6:30:13,  6.81s/it]                                                        47%|     | 3063/6500 [5:47:52<6:30:13,  6.81s/it] 47%|     | 3064/6500 [5:47:59<6:25:13,  6.73s/it]                                                        47%|     | 3064/6500 [5:47:59<6:25:13,  6.73s/it] 47%|     | 3065/6500 [5:48:05<6:21:53,  6.67s/it]                                                        47%|     | 3065/6500 [5:48:05<6:21:53,  6.67s/it] 47%|     | 3066/6500 [5:48:12<6:19:19,  6.63s/it]            {'loss': 0.4335, 'learning_rate': 5.446614370336639e-05, 'epoch': 0.47}
{'loss': 0.4305, 'learning_rate': 5.444206627530873e-05, 'epoch': 0.47}
{'loss': 0.4341, 'learning_rate': 5.441798780894508e-05, 'epoch': 0.47}
{'loss': 0.4219, 'learning_rate': 5.439390830990365e-05, 'epoch': 0.47}
{'loss': 0.414, 'learning_rate': 5.4369827783812864e-05, 'epoch': 0.47}
                                            47%|     | 3066/6500 [5:48:12<6:19:19,  6.63s/it] 47%|     | 3067/6500 [5:48:18<6:17:31,  6.60s/it]                                                        47%|     | 3067/6500 [5:48:18<6:17:31,  6.60s/it] 47%|     | 3068/6500 [5:48:25<6:16:11,  6.58s/it]                                                        47%|     | 3068/6500 [5:48:25<6:16:11,  6.58s/it] 47%|     | 3069/6500 [5:48:31<6:14:55,  6.56s/it]                                                        47%|     | 3069/6500 [5:48:31<6:14:55,  6.56s/it] 47%|     | 3070/6500 [5:48:38<6:14:07,  6.54s/it]                                                        47%|     | 3070/6500 [5:48:38<6:14:07,  6.54s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.851169764995575, 'eval_runtime': 1.4801, 'eval_samples_per_second': 8.108, 'eval_steps_per_second': 2.027, 'epoch': 0.47}
                                                        47%|     | 3070/6500 [5:48:39<6:14:07,  6.54s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-3070
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3070I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3070

I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3070
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3070
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3070/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3070/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3070/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3070/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.4579, 'learning_rate': 5.434574623630141e-05, 'epoch': 0.47}
{'loss': 0.4299, 'learning_rate': 5.432166367299818e-05, 'epoch': 0.47}
{'loss': 0.7077, 'learning_rate': 5.429758009953235e-05, 'epoch': 0.47}
{'loss': 0.4357, 'learning_rate': 5.4273495521533304e-05, 'epoch': 0.47}
{'loss': 0.4259, 'learning_rate': 5.424940994463066e-05, 'epoch': 0.47}
 47%|     | 3071/6500 [5:48:47<7:00:07,  7.35s/it]                                                        47%|     | 3071/6500 [5:48:47<7:00:07,  7.35s/it] 47%|     | 3072/6500 [5:48:54<6:45:52,  7.10s/it]                                                        47%|     | 3072/6500 [5:48:54<6:45:52,  7.10s/it] 47%|     | 3073/6500 [5:49:00<6:35:38,  6.93s/it]                                                        47%|     | 3073/6500 [5:49:00<6:35:38,  6.93s/it] 47%|     | 3074/6500 [5:49:07<6:28:55,  6.81s/it]                                                        47%|     | 3074/6500 [5:49:07<6:28:55,  6.81s/it] 47%|     | 3075/6500 [5:49:13<6:23:47,  6.72s/it]                                                        47%|     | 3075/6500 [5:49:13<6:23:47,  6.72s/it] 47%|     | 3076/6500 [5:49:20<6:20:23,  6.67s/it]            {'loss': 0.4129, 'learning_rate': 5.4225323374454286e-05, 'epoch': 0.47}
{'loss': 0.4142, 'learning_rate': 5.420123581663426e-05, 'epoch': 0.47}
{'loss': 0.4166, 'learning_rate': 5.4177147276800896e-05, 'epoch': 0.47}
{'loss': 0.4198, 'learning_rate': 5.4153057760584755e-05, 'epoch': 0.47}
{'loss': 0.4385, 'learning_rate': 5.4128967273616625e-05, 'epoch': 0.47}
                                            47%|     | 3076/6500 [5:49:20<6:20:23,  6.67s/it] 47%|     | 3077/6500 [5:49:26<6:17:51,  6.62s/it]                                                        47%|     | 3077/6500 [5:49:26<6:17:51,  6.62s/it] 47%|     | 3078/6500 [5:49:33<6:15:59,  6.59s/it]                                                        47%|     | 3078/6500 [5:49:33<6:15:59,  6.59s/it] 47%|     | 3079/6500 [5:49:39<6:14:47,  6.57s/it]                                                        47%|     | 3079/6500 [5:49:39<6:14:47,  6.57s/it] 47%|     | 3080/6500 [5:49:46<6:13:47,  6.56s/it]                                                        47%|     | 3080/6500 [5:49:46<6:13:47,  6.56s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8604676723480225, 'eval_runtime': 1.4805, 'eval_samples_per_second': 8.105, 'eval_steps_per_second': 2.026, 'epoch': 0.47}
                                                        47%|     | 3080/6500 [5:49:47<6:13:47,  6.56s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-3080
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3080I AM HERE AND I AM SAVING THE MODEL

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3080
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3080
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3080
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3080/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3080/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3080/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3080/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.4365, 'learning_rate': 5.410487582152749e-05, 'epoch': 0.47}
{'loss': 0.4269, 'learning_rate': 5.408078340994859e-05, 'epoch': 0.47}
{'loss': 0.4261, 'learning_rate': 5.4056690044511385e-05, 'epoch': 0.47}
{'loss': 0.4167, 'learning_rate': 5.403259573084753e-05, 'epoch': 0.47}
{'loss': 0.4219, 'learning_rate': 5.4008500474588965e-05, 'epoch': 0.47}
 47%|     | 3081/6500 [5:49:54<6:44:49,  7.10s/it]                                                        47%|     | 3081/6500 [5:49:54<6:44:49,  7.10s/it] 47%|     | 3082/6500 [5:50:01<6:35:28,  6.94s/it]                                                        47%|     | 3082/6500 [5:50:01<6:35:28,  6.94s/it] 47%|     | 3083/6500 [5:50:07<6:28:47,  6.83s/it]                                                        47%|     | 3083/6500 [5:50:07<6:28:47,  6.83s/it] 47%|     | 3084/6500 [5:50:14<6:24:07,  6.75s/it]                                                        47%|     | 3084/6500 [5:50:14<6:24:07,  6.75s/it] 47%|     | 3085/6500 [5:50:21<6:20:54,  6.69s/it]                                                        47%|     | 3085/6500 [5:50:21<6:20:54,  6.69s/it] 47%|     | 3086/6500 [5:50:27<6:18:44,  6.66s/it]            {'loss': 0.4492, 'learning_rate': 5.3984404281367786e-05, 'epoch': 0.47}
{'loss': 0.4315, 'learning_rate': 5.3960307156816324e-05, 'epoch': 0.47}
{'loss': 0.706, 'learning_rate': 5.393620910656714e-05, 'epoch': 0.48}
{'loss': 0.4204, 'learning_rate': 5.391211013625301e-05, 'epoch': 0.48}
{'loss': 0.4298, 'learning_rate': 5.3888010251506915e-05, 'epoch': 0.48}
                                            47%|     | 3086/6500 [5:50:27<6:18:44,  6.66s/it] 47%|     | 3087/6500 [5:50:35<6:32:38,  6.90s/it]                                                        47%|     | 3087/6500 [5:50:35<6:32:38,  6.90s/it] 48%|     | 3088/6500 [5:50:41<6:26:40,  6.80s/it]                                                        48%|     | 3088/6500 [5:50:41<6:26:40,  6.80s/it] 48%|     | 3089/6500 [5:50:48<6:22:30,  6.73s/it]                                                        48%|     | 3089/6500 [5:50:48<6:22:30,  6.73s/it] 48%|     | 3090/6500 [5:50:54<6:19:38,  6.68s/it]                                                        48%|     | 3090/6500 [5:50:54<6:19:38,  6.68s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8536235094070435, 'eval_runtime': 1.4752, 'eval_samples_per_second': 8.134, 'eval_steps_per_second': 2.034, 'epoch': 0.48}
                                                        48%|     | 3090/6500 [5:50:56<6:19:38,  6.68s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-3090
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3090
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3090
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3090
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3090
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3090/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3090/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3090/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3090/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.4124, 'learning_rate': 5.3863909457962094e-05, 'epoch': 0.48}
{'loss': 0.4088, 'learning_rate': 5.3839807761251906e-05, 'epoch': 0.48}
{'loss': 0.4177, 'learning_rate': 5.381570516701e-05, 'epoch': 0.48}
{'loss': 0.4199, 'learning_rate': 5.379160168087021e-05, 'epoch': 0.48}
{'loss': 0.4355, 'learning_rate': 5.376749730846657e-05, 'epoch': 0.48}
 48%|     | 3091/6500 [5:51:03<6:48:11,  7.18s/it]                                                        48%|     | 3091/6500 [5:51:03<6:48:11,  7.18s/it] 48%|     | 3092/6500 [5:51:09<6:38:41,  7.02s/it]                                                        48%|     | 3092/6500 [5:51:09<6:38:41,  7.02s/it] 48%|     | 3093/6500 [5:51:16<6:30:30,  6.88s/it]                                                        48%|     | 3093/6500 [5:51:16<6:30:30,  6.88s/it] 48%|     | 3094/6500 [5:51:22<6:24:51,  6.78s/it]                                                        48%|     | 3094/6500 [5:51:22<6:24:51,  6.78s/it] 48%|     | 3095/6500 [5:51:29<6:20:43,  6.71s/it]                                                        48%|     | 3095/6500 [5:51:29<6:20:43,  6.71s/it] 48%|     | 3096/6500 [5:51:35<6:17:42,  6.66s/it]            {'loss': 0.4296, 'learning_rate': 5.374339205543336e-05, 'epoch': 0.48}
{'loss': 0.4178, 'learning_rate': 5.371928592740503e-05, 'epoch': 0.48}
{'loss': 0.4337, 'learning_rate': 5.3695178930016196e-05, 'epoch': 0.48}
{'loss': 0.4145, 'learning_rate': 5.367107106890177e-05, 'epoch': 0.48}
{'loss': 0.4262, 'learning_rate': 5.3646962349696806e-05, 'epoch': 0.48}
                                            48%|     | 3096/6500 [5:51:35<6:17:42,  6.66s/it] 48%|     | 3097/6500 [5:51:42<6:15:39,  6.62s/it]                                                        48%|     | 3097/6500 [5:51:42<6:15:39,  6.62s/it] 48%|     | 3098/6500 [5:51:49<6:14:24,  6.60s/it]                                                        48%|     | 3098/6500 [5:51:49<6:14:24,  6.60s/it] 48%|     | 3099/6500 [5:51:55<6:13:16,  6.59s/it]                                                        48%|     | 3099/6500 [5:51:55<6:13:16,  6.59s/it] 48%|     | 3100/6500 [5:52:02<6:12:39,  6.58s/it]                                                        48%|     | 3100/6500 [5:52:02<6:12:39,  6.58s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8513258099555969, 'eval_runtime': 1.4715, 'eval_samples_per_second': 8.155, 'eval_steps_per_second': 2.039, 'epoch': 0.48}
                                                        48%|     | 3100/6500 [5:52:03<6:12:39,  6.58s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-3100
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3100I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3100

I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3100
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3100
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3100/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3100/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3100/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3100/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.435, 'learning_rate': 5.362285277803656e-05, 'epoch': 0.48}
{'loss': 0.4317, 'learning_rate': 5.3598742359556495e-05, 'epoch': 0.48}
{'loss': 0.714, 'learning_rate': 5.35746310998923e-05, 'epoch': 0.48}
{'loss': 0.4143, 'learning_rate': 5.3550519004679823e-05, 'epoch': 0.48}
{'loss': 0.4411, 'learning_rate': 5.35264060795551e-05, 'epoch': 0.48}
 48%|     | 3101/6500 [5:52:10<6:43:37,  7.12s/it]                                                        48%|     | 3101/6500 [5:52:10<6:43:37,  7.12s/it] 48%|     | 3102/6500 [5:52:17<6:33:44,  6.95s/it]                                                        48%|     | 3102/6500 [5:52:17<6:33:44,  6.95s/it] 48%|     | 3103/6500 [5:52:24<6:37:17,  7.02s/it]                                                        48%|     | 3103/6500 [5:52:24<6:37:17,  7.02s/it] 48%|     | 3104/6500 [5:52:30<6:29:19,  6.88s/it]                                                        48%|     | 3104/6500 [5:52:30<6:29:19,  6.88s/it] 48%|     | 3105/6500 [5:52:37<6:23:40,  6.78s/it]                                                        48%|     | 3105/6500 [5:52:37<6:23:40,  6.78s/it] 48%|     | 3106/6500 [5:52:43<6:19:40,  6.71s/it]            {'loss': 0.4099, 'learning_rate': 5.3502292330154404e-05, 'epoch': 0.48}
{'loss': 0.4169, 'learning_rate': 5.347817776211417e-05, 'epoch': 0.48}
{'loss': 0.4241, 'learning_rate': 5.3454062381071046e-05, 'epoch': 0.48}
{'loss': 0.4248, 'learning_rate': 5.342994619266182e-05, 'epoch': 0.48}
{'loss': 0.4341, 'learning_rate': 5.340582920252354e-05, 'epoch': 0.48}
                                            48%|     | 3106/6500 [5:52:43<6:19:40,  6.71s/it] 48%|     | 3107/6500 [5:52:50<6:17:07,  6.67s/it]                                                        48%|     | 3107/6500 [5:52:50<6:17:07,  6.67s/it] 48%|     | 3108/6500 [5:52:57<6:15:07,  6.64s/it]                                                        48%|     | 3108/6500 [5:52:57<6:15:07,  6.64s/it] 48%|     | 3109/6500 [5:53:03<6:13:41,  6.61s/it]                                                        48%|     | 3109/6500 [5:53:03<6:13:41,  6.61s/it] 48%|     | 3110/6500 [5:53:10<6:12:25,  6.59s/it]                                                        48%|     | 3110/6500 [5:53:10<6:12:25,  6.59s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.86346834897995, 'eval_runtime': 1.4843, 'eval_samples_per_second': 8.084, 'eval_steps_per_second': 2.021, 'epoch': 0.48}
                                                        48%|     | 3110/6500 [5:53:11<6:12:25,  6.59s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-3110
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3110I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3110

I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3110
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3110
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3110/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3110/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3110/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3110/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.436, 'learning_rate': 5.338171141629338e-05, 'epoch': 0.48}
{'loss': 0.4174, 'learning_rate': 5.335759283960874e-05, 'epoch': 0.48}
{'loss': 0.4269, 'learning_rate': 5.3333473478107184e-05, 'epoch': 0.48}
{'loss': 0.4068, 'learning_rate': 5.330935333742649e-05, 'epoch': 0.48}
{'loss': 0.4585, 'learning_rate': 5.328523242320456e-05, 'epoch': 0.48}
 48%|     | 3111/6500 [5:53:18<6:42:39,  7.13s/it]                                                        48%|     | 3111/6500 [5:53:18<6:42:39,  7.13s/it] 48%|     | 3112/6500 [5:53:25<6:32:53,  6.96s/it]                                                        48%|     | 3112/6500 [5:53:25<6:32:53,  6.96s/it] 48%|     | 3113/6500 [5:53:31<6:25:57,  6.84s/it]                                                        48%|     | 3113/6500 [5:53:31<6:25:57,  6.84s/it] 48%|     | 3114/6500 [5:53:38<6:20:56,  6.75s/it]                                                        48%|     | 3114/6500 [5:53:38<6:20:56,  6.75s/it] 48%|     | 3115/6500 [5:53:44<6:17:22,  6.69s/it]                                                        48%|     | 3115/6500 [5:53:44<6:17:22,  6.69s/it] 48%|     | 3116/6500 [5:53:51<6:14:49,  6.65s/it]            {'loss': 0.4154, 'learning_rate': 5.3261110741079525e-05, 'epoch': 0.48}
{'loss': 0.7036, 'learning_rate': 5.323698829668968e-05, 'epoch': 0.48}
{'loss': 0.4344, 'learning_rate': 5.3212865095673514e-05, 'epoch': 0.48}
{'loss': 0.414, 'learning_rate': 5.318874114366965e-05, 'epoch': 0.48}
{'loss': 0.4319, 'learning_rate': 5.316461644631694e-05, 'epoch': 0.48}
                                            48%|     | 3116/6500 [5:53:51<6:14:49,  6.65s/it] 48%|     | 3117/6500 [5:53:57<6:12:32,  6.61s/it]                                                        48%|     | 3117/6500 [5:53:57<6:12:32,  6.61s/it] 48%|     | 3118/6500 [5:54:04<6:11:14,  6.59s/it]                                                        48%|     | 3118/6500 [5:54:04<6:11:14,  6.59s/it] 48%|     | 3119/6500 [5:54:11<6:25:00,  6.83s/it]                                                        48%|     | 3119/6500 [5:54:11<6:25:00,  6.83s/it] 48%|     | 3120/6500 [5:54:18<6:19:57,  6.74s/it]                                                        48%|     | 3120/6500 [5:54:18<6:19:57,  6.74s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8562094569206238, 'eval_runtime': 1.4743, 'eval_samples_per_second': 8.139, 'eval_steps_per_second': 2.035, 'epoch': 0.48}
                                                        48%|     | 3120/6500 [5:54:19<6:19:57,  6.74s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-3120
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3120
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3120
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3120
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3120
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3120/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3120/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3120/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3120/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.4021, 'learning_rate': 5.3140491009254376e-05, 'epoch': 0.48}
{'loss': 0.4125, 'learning_rate': 5.311636483812114e-05, 'epoch': 0.48}
{'loss': 0.4211, 'learning_rate': 5.309223793855655e-05, 'epoch': 0.48}
{'loss': 0.4235, 'learning_rate': 5.306811031620017e-05, 'epoch': 0.48}
{'loss': 0.4254, 'learning_rate': 5.3043981976691645e-05, 'epoch': 0.48}
 48%|     | 3121/6500 [5:54:26<6:46:37,  7.22s/it]                                                        48%|     | 3121/6500 [5:54:26<6:46:37,  7.22s/it] 48%|     | 3122/6500 [5:54:33<6:34:36,  7.01s/it]                                                        48%|     | 3122/6500 [5:54:33<6:34:36,  7.01s/it] 48%|     | 3123/6500 [5:54:39<6:26:18,  6.86s/it]                                                        48%|     | 3123/6500 [5:54:39<6:26:18,  6.86s/it] 48%|     | 3124/6500 [5:54:46<6:20:34,  6.76s/it]                                                        48%|     | 3124/6500 [5:54:46<6:20:34,  6.76s/it] 48%|     | 3125/6500 [5:54:52<6:16:22,  6.69s/it]                                                        48%|     | 3125/6500 [5:54:52<6:16:22,  6.69s/it] 48%|     | 3126/6500 [5:54:59<6:13:23,  6.64s/it]            {'loss': 0.4291, 'learning_rate': 5.301985292567084e-05, 'epoch': 0.48}
{'loss': 0.4186, 'learning_rate': 5.299572316877778e-05, 'epoch': 0.48}
{'loss': 0.4277, 'learning_rate': 5.297159271165264e-05, 'epoch': 0.48}
{'loss': 0.4068, 'learning_rate': 5.2947461559935786e-05, 'epoch': 0.48}
{'loss': 0.4631, 'learning_rate': 5.292332971926769e-05, 'epoch': 0.48}
                                            48%|     | 3126/6500 [5:54:59<6:13:23,  6.64s/it] 48%|     | 3127/6500 [5:55:05<6:11:19,  6.61s/it]                                                        48%|     | 3127/6500 [5:55:05<6:11:19,  6.61s/it] 48%|     | 3128/6500 [5:55:12<6:09:45,  6.58s/it]                                                        48%|     | 3128/6500 [5:55:12<6:09:45,  6.58s/it] 48%|     | 3129/6500 [5:55:18<6:08:38,  6.56s/it]                                                        48%|     | 3129/6500 [5:55:18<6:08:38,  6.56s/it] 48%|     | 3130/6500 [5:55:25<6:07:49,  6.55s/it]                                                        48%|     | 3130/6500 [5:55:25<6:07:49,  6.55s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8518363237380981, 'eval_runtime': 1.4742, 'eval_samples_per_second': 8.14, 'eval_steps_per_second': 2.035, 'epoch': 0.48}
                                                        48%|     | 3130/6500 [5:55:26<6:07:49,  6.55s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-3130
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3130
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3130
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3130
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3130
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3130/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3130/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3130/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3130/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.4256, 'learning_rate': 5.289919719528905e-05, 'epoch': 0.48}
{'loss': 0.6948, 'learning_rate': 5.2875063993640707e-05, 'epoch': 0.48}
{'loss': 0.4374, 'learning_rate': 5.285093011996362e-05, 'epoch': 0.48}
{'loss': 0.4119, 'learning_rate': 5.2826795579898956e-05, 'epoch': 0.48}
{'loss': 0.4297, 'learning_rate': 5.280266037908802e-05, 'epoch': 0.48}
 48%|     | 3131/6500 [5:55:33<6:38:01,  7.09s/it]                                                        48%|     | 3131/6500 [5:55:33<6:38:01,  7.09s/it] 48%|     | 3132/6500 [5:55:40<6:28:25,  6.92s/it]                                                        48%|     | 3132/6500 [5:55:40<6:28:25,  6.92s/it] 48%|     | 3133/6500 [5:55:46<6:21:32,  6.80s/it]                                                        48%|     | 3133/6500 [5:55:46<6:21:32,  6.80s/it] 48%|     | 3134/6500 [5:55:53<6:16:40,  6.71s/it]                                                        48%|     | 3134/6500 [5:55:53<6:16:40,  6.71s/it] 48%|     | 3135/6500 [5:55:59<6:13:14,  6.66s/it]                                                        48%|     | 3135/6500 [5:55:59<6:13:14,  6.66s/it] 48%|     | 3136/6500 [5:56:07<6:25:23,  6.87s/it]            {'loss': 0.411, 'learning_rate': 5.277852452317226e-05, 'epoch': 0.48}
{'loss': 0.416, 'learning_rate': 5.2754388017793274e-05, 'epoch': 0.48}
{'loss': 0.4222, 'learning_rate': 5.2730250868592845e-05, 'epoch': 0.48}
{'loss': 0.4345, 'learning_rate': 5.270611308121287e-05, 'epoch': 0.48}
{'loss': 0.4233, 'learning_rate': 5.268197466129542e-05, 'epoch': 0.48}
                                            48%|     | 3136/6500 [5:56:07<6:25:23,  6.87s/it] 48%|     | 3137/6500 [5:56:13<6:19:20,  6.77s/it]                                                        48%|     | 3137/6500 [5:56:13<6:19:20,  6.77s/it] 48%|     | 3138/6500 [5:56:20<6:15:05,  6.69s/it]                                                        48%|     | 3138/6500 [5:56:20<6:15:05,  6.69s/it] 48%|     | 3139/6500 [5:56:26<6:12:14,  6.65s/it]                                                        48%|     | 3139/6500 [5:56:26<6:12:14,  6.65s/it] 48%|     | 3140/6500 [5:56:33<6:09:54,  6.61s/it]                                                        48%|     | 3140/6500 [5:56:33<6:09:54,  6.61s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8617235422134399, 'eval_runtime': 1.4703, 'eval_samples_per_second': 8.162, 'eval_steps_per_second': 2.04, 'epoch': 0.48}
                                                        48%|     | 3140/6500 [5:56:34<6:09:54,  6.61s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-3140
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3140I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3140

I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3140
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3140
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3140/pytorch_model.bin
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3140/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3140/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3140/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.428, 'learning_rate': 5.2657835614482706e-05, 'epoch': 0.48}
{'loss': 0.4291, 'learning_rate': 5.2633695946417075e-05, 'epoch': 0.48}
{'loss': 0.4198, 'learning_rate': 5.260955566274103e-05, 'epoch': 0.48}
{'loss': 0.4132, 'learning_rate': 5.2585414769097207e-05, 'epoch': 0.48}
{'loss': 0.4629, 'learning_rate': 5.2561273271128396e-05, 'epoch': 0.48}
 48%|     | 3141/6500 [5:56:41<6:38:29,  7.12s/it]                                                        48%|     | 3141/6500 [5:56:41<6:38:29,  7.12s/it] 48%|     | 3142/6500 [5:56:48<6:28:20,  6.94s/it]                                                        48%|     | 3142/6500 [5:56:48<6:28:20,  6.94s/it] 48%|     | 3143/6500 [5:56:54<6:21:21,  6.82s/it]                                                        48%|     | 3143/6500 [5:56:54<6:21:21,  6.82s/it] 48%|     | 3144/6500 [5:57:01<6:16:03,  6.72s/it]                                                        48%|     | 3144/6500 [5:57:01<6:16:03,  6.72s/it] 48%|     | 3145/6500 [5:57:07<6:12:26,  6.66s/it]                                                        48%|     | 3145/6500 [5:57:07<6:12:26,  6.66s/it] 48%|     | 3146/6500 [5:57:14<6:09:54,  6.62s/it]            {'loss': 0.4223, 'learning_rate': 5.253713117447755e-05, 'epoch': 0.48}
{'loss': 0.7023, 'learning_rate': 5.2512988484787704e-05, 'epoch': 0.48}
{'loss': 0.4377, 'learning_rate': 5.248884520770209e-05, 'epoch': 0.48}
{'loss': 0.424, 'learning_rate': 5.246470134886403e-05, 'epoch': 0.48}
{'loss': 0.4171, 'learning_rate': 5.2440556913917014e-05, 'epoch': 0.48}
                                            48%|     | 3146/6500 [5:57:14<6:09:54,  6.62s/it] 48%|     | 3147/6500 [5:57:20<6:08:07,  6.59s/it]                                                        48%|     | 3147/6500 [5:57:20<6:08:07,  6.59s/it] 48%|     | 3148/6500 [5:57:27<6:06:57,  6.57s/it]                                                        48%|     | 3148/6500 [5:57:27<6:06:57,  6.57s/it] 48%|     | 3149/6500 [5:57:33<6:06:20,  6.56s/it]                                                        48%|     | 3149/6500 [5:57:33<6:06:20,  6.56s/it] 48%|     | 3150/6500 [5:57:40<6:05:44,  6.55s/it]                                                        48%|     | 3150/6500 [5:57:40<6:05:44,  6.55s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8545543551445007, 'eval_runtime': 1.4733, 'eval_samples_per_second': 8.145, 'eval_steps_per_second': 2.036, 'epoch': 0.48}
                                                        48%|     | 3150/6500 [5:57:41<6:05:44,  6.55s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-3150
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3150
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3150
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3150
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3150
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3150/pytorch_model.bin
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3150/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3150/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3150/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.4108, 'learning_rate': 5.241641190850466e-05, 'epoch': 0.48}
{'loss': 0.4181, 'learning_rate': 5.2392266338270736e-05, 'epoch': 0.48}
{'loss': 0.4188, 'learning_rate': 5.236812020885907e-05, 'epoch': 0.49}
{'loss': 0.4451, 'learning_rate': 5.2343973525913716e-05, 'epoch': 0.49}
{'loss': 0.435, 'learning_rate': 5.23198262950788e-05, 'epoch': 0.49}
 48%|     | 3151/6500 [5:57:48<6:35:40,  7.09s/it]                                                        48%|     | 3151/6500 [5:57:48<6:35:40,  7.09s/it] 48%|     | 3152/6500 [5:57:55<6:40:46,  7.18s/it]                                                        48%|     | 3152/6500 [5:57:55<6:40:46,  7.18s/it] 49%|     | 3153/6500 [5:58:02<6:29:37,  6.98s/it]                                                        49%|     | 3153/6500 [5:58:02<6:29:37,  6.98s/it] 49%|     | 3154/6500 [5:58:09<6:21:52,  6.85s/it]                                                        49%|     | 3154/6500 [5:58:09<6:21:52,  6.85s/it] 49%|     | 3155/6500 [5:58:15<6:16:26,  6.75s/it]                                                        49%|     | 3155/6500 [5:58:15<6:16:26,  6.75s/it] 49%|     | 3156/6500 [5:58:22<6:12:47,  6.69s/it]            {'loss': 0.4248, 'learning_rate': 5.229567852199859e-05, 'epoch': 0.49}
{'loss': 0.4237, 'learning_rate': 5.2271530212317487e-05, 'epoch': 0.49}
{'loss': 0.4168, 'learning_rate': 5.2247381371680014e-05, 'epoch': 0.49}
{'loss': 0.4216, 'learning_rate': 5.222323200573081e-05, 'epoch': 0.49}
{'loss': 0.449, 'learning_rate': 5.219908212011463e-05, 'epoch': 0.49}
                                            49%|     | 3156/6500 [5:58:22<6:12:47,  6.69s/it] 49%|     | 3157/6500 [5:58:28<6:10:01,  6.64s/it]                                                        49%|     | 3157/6500 [5:58:28<6:10:01,  6.64s/it] 49%|     | 3158/6500 [5:58:35<6:07:51,  6.60s/it]                                                        49%|     | 3158/6500 [5:58:35<6:07:51,  6.60s/it] 49%|     | 3159/6500 [5:58:41<6:06:21,  6.58s/it]                                                        49%|     | 3159/6500 [5:58:41<6:06:21,  6.58s/it] 49%|     | 3160/6500 [5:58:48<6:05:36,  6.57s/it]                                                        49%|     | 3160/6500 [5:58:48<6:05:36,  6.57s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8524585366249084, 'eval_runtime': 1.4767, 'eval_samples_per_second': 8.126, 'eval_steps_per_second': 2.032, 'epoch': 0.49}
                                                        49%|     | 3160/6500 [5:58:49<6:05:36,  6.57s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-3160
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3160I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3160

I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3160
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3160
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3160/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3160/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3160/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3160/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.4293, 'learning_rate': 5.217493172047637e-05, 'epoch': 0.49}
{'loss': 0.704, 'learning_rate': 5.2150780812461075e-05, 'epoch': 0.49}
{'loss': 0.4176, 'learning_rate': 5.2126629401713814e-05, 'epoch': 0.49}
{'loss': 0.4339, 'learning_rate': 5.210247749387986e-05, 'epoch': 0.49}
{'loss': 0.4102, 'learning_rate': 5.2078325094604596e-05, 'epoch': 0.49}
 49%|     | 3161/6500 [5:58:56<6:35:20,  7.10s/it]                                                        49%|     | 3161/6500 [5:58:56<6:35:20,  7.10s/it] 49%|     | 3162/6500 [5:59:03<6:25:45,  6.93s/it]                                                        49%|     | 3162/6500 [5:59:03<6:25:45,  6.93s/it] 49%|     | 3163/6500 [5:59:09<6:19:09,  6.82s/it]                                                        49%|     | 3163/6500 [5:59:09<6:19:09,  6.82s/it] 49%|     | 3164/6500 [5:59:16<6:14:15,  6.73s/it]                                                        49%|     | 3164/6500 [5:59:16<6:14:15,  6.73s/it] 49%|     | 3165/6500 [5:59:22<6:10:41,  6.67s/it]                                                        49%|     | 3165/6500 [5:59:22<6:10:41,  6.67s/it] 49%|     | 3166/6500 [5:59:29<6:08:25,  6.63s/it]            {'loss': 0.3994, 'learning_rate': 5.205417220953346e-05, 'epoch': 0.49}
{'loss': 0.4175, 'learning_rate': 5.203001884431208e-05, 'epoch': 0.49}
{'loss': 0.4329, 'learning_rate': 5.200586500458612e-05, 'epoch': 0.49}
{'loss': 0.4314, 'learning_rate': 5.198171069600141e-05, 'epoch': 0.49}
{'loss': 0.4181, 'learning_rate': 5.195755592420387e-05, 'epoch': 0.49}
                                            49%|     | 3166/6500 [5:59:29<6:08:25,  6.63s/it] 49%|     | 3167/6500 [5:59:35<6:06:41,  6.60s/it]                                                        49%|     | 3167/6500 [5:59:35<6:06:41,  6.60s/it] 49%|     | 3168/6500 [5:59:43<6:20:40,  6.86s/it]                                                        49%|     | 3168/6500 [5:59:43<6:20:40,  6.86s/it] 49%|     | 3169/6500 [5:59:49<6:15:07,  6.76s/it]                                                        49%|     | 3169/6500 [5:59:49<6:15:07,  6.76s/it] 49%|     | 3170/6500 [5:59:56<6:11:13,  6.69s/it]                                                        49%|     | 3170/6500 [5:59:56<6:11:13,  6.69s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8615925908088684, 'eval_runtime': 1.4761, 'eval_samples_per_second': 8.129, 'eval_steps_per_second': 2.032, 'epoch': 0.49}
                                                        49%|     | 3170/6500 [5:59:57<6:11:13,  6.69s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-3170
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3170
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3170
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3170
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3170
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3170/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3170/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3170/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3170/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.4153, 'learning_rate': 5.193340069483955e-05, 'epoch': 0.49}
{'loss': 0.4294, 'learning_rate': 5.1909245013554564e-05, 'epoch': 0.49}
{'loss': 0.4067, 'learning_rate': 5.188508888599517e-05, 'epoch': 0.49}
{'loss': 0.4212, 'learning_rate': 5.186093231780771e-05, 'epoch': 0.49}
{'loss': 0.4362, 'learning_rate': 5.183677531463863e-05, 'epoch': 0.49}
 49%|     | 3171/6500 [6:00:04<6:38:40,  7.19s/it]                                                        49%|     | 3171/6500 [6:00:04<6:38:40,  7.19s/it] 49%|     | 3172/6500 [6:00:11<6:28:24,  7.00s/it]                                                        49%|     | 3172/6500 [6:00:11<6:28:24,  7.00s/it] 49%|     | 3173/6500 [6:00:17<6:20:22,  6.86s/it]                                                        49%|     | 3173/6500 [6:00:17<6:20:22,  6.86s/it] 49%|     | 3174/6500 [6:00:24<6:14:41,  6.76s/it]                                                        49%|     | 3174/6500 [6:00:24<6:14:41,  6.76s/it] 49%|     | 3175/6500 [6:00:30<6:10:49,  6.69s/it]                                                        49%|     | 3175/6500 [6:00:30<6:10:49,  6.69s/it] 49%|     | 3176/6500 [6:00:37<6:07:59,  6.64s/it]            {'loss': 0.4189, 'learning_rate': 5.1812617882134486e-05, 'epoch': 0.49}
{'loss': 0.6997, 'learning_rate': 5.1788460025941934e-05, 'epoch': 0.49}
{'loss': 0.4179, 'learning_rate': 5.1764301751707735e-05, 'epoch': 0.49}
{'loss': 0.4356, 'learning_rate': 5.174014306507873e-05, 'epoch': 0.49}
{'loss': 0.4024, 'learning_rate': 5.171598397170184e-05, 'epoch': 0.49}
                                            49%|     | 3176/6500 [6:00:37<6:07:59,  6.64s/it] 49%|     | 3177/6500 [6:00:43<6:06:04,  6.61s/it]                                                        49%|     | 3177/6500 [6:00:43<6:06:04,  6.61s/it] 49%|     | 3178/6500 [6:00:50<6:04:37,  6.59s/it]                                                        49%|     | 3178/6500 [6:00:50<6:04:37,  6.59s/it] 49%|     | 3179/6500 [6:00:56<6:03:16,  6.56s/it]                                                        49%|     | 3179/6500 [6:00:56<6:03:16,  6.56s/it] 49%|     | 3180/6500 [6:01:03<6:02:36,  6.55s/it]                                                        49%|     | 3180/6500 [6:01:03<6:02:36,  6.55s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8561866879463196, 'eval_runtime': 1.47, 'eval_samples_per_second': 8.163, 'eval_steps_per_second': 2.041, 'epoch': 0.49}
                                                        49%|     | 3180/6500 [6:01:04<6:02:36,  6.55s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-3180
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3180
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3180
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3180
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3180
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3180/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3180/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3180/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3180/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.4035, 'learning_rate': 5.169182447722415e-05, 'epoch': 0.49}
{'loss': 0.4194, 'learning_rate': 5.1667664587292776e-05, 'epoch': 0.49}
{'loss': 0.4095, 'learning_rate': 5.164350430755494e-05, 'epoch': 0.49}
{'loss': 0.4299, 'learning_rate': 5.161934364365796e-05, 'epoch': 0.49}
{'loss': 0.4369, 'learning_rate': 5.159518260124925e-05, 'epoch': 0.49}
 49%|     | 3181/6500 [6:01:11<6:32:15,  7.09s/it]                                                        49%|     | 3181/6500 [6:01:11<6:32:15,  7.09s/it] 49%|     | 3182/6500 [6:01:18<6:22:57,  6.93s/it]                                                        49%|     | 3182/6500 [6:01:18<6:22:57,  6.93s/it] 49%|     | 3183/6500 [6:01:24<6:16:20,  6.81s/it]                                                        49%|     | 3183/6500 [6:01:24<6:16:20,  6.81s/it] 49%|     | 3184/6500 [6:01:32<6:23:54,  6.95s/it]                                                        49%|     | 3184/6500 [6:01:32<6:23:54,  6.95s/it] 49%|     | 3185/6500 [6:01:38<6:17:05,  6.83s/it]                                                        49%|     | 3185/6500 [6:01:38<6:17:05,  6.83s/it] 49%|     | 3186/6500 [6:01:45<6:12:17,  6.74s/it]            {'loss': 0.412, 'learning_rate': 5.157102118597631e-05, 'epoch': 0.49}
{'loss': 0.4274, 'learning_rate': 5.154685940348671e-05, 'epoch': 0.49}
{'loss': 0.4102, 'learning_rate': 5.1522697259428146e-05, 'epoch': 0.49}
{'loss': 0.4545, 'learning_rate': 5.1498534759448346e-05, 'epoch': 0.49}
{'loss': 0.4093, 'learning_rate': 5.147437190919516e-05, 'epoch': 0.49}
                                            49%|     | 3186/6500 [6:01:45<6:12:17,  6.74s/it] 49%|     | 3187/6500 [6:01:51<6:09:00,  6.68s/it]                                                        49%|     | 3187/6500 [6:01:51<6:09:00,  6.68s/it] 49%|     | 3188/6500 [6:01:58<6:06:30,  6.64s/it]                                                        49%|     | 3188/6500 [6:01:58<6:06:30,  6.64s/it] 49%|     | 3189/6500 [6:02:04<6:04:34,  6.61s/it]                                                        49%|     | 3189/6500 [6:02:04<6:04:34,  6.61s/it] 49%|     | 3190/6500 [6:02:11<6:03:29,  6.59s/it]                                                        49%|     | 3190/6500 [6:02:11<6:03:29,  6.59s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8515336513519287, 'eval_runtime': 1.495, 'eval_samples_per_second': 8.027, 'eval_steps_per_second': 2.007, 'epoch': 0.49}
                                                        49%|     | 3190/6500 [6:02:12<6:03:29,  6.59s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-3190
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3190
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3190
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3190
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3190
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3190/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3190/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3190/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3190/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.6971, 'learning_rate': 5.1450208714316504e-05, 'epoch': 0.49}
{'loss': 0.4385, 'learning_rate': 5.142604518046038e-05, 'epoch': 0.49}
{'loss': 0.4172, 'learning_rate': 5.140188131327486e-05, 'epoch': 0.49}
{'loss': 0.4352, 'learning_rate': 5.1377717118408105e-05, 'epoch': 0.49}
{'loss': 0.4003, 'learning_rate': 5.1353552601508356e-05, 'epoch': 0.49}
 49%|     | 3191/6500 [6:02:19<6:33:54,  7.14s/it]                                                        49%|     | 3191/6500 [6:02:19<6:33:54,  7.14s/it] 49%|     | 3192/6500 [6:02:26<6:23:53,  6.96s/it]                                                        49%|     | 3192/6500 [6:02:26<6:23:53,  6.96s/it] 49%|     | 3193/6500 [6:02:32<6:16:31,  6.83s/it]                                                        49%|     | 3193/6500 [6:02:32<6:16:31,  6.83s/it] 49%|     | 3194/6500 [6:02:39<6:11:21,  6.74s/it]                                                        49%|     | 3194/6500 [6:02:39<6:11:21,  6.74s/it] 49%|     | 3195/6500 [6:02:45<6:07:50,  6.68s/it]                                                        49%|     | 3195/6500 [6:02:45<6:07:50,  6.68s/it] 49%|     | 3196/6500 [6:02:52<6:05:33,  6.64s/it]            {'loss': 0.4147, 'learning_rate': 5.132938776822391e-05, 'epoch': 0.49}
{'loss': 0.418, 'learning_rate': 5.130522262420316e-05, 'epoch': 0.49}
{'loss': 0.4221, 'learning_rate': 5.128105717509456e-05, 'epoch': 0.49}
{'loss': 0.4225, 'learning_rate': 5.1256891426546625e-05, 'epoch': 0.49}
{'loss': 0.426, 'learning_rate': 5.123272538420798e-05, 'epoch': 0.49}
                                            49%|     | 3196/6500 [6:02:52<6:05:33,  6.64s/it] 49%|     | 3197/6500 [6:02:58<6:03:55,  6.61s/it]                                                        49%|     | 3197/6500 [6:02:58<6:03:55,  6.61s/it] 49%|     | 3198/6500 [6:03:05<6:02:30,  6.59s/it]                                                        49%|     | 3198/6500 [6:03:05<6:02:30,  6.59s/it] 49%|     | 3199/6500 [6:03:12<6:01:27,  6.57s/it]                                                        49%|     | 3199/6500 [6:03:12<6:01:27,  6.57s/it] 49%|     | 3200/6500 [6:03:19<6:15:14,  6.82s/it]                                                        49%|     | 3200/6500 [6:03:19<6:15:14,  6.82s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8595191240310669, 'eval_runtime': 1.4741, 'eval_samples_per_second': 8.141, 'eval_steps_per_second': 2.035, 'epoch': 0.49}
                                                        49%|     | 3200/6500 [6:03:20<6:15:14,  6.82s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-3200
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3200
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3200
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3200
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3200
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3200/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3200/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3200/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3200/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.4062, 'learning_rate': 5.1208559053727257e-05, 'epoch': 0.49}
{'loss': 0.4282, 'learning_rate': 5.11843924407532e-05, 'epoch': 0.49}
{'loss': 0.4057, 'learning_rate': 5.1160225550934624e-05, 'epoch': 0.49}
{'loss': 0.454, 'learning_rate': 5.1136058389920374e-05, 'epoch': 0.49}
{'loss': 0.4091, 'learning_rate': 5.111189096335939e-05, 'epoch': 0.49}
 49%|     | 3201/6500 [6:03:27<6:39:56,  7.27s/it]                                                        49%|     | 3201/6500 [6:03:27<6:39:56,  7.27s/it] 49%|     | 3202/6500 [6:03:34<6:27:29,  7.05s/it]                                                        49%|     | 3202/6500 [6:03:34<6:27:29,  7.05s/it] 49%|     | 3203/6500 [6:03:40<6:18:56,  6.90s/it]                                                        49%|     | 3203/6500 [6:03:40<6:18:56,  6.90s/it] 49%|     | 3204/6500 [6:03:47<6:12:38,  6.78s/it]                                                        49%|     | 3204/6500 [6:03:47<6:12:38,  6.78s/it] 49%|     | 3205/6500 [6:03:53<6:08:19,  6.71s/it]                                                        49%|     | 3205/6500 [6:03:53<6:08:19,  6.71s/it] 49%|     | 3206/6500 [6:04:00<6:05:10,  6.65s/it]            {'loss': 0.6911, 'learning_rate': 5.1087723276900646e-05, 'epoch': 0.49}
{'loss': 0.4386, 'learning_rate': 5.106355533619319e-05, 'epoch': 0.49}
{'loss': 0.413, 'learning_rate': 5.1039387146886154e-05, 'epoch': 0.49}
{'loss': 0.4287, 'learning_rate': 5.101521871462869e-05, 'epoch': 0.49}
{'loss': 0.4136, 'learning_rate': 5.0991050045070024e-05, 'epoch': 0.49}
                                            49%|     | 3206/6500 [6:04:00<6:05:10,  6.65s/it] 49%|     | 3207/6500 [6:04:06<6:03:37,  6.63s/it]                                                        49%|     | 3207/6500 [6:04:06<6:03:37,  6.63s/it] 49%|     | 3208/6500 [6:04:13<6:01:52,  6.60s/it]                                                        49%|     | 3208/6500 [6:04:13<6:01:52,  6.60s/it] 49%|     | 3209/6500 [6:04:20<6:00:46,  6.58s/it]                                                        49%|     | 3209/6500 [6:04:20<6:00:46,  6.58s/it] 49%|     | 3210/6500 [6:04:26<5:59:57,  6.56s/it]                                                        49%|     | 3210/6500 [6:04:26<5:59:57,  6.56s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8551218509674072, 'eval_runtime': 1.4759, 'eval_samples_per_second': 8.13, 'eval_steps_per_second': 2.033, 'epoch': 0.49}
                                                        49%|     | 3210/6500 [6:04:28<5:59:57,  6.56s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-3210
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3210I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3210

I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3210
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3210
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3210/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3210/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3210/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3210/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.4197, 'learning_rate': 5.0966881143859435e-05, 'epoch': 0.49}
{'loss': 0.4235, 'learning_rate': 5.094271201664625e-05, 'epoch': 0.49}
{'loss': 0.4219, 'learning_rate': 5.091854266907987e-05, 'epoch': 0.49}
{'loss': 0.4235, 'learning_rate': 5.089437310680972e-05, 'epoch': 0.49}
{'loss': 0.4257, 'learning_rate': 5.08702033354853e-05, 'epoch': 0.49}
 49%|     | 3211/6500 [6:04:34<6:30:01,  7.12s/it]                                                        49%|     | 3211/6500 [6:04:34<6:30:01,  7.12s/it] 49%|     | 3212/6500 [6:04:41<6:20:31,  6.94s/it]                                                        49%|     | 3212/6500 [6:04:41<6:20:31,  6.94s/it] 49%|     | 3213/6500 [6:04:48<6:13:32,  6.82s/it]                                                        49%|     | 3213/6500 [6:04:48<6:13:32,  6.82s/it] 49%|     | 3214/6500 [6:04:54<6:08:51,  6.74s/it]                                                        49%|     | 3214/6500 [6:04:54<6:08:51,  6.74s/it] 49%|     | 3215/6500 [6:05:01<6:05:31,  6.68s/it]                                                        49%|     | 3215/6500 [6:05:01<6:05:31,  6.68s/it] 49%|     | 3216/6500 [6:05:08<6:17:32,  6.90s/it]            {'loss': 0.4204, 'learning_rate': 5.0846033360756155e-05, 'epoch': 0.49}
{'loss': 0.4136, 'learning_rate': 5.082186318827184e-05, 'epoch': 0.49}
{'loss': 0.4059, 'learning_rate': 5.0797692823682e-05, 'epoch': 0.5}
{'loss': 0.4522, 'learning_rate': 5.077352227263632e-05, 'epoch': 0.5}
{'loss': 0.4218, 'learning_rate': 5.07493515407845e-05, 'epoch': 0.5}
                                            49%|     | 3216/6500 [6:05:08<6:17:32,  6.90s/it] 49%|     | 3217/6500 [6:05:15<6:11:29,  6.79s/it]                                                        49%|     | 3217/6500 [6:05:15<6:11:29,  6.79s/it] 50%|     | 3218/6500 [6:05:21<6:07:07,  6.71s/it]                                                        50%|     | 3218/6500 [6:05:21<6:07:07,  6.71s/it] 50%|     | 3219/6500 [6:05:28<6:03:49,  6.65s/it]                                                        50%|     | 3219/6500 [6:05:28<6:03:49,  6.65s/it] 50%|     | 3220/6500 [6:05:34<6:01:39,  6.62s/it]                                                        50%|     | 3220/6500 [6:05:34<6:01:39,  6.62s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8557641506195068, 'eval_runtime': 1.4766, 'eval_samples_per_second': 8.127, 'eval_steps_per_second': 2.032, 'epoch': 0.5}
                                                        50%|     | 3220/6500 [6:05:36<6:01:39,  6.62s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-3220
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3220I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in 
 /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3220
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3220
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3220
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3220/pytorch_model.bin
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3220/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3220/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3220/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.6975, 'learning_rate': 5.0725180633776315e-05, 'epoch': 0.5}
{'loss': 0.4277, 'learning_rate': 5.070100955726159e-05, 'epoch': 0.5}
{'loss': 0.4181, 'learning_rate': 5.0676838316890116e-05, 'epoch': 0.5}
{'loss': 0.4112, 'learning_rate': 5.065266691831181e-05, 'epoch': 0.5}
{'loss': 0.407, 'learning_rate': 5.0628495367176576e-05, 'epoch': 0.5}
 50%|     | 3221/6500 [6:05:42<6:29:42,  7.13s/it]                                                        50%|     | 3221/6500 [6:05:43<6:29:42,  7.13s/it] 50%|     | 3222/6500 [6:05:49<6:19:44,  6.95s/it]                                                        50%|     | 3222/6500 [6:05:49<6:19:44,  6.95s/it] 50%|     | 3223/6500 [6:05:56<6:12:27,  6.82s/it]                                                        50%|     | 3223/6500 [6:05:56<6:12:27,  6.82s/it] 50%|     | 3224/6500 [6:06:02<6:07:36,  6.73s/it]                                                        50%|     | 3224/6500 [6:06:02<6:07:36,  6.73s/it] 50%|     | 3225/6500 [6:06:09<6:04:49,  6.68s/it]                                                        50%|     | 3225/6500 [6:06:09<6:04:49,  6.68s/it] 50%|     | 3226/6500 [6:06:15<6:01:58,  6.63s/it]            {'loss': 0.4155, 'learning_rate': 5.060432366913438e-05, 'epoch': 0.5}
{'loss': 0.4126, 'learning_rate': 5.058015182983519e-05, 'epoch': 0.5}
{'loss': 0.4255, 'learning_rate': 5.055597985492906e-05, 'epoch': 0.5}
{'loss': 0.4131, 'learning_rate': 5.053180775006599e-05, 'epoch': 0.5}
{'loss': 0.4177, 'learning_rate': 5.050763552089611e-05, 'epoch': 0.5}
                                            50%|     | 3226/6500 [6:06:15<6:01:58,  6.63s/it] 50%|     | 3227/6500 [6:06:22<6:00:06,  6.60s/it]                                                        50%|     | 3227/6500 [6:06:22<6:00:06,  6.60s/it] 50%|     | 3228/6500 [6:06:28<5:58:47,  6.58s/it]                                                        50%|     | 3228/6500 [6:06:28<5:58:47,  6.58s/it] 50%|     | 3229/6500 [6:06:35<5:57:38,  6.56s/it]                                                        50%|     | 3229/6500 [6:06:35<5:57:38,  6.56s/it] 50%|     | 3230/6500 [6:06:41<5:57:05,  6.55s/it]                                                        50%|     | 3230/6500 [6:06:41<5:57:05,  6.55s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8634620308876038, 'eval_runtime': 1.4711, 'eval_samples_per_second': 8.157, 'eval_steps_per_second': 2.039, 'epoch': 0.5}
                                                        50%|     | 3230/6500 [6:06:43<5:57:05,  6.55s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-3230
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3230I AM HERE AND I AM SAVING THE MODEL

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3230
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3230
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3230
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3230/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3230/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3230/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3230/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.4249, 'learning_rate': 5.04834631730695e-05, 'epoch': 0.5}
{'loss': 0.416, 'learning_rate': 5.0459290712236326e-05, 'epoch': 0.5}
{'loss': 0.4203, 'learning_rate': 5.043511814404673e-05, 'epoch': 0.5}
{'loss': 0.4427, 'learning_rate': 5.0410945474150916e-05, 'epoch': 0.5}
{'loss': 0.4218, 'learning_rate': 5.0386772708199104e-05, 'epoch': 0.5}
 50%|     | 3231/6500 [6:06:50<6:25:53,  7.08s/it]                                                        50%|     | 3231/6500 [6:06:50<6:25:53,  7.08s/it] 50%|     | 3232/6500 [6:06:56<6:16:46,  6.92s/it]                                                        50%|     | 3232/6500 [6:06:56<6:16:46,  6.92s/it] 50%|     | 3233/6500 [6:07:04<6:24:57,  7.07s/it]                                                        50%|     | 3233/6500 [6:07:04<6:24:57,  7.07s/it] 50%|     | 3234/6500 [6:07:10<6:16:13,  6.91s/it]                                                        50%|     | 3234/6500 [6:07:10<6:16:13,  6.91s/it] 50%|     | 3235/6500 [6:07:17<6:10:02,  6.80s/it]                                                        50%|     | 3235/6500 [6:07:17<6:10:02,  6.80s/it] 50%|     | 3236/6500 [6:07:23<6:05:27,  6.72s/it]            {'loss': 0.6958, 'learning_rate': 5.036259985184151e-05, 'epoch': 0.5}
{'loss': 0.4227, 'learning_rate': 5.033842691072841e-05, 'epoch': 0.5}
{'loss': 0.4174, 'learning_rate': 5.031425389051009e-05, 'epoch': 0.5}
{'loss': 0.4128, 'learning_rate': 5.0290080796836826e-05, 'epoch': 0.5}
{'loss': 0.4065, 'learning_rate': 5.0265907635358934e-05, 'epoch': 0.5}
                                            50%|     | 3236/6500 [6:07:23<6:05:27,  6.72s/it] 50%|     | 3237/6500 [6:07:30<6:02:15,  6.66s/it]                                                        50%|     | 3237/6500 [6:07:30<6:02:15,  6.66s/it] 50%|     | 3238/6500 [6:07:36<5:59:51,  6.62s/it]                                                        50%|     | 3238/6500 [6:07:36<5:59:51,  6.62s/it] 50%|     | 3239/6500 [6:07:43<5:58:27,  6.60s/it]                                                        50%|     | 3239/6500 [6:07:43<5:58:27,  6.60s/it] 50%|     | 3240/6500 [6:07:49<5:57:43,  6.58s/it]                                                        50%|     | 3240/6500 [6:07:49<5:57:43,  6.58s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.85838782787323, 'eval_runtime': 1.4732, 'eval_samples_per_second': 8.146, 'eval_steps_per_second': 2.036, 'epoch': 0.5}
                                                        50%|     | 3240/6500 [6:07:51<5:57:43,  6.58s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-3240
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3240I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3240

I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3240
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3240
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3240/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3240/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3240/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3240/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.4107, 'learning_rate': 5.024173441172675e-05, 'epoch': 0.5}
{'loss': 0.4266, 'learning_rate': 5.021756113159062e-05, 'epoch': 0.5}
{'loss': 0.4302, 'learning_rate': 5.01933878006009e-05, 'epoch': 0.5}
{'loss': 0.421, 'learning_rate': 5.0169214424407965e-05, 'epoch': 0.5}
{'loss': 0.4175, 'learning_rate': 5.0145041008662166e-05, 'epoch': 0.5}
 50%|     | 3241/6500 [6:07:58<6:26:57,  7.12s/it]                                                        50%|     | 3241/6500 [6:07:58<6:26:57,  7.12s/it] 50%|     | 3242/6500 [6:08:04<6:17:27,  6.95s/it]                                                        50%|     | 3242/6500 [6:08:04<6:17:27,  6.95s/it] 50%|     | 3243/6500 [6:08:11<6:10:47,  6.83s/it]                                                        50%|     | 3243/6500 [6:08:11<6:10:47,  6.83s/it] 50%|     | 3244/6500 [6:08:17<6:06:08,  6.75s/it]                                                        50%|     | 3244/6500 [6:08:17<6:06:08,  6.75s/it] 50%|     | 3245/6500 [6:08:24<6:02:45,  6.69s/it]                                                        50%|     | 3245/6500 [6:08:24<6:02:45,  6.69s/it] 50%|     | 3246/6500 [6:08:30<6:00:09,  6.64s/it]            {'loss': 0.4303, 'learning_rate': 5.012086755901393e-05, 'epoch': 0.5}
{'loss': 0.4041, 'learning_rate': 5.0096694081113625e-05, 'epoch': 0.5}
{'loss': 0.4159, 'learning_rate': 5.007252058061167e-05, 'epoch': 0.5}
{'loss': 0.4413, 'learning_rate': 5.0048347063158485e-05, 'epoch': 0.5}
{'loss': 0.4226, 'learning_rate': 5.002417353440445e-05, 'epoch': 0.5}
                                            50%|     | 3246/6500 [6:08:30<6:00:09,  6.64s/it] 50%|     | 3247/6500 [6:08:37<5:58:10,  6.61s/it]                                                        50%|     | 3247/6500 [6:08:37<5:58:10,  6.61s/it] 50%|     | 3248/6500 [6:08:44<5:57:25,  6.59s/it]                                                        50%|     | 3248/6500 [6:08:44<5:57:25,  6.59s/it] 50%|     | 3249/6500 [6:08:51<6:06:45,  6.77s/it]                                                        50%|     | 3249/6500 [6:08:51<6:06:45,  6.77s/it] 50%|     | 3250/6500 [6:08:57<6:02:46,  6.70s/it]                                                        50%|     | 3250/6500 [6:08:57<6:02:46,  6.70s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.855904221534729, 'eval_runtime': 1.7425, 'eval_samples_per_second': 6.887, 'eval_steps_per_second': 1.722, 'epoch': 0.5}
                                                        50%|     | 3250/6500 [6:08:59<6:02:46,  6.70s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-3250
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3250
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3250
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3250
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3250
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3250/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3250/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3250/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3250/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.699, 'learning_rate': 5e-05, 'epoch': 0.5}
{'loss': 0.4116, 'learning_rate': 4.997582646559556e-05, 'epoch': 0.5}
{'loss': 0.4306, 'learning_rate': 4.9951652936841527e-05, 'epoch': 0.5}
{'loss': 0.4024, 'learning_rate': 4.992747941938834e-05, 'epoch': 0.5}
{'loss': 0.4037, 'learning_rate': 4.990330591888639e-05, 'epoch': 0.5}
 50%|     | 3251/6500 [6:09:06<6:34:02,  7.28s/it]                                                        50%|     | 3251/6500 [6:09:06<6:34:02,  7.28s/it] 50%|     | 3252/6500 [6:09:12<6:21:48,  7.05s/it]                                                        50%|     | 3252/6500 [6:09:12<6:21:48,  7.05s/it] 50%|     | 3253/6500 [6:09:19<6:13:04,  6.89s/it]                                                        50%|     | 3253/6500 [6:09:19<6:13:04,  6.89s/it] 50%|     | 3254/6500 [6:09:25<6:07:02,  6.78s/it]                                                        50%|     | 3254/6500 [6:09:25<6:07:02,  6.78s/it] 50%|     | 3255/6500 [6:09:32<6:02:55,  6.71s/it]                                                        50%|     | 3255/6500 [6:09:32<6:02:55,  6.71s/it] 50%|     | 3256/6500 [6:09:39<6:00:10,  6.66s/it]            {'loss': 0.4089, 'learning_rate': 4.987913244098609e-05, 'epoch': 0.5}
{'loss': 0.4133, 'learning_rate': 4.985495899133784e-05, 'epoch': 0.5}
{'loss': 0.4289, 'learning_rate': 4.9830785575592054e-05, 'epoch': 0.5}
{'loss': 0.4243, 'learning_rate': 4.98066121993991e-05, 'epoch': 0.5}
{'loss': 0.4121, 'learning_rate': 4.978243886840939e-05, 'epoch': 0.5}
                                            50%|     | 3256/6500 [6:09:39<6:00:10,  6.66s/it] 50%|     | 3257/6500 [6:09:45<5:57:47,  6.62s/it]                                                        50%|     | 3257/6500 [6:09:45<5:57:47,  6.62s/it] 50%|     | 3258/6500 [6:09:52<5:56:19,  6.59s/it]                                                        50%|     | 3258/6500 [6:09:52<5:56:19,  6.59s/it] 50%|     | 3259/6500 [6:09:58<5:55:18,  6.58s/it]                                                        50%|     | 3259/6500 [6:09:58<5:55:18,  6.58s/it] 50%|     | 3260/6500 [6:10:05<5:54:45,  6.57s/it]                                                        50%|     | 3260/6500 [6:10:05<5:54:45,  6.57s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.858171820640564, 'eval_runtime': 1.4687, 'eval_samples_per_second': 8.171, 'eval_steps_per_second': 2.043, 'epoch': 0.5}
                                                        50%|     | 3260/6500 [6:10:06<5:54:45,  6.57s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-3260
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3260
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3260
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3260
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3260
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3260/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3260/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3260/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3260/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.4182, 'learning_rate': 4.9758265588273264e-05, 'epoch': 0.5}
{'loss': 0.4046, 'learning_rate': 4.973409236464108e-05, 'epoch': 0.5}
{'loss': 0.4419, 'learning_rate': 4.9709919203163185e-05, 'epoch': 0.5}
{'loss': 0.4059, 'learning_rate': 4.968574610948992e-05, 'epoch': 0.5}
{'loss': 0.6315, 'learning_rate': 4.96615730892716e-05, 'epoch': 0.5}
 50%|     | 3261/6500 [6:10:13<6:24:11,  7.12s/it]                                                        50%|     | 3261/6500 [6:10:13<6:24:11,  7.12s/it] 50%|     | 3262/6500 [6:10:20<6:14:32,  6.94s/it]                                                        50%|     | 3262/6500 [6:10:20<6:14:32,  6.94s/it] 50%|     | 3263/6500 [6:10:26<6:07:36,  6.81s/it]                                                        50%|     | 3263/6500 [6:10:26<6:07:36,  6.81s/it] 50%|     | 3264/6500 [6:10:33<6:02:51,  6.73s/it]                                                        50%|     | 3264/6500 [6:10:33<6:02:51,  6.73s/it] 50%|     | 3265/6500 [6:10:40<6:09:42,  6.86s/it]                                                        50%|     | 3265/6500 [6:10:40<6:09:42,  6.86s/it] 50%|     | 3266/6500 [6:10:46<6:04:27,  6.76s/it]            {'loss': 0.4954, 'learning_rate': 4.9637400148158504e-05, 'epoch': 0.5}
{'loss': 0.4064, 'learning_rate': 4.9613227291800914e-05, 'epoch': 0.5}
{'loss': 0.4255, 'learning_rate': 4.9589054525849096e-05, 'epoch': 0.5}
{'loss': 0.3938, 'learning_rate': 4.956488185595328e-05, 'epoch': 0.5}
{'loss': 0.402, 'learning_rate': 4.9540709287763685e-05, 'epoch': 0.5}
                                            50%|     | 3266/6500 [6:10:46<6:04:27,  6.76s/it] 50%|     | 3267/6500 [6:10:53<6:01:00,  6.70s/it]                                                        50%|     | 3267/6500 [6:10:53<6:01:00,  6.70s/it] 50%|     | 3268/6500 [6:10:59<5:57:56,  6.65s/it]                                                        50%|     | 3268/6500 [6:10:59<5:57:56,  6.65s/it] 50%|     | 3269/6500 [6:11:06<5:56:18,  6.62s/it]                                                        50%|     | 3269/6500 [6:11:06<5:56:18,  6.62s/it] 50%|     | 3270/6500 [6:11:12<5:55:04,  6.60s/it]                                                        50%|     | 3270/6500 [6:11:12<5:55:04,  6.60s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8610596060752869, 'eval_runtime': 1.4806, 'eval_samples_per_second': 8.105, 'eval_steps_per_second': 2.026, 'epoch': 0.5}
                                                        50%|     | 3270/6500 [6:11:14<5:55:04,  6.60s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-3270
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3270
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3270
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3270
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3270
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3270/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3270/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3270/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3270/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.4102, 'learning_rate': 4.9516536826930515e-05, 'epoch': 0.5}
{'loss': 0.4069, 'learning_rate': 4.9492364479103914e-05, 'epoch': 0.5}
{'loss': 0.4297, 'learning_rate': 4.9468192249934025e-05, 'epoch': 0.5}
{'loss': 0.4233, 'learning_rate': 4.944402014507097e-05, 'epoch': 0.5}
{'loss': 0.4068, 'learning_rate': 4.9419848170164815e-05, 'epoch': 0.5}
 50%|     | 3271/6500 [6:11:21<6:23:02,  7.12s/it]                                                        50%|     | 3271/6500 [6:11:21<6:23:02,  7.12s/it] 50%|     | 3272/6500 [6:11:27<6:13:26,  6.94s/it]                                                        50%|     | 3272/6500 [6:11:27<6:13:26,  6.94s/it] 50%|     | 3273/6500 [6:11:34<6:07:01,  6.82s/it]                                                        50%|     | 3273/6500 [6:11:34<6:07:01,  6.82s/it] 50%|     | 3274/6500 [6:11:40<6:02:15,  6.74s/it]                                                        50%|     | 3274/6500 [6:11:40<6:02:15,  6.74s/it] 50%|     | 3275/6500 [6:11:47<5:59:12,  6.68s/it]                                                        50%|     | 3275/6500 [6:11:47<5:59:12,  6.68s/it] 50%|     | 3276/6500 [6:11:54<5:56:44,  6.64s/it]            {'loss': 0.4218, 'learning_rate': 4.939567633086563e-05, 'epoch': 0.5}
{'loss': 0.4002, 'learning_rate': 4.937150463282344e-05, 'epoch': 0.5}
{'loss': 0.4481, 'learning_rate': 4.934733308168821e-05, 'epoch': 0.5}
{'loss': 0.4043, 'learning_rate': 4.93231616831099e-05, 'epoch': 0.5}
{'loss': 0.692, 'learning_rate': 4.929899044273843e-05, 'epoch': 0.5}
                                            50%|     | 3276/6500 [6:11:54<5:56:44,  6.64s/it] 50%|     | 3277/6500 [6:12:00<5:54:53,  6.61s/it]                                                        50%|     | 3277/6500 [6:12:00<5:54:53,  6.61s/it] 50%|     | 3278/6500 [6:12:07<5:53:39,  6.59s/it]                                                        50%|     | 3278/6500 [6:12:07<5:53:39,  6.59s/it] 50%|     | 3279/6500 [6:12:13<5:52:52,  6.57s/it]                                                        50%|     | 3279/6500 [6:12:13<5:52:52,  6.57s/it] 50%|     | 3280/6500 [6:12:20<5:52:15,  6.56s/it]                                                        50%|     | 3280/6500 [6:12:20<5:52:15,  6.56s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8567939400672913, 'eval_runtime': 1.7157, 'eval_samples_per_second': 6.994, 'eval_steps_per_second': 1.749, 'epoch': 0.5}
                                                        50%|     | 3280/6500 [6:12:21<5:52:15,  6.56s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-3280
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3280
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3280
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3280
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3280
the pytorch model path is the pytorch model path is/projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3280/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3280/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3280/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3280/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.4293, 'learning_rate': 4.927481936622369e-05, 'epoch': 0.5}
{'loss': 0.4071, 'learning_rate': 4.925064845921552e-05, 'epoch': 0.5}
{'loss': 0.4282, 'learning_rate': 4.922647772736371e-05, 'epoch': 0.51}
{'loss': 0.4015, 'learning_rate': 4.920230717631802e-05, 'epoch': 0.51}
{'loss': 0.4192, 'learning_rate': 4.917813681172818e-05, 'epoch': 0.51}
 50%|     | 3281/6500 [6:12:29<6:34:41,  7.36s/it]                                                        50%|     | 3281/6500 [6:12:29<6:34:41,  7.36s/it] 50%|     | 3282/6500 [6:12:35<6:21:34,  7.11s/it]                                                        50%|     | 3282/6500 [6:12:35<6:21:34,  7.11s/it] 51%|     | 3283/6500 [6:12:42<6:12:17,  6.94s/it]                                                        51%|     | 3283/6500 [6:12:42<6:12:17,  6.94s/it] 51%|     | 3284/6500 [6:12:49<6:05:57,  6.83s/it]                                                        51%|     | 3284/6500 [6:12:49<6:05:57,  6.83s/it] 51%|     | 3285/6500 [6:12:55<6:01:29,  6.75s/it]                                                        51%|     | 3285/6500 [6:12:55<6:01:29,  6.75s/it] 51%|     | 3286/6500 [6:13:02<5:58:48,  6.70s/it]            {'loss': 0.4215, 'learning_rate': 4.9153966639243864e-05, 'epoch': 0.51}
{'loss': 0.4223, 'learning_rate': 4.91297966645147e-05, 'epoch': 0.51}
{'loss': 0.4122, 'learning_rate': 4.910562689319029e-05, 'epoch': 0.51}
{'loss': 0.4219, 'learning_rate': 4.908145733092013e-05, 'epoch': 0.51}
{'loss': 0.4112, 'learning_rate': 4.9057287983353745e-05, 'epoch': 0.51}
                                            51%|     | 3286/6500 [6:13:02<5:58:48,  6.70s/it] 51%|     | 3287/6500 [6:13:08<5:56:51,  6.66s/it]                                                        51%|     | 3287/6500 [6:13:08<5:56:51,  6.66s/it] 51%|     | 3288/6500 [6:13:15<5:55:31,  6.64s/it]                                                        51%|     | 3288/6500 [6:13:15<5:55:31,  6.64s/it] 51%|     | 3289/6500 [6:13:21<5:54:37,  6.63s/it]                                                        51%|     | 3289/6500 [6:13:21<5:54:37,  6.63s/it] 51%|     | 3290/6500 [6:13:28<5:53:44,  6.61s/it]                                                        51%|     | 3290/6500 [6:13:28<5:53:44,  6.61s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8600690960884094, 'eval_runtime': 1.4735, 'eval_samples_per_second': 8.144, 'eval_steps_per_second': 2.036, 'epoch': 0.51}
                                                        51%|     | 3290/6500 [6:13:30<5:53:44,  6.61s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-3290
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3290
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3290
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3290
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3290
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3290/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3290/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3290/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3290/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.4136, 'learning_rate': 4.903311885614058e-05, 'epoch': 0.51}
{'loss': 0.3993, 'learning_rate': 4.900894995492998e-05, 'epoch': 0.51}
{'loss': 0.4473, 'learning_rate': 4.898478128537131e-05, 'epoch': 0.51}
{'loss': 0.4216, 'learning_rate': 4.8960612853113844e-05, 'epoch': 0.51}
{'loss': 0.682, 'learning_rate': 4.89364446638068e-05, 'epoch': 0.51}
 51%|     | 3291/6500 [6:13:36<6:21:48,  7.14s/it]                                                        51%|     | 3291/6500 [6:13:36<6:21:48,  7.14s/it] 51%|     | 3292/6500 [6:13:43<6:12:30,  6.97s/it]                                                        51%|     | 3292/6500 [6:13:43<6:12:30,  6.97s/it] 51%|     | 3293/6500 [6:13:50<6:06:07,  6.85s/it]                                                        51%|     | 3293/6500 [6:13:50<6:06:07,  6.85s/it] 51%|     | 3294/6500 [6:13:56<6:01:33,  6.77s/it]                                                        51%|     | 3294/6500 [6:13:56<6:01:33,  6.77s/it] 51%|     | 3295/6500 [6:14:03<5:58:13,  6.71s/it]                                                        51%|     | 3295/6500 [6:14:03<5:58:13,  6.71s/it] 51%|     | 3296/6500 [6:14:09<5:56:06,  6.67s/it]            {'loss': 0.4291, 'learning_rate': 4.891227672309935e-05, 'epoch': 0.51}
{'loss': 0.4123, 'learning_rate': 4.888810903664062e-05, 'epoch': 0.51}
{'loss': 0.4068, 'learning_rate': 4.886394161007963e-05, 'epoch': 0.51}
{'loss': 0.3956, 'learning_rate': 4.883977444906538e-05, 'epoch': 0.51}
{'loss': 0.4114, 'learning_rate': 4.881560755924679e-05, 'epoch': 0.51}
                                            51%|     | 3296/6500 [6:14:09<5:56:06,  6.67s/it] 51%|     | 3297/6500 [6:14:17<6:07:59,  6.89s/it]                                                        51%|     | 3297/6500 [6:14:17<6:07:59,  6.89s/it] 51%|     | 3298/6500 [6:14:23<6:02:48,  6.80s/it]                                                        51%|     | 3298/6500 [6:14:23<6:02:48,  6.80s/it] 51%|     | 3299/6500 [6:14:30<5:59:06,  6.73s/it]                                                        51%|     | 3299/6500 [6:14:30<5:59:06,  6.73s/it] 51%|     | 3300/6500 [6:14:36<5:56:32,  6.69s/it]                                                        51%|     | 3300/6500 [6:14:36<5:56:32,  6.69s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.861242949962616, 'eval_runtime': 1.4706, 'eval_samples_per_second': 8.16, 'eval_steps_per_second': 2.04, 'epoch': 0.51}
                                                        51%|     | 3300/6500 [6:14:38<5:56:32,  6.69s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-3300
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3300
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3300
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3300
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3300
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3300/pytorch_model.bin
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3300/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3300/pytorch_model.binthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3300/pytorch_model.bin

/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.41, 'learning_rate': 4.8791440946272735e-05, 'epoch': 0.51}
{'loss': 0.4204, 'learning_rate': 4.876727461579203e-05, 'epoch': 0.51}
{'loss': 0.4144, 'learning_rate': 4.874310857345337e-05, 'epoch': 0.51}
{'loss': 0.4192, 'learning_rate': 4.8718942824905445e-05, 'epoch': 0.51}
{'loss': 0.4215, 'learning_rate': 4.8694777375796844e-05, 'epoch': 0.51}
 51%|     | 3301/6500 [6:14:45<6:23:31,  7.19s/it]                                                        51%|     | 3301/6500 [6:14:45<6:23:31,  7.19s/it] 51%|     | 3302/6500 [6:14:51<6:13:30,  7.01s/it]                                                        51%|     | 3302/6500 [6:14:51<6:13:30,  7.01s/it] 51%|     | 3303/6500 [6:14:58<6:06:36,  6.88s/it]                                                        51%|     | 3303/6500 [6:14:58<6:06:36,  6.88s/it] 51%|     | 3304/6500 [6:15:05<6:01:38,  6.79s/it]                                                        51%|     | 3304/6500 [6:15:05<6:01:38,  6.79s/it] 51%|     | 3305/6500 [6:15:11<5:58:13,  6.73s/it]                                                        51%|     | 3305/6500 [6:15:11<5:58:13,  6.73s/it] 51%|     | 3306/6500 [6:15:18<5:55:35,  6.68s/it]            {'loss': 0.4132, 'learning_rate': 4.867061223177609e-05, 'epoch': 0.51}
{'loss': 0.3983, 'learning_rate': 4.864644739849165e-05, 'epoch': 0.51}
{'loss': 0.456, 'learning_rate': 4.8622282881591906e-05, 'epoch': 0.51}
{'loss': 0.4147, 'learning_rate': 4.859811868672515e-05, 'epoch': 0.51}
{'loss': 0.695, 'learning_rate': 4.8573954819539634e-05, 'epoch': 0.51}
                                            51%|     | 3306/6500 [6:15:18<5:55:35,  6.68s/it] 51%|     | 3307/6500 [6:15:24<5:53:44,  6.65s/it]                                                        51%|     | 3307/6500 [6:15:24<5:53:44,  6.65s/it] 51%|     | 3308/6500 [6:15:31<5:52:25,  6.62s/it]                                                        51%|     | 3308/6500 [6:15:31<5:52:25,  6.62s/it] 51%|     | 3309/6500 [6:15:37<5:51:21,  6.61s/it]                                                        51%|     | 3309/6500 [6:15:37<5:51:21,  6.61s/it] 51%|     | 3310/6500 [6:15:44<5:50:38,  6.60s/it]                                                        51%|     | 3310/6500 [6:15:44<5:50:38,  6.60s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8546438813209534, 'eval_runtime': 1.4677, 'eval_samples_per_second': 8.176, 'eval_steps_per_second': 2.044, 'epoch': 0.51}
                                                        51%|     | 3310/6500 [6:15:45<5:50:38,  6.60s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-3310
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  I AM HERE AND I AM SAVING THE MODEL/projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3310
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3310

I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3310
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3310
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3310/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3310/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3310/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3310/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.4219, 'learning_rate': 4.854979128568351e-05, 'epoch': 0.51}
{'loss': 0.4183, 'learning_rate': 4.852562809080485e-05, 'epoch': 0.51}
{'loss': 0.4045, 'learning_rate': 4.8501465240551666e-05, 'epoch': 0.51}
{'loss': 0.4012, 'learning_rate': 4.8477302740571866e-05, 'epoch': 0.51}
{'loss': 0.4087, 'learning_rate': 4.84531405965133e-05, 'epoch': 0.51}
 51%|     | 3311/6500 [6:15:52<6:19:03,  7.13s/it]                                                        51%|     | 3311/6500 [6:15:52<6:19:03,  7.13s/it] 51%|     | 3312/6500 [6:15:59<6:10:06,  6.97s/it]                                                        51%|     | 3312/6500 [6:15:59<6:10:06,  6.97s/it] 51%|     | 3313/6500 [6:16:06<6:17:48,  7.11s/it]                                                        51%|     | 3313/6500 [6:16:06<6:17:48,  7.11s/it] 51%|     | 3314/6500 [6:16:13<6:09:27,  6.96s/it]                                                        51%|     | 3314/6500 [6:16:13<6:09:27,  6.96s/it] 51%|     | 3315/6500 [6:16:20<6:03:30,  6.85s/it]                                                        51%|     | 3315/6500 [6:16:20<6:03:30,  6.85s/it] 51%|     | 3316/6500 [6:16:26<5:59:06,  6.77s/it]            {'loss': 0.4106, 'learning_rate': 4.84289788140237e-05, 'epoch': 0.51}
{'loss': 0.4246, 'learning_rate': 4.8404817398750756e-05, 'epoch': 0.51}
{'loss': 0.4212, 'learning_rate': 4.838065635634205e-05, 'epoch': 0.51}
{'loss': 0.4174, 'learning_rate': 4.835649569244508e-05, 'epoch': 0.51}
{'loss': 0.4255, 'learning_rate': 4.833233541270724e-05, 'epoch': 0.51}
                                            51%|     | 3316/6500 [6:16:26<5:59:06,  6.77s/it] 51%|     | 3317/6500 [6:16:33<5:55:56,  6.71s/it]                                                        51%|     | 3317/6500 [6:16:33<5:55:56,  6.71s/it] 51%|     | 3318/6500 [6:16:39<5:53:44,  6.67s/it]                                                        51%|     | 3318/6500 [6:16:39<5:53:44,  6.67s/it] 51%|     | 3319/6500 [6:16:46<5:52:13,  6.64s/it]                                                        51%|     | 3319/6500 [6:16:46<5:52:13,  6.64s/it] 51%|     | 3320/6500 [6:16:52<5:51:06,  6.62s/it]                                                        51%|     | 3320/6500 [6:16:52<5:51:06,  6.62s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.858707070350647, 'eval_runtime': 1.475, 'eval_samples_per_second': 8.135, 'eval_steps_per_second': 2.034, 'epoch': 0.51}
                                                        51%|     | 3320/6500 [6:16:54<5:51:06,  6.62s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-3320
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3320
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3320
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3320
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3320
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3320/pytorch_model.bin
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3320/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3320/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3320/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.4063, 'learning_rate': 4.8308175522775865e-05, 'epoch': 0.51}
{'loss': 0.4081, 'learning_rate': 4.828401602829816e-05, 'epoch': 0.51}
{'loss': 0.4427, 'learning_rate': 4.825985693492129e-05, 'epoch': 0.51}
{'loss': 0.4154, 'learning_rate': 4.823569824829227e-05, 'epoch': 0.51}
{'loss': 0.6982, 'learning_rate': 4.821153997405807e-05, 'epoch': 0.51}
 51%|     | 3321/6500 [6:17:01<6:18:39,  7.15s/it]                                                        51%|     | 3321/6500 [6:17:01<6:18:39,  7.15s/it] 51%|     | 3322/6500 [6:17:07<6:09:18,  6.97s/it]                                                        51%|     | 3322/6500 [6:17:07<6:09:18,  6.97s/it] 51%|     | 3323/6500 [6:17:14<6:02:49,  6.85s/it]                                                        51%|     | 3323/6500 [6:17:14<6:02:49,  6.85s/it] 51%|     | 3324/6500 [6:17:21<5:58:25,  6.77s/it]                                                        51%|     | 3324/6500 [6:17:21<5:58:25,  6.77s/it] 51%|     | 3325/6500 [6:17:27<5:55:00,  6.71s/it]                                                        51%|     | 3325/6500 [6:17:27<5:55:00,  6.71s/it] 51%|     | 3326/6500 [6:17:34<5:52:45,  6.67s/it]            {'loss': 0.4085, 'learning_rate': 4.8187382117865525e-05, 'epoch': 0.51}
{'loss': 0.4253, 'learning_rate': 4.816322468536139e-05, 'epoch': 0.51}
{'loss': 0.412, 'learning_rate': 4.8139067682192303e-05, 'epoch': 0.51}
{'loss': 0.4058, 'learning_rate': 4.811491111400484e-05, 'epoch': 0.51}
{'loss': 0.4002, 'learning_rate': 4.8090754986445454e-05, 'epoch': 0.51}
                                            51%|     | 3326/6500 [6:17:34<5:52:45,  6.67s/it] 51%|     | 3327/6500 [6:17:40<5:51:13,  6.64s/it]                                                        51%|     | 3327/6500 [6:17:40<5:51:13,  6.64s/it] 51%|     | 3328/6500 [6:17:47<5:50:08,  6.62s/it]                                                        51%|     | 3328/6500 [6:17:47<5:50:08,  6.62s/it] 51%|     | 3329/6500 [6:17:53<5:49:00,  6.60s/it]                                                        51%|     | 3329/6500 [6:17:53<5:49:00,  6.60s/it] 51%|     | 3330/6500 [6:18:01<6:02:29,  6.86s/it]                                                        51%|     | 3330/6500 [6:18:01<6:02:29,  6.86s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8593366742134094, 'eval_runtime': 1.4762, 'eval_samples_per_second': 8.129, 'eval_steps_per_second': 2.032, 'epoch': 0.51}
                                                        51%|     | 3330/6500 [6:18:02<6:02:29,  6.86s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-3330
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3330
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3330
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3330
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3330
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3330/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3330/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3330/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3330/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.4189, 'learning_rate': 4.806659930516047e-05, 'epoch': 0.51}
{'loss': 0.438, 'learning_rate': 4.804244407579613e-05, 'epoch': 0.51}
{'loss': 0.4184, 'learning_rate': 4.8018289303998604e-05, 'epoch': 0.51}
{'loss': 0.414, 'learning_rate': 4.79941349954139e-05, 'epoch': 0.51}
{'loss': 0.4239, 'learning_rate': 4.796998115568794e-05, 'epoch': 0.51}
 51%|     | 3331/6500 [6:18:09<6:27:12,  7.33s/it]                                                        51%|     | 3331/6500 [6:18:09<6:27:12,  7.33s/it] 51%|    | 3332/6500 [6:18:16<6:15:12,  7.11s/it]                                                        51%|    | 3332/6500 [6:18:16<6:15:12,  7.11s/it] 51%|    | 3333/6500 [6:18:22<6:06:47,  6.95s/it]                                                        51%|    | 3333/6500 [6:18:22<6:06:47,  6.95s/it] 51%|    | 3334/6500 [6:18:29<6:00:48,  6.84s/it]                                                        51%|    | 3334/6500 [6:18:29<6:00:48,  6.84s/it] 51%|    | 3335/6500 [6:18:36<5:56:34,  6.76s/it]                                                        51%|    | 3335/6500 [6:18:36<5:56:34,  6.76s/it] 51%|    | 3336/6500 [6:18:42<5:53:26,  6.70{'loss': 0.3986, 'learning_rate': 4.7945827790466554e-05, 'epoch': 0.51}
{'loss': 0.4233, 'learning_rate': 4.792167490539542e-05, 'epoch': 0.51}
{'loss': 0.4268, 'learning_rate': 4.789752250612014e-05, 'epoch': 0.51}
{'loss': 0.4266, 'learning_rate': 4.787337059828619e-05, 'epoch': 0.51}
{'loss': 0.6935, 'learning_rate': 4.7849219187538944e-05, 'epoch': 0.51}
s/it]                                                        51%|    | 3336/6500 [6:18:42<5:53:26,  6.70s/it] 51%|    | 3337/6500 [6:18:49<5:51:28,  6.67s/it]                                                        51%|    | 3337/6500 [6:18:49<5:51:28,  6.67s/it] 51%|    | 3338/6500 [6:18:55<5:49:49,  6.64s/it]                                                        51%|    | 3338/6500 [6:18:55<5:49:49,  6.64s/it] 51%|    | 3339/6500 [6:19:02<5:48:45,  6.62s/it]                                                        51%|    | 3339/6500 [6:19:02<5:48:45,  6.62s/it] 51%|    | 3340/6500 [6:19:08<5:47:56,  6.61s/it]                                                        51%|    | 3340/6500 [6:19:08<5:47:56,  6.61s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8545994758605957, 'eval_runtime': 1.4711, 'eval_samples_per_second': 8.157, 'eval_steps_per_second': 2.039, 'epoch': 0.51}
                                                        51%|    | 3340/6500 [6:19:10<5:47:56,  6.61s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-3340
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3340
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3340
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3340
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3340
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3340/pytorch_model.bin
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3340/pytorch_model.bin 
/projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3340/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3340/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.4112, 'learning_rate': 4.782506827952364e-05, 'epoch': 0.51}
{'loss': 0.4269, 'learning_rate': 4.780091787988539e-05, 'epoch': 0.51}
{'loss': 0.3914, 'learning_rate': 4.777676799426921e-05, 'epoch': 0.51}
{'loss': 0.4025, 'learning_rate': 4.775261862832e-05, 'epoch': 0.51}
{'loss': 0.4124, 'learning_rate': 4.772846978768252e-05, 'epoch': 0.51}
 51%|    | 3341/6500 [6:19:17<6:15:49,  7.14s/it]                                                        51%|    | 3341/6500 [6:19:17<6:15:49,  7.14s/it] 51%|    | 3342/6500 [6:19:23<6:06:51,  6.97s/it]                                                        51%|    | 3342/6500 [6:19:23<6:06:51,  6.97s/it] 51%|    | 3343/6500 [6:19:30<6:00:33,  6.85s/it]                                                        51%|    | 3343/6500 [6:19:30<6:00:33,  6.85s/it] 51%|    | 3344/6500 [6:19:37<5:55:49,  6.76s/it]                                                        51%|    | 3344/6500 [6:19:37<5:55:49,  6.76s/it] 51%|    | 3345/6500 [6:19:43<5:52:39,  6.71s/it]                                                        51%|    | 3345/6500 [6:19:43<5:52:39,  6.71s/it] 51%|    | 3346/6500 [6:19:50<5:59:49,  {'loss': 0.4073, 'learning_rate': 4.7704321478001415e-05, 'epoch': 0.51}
{'loss': 0.4217, 'learning_rate': 4.768017370492121e-05, 'epoch': 0.51}
{'loss': 0.4241, 'learning_rate': 4.76560264740863e-05, 'epoch': 0.52}
{'loss': 0.3992, 'learning_rate': 4.7631879791140946e-05, 'epoch': 0.52}
{'loss': 0.4149, 'learning_rate': 4.760773366172929e-05, 'epoch': 0.52}
6.85s/it]                                                        51%|    | 3346/6500 [6:19:50<5:59:49,  6.85s/it] 51%|    | 3347/6500 [6:19:57<5:55:40,  6.77s/it]                                                        51%|    | 3347/6500 [6:19:57<5:55:40,  6.77s/it] 52%|    | 3348/6500 [6:20:03<5:52:40,  6.71s/it]                                                        52%|    | 3348/6500 [6:20:03<5:52:40,  6.71s/it] 52%|    | 3349/6500 [6:20:10<5:50:41,  6.68s/it]                                                        52%|    | 3349/6500 [6:20:10<5:50:41,  6.68s/it] 52%|    | 3350/6500 [6:20:17<5:48:50,  6.64s/it]                                                        52%|    | 3350/6500 [6:20:17<5:48:50,  6.64s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8605403900146484, 'eval_runtime': 1.7331, 'eval_samples_per_second': 6.924, 'eval_steps_per_second': 1.731, 'epoch': 0.52}
                                                        52%|    | 3350/6500 [6:20:18<5:48:50,  6.64s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-3350
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3350
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3350
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3350
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3350
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3350/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3350/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3350/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3350/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3959, 'learning_rate': 4.7583588091495344e-05, 'epoch': 0.52}
{'loss': 0.4496, 'learning_rate': 4.7559443086083005e-05, 'epoch': 0.52}
{'loss': 0.3997, 'learning_rate': 4.7535298651136e-05, 'epoch': 0.52}
{'loss': 0.6863, 'learning_rate': 4.751115479229794e-05, 'epoch': 0.52}
{'loss': 0.42, 'learning_rate': 4.7487011515212315e-05, 'epoch': 0.52}
 52%|    | 3351/6500 [6:20:25<6:19:57,  7.24s/it]                                                        52%|    | 3351/6500 [6:20:25<6:19:57,  7.24s/it] 52%|    | 3352/6500 [6:20:32<6:09:14,  7.04s/it]                                                        52%|    | 3352/6500 [6:20:32<6:09:14,  7.04s/it] 52%|    | 3353/6500 [6:20:38<6:01:56,  6.90s/it]                                                        52%|    | 3353/6500 [6:20:38<6:01:56,  6.90s/it] 52%|    | 3354/6500 [6:20:45<5:56:30,  6.80s/it]                                                        52%|    | 3354/6500 [6:20:45<5:56:30,  6.80s/it] 52%|    | 3355/6500 [6:20:52<5:52:55,  6.73s/it]                                                        52%|    | 3355/6500 [6:20:52<5:52:55,  6.73s/it] 52%|    | 3356/6500 [6:20:58<5:50:04,  {'loss': 0.4032, 'learning_rate': 4.7462868825522466e-05, 'epoch': 0.52}
{'loss': 0.4222, 'learning_rate': 4.7438726728871615e-05, 'epoch': 0.52}
{'loss': 0.3987, 'learning_rate': 4.741458523090282e-05, 'epoch': 0.52}
{'loss': 0.4043, 'learning_rate': 4.7390444337259e-05, 'epoch': 0.52}
{'loss': 0.4084, 'learning_rate': 4.7366304053582943e-05, 'epoch': 0.52}
6.68s/it]                                                        52%|    | 3356/6500 [6:20:58<5:50:04,  6.68s/it] 52%|    | 3357/6500 [6:21:05<5:48:14,  6.65s/it]                                                        52%|    | 3357/6500 [6:21:05<5:48:14,  6.65s/it] 52%|    | 3358/6500 [6:21:11<5:46:51,  6.62s/it]                                                        52%|    | 3358/6500 [6:21:11<5:46:51,  6.62s/it] 52%|    | 3359/6500 [6:21:18<5:45:58,  6.61s/it]                                                        52%|    | 3359/6500 [6:21:18<5:45:58,  6.61s/it] 52%|    | 3360/6500 [6:21:24<5:45:06,  6.59s/it]                                                        52%|    | 3360/6500 [6:21:24<5:45:06,  6.59s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8623231649398804, 'eval_runtime': 1.4766, 'eval_samples_per_second': 8.127, 'eval_steps_per_second': 2.032, 'epoch': 0.52}
                                                        52%|    | 3360/6500 [6:21:26<5:45:06,  6.59s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-3360
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3360
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3360
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3360
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3360
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3360/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3360/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3360/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3360/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.4106, 'learning_rate': 4.7342164385517305e-05, 'epoch': 0.52}
{'loss': 0.4181, 'learning_rate': 4.731802533870459e-05, 'epoch': 0.52}
{'loss': 0.4162, 'learning_rate': 4.729388691878714e-05, 'epoch': 0.52}
{'loss': 0.4134, 'learning_rate': 4.726974913140717e-05, 'epoch': 0.52}
{'loss': 0.4134, 'learning_rate': 4.7245611982206724e-05, 'epoch': 0.52}
 52%|    | 3361/6500 [6:21:33<6:13:10,  7.13s/it]                                                        52%|    | 3361/6500 [6:21:33<6:13:10,  7.13s/it] 52%|    | 3362/6500 [6:21:40<6:16:04,  7.19s/it]                                                        52%|    | 3362/6500 [6:21:40<6:16:04,  7.19s/it] 52%|    | 3363/6500 [6:21:47<6:05:38,  6.99s/it]                                                        52%|    | 3363/6500 [6:21:47<6:05:38,  6.99s/it] 52%|    | 3364/6500 [6:21:53<5:58:24,  6.86s/it]                                                        52%|    | 3364/6500 [6:21:53<5:58:24,  6.86s/it] 52%|    | 3365/6500 [6:22:00<5:53:17,  6.76s/it]                                                        52%|    | 3365/6500 [6:22:00<5:53:17,  6.76s/it] 52%|    | 3366/6500 [6:22:06<5:49:28,  {'loss': 0.396, 'learning_rate': 4.7221475476827745e-05, 'epoch': 0.52}
{'loss': 0.4502, 'learning_rate': 4.719733962091198e-05, 'epoch': 0.52}
{'loss': 0.4072, 'learning_rate': 4.717320442010105e-05, 'epoch': 0.52}
{'loss': 0.6923, 'learning_rate': 4.714906988003638e-05, 'epoch': 0.52}
{'loss': 0.4283, 'learning_rate': 4.71249360063593e-05, 'epoch': 0.52}
6.69s/it]                                                        52%|    | 3366/6500 [6:22:06<5:49:28,  6.69s/it] 52%|    | 3367/6500 [6:22:13<5:47:01,  6.65s/it]                                                        52%|    | 3367/6500 [6:22:13<5:47:01,  6.65s/it] 52%|    | 3368/6500 [6:22:19<5:45:10,  6.61s/it]                                                        52%|    | 3368/6500 [6:22:19<5:45:10,  6.61s/it] 52%|    | 3369/6500 [6:22:26<5:43:46,  6.59s/it]                                                        52%|    | 3369/6500 [6:22:26<5:43:46,  6.59s/it] 52%|    | 3370/6500 [6:22:32<5:42:47,  6.57s/it]                                                        52%|    | 3370/6500 [6:22:32<5:42:47,  6.57s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8574245572090149, 'eval_runtime': 1.4837, 'eval_samples_per_second': 8.088, 'eval_steps_per_second': 2.022, 'epoch': 0.52}
                                                        52%|    | 3370/6500 [6:22:34<5:42:47,  6.57s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-3370
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3370
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3370
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3370
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3370
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3370/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3370/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3370/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3370/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.4, 'learning_rate': 4.7100802804710946e-05, 'epoch': 0.52}
{'loss': 0.4239, 'learning_rate': 4.707667028073232e-05, 'epoch': 0.52}
{'loss': 0.3991, 'learning_rate': 4.705253844006423e-05, 'epoch': 0.52}
{'loss': 0.4118, 'learning_rate': 4.702840728834736e-05, 'epoch': 0.52}
{'loss': 0.4098, 'learning_rate': 4.7004276831222224e-05, 'epoch': 0.52}
 52%|    | 3371/6500 [6:22:41<6:10:10,  7.10s/it]                                                        52%|    | 3371/6500 [6:22:41<6:10:10,  7.10s/it] 52%|    | 3372/6500 [6:22:47<6:01:21,  6.93s/it]                                                        52%|    | 3372/6500 [6:22:47<6:01:21,  6.93s/it] 52%|    | 3373/6500 [6:22:54<5:54:57,  6.81s/it]                                                        52%|    | 3373/6500 [6:22:54<5:54:57,  6.81s/it] 52%|    | 3374/6500 [6:23:00<5:50:37,  6.73s/it]                                                        52%|    | 3374/6500 [6:23:00<5:50:37,  6.73s/it] 52%|    | 3375/6500 [6:23:07<5:47:26,  6.67s/it]                                                        52%|    | 3375/6500 [6:23:07<5:47:26,  6.67s/it] 52%|    | 3376/6500 [6:23:13<5:45:16,  {'loss': 0.4176, 'learning_rate': 4.698014707432916e-05, 'epoch': 0.52}
{'loss': 0.4107, 'learning_rate': 4.695601802330835e-05, 'epoch': 0.52}
{'loss': 0.4131, 'learning_rate': 4.693188968379983e-05, 'epoch': 0.52}
{'loss': 0.4034, 'learning_rate': 4.6907762061443446e-05, 'epoch': 0.52}
{'loss': 0.4054, 'learning_rate': 4.688363516187886e-05, 'epoch': 0.52}
6.63s/it]                                                        52%|    | 3376/6500 [6:23:13<5:45:16,  6.63s/it] 52%|    | 3377/6500 [6:23:20<5:43:46,  6.60s/it]                                                        52%|    | 3377/6500 [6:23:20<5:43:46,  6.60s/it] 52%|    | 3378/6500 [6:23:27<5:56:30,  6.85s/it]                                                        52%|    | 3378/6500 [6:23:27<5:56:30,  6.85s/it] 52%|    | 3379/6500 [6:23:34<5:51:37,  6.76s/it]                                                        52%|    | 3379/6500 [6:23:34<5:51:37,  6.76s/it] 52%|    | 3380/6500 [6:23:40<5:48:09,  6.70s/it]                                                        52%|    | 3380/6500 [6:23:40<5:48:09,  6.70s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8607357144355774, 'eval_runtime': 1.4711, 'eval_samples_per_second': 8.157, 'eval_steps_per_second': 2.039, 'epoch': 0.52}
                                                        52%|    | 3380/6500 [6:23:42<5:48:09,  6.70s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-3380
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3380
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3380
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3380
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3380
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3380/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3380/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3380/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3380/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.4009, 'learning_rate': 4.685950899074562e-05, 'epoch': 0.52}
{'loss': 0.4481, 'learning_rate': 4.683538355368306e-05, 'epoch': 0.52}
{'loss': 0.4058, 'learning_rate': 4.681125885633035e-05, 'epoch': 0.52}
{'loss': 0.6842, 'learning_rate': 4.6787134904326504e-05, 'epoch': 0.52}
{'loss': 0.4254, 'learning_rate': 4.676301170331033e-05, 'epoch': 0.52}
 52%|    | 3381/6500 [6:23:49<6:13:37,  7.19s/it]                                                        52%|    | 3381/6500 [6:23:49<6:13:37,  7.19s/it] 52%|    | 3382/6500 [6:23:55<6:03:19,  6.99s/it]                                                        52%|    | 3382/6500 [6:23:55<6:03:19,  6.99s/it] 52%|    | 3383/6500 [6:24:02<5:56:09,  6.86s/it]                                                        52%|    | 3383/6500 [6:24:02<5:56:09,  6.86s/it] 52%|    | 3384/6500 [6:24:08<5:51:03,  6.76s/it]                                                        52%|    | 3384/6500 [6:24:08<5:51:03,  6.76s/it] 52%|    | 3385/6500 [6:24:15<5:47:21,  6.69s/it]                                                        52%|    | 3385/6500 [6:24:15<5:47:21,  6.69s/it] 52%|    | 3386/6500 [6:24:21<5:44:36,  {'loss': 0.4039, 'learning_rate': 4.673888925892048e-05, 'epoch': 0.52}
{'loss': 0.4079, 'learning_rate': 4.6714767576795446e-05, 'epoch': 0.52}
{'loss': 0.4052, 'learning_rate': 4.669064666257352e-05, 'epoch': 0.52}
{'loss': 0.4069, 'learning_rate': 4.666652652189282e-05, 'epoch': 0.52}
{'loss': 0.4085, 'learning_rate': 4.664240716039127e-05, 'epoch': 0.52}
6.64s/it]                                                        52%|    | 3386/6500 [6:24:21<5:44:36,  6.64s/it] 52%|    | 3387/6500 [6:24:28<5:43:00,  6.61s/it]                                                        52%|    | 3387/6500 [6:24:28<5:43:00,  6.61s/it] 52%|    | 3388/6500 [6:24:35<5:41:48,  6.59s/it]                                                        52%|    | 3388/6500 [6:24:35<5:41:48,  6.59s/it] 52%|    | 3389/6500 [6:24:41<5:40:54,  6.57s/it]                                                        52%|    | 3389/6500 [6:24:41<5:40:54,  6.57s/it] 52%|    | 3390/6500 [6:24:48<5:40:20,  6.57s/it]                                                        52%|    | 3390/6500 [6:24:48<5:40:20,  6.57s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8605982065200806, 'eval_runtime': 1.4705, 'eval_samples_per_second': 8.161, 'eval_steps_per_second': 2.04, 'epoch': 0.52}
                                                        52%|    | 3390/6500 [6:24:49<5:40:20,  6.57s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-3390
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3390
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3390
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3390
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3390
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3390/pytorch_model.bin
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3390/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3390/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3390/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.4252, 'learning_rate': 4.6618288583706634e-05, 'epoch': 0.52}
{'loss': 0.4175, 'learning_rate': 4.659417079747648e-05, 'epoch': 0.52}
{'loss': 0.4072, 'learning_rate': 4.6570053807338186e-05, 'epoch': 0.52}
{'loss': 0.4153, 'learning_rate': 4.654593761892897e-05, 'epoch': 0.52}
{'loss': 0.4002, 'learning_rate': 4.652182223788584e-05, 'epoch': 0.52}
 52%|    | 3391/6500 [6:24:56<6:07:42,  7.10s/it]                                                        52%|    | 3391/6500 [6:24:56<6:07:42,  7.10s/it] 52%|    | 3392/6500 [6:25:02<5:58:55,  6.93s/it]                                                        52%|    | 3392/6500 [6:25:02<5:58:55,  6.93s/it] 52%|    | 3393/6500 [6:25:09<5:53:10,  6.82s/it]                                                        52%|    | 3393/6500 [6:25:09<5:53:10,  6.82s/it] 52%|    | 3394/6500 [6:25:16<6:02:19,  7.00s/it]                                                        52%|    | 3394/6500 [6:25:16<6:02:19,  7.00s/it] 52%|    | 3395/6500 [6:25:23<5:55:02,  6.86s/it]                                                        52%|    | 3395/6500 [6:25:23<5:55:02,  6.86s/it] 52%|    | 3396/6500 [6:25:30<5:49:57,  {'loss': 0.4019, 'learning_rate': 4.64977076698456e-05, 'epoch': 0.52}
{'loss': 0.4395, 'learning_rate': 4.647359392044491e-05, 'epoch': 0.52}
{'loss': 0.4094, 'learning_rate': 4.644948099532019e-05, 'epoch': 0.52}
{'loss': 0.6919, 'learning_rate': 4.64253689001077e-05, 'epoch': 0.52}
{'loss': 0.4149, 'learning_rate': 4.640125764044351e-05, 'epoch': 0.52}
6.76s/it]                                                        52%|    | 3396/6500 [6:25:30<5:49:57,  6.76s/it] 52%|    | 3397/6500 [6:25:36<5:46:23,  6.70s/it]                                                        52%|    | 3397/6500 [6:25:36<5:46:23,  6.70s/it] 52%|    | 3398/6500 [6:25:43<5:43:45,  6.65s/it]                                                        52%|    | 3398/6500 [6:25:43<5:43:45,  6.65s/it] 52%|    | 3399/6500 [6:25:49<5:41:43,  6.61s/it]                                                        52%|    | 3399/6500 [6:25:49<5:41:43,  6.61s/it] 52%|    | 3400/6500 [6:25:56<5:40:24,  6.59s/it]                                                        52%|    | 3400/6500 [6:25:56<5:40:24,  6.59s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.859673798084259, 'eval_runtime': 1.4784, 'eval_samples_per_second': 8.117, 'eval_steps_per_second': 2.029, 'epoch': 0.52}
                                                        52%|    | 3400/6500 [6:25:57<5:40:24,  6.59s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-3400
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3400
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3400
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3400
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3400
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3400/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3400/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3400/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3400/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.4146, 'learning_rate': 4.6377147221963455e-05, 'epoch': 0.52}
{'loss': 0.399, 'learning_rate': 4.635303765030321e-05, 'epoch': 0.52}
{'loss': 0.3964, 'learning_rate': 4.6328928931098236e-05, 'epoch': 0.52}
{'loss': 0.4046, 'learning_rate': 4.630482106998381e-05, 'epoch': 0.52}
{'loss': 0.4057, 'learning_rate': 4.628071407259499e-05, 'epoch': 0.52}
 52%|    | 3401/6500 [6:26:04<6:08:13,  7.13s/it]                                                        52%|    | 3401/6500 [6:26:04<6:08:13,  7.13s/it] 52%|    | 3402/6500 [6:26:11<6:00:11,  6.98s/it]                                                        52%|    | 3402/6500 [6:26:11<6:00:11,  6.98s/it] 52%|    | 3403/6500 [6:26:17<5:53:31,  6.85s/it]                                                        52%|    | 3403/6500 [6:26:17<5:53:31,  6.85s/it] 52%|    | 3404/6500 [6:26:24<5:48:41,  6.76s/it]                                                        52%|    | 3404/6500 [6:26:24<5:48:41,  6.76s/it] 52%|    | 3405/6500 [6:26:30<5:45:18,  6.69s/it]                                                        52%|    | 3405/6500 [6:26:30<5:45:18,  6.69s/it] 52%|    | 3406/6500 [6:26:37<5:42:49,  {'loss': 0.4241, 'learning_rate': 4.625660794456665e-05, 'epoch': 0.52}
{'loss': 0.4044, 'learning_rate': 4.623250269153343e-05, 'epoch': 0.52}
{'loss': 0.4044, 'learning_rate': 4.6208398319129804e-05, 'epoch': 0.52}
{'loss': 0.4251, 'learning_rate': 4.6184294832990016e-05, 'epoch': 0.52}
{'loss': 0.4032, 'learning_rate': 4.616019223874811e-05, 'epoch': 0.52}
6.65s/it]                                                        52%|    | 3406/6500 [6:26:37<5:42:49,  6.65s/it] 52%|    | 3407/6500 [6:26:43<5:41:13,  6.62s/it]                                                        52%|    | 3407/6500 [6:26:43<5:41:13,  6.62s/it] 52%|    | 3408/6500 [6:26:50<5:39:48,  6.59s/it]                                                        52%|    | 3408/6500 [6:26:50<5:39:48,  6.59s/it] 52%|    | 3409/6500 [6:26:57<5:38:50,  6.58s/it]                                                        52%|    | 3409/6500 [6:26:57<5:38:50,  6.58s/it] 52%|    | 3410/6500 [6:27:04<5:52:07,  6.84s/it]                                                        52%|    | 3410/6500 [6:27:04<5:52:07,  6.84s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8585035800933838, 'eval_runtime': 1.4815, 'eval_samples_per_second': 8.1, 'eval_steps_per_second': 2.025, 'epoch': 0.52}
                                                        52%|    | 3410/6500 [6:27:05<5:52:07,  6.84s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-3410
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3410
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3410
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3410
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3410
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3410/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3410/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3410/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3410/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.4132, 'learning_rate': 4.6136090542037924e-05, 'epoch': 0.52}
{'loss': 0.4299, 'learning_rate': 4.611198974849309e-05, 'epoch': 0.52}
{'loss': 0.4105, 'learning_rate': 4.6087889863747e-05, 'epoch': 0.53}
{'loss': 0.6958, 'learning_rate': 4.6063790893432874e-05, 'epoch': 0.53}
{'loss': 0.4006, 'learning_rate': 4.603969284318369e-05, 'epoch': 0.53}
 52%|    | 3411/6500 [6:27:12<6:16:33,  7.31s/it]                                                        52%|    | 3411/6500 [6:27:12<6:16:33,  7.31s/it] 52%|    | 3412/6500 [6:27:19<6:04:08,  7.08s/it]                                                        52%|    | 3412/6500 [6:27:19<6:04:08,  7.08s/it] 53%|    | 3413/6500 [6:27:25<5:55:33,  6.91s/it]                                                        53%|    | 3413/6500 [6:27:25<5:55:33,  6.91s/it] 53%|    | 3414/6500 [6:27:32<5:49:29,  6.80s/it]                                                        53%|    | 3414/6500 [6:27:32<5:49:29,  6.80s/it] 53%|    | 3415/6500 [6:27:38<5:45:15,  6.71s/it]                                                        53%|    | 3415/6500 [6:27:38<5:45:15,  6.71s/it] 53%|    | 3416/6500 [6:27:45<5:42:14,  {'loss': 0.4219, 'learning_rate': 4.6015595718632226e-05, 'epoch': 0.53}
{'loss': 0.3965, 'learning_rate': 4.5991499525411046e-05, 'epoch': 0.53}
{'loss': 0.4012, 'learning_rate': 4.596740426915247e-05, 'epoch': 0.53}
{'loss': 0.4073, 'learning_rate': 4.594330995548863e-05, 'epoch': 0.53}
{'loss': 0.4113, 'learning_rate': 4.591921659005142e-05, 'epoch': 0.53}
6.66s/it]                                                        53%|    | 3416/6500 [6:27:45<5:42:14,  6.66s/it] 53%|    | 3417/6500 [6:27:52<5:40:14,  6.62s/it]                                                        53%|    | 3417/6500 [6:27:52<5:40:14,  6.62s/it] 53%|    | 3418/6500 [6:27:58<5:38:39,  6.59s/it]                                                        53%|    | 3418/6500 [6:27:58<5:38:39,  6.59s/it] 53%|    | 3419/6500 [6:28:05<5:37:26,  6.57s/it]                                                        53%|    | 3419/6500 [6:28:05<5:37:26,  6.57s/it] 53%|    | 3420/6500 [6:28:11<5:36:35,  6.56s/it]                                                        53%|    | 3420/6500 [6:28:11<5:36:35,  6.56s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8633333444595337, 'eval_runtime': 1.4697, 'eval_samples_per_second': 8.165, 'eval_steps_per_second': 2.041, 'epoch': 0.53}
                                                        53%|    | 3420/6500 [6:28:13<5:36:35,  6.56s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-3420
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3420I AM HERE AND I AM SAVING THE MODEL

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3420
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3420
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3420
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3420/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3420/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3420/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3420/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.4226, 'learning_rate': 4.589512417847252e-05, 'epoch': 0.53}
{'loss': 0.4277, 'learning_rate': 4.5871032726383386e-05, 'epoch': 0.53}
{'loss': 0.4055, 'learning_rate': 4.584694223941526e-05, 'epoch': 0.53}
{'loss': 0.4152, 'learning_rate': 4.582285272319913e-05, 'epoch': 0.53}
{'loss': 0.3955, 'learning_rate': 4.579876418336577e-05, 'epoch': 0.53}
 53%|    | 3421/6500 [6:28:19<6:03:55,  7.09s/it]                                                        53%|    | 3421/6500 [6:28:19<6:03:55,  7.09s/it] 53%|    | 3422/6500 [6:28:26<5:55:06,  6.92s/it]                                                        53%|    | 3422/6500 [6:28:26<5:55:06,  6.92s/it] 53%|    | 3423/6500 [6:28:33<5:49:03,  6.81s/it]                                                        53%|    | 3423/6500 [6:28:33<5:49:03,  6.81s/it] 53%|    | 3424/6500 [6:28:39<5:44:44,  6.72s/it]                                                        53%|    | 3424/6500 [6:28:39<5:44:44,  6.72s/it] 53%|    | 3425/6500 [6:28:46<5:41:34,  6.66s/it]                                                        53%|    | 3425/6500 [6:28:46<5:41:34,  6.66s/it] 53%|    | 3426/6500 [6:28:53<5:53:00,  {'loss': 0.4357, 'learning_rate': 4.577467662554574e-05, 'epoch': 0.53}
{'loss': 0.4003, 'learning_rate': 4.575059005536935e-05, 'epoch': 0.53}
{'loss': 0.6856, 'learning_rate': 4.572650447846672e-05, 'epoch': 0.53}
{'loss': 0.4293, 'learning_rate': 4.570241990046767e-05, 'epoch': 0.53}
{'loss': 0.4018, 'learning_rate': 4.5678336327001844e-05, 'epoch': 0.53}
6.89s/it]                                                        53%|    | 3426/6500 [6:28:53<5:53:00,  6.89s/it] 53%|    | 3427/6500 [6:29:00<5:47:41,  6.79s/it]                                                        53%|    | 3427/6500 [6:29:00<5:47:41,  6.79s/it] 53%|    | 3428/6500 [6:29:06<5:43:37,  6.71s/it]                                                        53%|    | 3428/6500 [6:29:06<5:43:37,  6.71s/it] 53%|    | 3429/6500 [6:29:13<5:41:16,  6.67s/it]                                                        53%|    | 3429/6500 [6:29:13<5:41:16,  6.67s/it] 53%|    | 3430/6500 [6:29:19<5:39:15,  6.63s/it]                                                        53%|    | 3430/6500 [6:29:19<5:39:15,  6.63s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8629167675971985, 'eval_runtime': 1.4679, 'eval_samples_per_second': 8.175, 'eval_steps_per_second': 2.044, 'epoch': 0.53}
                                                        53%|    | 3430/6500 [6:29:21<5:39:15,  6.63s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-3430
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3430
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3430
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3430
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3430
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3430/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3430/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3430/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3430/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.4175, 'learning_rate': 4.565425376369862e-05, 'epoch': 0.53}
{'loss': 0.3921, 'learning_rate': 4.563017221618715e-05, 'epoch': 0.53}
{'loss': 0.3989, 'learning_rate': 4.560609169009636e-05, 'epoch': 0.53}
{'loss': 0.4061, 'learning_rate': 4.558201219105494e-05, 'epoch': 0.53}
{'loss': 0.3997, 'learning_rate': 4.555793372469129e-05, 'epoch': 0.53}
 53%|    | 3431/6500 [6:29:28<6:06:28,  7.16s/it]                                                        53%|    | 3431/6500 [6:29:28<6:06:28,  7.16s/it] 53%|    | 3432/6500 [6:29:34<5:56:45,  6.98s/it]                                                        53%|    | 3432/6500 [6:29:34<5:56:45,  6.98s/it] 53%|    | 3433/6500 [6:29:41<5:49:49,  6.84s/it]                                                        53%|    | 3433/6500 [6:29:41<5:49:49,  6.84s/it] 53%|    | 3434/6500 [6:29:47<5:44:55,  6.75s/it]                                                        53%|    | 3434/6500 [6:29:47<5:44:55,  6.75s/it] 53%|    | 3435/6500 [6:29:54<5:41:38,  6.69s/it]                                                        53%|    | 3435/6500 [6:29:54<5:41:38,  6.69s/it] 53%|    | 3436/6500 [6:30:00<5:39:15,  {'loss': 0.4171, 'learning_rate': 4.553385629663363e-05, 'epoch': 0.53}
{'loss': 0.4152, 'learning_rate': 4.55097799125099e-05, 'epoch': 0.53}
{'loss': 0.3949, 'learning_rate': 4.548570457794782e-05, 'epoch': 0.53}
{'loss': 0.414, 'learning_rate': 4.546163029857485e-05, 'epoch': 0.53}
{'loss': 0.3942, 'learning_rate': 4.5437557080018175e-05, 'epoch': 0.53}
6.64s/it]                                                        53%|    | 3436/6500 [6:30:00<5:39:15,  6.64s/it] 53%|    | 3437/6500 [6:30:07<5:39:46,  6.66s/it]                                                        53%|    | 3437/6500 [6:30:07<5:39:46,  6.66s/it] 53%|    | 3438/6500 [6:30:14<5:37:57,  6.62s/it]                                                        53%|    | 3438/6500 [6:30:14<5:37:57,  6.62s/it] 53%|    | 3439/6500 [6:30:20<5:36:53,  6.60s/it]                                                        53%|    | 3439/6500 [6:30:20<5:36:53,  6.60s/it] 53%|    | 3440/6500 [6:30:27<5:35:45,  6.58s/it]                                                        53%|    | 3440/6500 [6:30:27<5:35:45,  6.58s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8584519028663635, 'eval_runtime': 1.4693, 'eval_samples_per_second': 8.167, 'eval_steps_per_second': 2.042, 'epoch': 0.53}
                                                        53%|    | 3440/6500 [6:30:28<5:35:45,  6.58s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-3440
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3440
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3440
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3440
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3440
the pytorch model path isthe pytorch model path is  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3440/pytorch_model.bin/projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3440/pytorch_model.bin

the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3440/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3440/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.4417, 'learning_rate': 4.541348492790482e-05, 'epoch': 0.53}
{'loss': 0.3963, 'learning_rate': 4.538941384786147e-05, 'epoch': 0.53}
{'loss': 0.6825, 'learning_rate': 4.536534384551462e-05, 'epoch': 0.53}
{'loss': 0.4252, 'learning_rate': 4.5341274926490446e-05, 'epoch': 0.53}
{'loss': 0.3909, 'learning_rate': 4.5317207096414934e-05, 'epoch': 0.53}
 53%|    | 3441/6500 [6:30:35<6:03:25,  7.13s/it]                                                        53%|    | 3441/6500 [6:30:35<6:03:25,  7.13s/it] 53%|    | 3442/6500 [6:30:42<5:54:19,  6.95s/it]                                                        53%|    | 3442/6500 [6:30:42<5:54:19,  6.95s/it] 53%|    | 3443/6500 [6:30:49<5:57:40,  7.02s/it]                                                        53%|    | 3443/6500 [6:30:49<5:57:40,  7.02s/it] 53%|    | 3444/6500 [6:30:55<5:50:13,  6.88s/it]                                                        53%|    | 3444/6500 [6:30:55<5:50:13,  6.88s/it] 53%|    | 3445/6500 [6:31:02<5:44:57,  6.77s/it]                                                        53%|    | 3445/6500 [6:31:02<5:44:57,  6.77s/it] 53%|    | 3446/6500 [6:31:08<5:41:26,  {'loss': 0.4141, 'learning_rate': 4.529314036091379e-05, 'epoch': 0.53}
{'loss': 0.3931, 'learning_rate': 4.5269074725612474e-05, 'epoch': 0.53}
{'loss': 0.3956, 'learning_rate': 4.524501019613619e-05, 'epoch': 0.53}
{'loss': 0.4113, 'learning_rate': 4.522094677810985e-05, 'epoch': 0.53}
{'loss': 0.4063, 'learning_rate': 4.519688447715814e-05, 'epoch': 0.53}
6.71s/it]                                                        53%|    | 3446/6500 [6:31:08<5:41:26,  6.71s/it] 53%|    | 3447/6500 [6:31:15<5:38:46,  6.66s/it]                                                        53%|    | 3447/6500 [6:31:15<5:38:46,  6.66s/it] 53%|    | 3448/6500 [6:31:21<5:36:51,  6.62s/it]                                                        53%|    | 3448/6500 [6:31:21<5:36:51,  6.62s/it] 53%|    | 3449/6500 [6:31:28<5:35:23,  6.60s/it]                                                        53%|    | 3449/6500 [6:31:28<5:35:23,  6.60s/it] 53%|    | 3450/6500 [6:31:35<5:34:26,  6.58s/it]                                                        53%|    | 3450/6500 [6:31:35<5:34:26,  6.58s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8637558221817017, 'eval_runtime': 1.4837, 'eval_samples_per_second': 8.088, 'eval_steps_per_second': 2.022, 'epoch': 0.53}
                                                        53%|    | 3450/6500 [6:31:36<5:34:26,  6.58s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-3450
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3450
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3450
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3450
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3450
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3450/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3450/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3450/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3450/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.4111, 'learning_rate': 4.517282329890548e-05, 'epoch': 0.53}
{'loss': 0.4107, 'learning_rate': 4.514876324897602e-05, 'epoch': 0.53}
{'loss': 0.3993, 'learning_rate': 4.512470433299366e-05, 'epoch': 0.53}
{'loss': 0.4058, 'learning_rate': 4.510064655658203e-05, 'epoch': 0.53}
{'loss': 0.397, 'learning_rate': 4.5076589925364465e-05, 'epoch': 0.53}
 53%|    | 3451/6500 [6:31:43<6:01:23,  7.11s/it]                                                        53%|    | 3451/6500 [6:31:43<6:01:23,  7.11s/it] 53%|    | 3452/6500 [6:31:49<5:52:34,  6.94s/it]                                                        53%|    | 3452/6500 [6:31:49<5:52:34,  6.94s/it] 53%|    | 3453/6500 [6:31:56<5:46:19,  6.82s/it]                                                        53%|    | 3453/6500 [6:31:56<5:46:19,  6.82s/it] 53%|    | 3454/6500 [6:32:02<5:41:54,  6.73s/it]                                                        53%|    | 3454/6500 [6:32:02<5:41:54,  6.73s/it] 53%|    | 3455/6500 [6:32:09<5:39:20,  6.69s/it]                                                        53%|    | 3455/6500 [6:32:09<5:39:20,  6.69s/it] 53%|    | 3456/6500 [6:32:16<5:36:58,  {'loss': 0.4401, 'learning_rate': 4.505253444496407e-05, 'epoch': 0.53}
{'loss': 0.4059, 'learning_rate': 4.502848012100367e-05, 'epoch': 0.53}
{'loss': 0.6797, 'learning_rate': 4.500442695910582e-05, 'epoch': 0.53}
{'loss': 0.4162, 'learning_rate': 4.4980374964892794e-05, 'epoch': 0.53}
{'loss': 0.4027, 'learning_rate': 4.4956324143986596e-05, 'epoch': 0.53}
6.64s/it]                                                        53%|    | 3456/6500 [6:32:16<5:36:58,  6.64s/it] 53%|    | 3457/6500 [6:32:22<5:35:06,  6.61s/it]                                                        53%|    | 3457/6500 [6:32:22<5:35:06,  6.61s/it] 53%|    | 3458/6500 [6:32:29<5:34:00,  6.59s/it]                                                        53%|    | 3458/6500 [6:32:29<5:34:00,  6.59s/it] 53%|    | 3459/6500 [6:32:36<5:49:58,  6.91s/it]                                                        53%|    | 3459/6500 [6:32:36<5:49:58,  6.91s/it] 53%|    | 3460/6500 [6:32:43<5:44:11,  6.79s/it]                                                        53%|    | 3460/6500 [6:32:43<5:44:11,  6.79s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8612211346626282, 'eval_runtime': 1.4686, 'eval_samples_per_second': 8.171, 'eval_steps_per_second': 2.043, 'epoch': 0.53}
                                                        53%|    | 3460/6500 [6:32:44<5:44:11,  6.79s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-3460
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3460I AM HERE AND I AM SAVING THE MODEL

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3460
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3460
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3460
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3460/pytorch_model.bin
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3460/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3460/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3460/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.402, 'learning_rate': 4.4932274502008956e-05, 'epoch': 0.53}
{'loss': 0.3998, 'learning_rate': 4.4908226044581346e-05, 'epoch': 0.53}
{'loss': 0.4101, 'learning_rate': 4.488417877732494e-05, 'epoch': 0.53}
{'loss': 0.4096, 'learning_rate': 4.4860132705860644e-05, 'epoch': 0.53}
{'loss': 0.4178, 'learning_rate': 4.4836087835809083e-05, 'epoch': 0.53}
 53%|    | 3461/6500 [6:32:51<6:07:31,  7.26s/it]                                                        53%|    | 3461/6500 [6:32:51<6:07:31,  7.26s/it] 53%|    | 3462/6500 [6:32:58<5:56:30,  7.04s/it]                                                        53%|    | 3462/6500 [6:32:58<5:56:30,  7.04s/it] 53%|    | 3463/6500 [6:33:04<5:48:40,  6.89s/it]                                                        53%|    | 3463/6500 [6:33:04<5:48:40,  6.89s/it] 53%|    | 3464/6500 [6:33:11<5:44:00,  6.80s/it]                                                        53%|    | 3464/6500 [6:33:11<5:44:00,  6.80s/it] 53%|    | 3465/6500 [6:33:17<5:39:54,  6.72s/it]                                                        53%|    | 3465/6500 [6:33:17<5:39:54,  6.72s/it] 53%|    | 3466/6500 [6:33:24<5:37:00,  {'loss': 0.4066, 'learning_rate': 4.481204417279058e-05, 'epoch': 0.53}
{'loss': 0.4111, 'learning_rate': 4.478800172242521e-05, 'epoch': 0.53}
{'loss': 0.4134, 'learning_rate': 4.476396049033275e-05, 'epoch': 0.53}
{'loss': 0.3982, 'learning_rate': 4.473992048213269e-05, 'epoch': 0.53}
{'loss': 0.3951, 'learning_rate': 4.471588170344423e-05, 'epoch': 0.53}
6.66s/it]                                                        53%|    | 3466/6500 [6:33:24<5:37:00,  6.66s/it] 53%|    | 3467/6500 [6:33:30<5:35:14,  6.63s/it]                                                        53%|    | 3467/6500 [6:33:30<5:35:14,  6.63s/it] 53%|    | 3468/6500 [6:33:37<5:33:47,  6.61s/it]                                                        53%|    | 3468/6500 [6:33:37<5:33:47,  6.61s/it] 53%|    | 3469/6500 [6:33:44<5:32:46,  6.59s/it]                                                        53%|    | 3469/6500 [6:33:44<5:32:46,  6.59s/it] 53%|    | 3470/6500 [6:33:50<5:31:41,  6.57s/it]                                                        53%|    | 3470/6500 [6:33:50<5:31:41,  6.57s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8609697818756104, 'eval_runtime': 1.4849, 'eval_samples_per_second': 8.081, 'eval_steps_per_second': 2.02, 'epoch': 0.53}
                                                        53%|    | 3470/6500 [6:33:52<5:31:41,  6.57s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-3470
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3470I AM HERE AND I AM SAVING THE MODEL

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3470
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3470
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3470
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3470/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3470/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3470/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3470/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.4394, 'learning_rate': 4.469184415988631e-05, 'epoch': 0.53}
{'loss': 0.409, 'learning_rate': 4.466780785707752e-05, 'epoch': 0.53}
{'loss': 0.6836, 'learning_rate': 4.464377280063624e-05, 'epoch': 0.53}
{'loss': 0.4127, 'learning_rate': 4.46197389961805e-05, 'epoch': 0.53}
{'loss': 0.4071, 'learning_rate': 4.459570644932805e-05, 'epoch': 0.53}
 53%|    | 3471/6500 [6:33:59<6:00:18,  7.14s/it]                                                        53%|    | 3471/6500 [6:33:59<6:00:18,  7.14s/it] 53%|    | 3472/6500 [6:34:05<5:51:08,  6.96s/it]                                                        53%|    | 3472/6500 [6:34:05<5:51:08,  6.96s/it] 53%|    | 3473/6500 [6:34:12<5:45:14,  6.84s/it]                                                        53%|    | 3473/6500 [6:34:12<5:45:14,  6.84s/it] 53%|    | 3474/6500 [6:34:18<5:40:36,  6.75s/it]                                                        53%|    | 3474/6500 [6:34:18<5:40:36,  6.75s/it] 53%|    | 3475/6500 [6:34:26<5:51:39,  6.98s/it]                                                        53%|    | 3475/6500 [6:34:26<5:51:39,  6.98s/it] 53%|    | 3476/6500 [6:34:32<5:44:55,  {'loss': 0.3957, 'learning_rate': 4.457167516569637e-05, 'epoch': 0.53}
{'loss': 0.3907, 'learning_rate': 4.454764515090261e-05, 'epoch': 0.53}
{'loss': 0.3954, 'learning_rate': 4.452361641056364e-05, 'epoch': 0.54}
{'loss': 0.4031, 'learning_rate': 4.449958895029604e-05, 'epoch': 0.54}
{'loss': 0.4092, 'learning_rate': 4.447556277571608e-05, 'epoch': 0.54}
6.84s/it]                                                        53%|    | 3476/6500 [6:34:32<5:44:55,  6.84s/it] 53%|    | 3477/6500 [6:34:39<5:40:17,  6.75s/it]                                                        53%|    | 3477/6500 [6:34:39<5:40:17,  6.75s/it] 54%|    | 3478/6500 [6:34:45<5:36:55,  6.69s/it]                                                        54%|    | 3478/6500 [6:34:45<5:36:55,  6.69s/it] 54%|    | 3479/6500 [6:34:52<5:34:36,  6.65s/it]                                                        54%|    | 3479/6500 [6:34:52<5:34:36,  6.65s/it] 54%|    | 3480/6500 [6:34:58<5:32:57,  6.62s/it]                                                        54%|    | 3480/6500 [6:34:58<5:32:57,  6.62s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.867411732673645, 'eval_runtime': 1.4752, 'eval_samples_per_second': 8.134, 'eval_steps_per_second': 2.034, 'epoch': 0.54}
                                                        54%|    | 3480/6500 [6:35:00<5:32:57,  6.62s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-3480
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  I AM HERE AND I AM SAVING THE MODEL
/projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3480the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3480

I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3480
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3480
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3480/pytorch_model.bin
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3480/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3480/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3480/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.416, 'learning_rate': 4.4451537892439735e-05, 'epoch': 0.54}
{'loss': 0.4076, 'learning_rate': 4.442751430608267e-05, 'epoch': 0.54}
{'loss': 0.413, 'learning_rate': 4.440349202226026e-05, 'epoch': 0.54}
{'loss': 0.396, 'learning_rate': 4.437947104658755e-05, 'epoch': 0.54}
{'loss': 0.4043, 'learning_rate': 4.4355451384679313e-05, 'epoch': 0.54}
 54%|    | 3481/6500 [6:35:07<5:59:04,  7.14s/it]                                                        54%|    | 3481/6500 [6:35:07<5:59:04,  7.14s/it] 54%|    | 3482/6500 [6:35:13<5:49:46,  6.95s/it]                                                        54%|    | 3482/6500 [6:35:13<5:49:46,  6.95s/it] 54%|    | 3483/6500 [6:35:20<5:43:14,  6.83s/it]                                                        54%|    | 3483/6500 [6:35:20<5:43:14,  6.83s/it] 54%|    | 3484/6500 [6:35:26<5:40:02,  6.76s/it]                                                        54%|    | 3484/6500 [6:35:26<5:40:02,  6.76s/it] 54%|    | 3485/6500 [6:35:33<5:36:29,  6.70s/it]                                                        54%|    | 3485/6500 [6:35:33<5:36:29,  6.70s/it] 54%|    | 3486/6500 [6:35:40<5:34:04,  {'loss': 0.4343, 'learning_rate': 4.4331433042150003e-05, 'epoch': 0.54}
{'loss': 0.407, 'learning_rate': 4.430741602461376e-05, 'epoch': 0.54}
{'loss': 0.6918, 'learning_rate': 4.428340033768439e-05, 'epoch': 0.54}
{'loss': 0.3998, 'learning_rate': 4.4259385986975446e-05, 'epoch': 0.54}
{'loss': 0.4194, 'learning_rate': 4.423537297810012e-05, 'epoch': 0.54}
6.65s/it]                                                        54%|    | 3486/6500 [6:35:40<5:34:04,  6.65s/it] 54%|    | 3487/6500 [6:35:46<5:34:13,  6.66s/it]                                                        54%|    | 3487/6500 [6:35:46<5:34:13,  6.66s/it] 54%|    | 3488/6500 [6:35:53<5:32:10,  6.62s/it]                                                        54%|    | 3488/6500 [6:35:53<5:32:10,  6.62s/it] 54%|    | 3489/6500 [6:35:59<5:30:49,  6.59s/it]                                                        54%|    | 3489/6500 [6:35:59<5:30:49,  6.59s/it] 54%|    | 3490/6500 [6:36:06<5:29:50,  6.57s/it]                                                        54%|    | 3490/6500 [6:36:06<5:29:50,  6.57s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8632245063781738, 'eval_runtime': 1.6383, 'eval_samples_per_second': 7.325, 'eval_steps_per_second': 1.831, 'epoch': 0.54}
                                                        54%|    | 3490/6500 [6:36:07<5:29:50,  6.57s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-3490
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3490
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3490
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3490
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3490
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3490/pytorch_model.bin
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3490/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3490/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3490/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3908, 'learning_rate': 4.421136131667131e-05, 'epoch': 0.54}
{'loss': 0.3993, 'learning_rate': 4.4187351008301596e-05, 'epoch': 0.54}
{'loss': 0.3998, 'learning_rate': 4.4163342058603255e-05, 'epoch': 0.54}
{'loss': 0.4051, 'learning_rate': 4.413933447318821e-05, 'epoch': 0.54}
{'loss': 0.4211, 'learning_rate': 4.41153282576681e-05, 'epoch': 0.54}
 54%|    | 3491/6500 [6:36:16<6:21:02,  7.60s/it]                                                        54%|    | 3491/6500 [6:36:16<6:21:02,  7.60s/it] 54%|    | 3492/6500 [6:36:22<6:05:04,  7.28s/it]                                                        54%|    | 3492/6500 [6:36:22<6:05:04,  7.28s/it] 54%|    | 3493/6500 [6:36:29<5:53:59,  7.06s/it]                                                        54%|    | 3493/6500 [6:36:29<5:53:59,  7.06s/it] 54%|    | 3494/6500 [6:36:35<5:46:01,  6.91s/it]                                                        54%|    | 3494/6500 [6:36:35<5:46:01,  6.91s/it] 54%|    | 3495/6500 [6:36:42<5:40:30,  6.80s/it]                                                        54%|    | 3495/6500 [6:36:42<5:40:30,  6.80s/it] 54%|    | 3496/6500 [6:36:48<5:36:36,  {'loss': 0.4108, 'learning_rate': 4.4091323417654225e-05, 'epoch': 0.54}
{'loss': 0.4063, 'learning_rate': 4.406731995875758e-05, 'epoch': 0.54}
{'loss': 0.4123, 'learning_rate': 4.404331788658882e-05, 'epoch': 0.54}
{'loss': 0.3963, 'learning_rate': 4.4019317206758297e-05, 'epoch': 0.54}
{'loss': 0.4112, 'learning_rate': 4.399531792487601e-05, 'epoch': 0.54}
6.72s/it]                                                        54%|    | 3496/6500 [6:36:48<5:36:36,  6.72s/it] 54%|    | 3497/6500 [6:36:55<5:33:49,  6.67s/it]                                                        54%|    | 3497/6500 [6:36:55<5:33:49,  6.67s/it] 54%|    | 3498/6500 [6:37:02<5:31:56,  6.63s/it]                                                        54%|    | 3498/6500 [6:37:02<5:31:56,  6.63s/it] 54%|    | 3499/6500 [6:37:08<5:30:24,  6.61s/it]                                                        54%|    | 3499/6500 [6:37:08<5:30:24,  6.61s/it] 54%|    | 3500/6500 [6:37:15<5:29:25,  6.59s/it]                                                        54%|    | 3500/6500 [6:37:15<5:29:25,  6.59s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8628902435302734, 'eval_runtime': 1.4817, 'eval_samples_per_second': 8.099, 'eval_steps_per_second': 2.025, 'epoch': 0.54}
                                                        54%|    | 3500/6500 [6:37:16<5:29:25,  6.59s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-3500
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3500
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3500
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3500
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3500
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3500/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3500/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3500/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3500/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.4208, 'learning_rate': 4.397132004655165e-05, 'epoch': 0.54}
{'loss': 0.4841, 'learning_rate': 4.394732357739456e-05, 'epoch': 0.54}
{'loss': 0.6234, 'learning_rate': 4.392332852301379e-05, 'epoch': 0.54}
{'loss': 0.399, 'learning_rate': 4.389933488901805e-05, 'epoch': 0.54}
{'loss': 0.4239, 'learning_rate': 4.387534268101566e-05, 'epoch': 0.54}
 54%|    | 3501/6500 [6:37:23<5:56:45,  7.14s/it]                                                        54%|    | 3501/6500 [6:37:23<5:56:45,  7.14s/it] 54%|    | 3502/6500 [6:37:30<5:47:40,  6.96s/it]                                                        54%|    | 3502/6500 [6:37:30<5:47:40,  6.96s/it] 54%|    | 3503/6500 [6:37:36<5:41:15,  6.83s/it]                                                        54%|    | 3503/6500 [6:37:36<5:41:15,  6.83s/it] 54%|    | 3504/6500 [6:37:43<5:36:53,  6.75s/it]                                                        54%|    | 3504/6500 [6:37:43<5:36:53,  6.75s/it] 54%|    | 3505/6500 [6:37:49<5:33:41,  6.68s/it]                                                        54%|    | 3505/6500 [6:37:49<5:33:41,  6.68s/it] 54%|    | 3506/6500 [6:37:56<5:34:10,  {'loss': 0.3944, 'learning_rate': 4.38513519046147e-05, 'epoch': 0.54}
{'loss': 0.3956, 'learning_rate': 4.382736256542283e-05, 'epoch': 0.54}
{'loss': 0.4035, 'learning_rate': 4.3803374669047436e-05, 'epoch': 0.54}
{'loss': 0.4048, 'learning_rate': 4.377938822109554e-05, 'epoch': 0.54}
{'loss': 0.4295, 'learning_rate': 4.3755403227173836e-05, 'epoch': 0.54}
6.70s/it]                                                        54%|    | 3506/6500 [6:37:56<5:34:10,  6.70s/it] 54%|    | 3507/6500 [6:38:04<5:49:58,  7.02s/it]                                                        54%|    | 3507/6500 [6:38:04<5:49:58,  7.02s/it] 54%|    | 3508/6500 [6:38:10<5:43:07,  6.88s/it]                                                        54%|    | 3508/6500 [6:38:10<5:43:07,  6.88s/it] 54%|    | 3509/6500 [6:38:17<5:38:05,  6.78s/it]                                                        54%|    | 3509/6500 [6:38:17<5:38:05,  6.78s/it] 54%|    | 3510/6500 [6:38:23<5:34:29,  6.71s/it]                                                        54%|    | 3510/6500 [6:38:23<5:34:29,  6.71s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8642618656158447, 'eval_runtime': 1.4806, 'eval_samples_per_second': 8.105, 'eval_steps_per_second': 2.026, 'epoch': 0.54}
                                                        54%|    | 3510/6500 [6:38:25<5:34:29,  6.71s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-3510
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3510
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3510
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3510
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3510
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3510/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3510/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3510/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3510/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.4169, 'learning_rate': 4.373141969288865e-05, 'epoch': 0.54}
{'loss': 0.3972, 'learning_rate': 4.3707437623845995e-05, 'epoch': 0.54}
{'loss': 0.4097, 'learning_rate': 4.368345702565153e-05, 'epoch': 0.54}
{'loss': 0.3952, 'learning_rate': 4.365947790391059e-05, 'epoch': 0.54}
{'loss': 0.4419, 'learning_rate': 4.3635500264228146e-05, 'epoch': 0.54}
 54%|    | 3511/6500 [6:38:32<5:59:20,  7.21s/it]                                                        54%|    | 3511/6500 [6:38:32<5:59:20,  7.21s/it] 54%|    | 3512/6500 [6:38:38<5:49:16,  7.01s/it]                                                        54%|    | 3512/6500 [6:38:38<5:49:16,  7.01s/it] 54%|    | 3513/6500 [6:38:45<5:42:21,  6.88s/it]                                                        54%|    | 3513/6500 [6:38:45<5:42:21,  6.88s/it] 54%|    | 3514/6500 [6:38:51<5:37:19,  6.78s/it]                                                        54%|    | 3514/6500 [6:38:51<5:37:19,  6.78s/it] 54%|    | 3515/6500 [6:38:58<5:33:49,  6.71s/it]                                                        54%|    | 3515/6500 [6:38:58<5:33:49,  6.71s/it] 54%|    | 3516/6500 [6:39:05<5:31:10,  {'loss': 0.394, 'learning_rate': 4.361152411220878e-05, 'epoch': 0.54}
{'loss': 0.6839, 'learning_rate': 4.358754945345684e-05, 'epoch': 0.54}
{'loss': 0.4182, 'learning_rate': 4.356357629357624e-05, 'epoch': 0.54}
{'loss': 0.3996, 'learning_rate': 4.353960463817053e-05, 'epoch': 0.54}
{'loss': 0.412, 'learning_rate': 4.3515634492842956e-05, 'epoch': 0.54}
6.66s/it]                                                        54%|    | 3516/6500 [6:39:05<5:31:10,  6.66s/it] 54%|    | 3517/6500 [6:39:11<5:29:11,  6.62s/it]                                                        54%|    | 3517/6500 [6:39:11<5:29:11,  6.62s/it] 54%|    | 3518/6500 [6:39:18<5:28:04,  6.60s/it]                                                        54%|    | 3518/6500 [6:39:18<5:28:04,  6.60s/it] 54%|    | 3519/6500 [6:39:24<5:27:02,  6.58s/it]                                                        54%|    | 3519/6500 [6:39:24<5:27:02,  6.58s/it] 54%|    | 3520/6500 [6:39:31<5:26:16,  6.57s/it]                                                        54%|    | 3520/6500 [6:39:31<5:26:16,  6.57s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8637542128562927, 'eval_runtime': 1.4832, 'eval_samples_per_second': 8.09, 'eval_steps_per_second': 2.023, 'epoch': 0.54}
                                                        54%|    | 3520/6500 [6:39:32<5:26:16,  6.57s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-3520
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3520
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3520
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3520
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3520
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3520/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3520/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3520/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3520/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3883, 'learning_rate': 4.3491665863196396e-05, 'epoch': 0.54}
{'loss': 0.3978, 'learning_rate': 4.346769875483336e-05, 'epoch': 0.54}
{'loss': 0.4073, 'learning_rate': 4.3443733173356035e-05, 'epoch': 0.54}
{'loss': 0.4116, 'learning_rate': 4.3419769124366225e-05, 'epoch': 0.54}
{'loss': 0.4017, 'learning_rate': 4.339580661346536e-05, 'epoch': 0.54}
 54%|    | 3521/6500 [6:39:39<5:53:00,  7.11s/it]                                                        54%|    | 3521/6500 [6:39:39<5:53:00,  7.11s/it] 54%|    | 3522/6500 [6:39:46<5:44:20,  6.94s/it]                                                        54%|    | 3522/6500 [6:39:46<5:44:20,  6.94s/it] 54%|    | 3523/6500 [6:39:53<5:47:11,  7.00s/it]                                                        54%|    | 3523/6500 [6:39:53<5:47:11,  7.00s/it] 54%|    | 3524/6500 [6:39:59<5:40:10,  6.86s/it]                                                        54%|    | 3524/6500 [6:39:59<5:40:10,  6.86s/it] 54%|    | 3525/6500 [6:40:06<5:35:18,  6.76s/it]                                                        54%|    | 3525/6500 [6:40:06<5:35:18,  6.76s/it] 54%|    | 3526/6500 [6:40:12<5:31:48,  {'loss': 0.4085, 'learning_rate': 4.337184564625455e-05, 'epoch': 0.54}
{'loss': 0.3989, 'learning_rate': 4.334788622833452e-05, 'epoch': 0.54}
{'loss': 0.4028, 'learning_rate': 4.3323928365305636e-05, 'epoch': 0.54}
{'loss': 0.3901, 'learning_rate': 4.3299972062767905e-05, 'epoch': 0.54}
{'loss': 0.4408, 'learning_rate': 4.3276017326320985e-05, 'epoch': 0.54}
6.69s/it]                                                        54%|    | 3526/6500 [6:40:12<5:31:48,  6.69s/it] 54%|    | 3527/6500 [6:40:19<5:29:26,  6.65s/it]                                                        54%|    | 3527/6500 [6:40:19<5:29:26,  6.65s/it] 54%|    | 3528/6500 [6:40:25<5:27:44,  6.62s/it]                                                        54%|    | 3528/6500 [6:40:25<5:27:44,  6.62s/it] 54%|    | 3529/6500 [6:40:32<5:26:25,  6.59s/it]                                                        54%|    | 3529/6500 [6:40:32<5:26:25,  6.59s/it] 54%|    | 3530/6500 [6:40:39<5:25:33,  6.58s/it]                                                        54%|    | 3530/6500 [6:40:39<5:25:33,  6.58s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8616135716438293, 'eval_runtime': 1.4918, 'eval_samples_per_second': 8.044, 'eval_steps_per_second': 2.011, 'epoch': 0.54}
                                                        54%|    | 3530/6500 [6:40:40<5:25:33,  6.58s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-3530
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3530
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3530
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3530
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3530
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3530/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3530/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3530/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3530/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.4071, 'learning_rate': 4.3252064161564115e-05, 'epoch': 0.54}
{'loss': 0.671, 'learning_rate': 4.322811257409622e-05, 'epoch': 0.54}
{'loss': 0.4158, 'learning_rate': 4.320416256951584e-05, 'epoch': 0.54}
{'loss': 0.3895, 'learning_rate': 4.3180214153421135e-05, 'epoch': 0.54}
{'loss': 0.4063, 'learning_rate': 4.315626733140992e-05, 'epoch': 0.54}
 54%|    | 3531/6500 [6:40:47<5:52:31,  7.12s/it]                                                        54%|    | 3531/6500 [6:40:47<5:52:31,  7.12s/it] 54%|    | 3532/6500 [6:40:53<5:43:31,  6.94s/it]                                                        54%|    | 3532/6500 [6:40:53<5:43:31,  6.94s/it] 54%|    | 3533/6500 [6:41:00<5:40:15,  6.88s/it]                                                        54%|    | 3533/6500 [6:41:00<5:40:15,  6.88s/it] 54%|    | 3534/6500 [6:41:07<5:35:16,  6.78s/it]                                                        54%|    | 3534/6500 [6:41:07<5:35:16,  6.78s/it] 54%|    | 3535/6500 [6:41:13<5:31:18,  6.70s/it]                                                        54%|    | 3535/6500 [6:41:13<5:31:18,  6.70s/it] 54%|    | 3536/6500 [6:41:20<5:28:38,  {'loss': 0.3906, 'learning_rate': 4.3132322109079596e-05, 'epoch': 0.54}
{'loss': 0.3991, 'learning_rate': 4.3108378492027224e-05, 'epoch': 0.54}
{'loss': 0.3966, 'learning_rate': 4.3084436485849475e-05, 'epoch': 0.54}
{'loss': 0.4089, 'learning_rate': 4.306049609614265e-05, 'epoch': 0.54}
{'loss': 0.4047, 'learning_rate': 4.303655732850267e-05, 'epoch': 0.54}
6.65s/it]                                                        54%|    | 3536/6500 [6:41:20<5:28:38,  6.65s/it] 54%|    | 3537/6500 [6:41:26<5:26:43,  6.62s/it]                                                        54%|    | 3537/6500 [6:41:26<5:26:43,  6.62s/it] 54%|    | 3538/6500 [6:41:33<5:25:25,  6.59s/it]                                                        54%|    | 3538/6500 [6:41:33<5:25:25,  6.59s/it] 54%|    | 3539/6500 [6:41:39<5:24:24,  6.57s/it]                                                        54%|    | 3539/6500 [6:41:39<5:24:24,  6.57s/it] 54%|    | 3540/6500 [6:41:48<5:48:56,  7.07s/it]                                                        54%|    | 3540/6500 [6:41:48<5:48:56,  7.07s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8637916445732117, 'eval_runtime': 1.493, 'eval_samples_per_second': 8.037, 'eval_steps_per_second': 2.009, 'epoch': 0.54}
                                                        54%|    | 3540/6500 [6:41:49<5:48:56,  7.07s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-3540
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3540
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3540
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3540
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3540
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3540/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3540/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3540/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3540/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.4062, 'learning_rate': 4.301262018852509e-05, 'epoch': 0.54}
{'loss': 0.4096, 'learning_rate': 4.2988684681805036e-05, 'epoch': 0.54}
{'loss': 0.4012, 'learning_rate': 4.296475081393731e-05, 'epoch': 0.55}
{'loss': 0.3896, 'learning_rate': 4.294081859051632e-05, 'epoch': 0.55}
{'loss': 0.4429, 'learning_rate': 4.2916888017136055e-05, 'epoch': 0.55}
 54%|    | 3541/6500 [6:41:56<6:08:43,  7.48s/it]                                                        54%|    | 3541/6500 [6:41:56<6:08:43,  7.48s/it] 54%|    | 3542/6500 [6:42:03<5:55:18,  7.21s/it]                                                        54%|    | 3542/6500 [6:42:03<5:55:18,  7.21s/it] 55%|    | 3543/6500 [6:42:09<5:45:35,  7.01s/it]                                                        55%|    | 3543/6500 [6:42:09<5:45:35,  7.01s/it] 55%|    | 3544/6500 [6:42:16<5:38:41,  6.87s/it]                                                        55%|    | 3544/6500 [6:42:16<5:38:41,  6.87s/it] 55%|    | 3545/6500 [6:42:22<5:33:28,  6.77s/it]                                                        55%|    | 3545/6500 [6:42:22<5:33:28,  6.77s/it] 55%|    | 3546/6500 [6:42:29<5:29:49,  {'loss': 0.4046, 'learning_rate': 4.289295909939016e-05, 'epoch': 0.55}
{'loss': 0.6844, 'learning_rate': 4.286903184287185e-05, 'epoch': 0.55}
{'loss': 0.4163, 'learning_rate': 4.2845106253174e-05, 'epoch': 0.55}
{'loss': 0.4056, 'learning_rate': 4.282118233588905e-05, 'epoch': 0.55}
{'loss': 0.3961, 'learning_rate': 4.279726009660909e-05, 'epoch': 0.55}
6.70s/it]                                                        55%|    | 3546/6500 [6:42:29<5:29:49,  6.70s/it] 55%|    | 3547/6500 [6:42:35<5:27:18,  6.65s/it]                                                        55%|    | 3547/6500 [6:42:35<5:27:18,  6.65s/it] 55%|    | 3548/6500 [6:42:42<5:25:37,  6.62s/it]                                                        55%|    | 3548/6500 [6:42:42<5:25:37,  6.62s/it] 55%|    | 3549/6500 [6:42:49<5:26:23,  6.64s/it]                                                        55%|    | 3549/6500 [6:42:49<5:26:23,  6.64s/it] 55%|    | 3550/6500 [6:42:55<5:25:19,  6.62s/it]                                                        55%|    | 3550/6500 [6:42:55<5:25:19,  6.62s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8638373613357544, 'eval_runtime': 1.5618, 'eval_samples_per_second': 7.684, 'eval_steps_per_second': 1.921, 'epoch': 0.55}
                                                        55%|    | 3550/6500 [6:42:57<5:25:19,  6.62s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-3550
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3550I AM HERE AND I AM SAVING THE MODEL

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3550
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3550
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3550
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3550/pytorch_model.binthe pytorch model path is
 /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3550/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3550/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3550/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3889, 'learning_rate': 4.277333954092579e-05, 'epoch': 0.55}
{'loss': 0.4047, 'learning_rate': 4.274942067443044e-05, 'epoch': 0.55}
{'loss': 0.4004, 'learning_rate': 4.272550350271391e-05, 'epoch': 0.55}
{'loss': 0.4183, 'learning_rate': 4.2701588031366706e-05, 'epoch': 0.55}
{'loss': 0.4062, 'learning_rate': 4.267767426597893e-05, 'epoch': 0.55}
 55%|    | 3551/6500 [6:43:04<5:53:34,  7.19s/it]                                                        55%|    | 3551/6500 [6:43:04<5:53:34,  7.19s/it] 55%|    | 3552/6500 [6:43:10<5:43:53,  7.00s/it]                                                        55%|    | 3552/6500 [6:43:10<5:43:53,  7.00s/it] 55%|    | 3553/6500 [6:43:17<5:36:43,  6.86s/it]                                                        55%|    | 3553/6500 [6:43:17<5:36:43,  6.86s/it] 55%|    | 3554/6500 [6:43:23<5:31:47,  6.76s/it]                                                        55%|    | 3554/6500 [6:43:23<5:31:47,  6.76s/it] 55%|    | 3555/6500 [6:43:30<5:28:22,  6.69s/it]                                                        55%|    | 3555/6500 [6:43:30<5:28:22,  6.69s/it] 55%|    | 3556/6500 [6:43:37<5:38:31,  {'loss': 0.4003, 'learning_rate': 4.2653762212140266e-05, 'epoch': 0.55}
{'loss': 0.4041, 'learning_rate': 4.262985187544003e-05, 'epoch': 0.55}
{'loss': 0.3928, 'learning_rate': 4.2605943261467106e-05, 'epoch': 0.55}
{'loss': 0.4011, 'learning_rate': 4.2582036375809984e-05, 'epoch': 0.55}
{'loss': 0.429, 'learning_rate': 4.2558131224056755e-05, 'epoch': 0.55}
6.90s/it]                                                        55%|    | 3556/6500 [6:43:37<5:38:31,  6.90s/it] 55%|    | 3557/6500 [6:43:44<5:33:18,  6.80s/it]                                                        55%|    | 3557/6500 [6:43:44<5:33:18,  6.80s/it] 55%|    | 3558/6500 [6:43:50<5:29:19,  6.72s/it]                                                        55%|    | 3558/6500 [6:43:50<5:29:19,  6.72s/it] 55%|    | 3559/6500 [6:43:57<5:26:34,  6.66s/it]                                                        55%|    | 3559/6500 [6:43:57<5:26:34,  6.66s/it] 55%|    | 3560/6500 [6:44:03<5:24:17,  6.62s/it]                                                        55%|    | 3560/6500 [6:44:03<5:24:17,  6.62s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.86138516664505, 'eval_runtime': 1.4747, 'eval_samples_per_second': 8.137, 'eval_steps_per_second': 2.034, 'epoch': 0.55}
                                                        55%|    | 3560/6500 [6:44:05<5:24:17,  6.62s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-3560
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3560/projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3560

I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3560
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3560
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3560/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3560/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3560/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3560/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.398, 'learning_rate': 4.253422781179511e-05, 'epoch': 0.55}
{'loss': 0.6858, 'learning_rate': 4.251032614461232e-05, 'epoch': 0.55}
{'loss': 0.4005, 'learning_rate': 4.248642622809526e-05, 'epoch': 0.55}
{'loss': 0.415, 'learning_rate': 4.246252806783038e-05, 'epoch': 0.55}
{'loss': 0.4009, 'learning_rate': 4.243863166940374e-05, 'epoch': 0.55}
 55%|    | 3561/6500 [6:44:12<5:49:57,  7.14s/it]                                                        55%|    | 3561/6500 [6:44:12<5:49:57,  7.14s/it] 55%|    | 3562/6500 [6:44:18<5:40:47,  6.96s/it]                                                        55%|    | 3562/6500 [6:44:18<5:40:47,  6.96s/it] 55%|    | 3563/6500 [6:44:25<5:34:16,  6.83s/it]                                                        55%|    | 3563/6500 [6:44:25<5:34:16,  6.83s/it] 55%|    | 3564/6500 [6:44:31<5:29:50,  6.74s/it]                                                        55%|    | 3564/6500 [6:44:31<5:29:50,  6.74s/it] 55%|    | 3565/6500 [6:44:38<5:26:37,  6.68s/it]                                                        55%|    | 3565/6500 [6:44:38<5:26:37,  6.68s/it] 55%|    | 3566/6500 [6:44:44<5:24:25,  {'loss': 0.3962, 'learning_rate': 4.2414737038400964e-05, 'epoch': 0.55}
{'loss': 0.3999, 'learning_rate': 4.2390844180407285e-05, 'epoch': 0.55}
{'loss': 0.4096, 'learning_rate': 4.236695310100752e-05, 'epoch': 0.55}
{'loss': 0.4158, 'learning_rate': 4.234306380578607e-05, 'epoch': 0.55}
{'loss': 0.4057, 'learning_rate': 4.231917630032689e-05, 'epoch': 0.55}
6.63s/it]                                                        55%|    | 3566/6500 [6:44:44<5:24:25,  6.63s/it] 55%|    | 3567/6500 [6:44:51<5:22:50,  6.60s/it]                                                        55%|    | 3567/6500 [6:44:51<5:22:50,  6.60s/it] 55%|    | 3568/6500 [6:44:57<5:21:32,  6.58s/it]                                                        55%|    | 3568/6500 [6:44:57<5:21:32,  6.58s/it] 55%|    | 3569/6500 [6:45:04<5:20:52,  6.57s/it]                                                        55%|    | 3569/6500 [6:45:04<5:20:52,  6.57s/it] 55%|    | 3570/6500 [6:45:10<5:20:09,  6.56s/it]                                                        55%|    | 3570/6500 [6:45:10<5:20:09,  6.56s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8638213872909546, 'eval_runtime': 1.4769, 'eval_samples_per_second': 8.125, 'eval_steps_per_second': 2.031, 'epoch': 0.55}
                                                        55%|    | 3570/6500 [6:45:12<5:20:09,  6.56s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-3570
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3570I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3570

I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3570
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3570
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3570/pytorch_model.bin
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3570/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3570/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3570/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.4015, 'learning_rate': 4.229529059021354e-05, 'epoch': 0.55}
{'loss': 0.4147, 'learning_rate': 4.227140668102918e-05, 'epoch': 0.55}
{'loss': 0.3854, 'learning_rate': 4.224752457835652e-05, 'epoch': 0.55}
{'loss': 0.4057, 'learning_rate': 4.222364428777786e-05, 'epoch': 0.55}
{'loss': 0.4202, 'learning_rate': 4.219976581487505e-05, 'epoch': 0.55}
 55%|    | 3571/6500 [6:45:19<5:47:10,  7.11s/it]                                                        55%|    | 3571/6500 [6:45:19<5:47:10,  7.11s/it] 55%|    | 3572/6500 [6:45:26<5:51:32,  7.20s/it]                                                        55%|    | 3572/6500 [6:45:26<5:51:32,  7.20s/it] 55%|    | 3573/6500 [6:45:33<5:41:43,  7.00s/it]                                                        55%|    | 3573/6500 [6:45:33<5:41:43,  7.00s/it] 55%|    | 3574/6500 [6:45:39<5:34:41,  6.86s/it]                                                        55%|    | 3574/6500 [6:45:39<5:34:41,  6.86s/it] 55%|    | 3575/6500 [6:45:46<5:29:37,  6.76s/it]                                                        55%|    | 3575/6500 [6:45:46<5:29:37,  6.76s/it] 55%|    | 3576/6500 [6:45:52<5:26:15,  {'loss': 0.41, 'learning_rate': 4.217588916522956e-05, 'epoch': 0.55}
{'loss': 0.6824, 'learning_rate': 4.215201434442241e-05, 'epoch': 0.55}
{'loss': 0.4016, 'learning_rate': 4.2128141358034186e-05, 'epoch': 0.55}
{'loss': 0.4208, 'learning_rate': 4.210427021164506e-05, 'epoch': 0.55}
{'loss': 0.3807, 'learning_rate': 4.2080400910834773e-05, 'epoch': 0.55}
6.69s/it]                                                        55%|    | 3576/6500 [6:45:52<5:26:15,  6.69s/it] 55%|    | 3577/6500 [6:45:59<5:23:43,  6.65s/it]                                                        55%|    | 3577/6500 [6:45:59<5:23:43,  6.65s/it] 55%|    | 3578/6500 [6:46:05<5:22:03,  6.61s/it]                                                        55%|    | 3578/6500 [6:46:05<5:22:03,  6.61s/it] 55%|    | 3579/6500 [6:46:12<5:20:48,  6.59s/it]                                                        55%|    | 3579/6500 [6:46:12<5:20:48,  6.59s/it] 55%|    | 3580/6500 [6:46:19<5:19:49,  6.57s/it]                                                        55%|    | 3580/6500 [6:46:19<5:19:49,  6.57s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8642008304595947, 'eval_runtime': 1.4785, 'eval_samples_per_second': 8.117, 'eval_steps_per_second': 2.029, 'epoch': 0.55}
                                                        55%|    | 3580/6500 [6:46:20<5:19:49,  6.57s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-3580
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3580I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3580

I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3580
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3580
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3580/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3580/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3580/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3580/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3933, 'learning_rate': 4.205653346118261e-05, 'epoch': 0.55}
{'loss': 0.4034, 'learning_rate': 4.203266786826745e-05, 'epoch': 0.55}
{'loss': 0.3934, 'learning_rate': 4.2008804137667744e-05, 'epoch': 0.55}
{'loss': 0.4075, 'learning_rate': 4.198494227496148e-05, 'epoch': 0.55}
{'loss': 0.4162, 'learning_rate': 4.1961082285726234e-05, 'epoch': 0.55}
 55%|    | 3581/6500 [6:46:27<5:47:19,  7.14s/it]                                                        55%|    | 3581/6500 [6:46:27<5:47:19,  7.14s/it] 55%|    | 3582/6500 [6:46:34<5:38:15,  6.96s/it]                                                        55%|    | 3582/6500 [6:46:34<5:38:15,  6.96s/it] 55%|    | 3583/6500 [6:46:40<5:32:01,  6.83s/it]                                                        55%|    | 3583/6500 [6:46:40<5:32:01,  6.83s/it] 55%|    | 3584/6500 [6:46:47<5:27:44,  6.74s/it]                                                        55%|    | 3584/6500 [6:46:47<5:27:44,  6.74s/it] 55%|    | 3585/6500 [6:46:53<5:24:25,  6.68s/it]                                                        55%|    | 3585/6500 [6:46:53<5:24:25,  6.68s/it] 55%|    | 3586/6500 [6:47:00<5:22:15,  {'loss': 0.3883, 'learning_rate': 4.1937224175539116e-05, 'epoch': 0.55}
{'loss': 0.4142, 'learning_rate': 4.1913367949976826e-05, 'epoch': 0.55}
{'loss': 0.4027, 'learning_rate': 4.188951361461561e-05, 'epoch': 0.55}
{'loss': 0.437, 'learning_rate': 4.1865661175031276e-05, 'epoch': 0.55}
{'loss': 0.3902, 'learning_rate': 4.184181063679918e-05, 'epoch': 0.55}
6.64s/it]                                                        55%|    | 3586/6500 [6:47:00<5:22:15,  6.64s/it] 55%|    | 3587/6500 [6:47:06<5:20:38,  6.60s/it]                                                        55%|    | 3587/6500 [6:47:06<5:20:38,  6.60s/it] 55%|    | 3588/6500 [6:47:14<5:32:03,  6.84s/it]                                                        55%|    | 3588/6500 [6:47:14<5:32:03,  6.84s/it] 55%|    | 3589/6500 [6:47:20<5:27:42,  6.75s/it]                                                        55%|    | 3589/6500 [6:47:20<5:27:42,  6.75s/it] 55%|    | 3590/6500 [6:47:27<5:24:17,  6.69s/it]                                                        55%|    | 3590/6500 [6:47:27<5:24:17,  6.69s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8603971600532532, 'eval_runtime': 1.478, 'eval_samples_per_second': 8.119, 'eval_steps_per_second': 2.03, 'epoch': 0.55}
                                                        55%|    | 3590/6500 [6:47:28<5:24:17,  6.69s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-3590
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3590
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3590
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3590
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3590
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3590/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3590/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3590/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3590/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.6824, 'learning_rate': 4.181796200549426e-05, 'epoch': 0.55}
{'loss': 0.4147, 'learning_rate': 4.179411528669094e-05, 'epoch': 0.55}
{'loss': 0.3945, 'learning_rate': 4.17702704859633e-05, 'epoch': 0.55}
{'loss': 0.4109, 'learning_rate': 4.174642760888491e-05, 'epoch': 0.55}
{'loss': 0.3885, 'learning_rate': 4.172258666102887e-05, 'epoch': 0.55}
 55%|    | 3591/6500 [6:47:35<5:48:01,  7.18s/it]                                                        55%|    | 3591/6500 [6:47:35<5:48:01,  7.18s/it] 55%|    | 3592/6500 [6:47:42<5:38:24,  6.98s/it]                                                        55%|    | 3592/6500 [6:47:42<5:38:24,  6.98s/it] 55%|    | 3593/6500 [6:47:48<5:31:36,  6.84s/it]                                                        55%|    | 3593/6500 [6:47:48<5:31:36,  6.84s/it] 55%|    | 3594/6500 [6:47:55<5:26:55,  6.75s/it]                                                        55%|    | 3594/6500 [6:47:55<5:26:55,  6.75s/it] 55%|    | 3595/6500 [6:48:01<5:23:47,  6.69s/it]                                                        55%|    | 3595/6500 [6:48:01<5:23:47,  6.69s/it] 55%|    | 3596/6500 [6:48:08<5:21:31,  {'loss': 0.4032, 'learning_rate': 4.169874764796787e-05, 'epoch': 0.55}
{'loss': 0.4059, 'learning_rate': 4.167491057527413e-05, 'epoch': 0.55}
{'loss': 0.4087, 'learning_rate': 4.165107544851944e-05, 'epoch': 0.55}
{'loss': 0.4051, 'learning_rate': 4.162724227327509e-05, 'epoch': 0.55}
{'loss': 0.4128, 'learning_rate': 4.160341105511196e-05, 'epoch': 0.55}
6.64s/it]                                                        55%|    | 3596/6500 [6:48:08<5:21:31,  6.64s/it] 55%|    | 3597/6500 [6:48:14<5:19:49,  6.61s/it]                                                        55%|    | 3597/6500 [6:48:14<5:19:49,  6.61s/it] 55%|    | 3598/6500 [6:48:21<5:18:32,  6.59s/it]                                                        55%|    | 3598/6500 [6:48:21<5:18:32,  6.59s/it] 55%|    | 3599/6500 [6:48:27<5:17:34,  6.57s/it]                                                        55%|    | 3599/6500 [6:48:27<5:17:34,  6.57s/it] 55%|    | 3600/6500 [6:48:34<5:16:58,  6.56s/it]                                                        55%|    | 3600/6500 [6:48:34<5:16:58,  6.56s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8657241463661194, 'eval_runtime': 1.4766, 'eval_samples_per_second': 8.127, 'eval_steps_per_second': 2.032, 'epoch': 0.55}
                                                        55%|    | 3600/6500 [6:48:35<5:16:58,  6.56s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-3600
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3600I AM HERE AND I AM SAVING THE MODEL

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3600
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3600
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3600
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3600/pytorch_model.bin
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3600/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3600/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3600/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.396, 'learning_rate': 4.157958179960044e-05, 'epoch': 0.55}
{'loss': 0.4105, 'learning_rate': 4.155575451231048e-05, 'epoch': 0.55}
{'loss': 0.3807, 'learning_rate': 4.1531929198811556e-05, 'epoch': 0.55}
{'loss': 0.4364, 'learning_rate': 4.15081058646727e-05, 'epoch': 0.55}
{'loss': 0.3957, 'learning_rate': 4.148428451546247e-05, 'epoch': 0.55}
 55%|    | 3601/6500 [6:48:42<5:43:51,  7.12s/it]                                                        55%|    | 3601/6500 [6:48:42<5:43:51,  7.12s/it] 55%|    | 3602/6500 [6:48:49<5:35:14,  6.94s/it]                                                        55%|    | 3602/6500 [6:48:49<5:35:14,  6.94s/it] 55%|    | 3603/6500 [6:48:55<5:29:08,  6.82s/it]                                                        55%|    | 3603/6500 [6:48:55<5:29:08,  6.82s/it] 55%|    | 3604/6500 [6:49:03<5:38:02,  7.00s/it]                                                        55%|    | 3604/6500 [6:49:03<5:38:02,  7.00s/it] 55%|    | 3605/6500 [6:49:09<5:31:12,  6.86s/it]                                                        55%|    | 3605/6500 [6:49:09<5:31:12,  6.86s/it] 55%|    | 3606/6500 [6:49:16<5:26:04,  {'loss': 0.682, 'learning_rate': 4.1460465156748954e-05, 'epoch': 0.55}
{'loss': 0.4164, 'learning_rate': 4.143664779409978e-05, 'epoch': 0.55}
{'loss': 0.3889, 'learning_rate': 4.1412832433082124e-05, 'epoch': 0.56}
{'loss': 0.406, 'learning_rate': 4.138901907926267e-05, 'epoch': 0.56}
{'loss': 0.3866, 'learning_rate': 4.136520773820765e-05, 'epoch': 0.56}
6.76s/it]                                                        55%|    | 3606/6500 [6:49:16<5:26:04,  6.76s/it] 55%|    | 3607/6500 [6:49:22<5:22:39,  6.69s/it]                                                        55%|    | 3607/6500 [6:49:22<5:22:39,  6.69s/it] 56%|    | 3608/6500 [6:49:29<5:20:11,  6.64s/it]                                                        56%|    | 3608/6500 [6:49:29<5:20:11,  6.64s/it] 56%|    | 3609/6500 [6:49:35<5:18:22,  6.61s/it]                                                        56%|    | 3609/6500 [6:49:35<5:18:22,  6.61s/it] 56%|    | 3610/6500 [6:49:42<5:17:23,  6.59s/it]                                                        56%|    | 3610/6500 [6:49:42<5:17:23,  6.59s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8678885102272034, 'eval_runtime': 1.4781, 'eval_samples_per_second': 8.118, 'eval_steps_per_second': 2.03, 'epoch': 0.56}
                                                        56%|    | 3610/6500 [6:49:43<5:17:23,  6.59s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-3610
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3610
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3610
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3610
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3610
the pytorch model path isthe pytorch model path isthe pytorch model path is  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3610/pytorch_model.bin/projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3610/pytorch_model.bin

 /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3610/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3610/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3924, 'learning_rate': 4.134139841548283e-05, 'epoch': 0.56}
{'loss': 0.4014, 'learning_rate': 4.131759111665349e-05, 'epoch': 0.56}
{'loss': 0.3996, 'learning_rate': 4.129378584728442e-05, 'epoch': 0.56}
{'loss': 0.4053, 'learning_rate': 4.1269982612939983e-05, 'epoch': 0.56}
{'loss': 0.4063, 'learning_rate': 4.124618141918403e-05, 'epoch': 0.56}
 56%|    | 3611/6500 [6:49:50<5:44:25,  7.15s/it]                                                        56%|    | 3611/6500 [6:49:50<5:44:25,  7.15s/it] 56%|    | 3612/6500 [6:49:57<5:35:22,  6.97s/it]                                                        56%|    | 3612/6500 [6:49:57<5:35:22,  6.97s/it] 56%|    | 3613/6500 [6:50:03<5:28:54,  6.84s/it]                                                        56%|    | 3613/6500 [6:50:03<5:28:54,  6.84s/it] 56%|    | 3614/6500 [6:50:10<5:24:24,  6.74s/it]                                                        56%|    | 3614/6500 [6:50:10<5:24:24,  6.74s/it] 56%|    | 3615/6500 [6:50:16<5:21:12,  6.68s/it]                                                        56%|    | 3615/6500 [6:50:16<5:21:12,  6.68s/it] 56%|    | 3616/6500 [6:50:23<5:19:07,  {'loss': 0.3928, 'learning_rate': 4.122238227157994e-05, 'epoch': 0.56}
{'loss': 0.393, 'learning_rate': 4.119858517569064e-05, 'epoch': 0.56}
{'loss': 0.3889, 'learning_rate': 4.117479013707854e-05, 'epoch': 0.56}
{'loss': 0.4306, 'learning_rate': 4.115099716130557e-05, 'epoch': 0.56}
{'loss': 0.3995, 'learning_rate': 4.112720625393322e-05, 'epoch': 0.56}
6.64s/it]                                                        56%|    | 3616/6500 [6:50:23<5:19:07,  6.64s/it] 56%|    | 3617/6500 [6:50:30<5:17:29,  6.61s/it]                                                        56%|    | 3617/6500 [6:50:30<5:17:29,  6.61s/it] 56%|    | 3618/6500 [6:50:36<5:16:17,  6.58s/it]                                                        56%|    | 3618/6500 [6:50:36<5:16:17,  6.58s/it] 56%|    | 3619/6500 [6:50:43<5:15:23,  6.57s/it]                                                        56%|    | 3619/6500 [6:50:43<5:15:23,  6.57s/it] 56%|    | 3620/6500 [6:50:50<5:23:44,  6.74s/it]                                                        56%|    | 3620/6500 [6:50:50<5:23:44,  6.74s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8613690137863159, 'eval_runtime': 2.8206, 'eval_samples_per_second': 4.254, 'eval_steps_per_second': 1.064, 'epoch': 0.56}
                                                        56%|    | 3620/6500 [6:50:53<5:23:44,  6.74s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-3620
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3620I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3620

I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3620
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3620
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3620/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3620/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3620/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3620/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.6802, 'learning_rate': 4.110341742052245e-05, 'epoch': 0.56}
{'loss': 0.4165, 'learning_rate': 4.1079630666633796e-05, 'epoch': 0.56}
{'loss': 0.3938, 'learning_rate': 4.105584599782721e-05, 'epoch': 0.56}
{'loss': 0.3892, 'learning_rate': 4.1032063419662245e-05, 'epoch': 0.56}
{'loss': 0.3885, 'learning_rate': 4.100828293769794e-05, 'epoch': 0.56}
 56%|    | 3621/6500 [6:51:00<6:09:49,  7.71s/it]                                                        56%|    | 3621/6500 [6:51:00<6:09:49,  7.71s/it] 56%|    | 3622/6500 [6:51:06<5:52:41,  7.35s/it]                                                        56%|    | 3622/6500 [6:51:06<5:52:41,  7.35s/it] 56%|    | 3623/6500 [6:51:13<5:40:46,  7.11s/it]                                                        56%|    | 3623/6500 [6:51:13<5:40:46,  7.11s/it] 56%|    | 3624/6500 [6:51:20<5:35:08,  6.99s/it]                                                        56%|    | 3624/6500 [6:51:20<5:35:08,  6.99s/it] 56%|    | 3625/6500 [6:51:26<5:28:33,  6.86s/it]                                                        56%|    | 3625/6500 [6:51:26<5:28:33,  6.86s/it] 56%|    | 3626/6500 [6:51:33<5:23:49,  {'loss': 0.395, 'learning_rate': 4.098450455749281e-05, 'epoch': 0.56}
{'loss': 0.393, 'learning_rate': 4.096072828460494e-05, 'epoch': 0.56}
{'loss': 0.4096, 'learning_rate': 4.093695412459188e-05, 'epoch': 0.56}
{'loss': 0.4064, 'learning_rate': 4.0913182083010676e-05, 'epoch': 0.56}
{'loss': 0.3965, 'learning_rate': 4.088941216541791e-05, 'epoch': 0.56}
6.76s/it]                                                        56%|    | 3626/6500 [6:51:33<5:23:49,  6.76s/it] 56%|    | 3627/6500 [6:51:39<5:20:32,  6.69s/it]                                                        56%|    | 3627/6500 [6:51:39<5:20:32,  6.69s/it] 56%|    | 3628/6500 [6:51:46<5:18:12,  6.65s/it]                                                        56%|    | 3628/6500 [6:51:46<5:18:12,  6.65s/it] 56%|    | 3629/6500 [6:51:52<5:16:31,  6.61s/it]                                                        56%|    | 3629/6500 [6:51:52<5:16:31,  6.61s/it] 56%|    | 3630/6500 [6:51:59<5:15:20,  6.59s/it]                                                        56%|    | 3630/6500 [6:51:59<5:15:20,  6.59s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8657942414283752, 'eval_runtime': 1.4736, 'eval_samples_per_second': 8.143, 'eval_steps_per_second': 2.036, 'epoch': 0.56}
                                                        56%|    | 3630/6500 [6:52:00<5:15:20,  6.59s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-3630
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3630
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3630
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3630
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3630
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3630/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3630/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3630/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3630/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.4005, 'learning_rate': 4.086564437736966e-05, 'epoch': 0.56}
{'loss': 0.3944, 'learning_rate': 4.08418787244215e-05, 'epoch': 0.56}
{'loss': 0.3956, 'learning_rate': 4.081811521212851e-05, 'epoch': 0.56}
{'loss': 0.4245, 'learning_rate': 4.079435384604526e-05, 'epoch': 0.56}
{'loss': 0.3984, 'learning_rate': 4.077059463172582e-05, 'epoch': 0.56}
 56%|    | 3631/6500 [6:52:07<5:40:58,  7.13s/it]                                                        56%|    | 3631/6500 [6:52:07<5:40:58,  7.13s/it] 56%|    | 3632/6500 [6:52:14<5:32:26,  6.95s/it]                                                        56%|    | 3632/6500 [6:52:14<5:32:26,  6.95s/it] 56%|    | 3633/6500 [6:52:20<5:26:28,  6.83s/it]                                                        56%|    | 3633/6500 [6:52:20<5:26:28,  6.83s/it] 56%|    | 3634/6500 [6:52:27<5:21:58,  6.74s/it]                                                        56%|    | 3634/6500 [6:52:27<5:21:58,  6.74s/it] 56%|    | 3635/6500 [6:52:33<5:18:53,  6.68s/it]                                                        56%|    | 3635/6500 [6:52:33<5:18:53,  6.68s/it] 56%|    | 3636/6500 [6:52:40<5:16:40,  {'loss': 0.6799, 'learning_rate': 4.0746837574723776e-05, 'epoch': 0.56}
{'loss': 0.4052, 'learning_rate': 4.072308268059219e-05, 'epoch': 0.56}
{'loss': 0.4001, 'learning_rate': 4.069932995488361e-05, 'epoch': 0.56}
{'loss': 0.3922, 'learning_rate': 4.0675579403150125e-05, 'epoch': 0.56}
{'loss': 0.3923, 'learning_rate': 4.0651831030943246e-05, 'epoch': 0.56}
6.63s/it]                                                        56%|    | 3636/6500 [6:52:40<5:16:40,  6.63s/it] 56%|    | 3637/6500 [6:52:47<5:29:46,  6.91s/it]                                                        56%|    | 3637/6500 [6:52:47<5:29:46,  6.91s/it] 56%|    | 3638/6500 [6:52:54<5:24:21,  6.80s/it]                                                        56%|    | 3638/6500 [6:52:54<5:24:21,  6.80s/it] 56%|    | 3639/6500 [6:53:00<5:20:44,  6.73s/it]                                                        56%|    | 3639/6500 [6:53:00<5:20:44,  6.73s/it] 56%|    | 3640/6500 [6:53:07<5:18:04,  6.67s/it]                                                        56%|    | 3640/6500 [6:53:07<5:18:04,  6.67s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.868613600730896, 'eval_runtime': 1.4748, 'eval_samples_per_second': 8.137, 'eval_steps_per_second': 2.034, 'epoch': 0.56}
                                                        56%|    | 3640/6500 [6:53:09<5:18:04,  6.67s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-3640
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3640I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3640

I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3640
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3640
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3640/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3640/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3640/pytorch_model.binthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3640/pytorch_model.bin

/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.397, 'learning_rate': 4.062808484381403e-05, 'epoch': 0.56}
{'loss': 0.4102, 'learning_rate': 4.0604340847313e-05, 'epoch': 0.56}
{'loss': 0.413, 'learning_rate': 4.058059904699017e-05, 'epoch': 0.56}
{'loss': 0.3986, 'learning_rate': 4.0556859448395037e-05, 'epoch': 0.56}
{'loss': 0.3958, 'learning_rate': 4.0533122057076604e-05, 'epoch': 0.56}
 56%|    | 3641/6500 [6:53:15<5:42:14,  7.18s/it]                                                        56%|    | 3641/6500 [6:53:15<5:42:14,  7.18s/it] 56%|    | 3642/6500 [6:53:22<5:32:45,  6.99s/it]                                                        56%|    | 3642/6500 [6:53:22<5:32:45,  6.99s/it] 56%|    | 3643/6500 [6:53:28<5:26:14,  6.85s/it]                                                        56%|    | 3643/6500 [6:53:28<5:26:14,  6.85s/it] 56%|    | 3644/6500 [6:53:35<5:21:42,  6.76s/it]                                                        56%|    | 3644/6500 [6:53:35<5:21:42,  6.76s/it] 56%|    | 3645/6500 [6:53:42<5:18:27,  6.69s/it]                                                        56%|    | 3645/6500 [6:53:42<5:18:27,  6.69s/it] 56%|    | 3646/6500 [6:53:48<5:16:04,  {'loss': 0.4102, 'learning_rate': 4.050938687858333e-05, 'epoch': 0.56}
{'loss': 0.3877, 'learning_rate': 4.048565391846316e-05, 'epoch': 0.56}
{'loss': 0.4013, 'learning_rate': 4.046192318226354e-05, 'epoch': 0.56}
{'loss': 0.4206, 'learning_rate': 4.043819467553138e-05, 'epoch': 0.56}
{'loss': 0.399, 'learning_rate': 4.0414468403813095e-05, 'epoch': 0.56}
6.64s/it]                                                        56%|    | 3646/6500 [6:53:48<5:16:04,  6.64s/it] 56%|    | 3647/6500 [6:53:55<5:14:33,  6.62s/it]                                                        56%|    | 3647/6500 [6:53:55<5:14:33,  6.62s/it] 56%|    | 3648/6500 [6:54:01<5:13:23,  6.59s/it]                                                        56%|    | 3648/6500 [6:54:01<5:13:23,  6.59s/it] 56%|    | 3649/6500 [6:54:08<5:12:29,  6.58s/it]                                                        56%|    | 3649/6500 [6:54:08<5:12:29,  6.58s/it] 56%|    | 3650/6500 [6:54:14<5:11:49,  6.56s/it]                                                        56%|    | 3650/6500 [6:54:14<5:11:49,  6.56s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8592389225959778, 'eval_runtime': 1.4691, 'eval_samples_per_second': 8.168, 'eval_steps_per_second': 2.042, 'epoch': 0.56}
                                                        56%|    | 3650/6500 [6:54:16<5:11:49,  6.56s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-3650
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3650
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3650
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3650
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3650
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3650/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3650/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3650/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3650/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.6799, 'learning_rate': 4.039074437265452e-05, 'epoch': 0.56}
{'loss': 0.3905, 'learning_rate': 4.0367022587601035e-05, 'epoch': 0.56}
{'loss': 0.4112, 'learning_rate': 4.034330305419744e-05, 'epoch': 0.56}
{'loss': 0.3813, 'learning_rate': 4.031958577798805e-05, 'epoch': 0.56}
{'loss': 0.3874, 'learning_rate': 4.029587076451662e-05, 'epoch': 0.56}
 56%|    | 3651/6500 [6:54:23<5:38:10,  7.12s/it]                                                        56%|    | 3651/6500 [6:54:23<5:38:10,  7.12s/it] 56%|    | 3652/6500 [6:54:29<5:29:36,  6.94s/it]                                                        56%|    | 3652/6500 [6:54:29<5:29:36,  6.94s/it] 56%|    | 3653/6500 [6:54:37<5:36:17,  7.09s/it]                                                        56%|    | 3653/6500 [6:54:37<5:36:17,  7.09s/it] 56%|    | 3654/6500 [6:54:43<5:28:25,  6.92s/it]                                                        56%|    | 3654/6500 [6:54:43<5:28:25,  6.92s/it] 56%|    | 3655/6500 [6:54:50<5:22:55,  6.81s/it]                                                        56%|    | 3655/6500 [6:54:50<5:22:55,  6.81s/it] 56%|    | 3656/6500 [6:54:56<5:18:58,  {'loss': 0.3934, 'learning_rate': 4.0272158019326414e-05, 'epoch': 0.56}
{'loss': 0.3947, 'learning_rate': 4.024844754796011e-05, 'epoch': 0.56}
{'loss': 0.4096, 'learning_rate': 4.0224739355959905e-05, 'epoch': 0.56}
{'loss': 0.4147, 'learning_rate': 4.020103344886744e-05, 'epoch': 0.56}
{'loss': 0.3988, 'learning_rate': 4.017732983222382e-05, 'epoch': 0.56}
6.73s/it]                                                        56%|    | 3656/6500 [6:54:56<5:18:58,  6.73s/it] 56%|    | 3657/6500 [6:55:03<5:16:18,  6.68s/it]                                                        56%|    | 3657/6500 [6:55:03<5:16:18,  6.68s/it] 56%|    | 3658/6500 [6:55:09<5:14:11,  6.63s/it]                                                        56%|    | 3658/6500 [6:55:09<5:14:11,  6.63s/it] 56%|    | 3659/6500 [6:55:16<5:12:58,  6.61s/it]                                                        56%|    | 3659/6500 [6:55:16<5:12:58,  6.61s/it] 56%|    | 3660/6500 [6:55:22<5:11:47,  6.59s/it]                                                        56%|    | 3660/6500 [6:55:22<5:11:47,  6.59s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8657344579696655, 'eval_runtime': 1.4716, 'eval_samples_per_second': 8.154, 'eval_steps_per_second': 2.039, 'epoch': 0.56}
                                                        56%|    | 3660/6500 [6:55:24<5:11:47,  6.59s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-3660
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3660
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3660
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3660
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3660
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3660/pytorch_model.bin
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3660/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3660/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3660/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.4009, 'learning_rate': 4.0153628511569646e-05, 'epoch': 0.56}
{'loss': 0.389, 'learning_rate': 4.012992949244493e-05, 'epoch': 0.56}
{'loss': 0.4275, 'learning_rate': 4.010623278038919e-05, 'epoch': 0.56}
{'loss': 0.3926, 'learning_rate': 4.008253838094137e-05, 'epoch': 0.56}
{'loss': 0.664, 'learning_rate': 4.005884629963991e-05, 'epoch': 0.56}
 56%|    | 3661/6500 [6:55:31<5:37:13,  7.13s/it]                                                        56%|    | 3661/6500 [6:55:31<5:37:13,  7.13s/it] 56%|    | 3662/6500 [6:55:37<5:28:45,  6.95s/it]                                                        56%|    | 3662/6500 [6:55:37<5:28:45,  6.95s/it] 56%|    | 3663/6500 [6:55:44<5:22:50,  6.83s/it]                                                        56%|    | 3663/6500 [6:55:44<5:22:50,  6.83s/it] 56%|    | 3664/6500 [6:55:50<5:18:21,  6.74s/it]                                                        56%|    | 3664/6500 [6:55:50<5:18:21,  6.74s/it] 56%|    | 3665/6500 [6:55:57<5:15:23,  6.68s/it]                                                        56%|    | 3665/6500 [6:55:57<5:15:23,  6.68s/it] 56%|    | 3666/6500 [6:56:03<5:13:22,  {'loss': 0.4248, 'learning_rate': 4.0035156542022684e-05, 'epoch': 0.56}
{'loss': 0.3956, 'learning_rate': 4.001146911362702e-05, 'epoch': 0.56}
{'loss': 0.4097, 'learning_rate': 3.998778401998973e-05, 'epoch': 0.56}
{'loss': 0.3818, 'learning_rate': 3.9964101266647044e-05, 'epoch': 0.56}
{'loss': 0.3947, 'learning_rate': 3.9940420859134686e-05, 'epoch': 0.56}
6.63s/it]                                                        56%|    | 3666/6500 [6:56:03<5:13:22,  6.63s/it] 56%|    | 3667/6500 [6:56:10<5:12:02,  6.61s/it]                                                        56%|    | 3667/6500 [6:56:10<5:12:02,  6.61s/it] 56%|    | 3668/6500 [6:56:17<5:10:49,  6.59s/it]                                                        56%|    | 3668/6500 [6:56:17<5:10:49,  6.59s/it] 56%|    | 3669/6500 [6:56:24<5:18:54,  6.76s/it]                                                        56%|    | 3669/6500 [6:56:24<5:18:54,  6.76s/it] 56%|    | 3670/6500 [6:56:30<5:15:43,  6.69s/it]                                                        56%|    | 3670/6500 [6:56:30<5:15:43,  6.69s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8679877519607544, 'eval_runtime': 1.7118, 'eval_samples_per_second': 7.01, 'eval_steps_per_second': 1.753, 'epoch': 0.56}
                                                        56%|    | 3670/6500 [6:56:32<5:15:43,  6.69s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-3670
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3670
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3670
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3670
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3670
the pytorch model path isthe pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3670/pytorch_model.bin 
/projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3670/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3670/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3670/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3958, 'learning_rate': 3.9916742802987774e-05, 'epoch': 0.56}
{'loss': 0.4031, 'learning_rate': 3.9893067103740925e-05, 'epoch': 0.56}
{'loss': 0.4073, 'learning_rate': 3.986939376692819e-05, 'epoch': 0.57}
{'loss': 0.4073, 'learning_rate': 3.9845722798083066e-05, 'epoch': 0.57}
{'loss': 0.3977, 'learning_rate': 3.98220542027385e-05, 'epoch': 0.57}
 56%|    | 3671/6500 [6:56:39<5:44:45,  7.31s/it]                                                        56%|    | 3671/6500 [6:56:39<5:44:45,  7.31s/it] 56%|    | 3672/6500 [6:56:46<5:33:40,  7.08s/it]                                                        56%|    | 3672/6500 [6:56:46<5:33:40,  7.08s/it] 57%|    | 3673/6500 [6:56:52<5:25:53,  6.92s/it]                                                        57%|    | 3673/6500 [6:56:52<5:25:53,  6.92s/it] 57%|    | 3674/6500 [6:56:59<5:20:31,  6.81s/it]                                                        57%|    | 3674/6500 [6:56:59<5:20:31,  6.81s/it] 57%|    | 3675/6500 [6:57:05<5:16:47,  6.73s/it]                                                        57%|    | 3675/6500 [6:57:05<5:16:47,  6.73s/it] 57%|    | 3676/6500 [6:57:12<5:13:57,  {'loss': 0.4034, 'learning_rate': 3.97983879864269e-05, 'epoch': 0.57}
{'loss': 0.3908, 'learning_rate': 3.977472415468006e-05, 'epoch': 0.57}
{'loss': 0.4328, 'learning_rate': 3.975106271302928e-05, 'epoch': 0.57}
{'loss': 0.3942, 'learning_rate': 3.972740366700528e-05, 'epoch': 0.57}
{'loss': 0.6745, 'learning_rate': 3.970374702213821e-05, 'epoch': 0.57}
6.67s/it]                                                        57%|    | 3676/6500 [6:57:12<5:13:57,  6.67s/it] 57%|    | 3677/6500 [6:57:18<5:11:57,  6.63s/it]                                                        57%|    | 3677/6500 [6:57:18<5:11:57,  6.63s/it] 57%|    | 3678/6500 [6:57:25<5:10:29,  6.60s/it]                                                        57%|    | 3678/6500 [6:57:25<5:10:29,  6.60s/it] 57%|    | 3679/6500 [6:57:31<5:09:20,  6.58s/it]                                                        57%|    | 3679/6500 [6:57:31<5:09:20,  6.58s/it] 57%|    | 3680/6500 [6:57:38<5:08:46,  6.57s/it]                                                        57%|    | 3680/6500 [6:57:38<5:08:46,  6.57s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8578575253486633, 'eval_runtime': 1.4792, 'eval_samples_per_second': 8.112, 'eval_steps_per_second': 2.028, 'epoch': 0.57}
                                                        57%|    | 3680/6500 [6:57:39<5:08:46,  6.57s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-3680
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3680I AM HERE AND I AM SAVING THE MODEL

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3680
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3680
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3680
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3680/pytorch_model.bin
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3680/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3680/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3680/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.4119, 'learning_rate': 3.968009278395768e-05, 'epoch': 0.57}
{'loss': 0.3911, 'learning_rate': 3.9656440957992716e-05, 'epoch': 0.57}
{'loss': 0.4102, 'learning_rate': 3.9632791549771776e-05, 'epoch': 0.57}
{'loss': 0.3919, 'learning_rate': 3.960914456482278e-05, 'epoch': 0.57}
{'loss': 0.3927, 'learning_rate': 3.958550000867307e-05, 'epoch': 0.57}
 57%|    | 3681/6500 [6:57:46<5:33:38,  7.10s/it]                                                        57%|    | 3681/6500 [6:57:46<5:33:38,  7.10s/it] 57%|    | 3682/6500 [6:57:53<5:25:29,  6.93s/it]                                                        57%|    | 3682/6500 [6:57:53<5:25:29,  6.93s/it] 57%|    | 3683/6500 [6:57:59<5:19:56,  6.81s/it]                                                        57%|    | 3683/6500 [6:57:59<5:19:56,  6.81s/it] 57%|    | 3684/6500 [6:58:06<5:15:58,  6.73s/it]                                                        57%|    | 3684/6500 [6:58:06<5:15:58,  6.73s/it] 57%|    | 3685/6500 [6:58:13<5:21:42,  6.86s/it]                                                        57%|    | 3685/6500 [6:58:13<5:21:42,  6.86s/it] 57%|    | 3686/6500 [6:58:20<5:17:01,  {'loss': 0.4013, 'learning_rate': 3.9561857886849405e-05, 'epoch': 0.57}
{'loss': 0.4074, 'learning_rate': 3.9538218204878015e-05, 'epoch': 0.57}
{'loss': 0.4086, 'learning_rate': 3.951458096828449e-05, 'epoch': 0.57}
{'loss': 0.4052, 'learning_rate': 3.9490946182593914e-05, 'epoch': 0.57}
{'loss': 0.3971, 'learning_rate': 3.9467313853330776e-05, 'epoch': 0.57}
6.76s/it]                                                        57%|    | 3686/6500 [6:58:20<5:17:01,  6.76s/it] 57%|    | 3687/6500 [6:58:26<5:13:52,  6.69s/it]                                                        57%|    | 3687/6500 [6:58:26<5:13:52,  6.69s/it] 57%|    | 3688/6500 [6:58:33<5:11:31,  6.65s/it]                                                        57%|    | 3688/6500 [6:58:33<5:11:31,  6.65s/it] 57%|    | 3689/6500 [6:58:39<5:10:08,  6.62s/it]                                                        57%|    | 3689/6500 [6:58:39<5:10:08,  6.62s/it] 57%|    | 3690/6500 [6:58:46<5:08:54,  6.60s/it]                                                        57%|    | 3690/6500 [6:58:46<5:08:54,  6.60s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8663150668144226, 'eval_runtime': 1.5037, 'eval_samples_per_second': 7.98, 'eval_steps_per_second': 1.995, 'epoch': 0.57}
                                                        57%|    | 3690/6500 [6:58:47<5:08:54,  6.60s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-3690
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3690
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3690
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3690
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3690
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3690/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3690/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3690/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3690/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3947, 'learning_rate': 3.944368398601898e-05, 'epoch': 0.57}
{'loss': 0.3906, 'learning_rate': 3.942005658618188e-05, 'epoch': 0.57}
{'loss': 0.4383, 'learning_rate': 3.9396431659342216e-05, 'epoch': 0.57}
{'loss': 0.4038, 'learning_rate': 3.937280921102218e-05, 'epoch': 0.57}
{'loss': 0.6683, 'learning_rate': 3.934918924674338e-05, 'epoch': 0.57}
 57%|    | 3691/6500 [6:58:54<5:35:18,  7.16s/it]                                                        57%|    | 3691/6500 [6:58:54<5:35:18,  7.16s/it] 57%|    | 3692/6500 [6:59:01<5:26:30,  6.98s/it]                                                        57%|    | 3692/6500 [6:59:01<5:26:30,  6.98s/it] 57%|    | 3693/6500 [6:59:07<5:20:24,  6.85s/it]                                                        57%|    | 3693/6500 [6:59:07<5:20:24,  6.85s/it] 57%|    | 3694/6500 [6:59:14<5:16:09,  6.76s/it]                                                        57%|    | 3694/6500 [6:59:14<5:16:09,  6.76s/it] 57%|    | 3695/6500 [6:59:20<5:13:00,  6.70s/it]                                                        57%|    | 3695/6500 [6:59:20<5:13:00,  6.70s/it] 57%|    | 3696/6500 [6:59:27<5:10:51,  {'loss': 0.4144, 'learning_rate': 3.9325571772026834e-05, 'epoch': 0.57}
{'loss': 0.4001, 'learning_rate': 3.930195679239298e-05, 'epoch': 0.57}
{'loss': 0.3871, 'learning_rate': 3.9278344313361696e-05, 'epoch': 0.57}
{'loss': 0.3849, 'learning_rate': 3.925473434045223e-05, 'epoch': 0.57}
{'loss': 0.3962, 'learning_rate': 3.923112687918328e-05, 'epoch': 0.57}
6.65s/it]                                                        57%|    | 3696/6500 [6:59:27<5:10:51,  6.65s/it] 57%|    | 3697/6500 [6:59:33<5:09:09,  6.62s/it]                                                        57%|    | 3697/6500 [6:59:33<5:09:09,  6.62s/it] 57%|    | 3698/6500 [6:59:40<5:08:03,  6.60s/it]                                                        57%|    | 3698/6500 [6:59:40<5:08:03,  6.60s/it] 57%|    | 3699/6500 [6:59:47<5:07:11,  6.58s/it]                                                        57%|    | 3699/6500 [6:59:47<5:07:11,  6.58s/it] 57%|    | 3700/6500 [6:59:53<5:06:30,  6.57s/it]                                                        57%|    | 3700/6500 [6:59:53<5:06:30,  6.57s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8666756749153137, 'eval_runtime': 1.7297, 'eval_samples_per_second': 6.938, 'eval_steps_per_second': 1.734, 'epoch': 0.57}
                                                        57%|    | 3700/6500 [6:59:55<5:06:30,  6.57s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-3700
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3700
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3700
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3700
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3700
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3700/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3700/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3700/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3700/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.397, 'learning_rate': 3.9207521935072946e-05, 'epoch': 0.57}
{'loss': 0.4072, 'learning_rate': 3.9183919513638734e-05, 'epoch': 0.57}
{'loss': 0.3971, 'learning_rate': 3.916031962039758e-05, 'epoch': 0.57}
{'loss': 0.3958, 'learning_rate': 3.913672226086581e-05, 'epoch': 0.57}
{'loss': 0.4013, 'learning_rate': 3.911312744055917e-05, 'epoch': 0.57}
 57%|    | 3701/6500 [7:00:02<5:44:17,  7.38s/it]                                                        57%|    | 3701/6500 [7:00:02<5:44:17,  7.38s/it] 57%|    | 3702/6500 [7:00:09<5:32:24,  7.13s/it]                                                        57%|    | 3702/6500 [7:00:09<5:32:24,  7.13s/it] 57%|    | 3703/6500 [7:00:15<5:24:04,  6.95s/it]                                                        57%|    | 3703/6500 [7:00:15<5:24:04,  6.95s/it] 57%|    | 3704/6500 [7:00:22<5:18:23,  6.83s/it]                                                        57%|    | 3704/6500 [7:00:22<5:18:23,  6.83s/it] 57%|    | 3705/6500 [7:00:29<5:14:16,  6.75s/it]                                                        57%|    | 3705/6500 [7:00:29<5:14:16,  6.75s/it] 57%|    | 3706/6500 [7:00:35<5:11:09,  {'loss': 0.3847, 'learning_rate': 3.908953516499278e-05, 'epoch': 0.57}
{'loss': 0.3862, 'learning_rate': 3.9065945439681214e-05, 'epoch': 0.57}
{'loss': 0.432, 'learning_rate': 3.904235827013843e-05, 'epoch': 0.57}
{'loss': 0.3984, 'learning_rate': 3.901877366187777e-05, 'epoch': 0.57}
{'loss': 0.671, 'learning_rate': 3.8995191620412e-05, 'epoch': 0.57}
6.68s/it]                                                        57%|    | 3706/6500 [7:00:35<5:11:09,  6.68s/it] 57%|    | 3707/6500 [7:00:42<5:08:55,  6.64s/it]                                                        57%|    | 3707/6500 [7:00:42<5:08:55,  6.64s/it] 57%|    | 3708/6500 [7:00:48<5:07:28,  6.61s/it]                                                        57%|    | 3708/6500 [7:00:48<5:07:28,  6.61s/it] 57%|    | 3709/6500 [7:00:55<5:06:33,  6.59s/it]                                                        57%|    | 3709/6500 [7:00:55<5:06:33,  6.59s/it] 57%|    | 3710/6500 [7:01:01<5:05:38,  6.57s/it]                                                        57%|    | 3710/6500 [7:01:01<5:05:38,  6.57s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8592671751976013, 'eval_runtime': 1.4806, 'eval_samples_per_second': 8.105, 'eval_steps_per_second': 2.026, 'epoch': 0.57}
                                                        57%|    | 3710/6500 [7:01:03<5:05:38,  6.57s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-3710
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3710
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3710
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3710
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3710
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3710/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3710/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3710/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3710/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.4074, 'learning_rate': 3.8971612151253275e-05, 'epoch': 0.57}
{'loss': 0.3983, 'learning_rate': 3.8948035259913154e-05, 'epoch': 0.57}
{'loss': 0.3906, 'learning_rate': 3.89244609519026e-05, 'epoch': 0.57}
{'loss': 0.3807, 'learning_rate': 3.8900889232731954e-05, 'epoch': 0.57}
{'loss': 0.3929, 'learning_rate': 3.887732010791098e-05, 'epoch': 0.57}
 57%|    | 3711/6500 [7:01:10<5:31:25,  7.13s/it]                                                        57%|    | 3711/6500 [7:01:10<5:31:25,  7.13s/it] 57%|    | 3712/6500 [7:01:16<5:23:08,  6.95s/it]                                                        57%|    | 3712/6500 [7:01:16<5:23:08,  6.95s/it] 57%|    | 3713/6500 [7:01:23<5:17:10,  6.83s/it]                                                        57%|    | 3713/6500 [7:01:23<5:17:10,  6.83s/it] 57%|    | 3714/6500 [7:01:29<5:13:08,  6.74s/it]                                                        57%|    | 3714/6500 [7:01:29<5:13:08,  6.74s/it] 57%|    | 3715/6500 [7:01:36<5:10:27,  6.69s/it]                                                        57%|    | 3715/6500 [7:01:36<5:10:27,  6.69s/it] 57%|    | 3716/6500 [7:01:42<5:08:18,  {'loss': 0.3918, 'learning_rate': 3.8853753582948785e-05, 'epoch': 0.57}
{'loss': 0.4065, 'learning_rate': 3.883018966335393e-05, 'epoch': 0.57}
{'loss': 0.4029, 'learning_rate': 3.880662835463432e-05, 'epoch': 0.57}
{'loss': 0.4061, 'learning_rate': 3.878306966229728e-05, 'epoch': 0.57}
{'loss': 0.4013, 'learning_rate': 3.875951359184951e-05, 'epoch': 0.57}
6.64s/it]                                                        57%|    | 3716/6500 [7:01:42<5:08:18,  6.64s/it] 57%|    | 3717/6500 [7:01:50<5:20:37,  6.91s/it]                                                        57%|    | 3717/6500 [7:01:50<5:20:37,  6.91s/it] 57%|    | 3718/6500 [7:01:56<5:15:40,  6.81s/it]                                                        57%|    | 3718/6500 [7:01:56<5:15:40,  6.81s/it] 57%|    | 3719/6500 [7:02:03<5:11:51,  6.73s/it]                                                        57%|    | 3719/6500 [7:02:03<5:11:51,  6.73s/it] 57%|    | 3720/6500 [7:02:10<5:09:19,  6.68s/it]                                                        57%|    | 3720/6500 [7:02:10<5:09:19,  6.68s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8667014837265015, 'eval_runtime': 1.4801, 'eval_samples_per_second': 8.108, 'eval_steps_per_second': 2.027, 'epoch': 0.57}
                                                        57%|    | 3720/6500 [7:02:11<5:09:19,  6.68s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-3720
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3720
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3720
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3720
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3720
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3720/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3720/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3720/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3720/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.387, 'learning_rate': 3.873596014879709e-05, 'epoch': 0.57}
{'loss': 0.3943, 'learning_rate': 3.87124093386455e-05, 'epoch': 0.57}
{'loss': 0.4248, 'learning_rate': 3.868886116689959e-05, 'epoch': 0.57}
{'loss': 0.4006, 'learning_rate': 3.866531563906362e-05, 'epoch': 0.57}
{'loss': 0.6831, 'learning_rate': 3.86417727606412e-05, 'epoch': 0.57}
 57%|    | 3721/6500 [7:02:18<5:32:52,  7.19s/it]                                                        57%|    | 3721/6500 [7:02:18<5:32:52,  7.19s/it] 57%|    | 3722/6500 [7:02:24<5:23:51,  6.99s/it]                                                        57%|    | 3722/6500 [7:02:24<5:23:51,  6.99s/it] 57%|    | 3723/6500 [7:02:31<5:17:46,  6.87s/it]                                                        57%|    | 3723/6500 [7:02:31<5:17:46,  6.87s/it] 57%|    | 3724/6500 [7:02:38<5:13:14,  6.77s/it]                                                        57%|    | 3724/6500 [7:02:38<5:13:14,  6.77s/it] 57%|    | 3725/6500 [7:02:44<5:10:00,  6.70s/it]                                                        57%|    | 3725/6500 [7:02:44<5:10:00,  6.70s/it] 57%|    | 3726/6500 [7:02:51<5:07:49,  {'loss': 0.3893, 'learning_rate': 3.861823253713535e-05, 'epoch': 0.57}
{'loss': 0.4092, 'learning_rate': 3.8594694974048426e-05, 'epoch': 0.57}
{'loss': 0.3877, 'learning_rate': 3.8571160076882204e-05, 'epoch': 0.57}
{'loss': 0.39, 'learning_rate': 3.8547627851137836e-05, 'epoch': 0.57}
{'loss': 0.393, 'learning_rate': 3.852409830231582e-05, 'epoch': 0.57}
6.66s/it]                                                        57%|    | 3726/6500 [7:02:51<5:07:49,  6.66s/it] 57%|    | 3727/6500 [7:02:57<5:06:08,  6.62s/it]                                                        57%|    | 3727/6500 [7:02:57<5:06:08,  6.62s/it] 57%|    | 3728/6500 [7:03:04<5:05:08,  6.60s/it]                                                        57%|    | 3728/6500 [7:03:04<5:05:08,  6.60s/it] 57%|    | 3729/6500 [7:03:10<5:04:10,  6.59s/it]                                                        57%|    | 3729/6500 [7:03:10<5:04:10,  6.59s/it] 57%|    | 3730/6500 [7:03:17<5:03:28,  6.57s/it]                                                        57%|    | 3730/6500 [7:03:17<5:03:28,  6.57s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8692741394042969, 'eval_runtime': 1.4825, 'eval_samples_per_second': 8.095, 'eval_steps_per_second': 2.024, 'epoch': 0.57}
                                                        57%|    | 3730/6500 [7:03:18<5:03:28,  6.57s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-3730
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3730
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3730
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3730
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3730
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3730/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3730/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3730/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3730/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3981, 'learning_rate': 3.850057143591605e-05, 'epoch': 0.57}
{'loss': 0.4102, 'learning_rate': 3.84770472574378e-05, 'epoch': 0.57}
{'loss': 0.4008, 'learning_rate': 3.845352577237969e-05, 'epoch': 0.57}
{'loss': 0.3892, 'learning_rate': 3.843000698623972e-05, 'epoch': 0.57}
{'loss': 0.4042, 'learning_rate': 3.840649090451527e-05, 'epoch': 0.57}
 57%|    | 3731/6500 [7:03:25<5:28:08,  7.11s/it]                                                        57%|    | 3731/6500 [7:03:25<5:28:08,  7.11s/it] 57%|    | 3732/6500 [7:03:32<5:20:20,  6.94s/it]                                                        57%|    | 3732/6500 [7:03:32<5:20:20,  6.94s/it] 57%|    | 3733/6500 [7:03:38<5:14:56,  6.83s/it]                                                        57%|    | 3733/6500 [7:03:38<5:14:56,  6.83s/it] 57%|    | 3734/6500 [7:03:46<5:23:28,  7.02s/it]                                                        57%|    | 3734/6500 [7:03:46<5:23:28,  7.02s/it] 57%|    | 3735/6500 [7:03:52<5:17:00,  6.88s/it]                                                        57%|    | 3735/6500 [7:03:52<5:17:00,  6.88s/it] 57%|    | 3736/6500 [7:03:59<5:12:21,  {'loss': 0.3867, 'learning_rate': 3.838297753270308e-05, 'epoch': 0.57}
{'loss': 0.4006, 'learning_rate': 3.835946687629927e-05, 'epoch': 0.57}
{'loss': 0.4096, 'learning_rate': 3.83359589407993e-05, 'epoch': 0.58}
{'loss': 0.3982, 'learning_rate': 3.8312453731698e-05, 'epoch': 0.58}
{'loss': 0.6836, 'learning_rate': 3.8288951254489583e-05, 'epoch': 0.58}
6.78s/it]                                                        57%|    | 3736/6500 [7:03:59<5:12:21,  6.78s/it] 57%|    | 3737/6500 [7:04:05<5:09:05,  6.71s/it]                                                        57%|    | 3737/6500 [7:04:05<5:09:05,  6.71s/it] 58%|    | 3738/6500 [7:04:12<5:06:55,  6.67s/it]                                                        58%|    | 3738/6500 [7:04:12<5:06:55,  6.67s/it] 58%|    | 3739/6500 [7:04:19<5:05:18,  6.63s/it]                                                        58%|    | 3739/6500 [7:04:19<5:05:18,  6.63s/it] 58%|    | 3740/6500 [7:04:25<5:04:08,  6.61s/it]                                                        58%|    | 3740/6500 [7:04:25<5:04:08,  6.61s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8607874512672424, 'eval_runtime': 1.4801, 'eval_samples_per_second': 8.108, 'eval_steps_per_second': 2.027, 'epoch': 0.58}
                                                        58%|    | 3740/6500 [7:04:27<5:04:08,  6.61s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-3740
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3740I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3740

I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3740
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3740
the pytorch model path isthe pytorch model path isthe pytorch model path is  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3740/pytorch_model.bin/projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3740/pytorch_model.bin

 /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3740/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3740/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3936, 'learning_rate': 3.82654515146676e-05, 'epoch': 0.58}
{'loss': 0.4111, 'learning_rate': 3.824195451772499e-05, 'epoch': 0.58}
{'loss': 0.3864, 'learning_rate': 3.8218460269154e-05, 'epoch': 0.58}
{'loss': 0.391, 'learning_rate': 3.81949687744463e-05, 'epoch': 0.58}
{'loss': 0.4018, 'learning_rate': 3.817148003909288e-05, 'epoch': 0.58}
 58%|    | 3741/6500 [7:04:34<5:27:58,  7.13s/it]                                                        58%|    | 3741/6500 [7:04:34<5:27:58,  7.13s/it] 58%|    | 3742/6500 [7:04:40<5:19:53,  6.96s/it]                                                        58%|    | 3742/6500 [7:04:40<5:19:53,  6.96s/it] 58%|    | 3743/6500 [7:04:47<5:14:16,  6.84s/it]                                                        58%|    | 3743/6500 [7:04:47<5:14:16,  6.84s/it] 58%|    | 3744/6500 [7:04:53<5:10:11,  6.75s/it]                                                        58%|    | 3744/6500 [7:04:53<5:10:11,  6.75s/it] 58%|    | 3745/6500 [7:05:00<5:07:17,  6.69s/it]                                                        58%|    | 3745/6500 [7:05:00<5:07:17,  6.69s/it] 58%|    | 3746/6500 [7:05:06<5:05:17,  {'loss': 0.3929, 'learning_rate': 3.8147994068584087e-05, 'epoch': 0.58}
{'loss': 0.409, 'learning_rate': 3.812451086840961e-05, 'epoch': 0.58}
{'loss': 0.4079, 'learning_rate': 3.8101030444058515e-05, 'epoch': 0.58}
{'loss': 0.3918, 'learning_rate': 3.807755280101921e-05, 'epoch': 0.58}
{'loss': 0.3968, 'learning_rate': 3.8054077944779434e-05, 'epoch': 0.58}
6.65s/it]                                                        58%|    | 3746/6500 [7:05:06<5:05:17,  6.65s/it] 58%|    | 3747/6500 [7:05:13<5:03:47,  6.62s/it]                                                        58%|    | 3747/6500 [7:05:13<5:03:47,  6.62s/it] 58%|    | 3748/6500 [7:05:19<5:02:40,  6.60s/it]                                                        58%|    | 3748/6500 [7:05:19<5:02:40,  6.60s/it] 58%|    | 3749/6500 [7:05:26<5:02:06,  6.59s/it]                                                        58%|    | 3749/6500 [7:05:26<5:02:06,  6.59s/it] 58%|    | 3750/6500 [7:05:33<5:13:38,  6.84s/it]                                                        58%|    | 3750/6500 [7:05:33<5:13:38,  6.84s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.866604208946228, 'eval_runtime': 1.4757, 'eval_samples_per_second': 8.132, 'eval_steps_per_second': 2.033, 'epoch': 0.58}
                                                        58%|    | 3750/6500 [7:05:35<5:13:38,  6.84s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-3750
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3750I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3750

I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3750
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3750
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3750/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3750/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3750/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3750/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3809, 'learning_rate': 3.803060588082633e-05, 'epoch': 0.58}
{'loss': 0.4276, 'learning_rate': 3.800713661464631e-05, 'epoch': 0.58}
{'loss': 0.3885, 'learning_rate': 3.7983670151725195e-05, 'epoch': 0.58}
{'loss': 0.6748, 'learning_rate': 3.7960206497548115e-05, 'epoch': 0.58}
{'loss': 0.4084, 'learning_rate': 3.793674565759957e-05, 'epoch': 0.58}
 58%|    | 3751/6500 [7:05:42<5:34:20,  7.30s/it]                                                        58%|    | 3751/6500 [7:05:42<5:34:20,  7.30s/it] 58%|    | 3752/6500 [7:05:48<5:23:41,  7.07s/it]                                                        58%|    | 3752/6500 [7:05:48<5:23:41,  7.07s/it] 58%|    | 3753/6500 [7:05:55<5:16:20,  6.91s/it]                                                        58%|    | 3753/6500 [7:05:55<5:16:20,  6.91s/it] 58%|    | 3754/6500 [7:06:01<5:11:09,  6.80s/it]                                                        58%|    | 3754/6500 [7:06:01<5:11:09,  6.80s/it] 58%|    | 3755/6500 [7:06:08<5:07:29,  6.72s/it]                                                        58%|    | 3755/6500 [7:06:08<5:07:29,  6.72s/it] 58%|    | 3756/6500 [7:06:14<5:04:57,  {'loss': 0.3904, 'learning_rate': 3.791328763736337e-05, 'epoch': 0.58}
{'loss': 0.406, 'learning_rate': 3.788983244232272e-05, 'epoch': 0.58}
{'loss': 0.3818, 'learning_rate': 3.7866380077960085e-05, 'epoch': 0.58}
{'loss': 0.3893, 'learning_rate': 3.784293054975734e-05, 'epoch': 0.58}
{'loss': 0.3976, 'learning_rate': 3.781948386319566e-05, 'epoch': 0.58}
6.67s/it]                                                        58%|    | 3756/6500 [7:06:14<5:04:57,  6.67s/it] 58%|    | 3757/6500 [7:06:21<5:02:58,  6.63s/it]                                                        58%|    | 3757/6500 [7:06:21<5:02:58,  6.63s/it] 58%|    | 3758/6500 [7:06:28<5:01:37,  6.60s/it]                                                        58%|    | 3758/6500 [7:06:28<5:01:37,  6.60s/it] 58%|    | 3759/6500 [7:06:34<5:00:45,  6.58s/it]                                                        58%|    | 3759/6500 [7:06:34<5:00:45,  6.58s/it] 58%|    | 3760/6500 [7:06:41<5:00:06,  6.57s/it]                                                        58%|    | 3760/6500 [7:06:41<5:00:06,  6.57s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8700525760650635, 'eval_runtime': 1.4736, 'eval_samples_per_second': 8.143, 'eval_steps_per_second': 2.036, 'epoch': 0.58}
                                                        58%|    | 3760/6500 [7:06:42<5:00:06,  6.57s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-3760
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3760
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3760
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3760
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3760
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3760/pytorch_model.bin
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3760/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3760/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3760/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3953, 'learning_rate': 3.7796040023755566e-05, 'epoch': 0.58}
{'loss': 0.3949, 'learning_rate': 3.777259903691692e-05, 'epoch': 0.58}
{'loss': 0.3984, 'learning_rate': 3.774916090815891e-05, 'epoch': 0.58}
{'loss': 0.3971, 'learning_rate': 3.772572564296005e-05, 'epoch': 0.58}
{'loss': 0.3956, 'learning_rate': 3.770229324679818e-05, 'epoch': 0.58}
 58%|    | 3761/6500 [7:06:49<5:24:08,  7.10s/it]                                                        58%|    | 3761/6500 [7:06:49<5:24:08,  7.10s/it] 58%|    | 3762/6500 [7:06:55<5:16:21,  6.93s/it]                                                        58%|    | 3762/6500 [7:06:55<5:16:21,  6.93s/it] 58%|    | 3763/6500 [7:07:02<5:11:01,  6.82s/it]                                                        58%|    | 3763/6500 [7:07:02<5:11:01,  6.82s/it] 58%|    | 3764/6500 [7:07:09<5:07:14,  6.74s/it]                                                        58%|    | 3764/6500 [7:07:09<5:07:14,  6.74s/it] 58%|    | 3765/6500 [7:07:15<5:04:17,  6.68s/it]                                                        58%|    | 3765/6500 [7:07:15<5:04:17,  6.68s/it] 58%|    | 3766/6500 [7:07:22<5:10:40,  {'loss': 0.3882, 'learning_rate': 3.7678863725150505e-05, 'epoch': 0.58}
{'loss': 0.4335, 'learning_rate': 3.765543708349351e-05, 'epoch': 0.58}
{'loss': 0.397, 'learning_rate': 3.7632013327303055e-05, 'epoch': 0.58}
{'loss': 0.6669, 'learning_rate': 3.760859246205427e-05, 'epoch': 0.58}
{'loss': 0.4113, 'learning_rate': 3.7585174493221664e-05, 'epoch': 0.58}
6.82s/it]                                                        58%|    | 3766/6500 [7:07:22<5:10:40,  6.82s/it] 58%|    | 3767/6500 [7:07:29<5:06:56,  6.74s/it]                                                        58%|    | 3767/6500 [7:07:29<5:06:56,  6.74s/it] 58%|    | 3768/6500 [7:07:35<5:04:10,  6.68s/it]                                                        58%|    | 3768/6500 [7:07:35<5:04:10,  6.68s/it] 58%|    | 3769/6500 [7:07:42<5:02:04,  6.64s/it]                                                        58%|    | 3769/6500 [7:07:42<5:02:04,  6.64s/it] 58%|    | 3770/6500 [7:07:48<5:00:49,  6.61s/it]                                                        58%|    | 3770/6500 [7:07:48<5:00:49,  6.61s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8595162034034729, 'eval_runtime': 1.4813, 'eval_samples_per_second': 8.101, 'eval_steps_per_second': 2.025, 'epoch': 0.58}
                                                        58%|    | 3770/6500 [7:07:50<5:00:49,  6.61s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-3770
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3770
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3770
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3770
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3770
the pytorch model path is the pytorch model path is/projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3770/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3770/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3770/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3770/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3834, 'learning_rate': 3.756175942627904e-05, 'epoch': 0.58}
{'loss': 0.4021, 'learning_rate': 3.753834726669951e-05, 'epoch': 0.58}
{'loss': 0.3895, 'learning_rate': 3.7514938019955554e-05, 'epoch': 0.58}
{'loss': 0.3936, 'learning_rate': 3.7491531691518935e-05, 'epoch': 0.58}
{'loss': 0.4029, 'learning_rate': 3.74681282868607e-05, 'epoch': 0.58}
 58%|    | 3771/6500 [7:07:57<5:24:03,  7.12s/it]                                                        58%|    | 3771/6500 [7:07:57<5:24:03,  7.12s/it] 58%|    | 3772/6500 [7:08:03<5:15:56,  6.95s/it]                                                        58%|    | 3772/6500 [7:08:03<5:15:56,  6.95s/it] 58%|    | 3773/6500 [7:08:10<5:10:11,  6.83s/it]                                                        58%|    | 3773/6500 [7:08:10<5:10:11,  6.83s/it] 58%|    | 3774/6500 [7:08:16<5:06:09,  6.74s/it]                                                        58%|    | 3774/6500 [7:08:16<5:06:09,  6.74s/it] 58%|    | 3775/6500 [7:08:23<5:03:16,  6.68s/it]                                                        58%|    | 3775/6500 [7:08:23<5:03:16,  6.68s/it] 58%|    | 3776/6500 [7:08:29<5:01:16,  {'loss': 0.4076, 'learning_rate': 3.744472781145131e-05, 'epoch': 0.58}
{'loss': 0.3959, 'learning_rate': 3.742133027076043e-05, 'epoch': 0.58}
{'loss': 0.3988, 'learning_rate': 3.739793567025714e-05, 'epoch': 0.58}
{'loss': 0.3966, 'learning_rate': 3.737454401540977e-05, 'epoch': 0.58}
{'loss': 0.3899, 'learning_rate': 3.735115531168596e-05, 'epoch': 0.58}
6.64s/it]                                                        58%|    | 3776/6500 [7:08:29<5:01:16,  6.64s/it] 58%|    | 3777/6500 [7:08:36<4:59:47,  6.61s/it]                                                        58%|    | 3777/6500 [7:08:36<4:59:47,  6.61s/it] 58%|    | 3778/6500 [7:08:43<4:58:48,  6.59s/it]                                                        58%|    | 3778/6500 [7:08:43<4:58:48,  6.59s/it] 58%|    | 3779/6500 [7:08:49<4:58:14,  6.58s/it]                                                        58%|    | 3779/6500 [7:08:49<4:58:14,  6.58s/it] 58%|    | 3780/6500 [7:08:56<4:57:47,  6.57s/it]                                                        58%|    | 3780/6500 [7:08:56<4:57:47,  6.57s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8687624931335449, 'eval_runtime': 1.7236, 'eval_samples_per_second': 6.962, 'eval_steps_per_second': 1.741, 'epoch': 0.58}
                                                        58%|    | 3780/6500 [7:08:57<4:57:47,  6.57s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-3780
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3780I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3780

I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3780
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3780
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3780/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3780/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3780/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3780/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3804, 'learning_rate': 3.73277695645527e-05, 'epoch': 0.58}
{'loss': 0.4275, 'learning_rate': 3.730438677947624e-05, 'epoch': 0.58}
{'loss': 0.3949, 'learning_rate': 3.728100696192218e-05, 'epoch': 0.58}
{'loss': 0.6756, 'learning_rate': 3.725763011735542e-05, 'epoch': 0.58}
{'loss': 0.4081, 'learning_rate': 3.723425625124015e-05, 'epoch': 0.58}
 58%|    | 3781/6500 [7:09:04<5:26:18,  7.20s/it]                                                        58%|    | 3781/6500 [7:09:04<5:26:18,  7.20s/it] 58%|    | 3782/6500 [7:09:11<5:25:37,  7.19s/it]                                                        58%|    | 3782/6500 [7:09:11<5:25:37,  7.19s/it] 58%|    | 3783/6500 [7:09:18<5:16:43,  6.99s/it]                                                        58%|    | 3783/6500 [7:09:18<5:16:43,  6.99s/it] 58%|    | 3784/6500 [7:09:25<5:10:21,  6.86s/it]                                                        58%|    | 3784/6500 [7:09:25<5:10:21,  6.86s/it] 58%|    | 3785/6500 [7:09:31<5:05:59,  6.76s/it]                                                        58%|    | 3785/6500 [7:09:31<5:05:59,  6.76s/it] 58%|    | 3786/6500 [7:09:38<5:02:45,  {'loss': 0.3882, 'learning_rate': 3.721088536903986e-05, 'epoch': 0.58}
{'loss': 0.3832, 'learning_rate': 3.718751747621735e-05, 'epoch': 0.58}
{'loss': 0.3823, 'learning_rate': 3.7164152578234734e-05, 'epoch': 0.58}
{'loss': 0.3875, 'learning_rate': 3.714079068055341e-05, 'epoch': 0.58}
{'loss': 0.3879, 'learning_rate': 3.711743178863407e-05, 'epoch': 0.58}
6.69s/it]                                                        58%|    | 3786/6500 [7:09:38<5:02:45,  6.69s/it] 58%|    | 3787/6500 [7:09:44<5:00:35,  6.65s/it]                                                        58%|    | 3787/6500 [7:09:44<5:00:35,  6.65s/it] 58%|    | 3788/6500 [7:09:51<4:58:50,  6.61s/it]                                                        58%|    | 3788/6500 [7:09:51<4:58:50,  6.61s/it] 58%|    | 3789/6500 [7:09:57<4:57:52,  6.59s/it]                                                        58%|    | 3789/6500 [7:09:57<4:57:52,  6.59s/it] 58%|    | 3790/6500 [7:10:04<4:57:04,  6.58s/it]                                                        58%|    | 3790/6500 [7:10:04<4:57:04,  6.58s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8710416555404663, 'eval_runtime': 1.4907, 'eval_samples_per_second': 8.05, 'eval_steps_per_second': 2.013, 'epoch': 0.58}
                                                        58%|    | 3790/6500 [7:10:05<4:57:04,  6.58s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-3790
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3790
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3790
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3790
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3790
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3790/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3790/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3790/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3790/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.409, 'learning_rate': 3.709407590793673e-05, 'epoch': 0.58}
{'loss': 0.4048, 'learning_rate': 3.707072304392068e-05, 'epoch': 0.58}
{'loss': 0.3924, 'learning_rate': 3.70473732020445e-05, 'epoch': 0.58}
{'loss': 0.3966, 'learning_rate': 3.702402638776609e-05, 'epoch': 0.58}
{'loss': 0.3855, 'learning_rate': 3.7000682606542605e-05, 'epoch': 0.58}
 58%|    | 3791/6500 [7:10:12<5:22:21,  7.14s/it]                                                        58%|    | 3791/6500 [7:10:12<5:22:21,  7.14s/it] 58%|    | 3792/6500 [7:10:19<5:14:09,  6.96s/it]                                                        58%|    | 3792/6500 [7:10:19<5:14:09,  6.96s/it] 58%|    | 3793/6500 [7:10:25<5:08:29,  6.84s/it]                                                        58%|    | 3793/6500 [7:10:25<5:08:29,  6.84s/it] 58%|    | 3794/6500 [7:10:32<5:04:26,  6.75s/it]                                                        58%|    | 3794/6500 [7:10:32<5:04:26,  6.75s/it] 58%|    | 3795/6500 [7:10:38<5:01:30,  6.69s/it]                                                        58%|    | 3795/6500 [7:10:38<5:01:30,  6.69s/it] 58%|    | 3796/6500 [7:10:45<4:59:20,  {'loss': 0.3882, 'learning_rate': 3.6977341863830534e-05, 'epoch': 0.58}
{'loss': 0.4204, 'learning_rate': 3.695400416508562e-05, 'epoch': 0.58}
{'loss': 0.3978, 'learning_rate': 3.6930669515762906e-05, 'epoch': 0.58}
{'loss': 0.6734, 'learning_rate': 3.690733792131673e-05, 'epoch': 0.58}
{'loss': 0.3954, 'learning_rate': 3.6884009387200714e-05, 'epoch': 0.58}
6.64s/it]                                                        58%|    | 3796/6500 [7:10:45<4:59:20,  6.64s/it] 58%|    | 3797/6500 [7:10:51<4:57:55,  6.61s/it]                                                        58%|    | 3797/6500 [7:10:51<4:57:55,  6.61s/it] 58%|    | 3798/6500 [7:10:59<5:08:45,  6.86s/it]                                                        58%|    | 3798/6500 [7:10:59<5:08:45,  6.86s/it] 58%|    | 3799/6500 [7:11:05<5:04:29,  6.76s/it]                                                        58%|    | 3799/6500 [7:11:05<5:04:29,  6.76s/it] 58%|    | 3800/6500 [7:11:12<5:01:19,  6.70s/it]                                                        58%|    | 3800/6500 [7:11:12<5:01:19,  6.70s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8625699877738953, 'eval_runtime': 1.4742, 'eval_samples_per_second': 8.14, 'eval_steps_per_second': 2.035, 'epoch': 0.58}
                                                        58%|    | 3800/6500 [7:11:13<5:01:19,  6.70s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-3800
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3800
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3800
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3800
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3800
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3800/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3800/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3800/pytorch_model.binthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3800/pytorch_model.bin

/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3997, 'learning_rate': 3.6860683918867756e-05, 'epoch': 0.58}
{'loss': 0.384, 'learning_rate': 3.683736152177005e-05, 'epoch': 0.58}
{'loss': 0.382, 'learning_rate': 3.6814042201359056e-05, 'epoch': 0.59}
{'loss': 0.3828, 'learning_rate': 3.6790725963085516e-05, 'epoch': 0.59}
{'loss': 0.3948, 'learning_rate': 3.6767412812399473e-05, 'epoch': 0.59}
 58%|    | 3801/6500 [7:11:20<5:23:34,  7.19s/it]                                                        58%|    | 3801/6500 [7:11:20<5:23:34,  7.19s/it] 58%|    | 3802/6500 [7:11:27<5:14:36,  7.00s/it]                                                        58%|    | 3802/6500 [7:11:27<5:14:36,  7.00s/it] 59%|    | 3803/6500 [7:11:33<5:08:15,  6.86s/it]                                                        59%|    | 3803/6500 [7:11:33<5:08:15,  6.86s/it] 59%|    | 3804/6500 [7:11:40<5:03:43,  6.76s/it]                                                        59%|    | 3804/6500 [7:11:40<5:03:43,  6.76s/it] 59%|    | 3805/6500 [7:11:47<5:00:39,  6.69s/it]                                                        59%|    | 3805/6500 [7:11:47<5:00:39,  6.69s/it] 59%|    | 3806/6500 [7:11:53<4:58:25,  {'loss': 0.4008, 'learning_rate': 3.674410275475023e-05, 'epoch': 0.59}
{'loss': 0.3968, 'learning_rate': 3.6720795795586384e-05, 'epoch': 0.59}
{'loss': 0.3866, 'learning_rate': 3.6697491940355765e-05, 'epoch': 0.59}
{'loss': 0.4021, 'learning_rate': 3.667419119450553e-05, 'epoch': 0.59}
{'loss': 0.3805, 'learning_rate': 3.665089356348208e-05, 'epoch': 0.59}
6.65s/it]                                                        59%|    | 3806/6500 [7:11:53<4:58:25,  6.65s/it] 59%|    | 3807/6500 [7:12:00<4:56:53,  6.61s/it]                                                        59%|    | 3807/6500 [7:12:00<4:56:53,  6.61s/it] 59%|    | 3808/6500 [7:12:06<4:55:40,  6.59s/it]                                                        59%|    | 3808/6500 [7:12:06<4:55:40,  6.59s/it] 59%|    | 3809/6500 [7:12:13<4:54:54,  6.58s/it]                                                        59%|    | 3809/6500 [7:12:13<4:54:54,  6.58s/it] 59%|    | 3810/6500 [7:12:19<4:54:11,  6.56s/it]                                                        59%|    | 3810/6500 [7:12:19<4:54:11,  6.56s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8701485395431519, 'eval_runtime': 1.4773, 'eval_samples_per_second': 8.123, 'eval_steps_per_second': 2.031, 'epoch': 0.59}
                                                        59%|    | 3810/6500 [7:12:21<4:54:11,  6.56s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-3810
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3810
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3810
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3810
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3810
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3810/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3810/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3810/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3810/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3952, 'learning_rate': 3.662759905273109e-05, 'epoch': 0.59}
{'loss': 0.4125, 'learning_rate': 3.660430766769752e-05, 'epoch': 0.59}
{'loss': 0.3969, 'learning_rate': 3.65810194138256e-05, 'epoch': 0.59}
{'loss': 0.6774, 'learning_rate': 3.655773429655879e-05, 'epoch': 0.59}
{'loss': 0.385, 'learning_rate': 3.6534452321339854e-05, 'epoch': 0.59}
 59%|    | 3811/6500 [7:12:28<5:17:57,  7.09s/it]                                                        59%|    | 3811/6500 [7:12:28<5:17:57,  7.09s/it] 59%|    | 3812/6500 [7:12:34<5:10:27,  6.93s/it]                                                        59%|    | 3812/6500 [7:12:34<5:10:27,  6.93s/it] 59%|    | 3813/6500 [7:12:41<5:05:08,  6.81s/it]                                                        59%|    | 3813/6500 [7:12:41<5:05:08,  6.81s/it] 59%|    | 3814/6500 [7:12:48<5:13:05,  6.99s/it]                                                        59%|    | 3814/6500 [7:12:48<5:13:05,  6.99s/it] 59%|    | 3815/6500 [7:12:55<5:06:54,  6.86s/it]                                                        59%|    | 3815/6500 [7:12:55<5:06:54,  6.86s/it] 59%|    | 3816/6500 [7:13:01<5:02:40,  {'loss': 0.4098, 'learning_rate': 3.6511173493610825e-05, 'epoch': 0.59}
{'loss': 0.378, 'learning_rate': 3.648789781881297e-05, 'epoch': 0.59}
{'loss': 0.3893, 'learning_rate': 3.646462530238684e-05, 'epoch': 0.59}
{'loss': 0.3936, 'learning_rate': 3.6441355949772253e-05, 'epoch': 0.59}
{'loss': 0.3949, 'learning_rate': 3.641808976640828e-05, 'epoch': 0.59}
6.77s/it]                                                        59%|    | 3816/6500 [7:13:01<5:02:40,  6.77s/it] 59%|    | 3817/6500 [7:13:08<4:59:40,  6.70s/it]                                                        59%|    | 3817/6500 [7:13:08<4:59:40,  6.70s/it] 59%|    | 3818/6500 [7:13:14<4:57:22,  6.65s/it]                                                        59%|    | 3818/6500 [7:13:14<4:57:22,  6.65s/it] 59%|    | 3819/6500 [7:13:21<4:55:53,  6.62s/it]                                                        59%|    | 3819/6500 [7:13:21<4:55:53,  6.62s/it] 59%|    | 3820/6500 [7:13:27<4:54:49,  6.60s/it]                                                        59%|    | 3820/6500 [7:13:27<4:54:49,  6.60s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8706440329551697, 'eval_runtime': 1.4771, 'eval_samples_per_second': 8.124, 'eval_steps_per_second': 2.031, 'epoch': 0.59}
                                                        59%|    | 3820/6500 [7:13:29<4:54:49,  6.60s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-3820
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3820
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3820
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3820
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3820
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3820/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3820/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3820/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3820/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.4004, 'learning_rate': 3.639482675773324e-05, 'epoch': 0.59}
{'loss': 0.405, 'learning_rate': 3.6371566929184744e-05, 'epoch': 0.59}
{'loss': 0.3851, 'learning_rate': 3.634831028619959e-05, 'epoch': 0.59}
{'loss': 0.399, 'learning_rate': 3.632505683421392e-05, 'epoch': 0.59}
{'loss': 0.3808, 'learning_rate': 3.630180657866306e-05, 'epoch': 0.59}
 59%|    | 3821/6500 [7:13:36<5:18:48,  7.14s/it]                                                        59%|    | 3821/6500 [7:13:36<5:18:48,  7.14s/it] 59%|    | 3822/6500 [7:13:42<5:10:45,  6.96s/it]                                                        59%|    | 3822/6500 [7:13:42<5:10:45,  6.96s/it] 59%|    | 3823/6500 [7:13:49<5:05:01,  6.84s/it]                                                        59%|    | 3823/6500 [7:13:49<5:05:01,  6.84s/it] 59%|    | 3824/6500 [7:13:55<5:01:01,  6.75s/it]                                                        59%|    | 3824/6500 [7:13:55<5:01:01,  6.75s/it] 59%|    | 3825/6500 [7:14:02<4:58:00,  6.68s/it]                                                        59%|    | 3825/6500 [7:14:02<4:58:00,  6.68s/it] 59%|    | 3826/6500 [7:14:08<4:55:45,  {'loss': 0.424, 'learning_rate': 3.627855952498163e-05, 'epoch': 0.59}
{'loss': 0.3834, 'learning_rate': 3.6255315678603494e-05, 'epoch': 0.59}
{'loss': 0.666, 'learning_rate': 3.6232075044961735e-05, 'epoch': 0.59}
{'loss': 0.4075, 'learning_rate': 3.620883762948873e-05, 'epoch': 0.59}
{'loss': 0.3892, 'learning_rate': 3.6185603437616065e-05, 'epoch': 0.59}
6.64s/it]                                                        59%|    | 3826/6500 [7:14:08<4:55:45,  6.64s/it] 59%|    | 3827/6500 [7:14:15<4:54:27,  6.61s/it]                                                        59%|    | 3827/6500 [7:14:15<4:54:27,  6.61s/it] 59%|    | 3828/6500 [7:14:21<4:53:26,  6.59s/it]                                                        59%|    | 3828/6500 [7:14:21<4:53:26,  6.59s/it] 59%|    | 3829/6500 [7:14:28<4:52:42,  6.58s/it]                                                        59%|    | 3829/6500 [7:14:28<4:52:42,  6.58s/it] 59%|    | 3830/6500 [7:14:36<5:04:35,  6.84s/it]                                                        59%|    | 3830/6500 [7:14:36<5:04:35,  6.84s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8623068332672119, 'eval_runtime': 1.4848, 'eval_samples_per_second': 8.082, 'eval_steps_per_second': 2.02, 'epoch': 0.59}
                                                        59%|    | 3830/6500 [7:14:37<5:04:35,  6.84s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-3830
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3830
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3830
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3830
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3830
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3830/pytorch_model.bin
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3830/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3830/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3830/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.4035, 'learning_rate': 3.616237247477462e-05, 'epoch': 0.59}
{'loss': 0.3742, 'learning_rate': 3.6139144746394464e-05, 'epoch': 0.59}
{'loss': 0.3904, 'learning_rate': 3.6115920257904963e-05, 'epoch': 0.59}
{'loss': 0.3897, 'learning_rate': 3.609269901473467e-05, 'epoch': 0.59}
{'loss': 0.3938, 'learning_rate': 3.606948102231143e-05, 'epoch': 0.59}
 59%|    | 3831/6500 [7:14:44<5:24:52,  7.30s/it]                                                        59%|    | 3831/6500 [7:14:44<5:24:52,  7.30s/it] 59%|    | 3832/6500 [7:14:50<5:14:43,  7.08s/it]                                                        59%|    | 3832/6500 [7:14:50<5:14:43,  7.08s/it] 59%|    | 3833/6500 [7:14:57<5:07:30,  6.92s/it]                                                        59%|    | 3833/6500 [7:14:57<5:07:30,  6.92s/it] 59%|    | 3834/6500 [7:15:04<5:02:35,  6.81s/it]                                                        59%|    | 3834/6500 [7:15:04<5:02:35,  6.81s/it] 59%|    | 3835/6500 [7:15:10<4:58:56,  6.73s/it]                                                        59%|    | 3835/6500 [7:15:10<4:58:56,  6.73s/it] 59%|    | 3836/6500 [7:15:17<4:56:21,  {'loss': 0.3983, 'learning_rate': 3.60462662860623e-05, 'epoch': 0.59}
{'loss': 0.4016, 'learning_rate': 3.6023054811413584e-05, 'epoch': 0.59}
{'loss': 0.3853, 'learning_rate': 3.599984660379084e-05, 'epoch': 0.59}
{'loss': 0.4017, 'learning_rate': 3.5976641668618816e-05, 'epoch': 0.59}
{'loss': 0.3749, 'learning_rate': 3.595344001132154e-05, 'epoch': 0.59}
6.67s/it]                                                        59%|    | 3836/6500 [7:15:17<4:56:21,  6.67s/it] 59%|    | 3837/6500 [7:15:23<4:54:28,  6.63s/it]                                                        59%|    | 3837/6500 [7:15:23<4:54:28,  6.63s/it] 59%|    | 3838/6500 [7:15:30<4:53:08,  6.61s/it]                                                        59%|    | 3838/6500 [7:15:30<4:53:08,  6.61s/it] 59%|    | 3839/6500 [7:15:36<4:52:12,  6.59s/it]                                                        59%|    | 3839/6500 [7:15:36<4:52:12,  6.59s/it] 59%|    | 3840/6500 [7:15:43<4:51:37,  6.58s/it]                                                        59%|    | 3840/6500 [7:15:43<4:51:37,  6.58s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8701826930046082, 'eval_runtime': 1.4795, 'eval_samples_per_second': 8.111, 'eval_steps_per_second': 2.028, 'epoch': 0.59}
                                                        59%|    | 3840/6500 [7:15:44<4:51:37,  6.58s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-3840
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3840
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3840
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3840
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3840
the pytorch model path isthe pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3840/pytorch_model.bin 
/projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3840/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3840/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3840/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.4351, 'learning_rate': 3.593024163732225e-05, 'epoch': 0.59}
{'loss': 0.3862, 'learning_rate': 3.5907046552043436e-05, 'epoch': 0.59}
{'loss': 0.6693, 'learning_rate': 3.588385476090681e-05, 'epoch': 0.59}
{'loss': 0.4059, 'learning_rate': 3.586066626933331e-05, 'epoch': 0.59}
{'loss': 0.3885, 'learning_rate': 3.583748108274309e-05, 'epoch': 0.59}
 59%|    | 3841/6500 [7:15:51<5:15:15,  7.11s/it]                                                        59%|    | 3841/6500 [7:15:51<5:15:15,  7.11s/it] 59%|    | 3842/6500 [7:15:58<5:07:39,  6.95s/it]                                                        59%|    | 3842/6500 [7:15:58<5:07:39,  6.95s/it] 59%|    | 3843/6500 [7:16:04<5:02:18,  6.83s/it]                                                        59%|    | 3843/6500 [7:16:04<5:02:18,  6.83s/it] 59%|    | 3844/6500 [7:16:11<4:58:29,  6.74s/it]                                                        59%|    | 3844/6500 [7:16:11<4:58:29,  6.74s/it] 59%|    | 3845/6500 [7:16:17<4:55:41,  6.68s/it]                                                        59%|    | 3845/6500 [7:16:17<4:55:41,  6.68s/it] 59%|    | 3846/6500 [7:16:24<4:53:46,  {'loss': 0.4005, 'learning_rate': 3.5814299206555555e-05, 'epoch': 0.59}
{'loss': 0.3803, 'learning_rate': 3.579112064618934e-05, 'epoch': 0.59}
{'loss': 0.389, 'learning_rate': 3.576794540706227e-05, 'epoch': 0.59}
{'loss': 0.3907, 'learning_rate': 3.5744773494591445e-05, 'epoch': 0.59}
{'loss': 0.3986, 'learning_rate': 3.5721604914193144e-05, 'epoch': 0.59}
6.64s/it]                                                        59%|    | 3846/6500 [7:16:24<4:53:46,  6.64s/it] 59%|    | 3847/6500 [7:16:31<5:00:35,  6.80s/it]                                                        59%|    | 3847/6500 [7:16:31<5:00:35,  6.80s/it] 59%|    | 3848/6500 [7:16:38<4:57:03,  6.72s/it]                                                        59%|    | 3848/6500 [7:16:38<4:57:03,  6.72s/it] 59%|    | 3849/6500 [7:16:44<4:54:40,  6.67s/it]                                                        59%|    | 3849/6500 [7:16:44<4:54:40,  6.67s/it] 59%|    | 3850/6500 [7:16:51<4:53:06,  6.64s/it]                                                        59%|    | 3850/6500 [7:16:51<4:53:06,  6.64s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8727386593818665, 'eval_runtime': 1.4967, 'eval_samples_per_second': 8.018, 'eval_steps_per_second': 2.004, 'epoch': 0.59}
                                                        59%|    | 3850/6500 [7:16:52<4:53:06,  6.64s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-3850
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3850
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3850
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3850
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3850
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3850/pytorch_model.bin
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3850/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3850/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3850/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.394, 'learning_rate': 3.569843967128287e-05, 'epoch': 0.59}
{'loss': 0.3996, 'learning_rate': 3.567527777127536e-05, 'epoch': 0.59}
{'loss': 0.3993, 'learning_rate': 3.5652119219584586e-05, 'epoch': 0.59}
{'loss': 0.392, 'learning_rate': 3.56289640216237e-05, 'epoch': 0.59}
{'loss': 0.3786, 'learning_rate': 3.5605812182805116e-05, 'epoch': 0.59}
 59%|    | 3851/6500 [7:16:59<5:16:06,  7.16s/it]                                                        59%|    | 3851/6500 [7:16:59<5:16:06,  7.16s/it] 59%|    | 3852/6500 [7:17:06<5:07:54,  6.98s/it]                                                        59%|    | 3852/6500 [7:17:06<5:07:54,  6.98s/it] 59%|    | 3853/6500 [7:17:12<5:02:14,  6.85s/it]                                                        59%|    | 3853/6500 [7:17:12<5:02:14,  6.85s/it] 59%|    | 3854/6500 [7:17:19<4:58:02,  6.76s/it]                                                        59%|    | 3854/6500 [7:17:19<4:58:02,  6.76s/it] 59%|    | 3855/6500 [7:17:25<4:55:05,  6.69s/it]                                                        59%|    | 3855/6500 [7:17:25<4:55:05,  6.69s/it] 59%|    | 3856/6500 [7:17:32<4:53:01,  {'loss': 0.4319, 'learning_rate': 3.55826637085404e-05, 'epoch': 0.59}
{'loss': 0.395, 'learning_rate': 3.5559518604240385e-05, 'epoch': 0.59}
{'loss': 0.6687, 'learning_rate': 3.5536376875315095e-05, 'epoch': 0.59}
{'loss': 0.4069, 'learning_rate': 3.551323852717378e-05, 'epoch': 0.59}
{'loss': 0.3913, 'learning_rate': 3.5490103565224865e-05, 'epoch': 0.59}
6.65s/it]                                                        59%|    | 3856/6500 [7:17:32<4:53:01,  6.65s/it] 59%|    | 3857/6500 [7:17:38<4:51:49,  6.62s/it]                                                        59%|    | 3857/6500 [7:17:38<4:51:49,  6.62s/it] 59%|    | 3858/6500 [7:17:45<4:50:49,  6.60s/it]                                                        59%|    | 3858/6500 [7:17:45<4:50:49,  6.60s/it] 59%|    | 3859/6500 [7:17:52<4:50:02,  6.59s/it]                                                        59%|    | 3859/6500 [7:17:52<4:50:02,  6.59s/it] 59%|    | 3860/6500 [7:17:58<4:49:23,  6.58s/it]                                                        59%|    | 3860/6500 [7:17:58<4:49:23,  6.58s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8620606064796448, 'eval_runtime': 1.7229, 'eval_samples_per_second': 6.965, 'eval_steps_per_second': 1.741, 'epoch': 0.59}
                                                        59%|    | 3860/6500 [7:18:00<4:49:23,  6.58s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-3860
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3860
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3860
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3860
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3860
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3860/pytorch_model.bin
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3860/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3860/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3860/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3883, 'learning_rate': 3.546697199487603e-05, 'epoch': 0.59}
{'loss': 0.3907, 'learning_rate': 3.544384382153413e-05, 'epoch': 0.59}
{'loss': 0.3912, 'learning_rate': 3.542071905060522e-05, 'epoch': 0.59}
{'loss': 0.3937, 'learning_rate': 3.5397597687494596e-05, 'epoch': 0.59}
{'loss': 0.411, 'learning_rate': 3.5374479737606733e-05, 'epoch': 0.59}
 59%|    | 3861/6500 [7:18:07<5:18:16,  7.24s/it]                                                        59%|    | 3861/6500 [7:18:07<5:18:16,  7.24s/it] 59%|    | 3862/6500 [7:18:13<5:08:55,  7.03s/it]                                                        59%|    | 3862/6500 [7:18:13<5:08:55,  7.03s/it] 59%|    | 3863/6500 [7:18:21<5:10:15,  7.06s/it]                                                        59%|    | 3863/6500 [7:18:21<5:10:15,  7.06s/it] 59%|    | 3864/6500 [7:18:27<5:03:14,  6.90s/it]                                                        59%|    | 3864/6500 [7:18:27<5:03:14,  6.90s/it] 59%|    | 3865/6500 [7:18:34<4:58:16,  6.79s/it]                                                        59%|    | 3865/6500 [7:18:34<4:58:16,  6.79s/it] 59%|    | 3866/6500 [7:18:40<4:55:00,  {'loss': 0.4015, 'learning_rate': 3.535136520634531e-05, 'epoch': 0.59}
{'loss': 0.395, 'learning_rate': 3.53282540991132e-05, 'epoch': 0.59}
{'loss': 0.3991, 'learning_rate': 3.530514642131249e-05, 'epoch': 0.6}
{'loss': 0.3844, 'learning_rate': 3.528204217834444e-05, 'epoch': 0.6}
{'loss': 0.3938, 'learning_rate': 3.5258941375609565e-05, 'epoch': 0.6}
6.72s/it]                                                        59%|    | 3866/6500 [7:18:40<4:55:00,  6.72s/it] 59%|    | 3867/6500 [7:18:47<4:52:30,  6.67s/it]                                                        59%|    | 3867/6500 [7:18:47<4:52:30,  6.67s/it] 60%|    | 3868/6500 [7:18:53<4:50:58,  6.63s/it]                                                        60%|    | 3868/6500 [7:18:53<4:50:58,  6.63s/it] 60%|    | 3869/6500 [7:19:00<4:49:58,  6.61s/it]                                                        60%|    | 3869/6500 [7:19:00<4:49:58,  6.61s/it] 60%|    | 3870/6500 [7:19:06<4:48:48,  6.59s/it]                                                        60%|    | 3870/6500 [7:19:06<4:48:48,  6.59s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.870268702507019, 'eval_runtime': 1.4967, 'eval_samples_per_second': 8.018, 'eval_steps_per_second': 2.004, 'epoch': 0.6}
                                                        60%|    | 3870/6500 [7:19:08<4:48:48,  6.59s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-3870
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3870
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3870
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3870
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3870
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3870/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3870/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3870/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3870/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.4166, 'learning_rate': 3.523584401850751e-05, 'epoch': 0.6}
{'loss': 0.3962, 'learning_rate': 3.521275011243715e-05, 'epoch': 0.6}
{'loss': 0.6732, 'learning_rate': 3.5189659662796546e-05, 'epoch': 0.6}
{'loss': 0.3967, 'learning_rate': 3.5166572674982944e-05, 'epoch': 0.6}
{'loss': 0.3986, 'learning_rate': 3.5143489154392785e-05, 'epoch': 0.6}
 60%|    | 3871/6500 [7:19:15<5:13:22,  7.15s/it]                                                        60%|    | 3871/6500 [7:19:15<5:13:22,  7.15s/it] 60%|    | 3872/6500 [7:19:21<5:05:21,  6.97s/it]                                                        60%|    | 3872/6500 [7:19:21<5:05:21,  6.97s/it] 60%|    | 3873/6500 [7:19:28<4:59:44,  6.85s/it]                                                        60%|    | 3873/6500 [7:19:28<4:59:44,  6.85s/it] 60%|    | 3874/6500 [7:19:34<4:55:48,  6.76s/it]                                                        60%|    | 3874/6500 [7:19:34<4:55:48,  6.76s/it] 60%|    | 3875/6500 [7:19:41<4:52:59,  6.70s/it]                                                        60%|    | 3875/6500 [7:19:41<4:52:59,  6.70s/it] 60%|    | 3876/6500 [7:19:48<4:51:00,  {'loss': 0.3792, 'learning_rate': 3.5120409106421716e-05, 'epoch': 0.6}
{'loss': 0.378, 'learning_rate': 3.509733253646454e-05, 'epoch': 0.6}
{'loss': 0.39, 'learning_rate': 3.5074259449915284e-05, 'epoch': 0.6}
{'loss': 0.3987, 'learning_rate': 3.505118985216713e-05, 'epoch': 0.6}
{'loss': 0.3996, 'learning_rate': 3.502812374861245e-05, 'epoch': 0.6}
6.65s/it]                                                        60%|    | 3876/6500 [7:19:48<4:51:00,  6.65s/it] 60%|    | 3877/6500 [7:19:54<4:49:34,  6.62s/it]                                                        60%|    | 3877/6500 [7:19:54<4:49:34,  6.62s/it] 60%|    | 3878/6500 [7:20:01<4:48:35,  6.60s/it]                                                        60%|    | 3878/6500 [7:20:01<4:48:35,  6.60s/it] 60%|    | 3879/6500 [7:20:08<4:59:11,  6.85s/it]                                                        60%|    | 3879/6500 [7:20:08<4:59:11,  6.85s/it] 60%|    | 3880/6500 [7:20:15<4:55:24,  6.77s/it]                                                        60%|    | 3880/6500 [7:20:15<4:55:24,  6.77s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.870394766330719, 'eval_runtime': 1.4807, 'eval_samples_per_second': 8.105, 'eval_steps_per_second': 2.026, 'epoch': 0.6}
                                                        60%|    | 3880/6500 [7:20:16<4:55:24,  6.77s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-3880
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3880
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3880
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3880
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3880
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3880/pytorch_model.bin
the pytorch model path isthe pytorch model path is  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3880/pytorch_model.bin/projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3880/pytorch_model.bin

the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3880/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3912, 'learning_rate': 3.500506114464282e-05, 'epoch': 0.6}
{'loss': 0.3876, 'learning_rate': 3.498200204564897e-05, 'epoch': 0.6}
{'loss': 0.3946, 'learning_rate': 3.495894645702084e-05, 'epoch': 0.6}
{'loss': 0.3783, 'learning_rate': 3.4935894384147516e-05, 'epoch': 0.6}
{'loss': 0.3894, 'learning_rate': 3.49128458324173e-05, 'epoch': 0.6}
 60%|    | 3881/6500 [7:20:23<5:16:26,  7.25s/it]                                                        60%|    | 3881/6500 [7:20:23<5:16:26,  7.25s/it] 60%|    | 3882/6500 [7:20:30<5:07:03,  7.04s/it]                                                        60%|    | 3882/6500 [7:20:30<5:07:03,  7.04s/it] 60%|    | 3883/6500 [7:20:36<5:00:36,  6.89s/it]                                                        60%|    | 3883/6500 [7:20:36<5:00:36,  6.89s/it] 60%|    | 3884/6500 [7:20:43<4:55:50,  6.79s/it]                                                        60%|    | 3884/6500 [7:20:43<4:55:50,  6.79s/it] 60%|    | 3885/6500 [7:20:49<4:52:20,  6.71s/it]                                                        60%|    | 3885/6500 [7:20:49<4:52:20,  6.71s/it] 60%|    | 3886/6500 [7:20:56<4:50:07,  {'loss': 0.413, 'learning_rate': 3.488980080721762e-05, 'epoch': 0.6}
{'loss': 0.3953, 'learning_rate': 3.486675931393514e-05, 'epoch': 0.6}
{'loss': 0.6653, 'learning_rate': 3.484372135795566e-05, 'epoch': 0.6}
{'loss': 0.3856, 'learning_rate': 3.482068694466417e-05, 'epoch': 0.6}
{'loss': 0.406, 'learning_rate': 3.4797656079444806e-05, 'epoch': 0.6}
6.66s/it]                                                        60%|    | 3886/6500 [7:20:56<4:50:07,  6.66s/it] 60%|    | 3887/6500 [7:21:02<4:48:36,  6.63s/it]                                                        60%|    | 3887/6500 [7:21:02<4:48:36,  6.63s/it] 60%|    | 3888/6500 [7:21:09<4:48:24,  6.63s/it]                                                        60%|    | 3888/6500 [7:21:09<4:48:24,  6.63s/it] 60%|    | 3889/6500 [7:21:15<4:47:20,  6.60s/it]                                                        60%|    | 3889/6500 [7:21:15<4:47:20,  6.60s/it] 60%|    | 3890/6500 [7:21:22<4:46:24,  6.58s/it]                                                        60%|    | 3890/6500 [7:21:22<4:46:24,  6.58s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8637758493423462, 'eval_runtime': 1.4778, 'eval_samples_per_second': 8.12, 'eval_steps_per_second': 2.03, 'epoch': 0.6}
                                                        60%|    | 3890/6500 [7:21:23<4:46:24,  6.58s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-3890
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3890
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3890
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3890
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3890
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3890/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3890/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3890/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3890/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3793, 'learning_rate': 3.47746287676809e-05, 'epoch': 0.6}
{'loss': 0.3745, 'learning_rate': 3.475160501475495e-05, 'epoch': 0.6}
{'loss': 0.3826, 'learning_rate': 3.472858482604861e-05, 'epoch': 0.6}
{'loss': 0.3861, 'learning_rate': 3.4705568206942706e-05, 'epoch': 0.6}
{'loss': 0.4054, 'learning_rate': 3.468255516281725e-05, 'epoch': 0.6}
 60%|    | 3891/6500 [7:21:30<5:09:50,  7.13s/it]                                                        60%|    | 3891/6500 [7:21:30<5:09:50,  7.13s/it] 60%|    | 3892/6500 [7:21:37<5:02:11,  6.95s/it]                                                        60%|    | 3892/6500 [7:21:37<5:02:11,  6.95s/it] 60%|    | 3893/6500 [7:21:44<4:56:54,  6.83s/it]                                                        60%|    | 3893/6500 [7:21:44<4:56:54,  6.83s/it] 60%|    | 3894/6500 [7:21:50<4:53:02,  6.75s/it]                                                        60%|    | 3894/6500 [7:21:50<4:53:02,  6.75s/it] 60%|    | 3895/6500 [7:21:58<5:03:09,  6.98s/it]                                                        60%|    | 3895/6500 [7:21:58<5:03:09,  6.98s/it] 60%|    | 3896/6500 [7:22:04<4:57:34,  {'loss': 0.396, 'learning_rate': 3.465954569905141e-05, 'epoch': 0.6}
{'loss': 0.3938, 'learning_rate': 3.463653982102347e-05, 'epoch': 0.6}
{'loss': 0.3924, 'learning_rate': 3.461353753411096e-05, 'epoch': 0.6}
{'loss': 0.3798, 'learning_rate': 3.4590538843690485e-05, 'epoch': 0.6}
{'loss': 0.4195, 'learning_rate': 3.456754375513786e-05, 'epoch': 0.6}
6.86s/it]                                                        60%|    | 3896/6500 [7:22:04<4:57:34,  6.86s/it] 60%|    | 3897/6500 [7:22:11<4:54:10,  6.78s/it]                                                        60%|    | 3897/6500 [7:22:11<4:54:10,  6.78s/it] 60%|    | 3898/6500 [7:22:17<4:51:08,  6.71s/it]                                                        60%|    | 3898/6500 [7:22:17<4:51:08,  6.71s/it] 60%|    | 3899/6500 [7:22:24<4:48:49,  6.66s/it]                                                        60%|    | 3899/6500 [7:22:24<4:48:49,  6.66s/it] 60%|    | 3900/6500 [7:22:30<4:46:57,  6.62s/it]                                                        60%|    | 3900/6500 [7:22:30<4:46:57,  6.62s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8690058588981628, 'eval_runtime': 1.4836, 'eval_samples_per_second': 8.089, 'eval_steps_per_second': 2.022, 'epoch': 0.6}
                                                        60%|    | 3900/6500 [7:22:32<4:46:57,  6.62s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-3900
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3900
/projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3900
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3900
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3900
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3900/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3900/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3900/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3900/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.387, 'learning_rate': 3.4544552273828056e-05, 'epoch': 0.6}
{'loss': 0.5817, 'learning_rate': 3.452156440513519e-05, 'epoch': 0.6}
{'loss': 0.4926, 'learning_rate': 3.449858015443254e-05, 'epoch': 0.6}
{'loss': 0.3883, 'learning_rate': 3.447559952709252e-05, 'epoch': 0.6}
{'loss': 0.4075, 'learning_rate': 3.445262252848672e-05, 'epoch': 0.6}
 60%|    | 3901/6500 [7:22:39<5:09:27,  7.14s/it]                                                        60%|    | 3901/6500 [7:22:39<5:09:27,  7.14s/it] 60%|    | 3902/6500 [7:22:45<5:01:49,  6.97s/it]                                                        60%|    | 3902/6500 [7:22:45<5:01:49,  6.97s/it] 60%|    | 3903/6500 [7:22:52<4:56:23,  6.85s/it]                                                        60%|    | 3903/6500 [7:22:52<4:56:23,  6.85s/it] 60%|    | 3904/6500 [7:22:58<4:52:39,  6.76s/it]                                                        60%|    | 3904/6500 [7:22:58<4:52:39,  6.76s/it] 60%|    | 3905/6500 [7:23:05<4:49:47,  6.70s/it]                                                        60%|    | 3905/6500 [7:23:05<4:49:47,  6.70s/it] 60%|    | 3906/6500 [7:23:12<4:47:50,  {'loss': 0.3794, 'learning_rate': 3.442964916398588e-05, 'epoch': 0.6}
{'loss': 0.3823, 'learning_rate': 3.440667943895986e-05, 'epoch': 0.6}
{'loss': 0.3897, 'learning_rate': 3.4383713358777735e-05, 'epoch': 0.6}
{'loss': 0.388, 'learning_rate': 3.4360750928807664e-05, 'epoch': 0.6}
{'loss': 0.4047, 'learning_rate': 3.4337792154416966e-05, 'epoch': 0.6}
6.66s/it]                                                        60%|    | 3906/6500 [7:23:12<4:47:50,  6.66s/it] 60%|    | 3907/6500 [7:23:18<4:46:32,  6.63s/it]                                                        60%|    | 3907/6500 [7:23:18<4:46:32,  6.63s/it] 60%|    | 3908/6500 [7:23:25<4:45:28,  6.61s/it]                                                        60%|    | 3908/6500 [7:23:25<4:45:28,  6.61s/it] 60%|    | 3909/6500 [7:23:31<4:44:44,  6.59s/it]                                                        60%|    | 3909/6500 [7:23:31<4:44:44,  6.59s/it] 60%|    | 3910/6500 [7:23:38<4:44:07,  6.58s/it]                                                        60%|    | 3910/6500 [7:23:38<4:44:07,  6.58s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8715243935585022, 'eval_runtime': 1.4866, 'eval_samples_per_second': 8.072, 'eval_steps_per_second': 2.018, 'epoch': 0.6}
                                                        60%|    | 3910/6500 [7:23:39<4:44:07,  6.58s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-3910
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3910
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3910
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3910
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3910
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3910/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3910/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3910/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3910/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3997, 'learning_rate': 3.431483704097212e-05, 'epoch': 0.6}
{'loss': 0.3801, 'learning_rate': 3.4291885593838755e-05, 'epoch': 0.6}
{'loss': 0.3956, 'learning_rate': 3.426893781838162e-05, 'epoch': 0.6}
{'loss': 0.375, 'learning_rate': 3.4245993719964634e-05, 'epoch': 0.6}
{'loss': 0.4252, 'learning_rate': 3.4223053303950827e-05, 'epoch': 0.6}
 60%|    | 3911/6500 [7:23:47<5:19:54,  7.41s/it]                                                        60%|    | 3911/6500 [7:23:47<5:19:54,  7.41s/it] 60%|    | 3912/6500 [7:23:54<5:08:59,  7.16s/it]                                                        60%|    | 3912/6500 [7:23:54<5:08:59,  7.16s/it] 60%|    | 3913/6500 [7:24:00<5:00:55,  6.98s/it]                                                        60%|    | 3913/6500 [7:24:00<5:00:55,  6.98s/it] 60%|    | 3914/6500 [7:24:07<4:55:28,  6.86s/it]                                                        60%|    | 3914/6500 [7:24:07<4:55:28,  6.86s/it] 60%|    | 3915/6500 [7:24:13<4:51:21,  6.76s/it]                                                        60%|    | 3915/6500 [7:24:13<4:51:21,  6.76s/it] 60%|    | 3916/6500 [7:24:20<4:48:40,  {'loss': 0.3784, 'learning_rate': 3.420011657570238e-05, 'epoch': 0.6}
{'loss': 0.6646, 'learning_rate': 3.417718354058062e-05, 'epoch': 0.6}
{'loss': 0.4062, 'learning_rate': 3.4154254203946e-05, 'epoch': 0.6}
{'loss': 0.3876, 'learning_rate': 3.413132857115812e-05, 'epoch': 0.6}
{'loss': 0.4006, 'learning_rate': 3.4108406647575706e-05, 'epoch': 0.6}
6.70s/it]                                                        60%|    | 3916/6500 [7:24:20<4:48:40,  6.70s/it] 60%|    | 3917/6500 [7:24:26<4:46:41,  6.66s/it]                                                        60%|    | 3917/6500 [7:24:27<4:46:41,  6.66s/it] 60%|    | 3918/6500 [7:24:33<4:45:23,  6.63s/it]                                                        60%|    | 3918/6500 [7:24:33<4:45:23,  6.63s/it] 60%|    | 3919/6500 [7:24:40<4:44:09,  6.61s/it]                                                        60%|    | 3919/6500 [7:24:40<4:44:09,  6.61s/it] 60%|    | 3920/6500 [7:24:46<4:43:21,  6.59s/it]                                                        60%|    | 3920/6500 [7:24:46<4:43:21,  6.59s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8651149868965149, 'eval_runtime': 1.4832, 'eval_samples_per_second': 8.091, 'eval_steps_per_second': 2.023, 'epoch': 0.6}
                                                        60%|    | 3920/6500 [7:24:48<4:43:21,  6.59s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-3920
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3920
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3920
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3920
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3920
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3920/pytorch_model.bin
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3920/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3920/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3920/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.384, 'learning_rate': 3.408548843855661e-05, 'epoch': 0.6}
{'loss': 0.3928, 'learning_rate': 3.406257394945783e-05, 'epoch': 0.6}
{'loss': 0.3936, 'learning_rate': 3.403966318563549e-05, 'epoch': 0.6}
{'loss': 0.3949, 'learning_rate': 3.401675615244483e-05, 'epoch': 0.6}
{'loss': 0.3909, 'learning_rate': 3.399385285524026e-05, 'epoch': 0.6}
 60%|    | 3921/6500 [7:24:55<5:06:58,  7.14s/it]                                                        60%|    | 3921/6500 [7:24:55<5:06:58,  7.14s/it] 60%|    | 3922/6500 [7:25:01<4:59:22,  6.97s/it]                                                        60%|    | 3922/6500 [7:25:01<4:59:22,  6.97s/it] 60%|    | 3923/6500 [7:25:08<4:54:00,  6.85s/it]                                                        60%|    | 3923/6500 [7:25:08<4:54:00,  6.85s/it] 60%|    | 3924/6500 [7:25:14<4:50:08,  6.76s/it]                                                        60%|    | 3924/6500 [7:25:14<4:50:08,  6.76s/it] 60%|    | 3925/6500 [7:25:21<4:47:37,  6.70s/it]                                                        60%|    | 3925/6500 [7:25:21<4:47:37,  6.70s/it] 60%|    | 3926/6500 [7:25:27<4:45:42,  {'loss': 0.3973, 'learning_rate': 3.397095329937526e-05, 'epoch': 0.6}
{'loss': 0.3891, 'learning_rate': 3.394805749020246e-05, 'epoch': 0.6}
{'loss': 0.3856, 'learning_rate': 3.3925165433073624e-05, 'epoch': 0.6}
{'loss': 0.3763, 'learning_rate': 3.3902277133339635e-05, 'epoch': 0.6}
{'loss': 0.4226, 'learning_rate': 3.387939259635049e-05, 'epoch': 0.6}
6.66s/it]                                                        60%|    | 3926/6500 [7:25:27<4:45:42,  6.66s/it] 60%|    | 3927/6500 [7:25:35<4:55:49,  6.90s/it]                                                        60%|    | 3927/6500 [7:25:35<4:55:49,  6.90s/it] 60%|    | 3928/6500 [7:25:41<4:51:07,  6.79s/it]                                                        60%|    | 3928/6500 [7:25:41<4:51:07,  6.79s/it] 60%|    | 3929/6500 [7:25:48<4:47:43,  6.71s/it]                                                        60%|    | 3929/6500 [7:25:48<4:47:43,  6.71s/it] 60%|    | 3930/6500 [7:25:54<4:45:33,  6.67s/it]                                                        60%|    | 3930/6500 [7:25:54<4:45:33,  6.67s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.871557891368866, 'eval_runtime': 1.4844, 'eval_samples_per_second': 8.084, 'eval_steps_per_second': 2.021, 'epoch': 0.6}
                                                        60%|    | 3930/6500 [7:25:56<4:45:33,  6.67s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-3930
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3930
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3930
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3930
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3930
the pytorch model path isthe pytorch model path isthe pytorch model path is  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3930/pytorch_model.bin/projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3930/pytorch_model.bin

 /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3930/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3930/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3963, 'learning_rate': 3.385651182745532e-05, 'epoch': 0.6}
{'loss': 0.6556, 'learning_rate': 3.383363483200235e-05, 'epoch': 0.6}
{'loss': 0.4048, 'learning_rate': 3.3810761615338934e-05, 'epoch': 0.61}
{'loss': 0.3845, 'learning_rate': 3.3787892182811564e-05, 'epoch': 0.61}
{'loss': 0.3915, 'learning_rate': 3.3765026539765834e-05, 'epoch': 0.61}
 60%|    | 3931/6500 [7:26:03<5:07:38,  7.19s/it]                                                        60%|    | 3931/6500 [7:26:03<5:07:38,  7.19s/it] 60%|    | 3932/6500 [7:26:09<4:59:08,  6.99s/it]                                                        60%|    | 3932/6500 [7:26:09<4:59:08,  6.99s/it] 61%|    | 3933/6500 [7:26:16<4:53:26,  6.86s/it]                                                        61%|    | 3933/6500 [7:26:16<4:53:26,  6.86s/it] 61%|    | 3934/6500 [7:26:22<4:49:03,  6.76s/it]                                                        61%|    | 3934/6500 [7:26:22<4:49:03,  6.76s/it] 61%|    | 3935/6500 [7:26:29<4:46:05,  6.69s/it]                                                        61%|    | 3935/6500 [7:26:29<4:46:05,  6.69s/it] 61%|    | 3936/6500 [7:26:36<4:44:02,  {'loss': 0.3783, 'learning_rate': 3.374216469154643e-05, 'epoch': 0.61}
{'loss': 0.3906, 'learning_rate': 3.371930664349719e-05, 'epoch': 0.61}
{'loss': 0.3852, 'learning_rate': 3.3696452400961023e-05, 'epoch': 0.61}
{'loss': 0.3964, 'learning_rate': 3.3673601969279986e-05, 'epoch': 0.61}
{'loss': 0.388, 'learning_rate': 3.3650755353795216e-05, 'epoch': 0.61}
6.65s/it]                                                        61%|    | 3936/6500 [7:26:36<4:44:02,  6.65s/it] 61%|    | 3937/6500 [7:26:42<4:42:28,  6.61s/it]                                                        61%|    | 3937/6500 [7:26:42<4:42:28,  6.61s/it] 61%|    | 3938/6500 [7:26:49<4:41:28,  6.59s/it]                                                        61%|    | 3938/6500 [7:26:49<4:41:28,  6.59s/it] 61%|    | 3939/6500 [7:26:55<4:40:45,  6.58s/it]                                                        61%|    | 3939/6500 [7:26:55<4:40:45,  6.58s/it] 61%|    | 3940/6500 [7:27:02<4:40:01,  6.56s/it]                                                        61%|    | 3940/6500 [7:27:02<4:40:01,  6.56s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8728559613227844, 'eval_runtime': 1.4794, 'eval_samples_per_second': 8.111, 'eval_steps_per_second': 2.028, 'epoch': 0.61}
                                                        61%|    | 3940/6500 [7:27:03<4:40:01,  6.56s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-3940
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3940I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3940

I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3940
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3940
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3940/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3940/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3940/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3940/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3856, 'learning_rate': 3.3627912559846985e-05, 'epoch': 0.61}
{'loss': 0.3941, 'learning_rate': 3.360507359277466e-05, 'epoch': 0.61}
{'loss': 0.3883, 'learning_rate': 3.358223845791668e-05, 'epoch': 0.61}
{'loss': 0.38, 'learning_rate': 3.355940716061064e-05, 'epoch': 0.61}
{'loss': 0.4303, 'learning_rate': 3.3536579706193224e-05, 'epoch': 0.61}
 61%|    | 3941/6500 [7:27:10<5:02:43,  7.10s/it]                                                        61%|    | 3941/6500 [7:27:10<5:02:43,  7.10s/it] 61%|    | 3942/6500 [7:27:17<4:55:16,  6.93s/it]                                                        61%|    | 3942/6500 [7:27:17<4:55:16,  6.93s/it] 61%|    | 3943/6500 [7:27:23<4:50:14,  6.81s/it]                                                        61%|    | 3943/6500 [7:27:23<4:50:14,  6.81s/it] 61%|    | 3944/6500 [7:27:30<4:54:32,  6.91s/it]                                                        61%|    | 3944/6500 [7:27:30<4:54:32,  6.91s/it] 61%|    | 3945/6500 [7:27:37<4:49:37,  6.80s/it]                                                        61%|    | 3945/6500 [7:27:37<4:49:37,  6.80s/it] 61%|    | 3946/6500 [7:27:43<4:46:09,  {'loss': 0.3888, 'learning_rate': 3.351375610000019e-05, 'epoch': 0.61}
{'loss': 0.6696, 'learning_rate': 3.349093634736644e-05, 'epoch': 0.61}
{'loss': 0.3988, 'learning_rate': 3.346812045362595e-05, 'epoch': 0.61}
{'loss': 0.3886, 'learning_rate': 3.344530842411178e-05, 'epoch': 0.61}
{'loss': 0.3831, 'learning_rate': 3.342250026415611e-05, 'epoch': 0.61}
6.72s/it]                                                        61%|    | 3946/6500 [7:27:43<4:46:09,  6.72s/it] 61%|    | 3947/6500 [7:27:50<4:43:55,  6.67s/it]                                                        61%|    | 3947/6500 [7:27:50<4:43:55,  6.67s/it] 61%|    | 3948/6500 [7:27:56<4:42:10,  6.63s/it]                                                        61%|    | 3948/6500 [7:27:56<4:42:10,  6.63s/it] 61%|    | 3949/6500 [7:28:03<4:40:51,  6.61s/it]                                                        61%|    | 3949/6500 [7:28:03<4:40:51,  6.61s/it] 61%|    | 3950/6500 [7:28:10<4:40:01,  6.59s/it]                                                        61%|    | 3950/6500 [7:28:10<4:40:01,  6.59s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8637489676475525, 'eval_runtime': 1.4911, 'eval_samples_per_second': 8.048, 'eval_steps_per_second': 2.012, 'epoch': 0.61}
                                                        61%|    | 3950/6500 [7:28:11<4:40:01,  6.59s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-3950
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3950
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3950
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3950
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3950
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3950/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3950/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3950/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3950/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3838, 'learning_rate': 3.339969597909021e-05, 'epoch': 0.61}
{'loss': 0.3904, 'learning_rate': 3.337689557424445e-05, 'epoch': 0.61}
{'loss': 0.3929, 'learning_rate': 3.335409905494828e-05, 'epoch': 0.61}
{'loss': 0.4061, 'learning_rate': 3.333130642653024e-05, 'epoch': 0.61}
{'loss': 0.3956, 'learning_rate': 3.330851769431798e-05, 'epoch': 0.61}
 61%|    | 3951/6500 [7:28:18<5:03:58,  7.16s/it]                                                        61%|    | 3951/6500 [7:28:18<5:03:58,  7.16s/it] 61%|    | 3952/6500 [7:28:25<4:56:02,  6.97s/it]                                                        61%|    | 3952/6500 [7:28:25<4:56:02,  6.97s/it] 61%|    | 3953/6500 [7:28:31<4:50:23,  6.84s/it]                                                        61%|    | 3953/6500 [7:28:31<4:50:23,  6.84s/it] 61%|    | 3954/6500 [7:28:38<4:46:32,  6.75s/it]                                                        61%|    | 3954/6500 [7:28:38<4:46:32,  6.75s/it] 61%|    | 3955/6500 [7:28:44<4:43:26,  6.68s/it]                                                        61%|    | 3955/6500 [7:28:44<4:43:26,  6.68s/it] 61%|    | 3956/6500 [7:28:51<4:41:30,  {'loss': 0.3948, 'learning_rate': 3.3285732863638215e-05, 'epoch': 0.61}
{'loss': 0.3961, 'learning_rate': 3.326295193981677e-05, 'epoch': 0.61}
{'loss': 0.3776, 'learning_rate': 3.3240174928178544e-05, 'epoch': 0.61}
{'loss': 0.385, 'learning_rate': 3.321740183404755e-05, 'epoch': 0.61}
{'loss': 0.4119, 'learning_rate': 3.319463266274682e-05, 'epoch': 0.61}
6.64s/it]                                                        61%|    | 3956/6500 [7:28:51<4:41:30,  6.64s/it] 61%|    | 3957/6500 [7:28:57<4:40:12,  6.61s/it]                                                        61%|    | 3957/6500 [7:28:57<4:40:12,  6.61s/it] 61%|    | 3958/6500 [7:29:04<4:39:00,  6.59s/it]                                                        61%|    | 3958/6500 [7:29:04<4:39:00,  6.59s/it] 61%|    | 3959/6500 [7:29:10<4:38:17,  6.57s/it]                                                        61%|    | 3959/6500 [7:29:10<4:38:17,  6.57s/it] 61%|    | 3960/6500 [7:29:18<4:49:09,  6.83s/it]                                                        61%|    | 3960/6500 [7:29:18<4:49:09,  6.83s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8731447458267212, 'eval_runtime': 1.4935, 'eval_samples_per_second': 8.035, 'eval_steps_per_second': 2.009, 'epoch': 0.61}
                                                        61%|    | 3960/6500 [7:29:19<4:49:09,  6.83s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-3960
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3960
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3960
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3960
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3960
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3960/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3960/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3960/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3960/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3965, 'learning_rate': 3.317186741959852e-05, 'epoch': 0.61}
{'loss': 0.6671, 'learning_rate': 3.31491061099239e-05, 'epoch': 0.61}
{'loss': 0.387, 'learning_rate': 3.312634873904327e-05, 'epoch': 0.61}
{'loss': 0.3922, 'learning_rate': 3.3103595312276035e-05, 'epoch': 0.61}
{'loss': 0.384, 'learning_rate': 3.3080845834940664e-05, 'epoch': 0.61}
 61%|    | 3961/6500 [7:29:26<5:08:23,  7.29s/it]                                                        61%|    | 3961/6500 [7:29:26<5:08:23,  7.29s/it] 61%|    | 3962/6500 [7:29:33<4:58:45,  7.06s/it]                                                        61%|    | 3962/6500 [7:29:33<4:58:45,  7.06s/it] 61%|    | 3963/6500 [7:29:39<4:52:02,  6.91s/it]                                                        61%|    | 3963/6500 [7:29:39<4:52:02,  6.91s/it] 61%|    | 3964/6500 [7:29:46<4:47:10,  6.79s/it]                                                        61%|    | 3964/6500 [7:29:46<4:47:10,  6.79s/it] 61%|    | 3965/6500 [7:29:52<4:43:47,  6.72s/it]                                                        61%|    | 3965/6500 [7:29:52<4:43:47,  6.72s/it] 61%|    | 3966/6500 [7:29:59<4:41:28,  {'loss': 0.3782, 'learning_rate': 3.305810031235471e-05, 'epoch': 0.61}
{'loss': 0.3828, 'learning_rate': 3.303535874983479e-05, 'epoch': 0.61}
{'loss': 0.386, 'learning_rate': 3.301262115269662e-05, 'epoch': 0.61}
{'loss': 0.399, 'learning_rate': 3.298988752625496e-05, 'epoch': 0.61}
{'loss': 0.394, 'learning_rate': 3.296715787582367e-05, 'epoch': 0.61}
6.66s/it]                                                        61%|    | 3966/6500 [7:29:59<4:41:28,  6.66s/it] 61%|    | 3967/6500 [7:30:05<4:39:49,  6.63s/it]                                                        61%|    | 3967/6500 [7:30:05<4:39:49,  6.63s/it] 61%|    | 3968/6500 [7:30:12<4:38:30,  6.60s/it]                                                        61%|    | 3968/6500 [7:30:12<4:38:30,  6.60s/it] 61%|    | 3969/6500 [7:30:18<4:37:36,  6.58s/it]                                                        61%|    | 3969/6500 [7:30:18<4:37:36,  6.58s/it] 61%|    | 3970/6500 [7:30:25<4:36:53,  6.57s/it]                                                        61%|    | 3970/6500 [7:30:25<4:36:53,  6.57s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8733716011047363, 'eval_runtime': 1.4829, 'eval_samples_per_second': 8.092, 'eval_steps_per_second': 2.023, 'epoch': 0.61}
                                                        61%|    | 3970/6500 [7:30:26<4:36:53,  6.57s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-3970
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3970
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3970
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3970
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3970
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3970/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3970/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3970/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3970/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.382, 'learning_rate': 3.2944432206715684e-05, 'epoch': 0.61}
{'loss': 0.3931, 'learning_rate': 3.2921710524242956e-05, 'epoch': 0.61}
{'loss': 0.3751, 'learning_rate': 3.289899283371657e-05, 'epoch': 0.61}
{'loss': 0.3903, 'learning_rate': 3.287627914044662e-05, 'epoch': 0.61}
{'loss': 0.4027, 'learning_rate': 3.28535694497423e-05, 'epoch': 0.61}
 61%|    | 3971/6500 [7:30:33<5:00:41,  7.13s/it]                                                        61%|    | 3971/6500 [7:30:33<5:00:41,  7.13s/it] 61%|    | 3972/6500 [7:30:40<4:52:59,  6.95s/it]                                                        61%|    | 3972/6500 [7:30:40<4:52:59,  6.95s/it] 61%|    | 3973/6500 [7:30:46<4:47:39,  6.83s/it]                                                        61%|    | 3973/6500 [7:30:46<4:47:39,  6.83s/it] 61%|    | 3974/6500 [7:30:53<4:43:48,  6.74s/it]                                                        61%|    | 3974/6500 [7:30:53<4:43:48,  6.74s/it] 61%|    | 3975/6500 [7:31:00<4:41:10,  6.68s/it]                                                        61%|    | 3975/6500 [7:31:00<4:41:10,  6.68s/it] 61%|    | 3976/6500 [7:31:07<4:50:36,  {'loss': 0.3999, 'learning_rate': 3.283086376691188e-05, 'epoch': 0.61}
{'loss': 0.668, 'learning_rate': 3.2808162097262664e-05, 'epoch': 0.61}
{'loss': 0.3862, 'learning_rate': 3.278546444610103e-05, 'epoch': 0.61}
{'loss': 0.397, 'learning_rate': 3.276277081873243e-05, 'epoch': 0.61}
{'loss': 0.37, 'learning_rate': 3.274008122046132e-05, 'epoch': 0.61}
6.91s/it]                                                        61%|    | 3976/6500 [7:31:07<4:50:36,  6.91s/it] 61%|    | 3977/6500 [7:31:14<4:45:50,  6.80s/it]                                                        61%|    | 3977/6500 [7:31:14<4:45:50,  6.80s/it] 61%|    | 3978/6500 [7:31:20<4:42:35,  6.72s/it]                                                        61%|    | 3978/6500 [7:31:20<4:42:35,  6.72s/it] 61%|    | 3979/6500 [7:31:27<4:40:02,  6.67s/it]                                                        61%|    | 3979/6500 [7:31:27<4:40:02,  6.67s/it] 61%|    | 3980/6500 [7:31:33<4:38:28,  6.63s/it]                                                        61%|    | 3980/6500 [7:31:33<4:38:28,  6.63s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8634915351867676, 'eval_runtime': 1.4835, 'eval_samples_per_second': 8.089, 'eval_steps_per_second': 2.022, 'epoch': 0.61}
                                                        61%|    | 3980/6500 [7:31:35<4:38:28,  6.63s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-3980
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3980
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3980
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3980
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3980
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3980/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3980/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3980/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3980/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3785, 'learning_rate': 3.271739565659129e-05, 'epoch': 0.61}
{'loss': 0.3882, 'learning_rate': 3.269471413242495e-05, 'epoch': 0.61}
{'loss': 0.3747, 'learning_rate': 3.267203665326396e-05, 'epoch': 0.61}
{'loss': 0.4018, 'learning_rate': 3.264936322440905e-05, 'epoch': 0.61}
{'loss': 0.4013, 'learning_rate': 3.262669385116001e-05, 'epoch': 0.61}
 61%|    | 3981/6500 [7:31:42<5:00:44,  7.16s/it]                                                        61%|    | 3981/6500 [7:31:42<5:00:44,  7.16s/it] 61%|   | 3982/6500 [7:31:48<4:52:57,  6.98s/it]                                                        61%|   | 3982/6500 [7:31:48<4:52:57,  6.98s/it] 61%|   | 3983/6500 [7:31:55<4:47:14,  6.85s/it]                                                        61%|   | 3983/6500 [7:31:55<4:47:14,  6.85s/it] 61%|   | 3984/6500 [7:32:01<4:43:20,  6.76s/it]                                                        61%|   | 3984/6500 [7:32:01<4:43:20,  6.76s/it] 61%|   | 3985/6500 [7:32:08<4:40:26,  6.69s/it]                                                        61%|   | 3985/6500 [7:32:08<4:40:26,  6.69s/it] 61%|   | 3986/6500 [{'loss': 0.3761, 'learning_rate': 3.260402853881562e-05, 'epoch': 0.61}
{'loss': 0.3933, 'learning_rate': 3.2581367292673806e-05, 'epoch': 0.61}
{'loss': 0.3748, 'learning_rate': 3.255871011803148e-05, 'epoch': 0.61}
{'loss': 0.4213, 'learning_rate': 3.253605702018461e-05, 'epoch': 0.61}
{'loss': 0.3755, 'learning_rate': 3.251340800442825e-05, 'epoch': 0.61}
7:32:14<4:38:27,  6.65s/it]                                                        61%|   | 3986/6500 [7:32:14<4:38:27,  6.65s/it] 61%|   | 3987/6500 [7:32:21<4:37:03,  6.61s/it]                                                        61%|   | 3987/6500 [7:32:21<4:37:03,  6.61s/it] 61%|   | 3988/6500 [7:32:27<4:35:51,  6.59s/it]                                                        61%|   | 3988/6500 [7:32:27<4:35:51,  6.59s/it] 61%|   | 3989/6500 [7:32:34<4:34:59,  6.57s/it]                                                        61%|   | 3989/6500 [7:32:34<4:34:59,  6.57s/it] 61%|   | 3990/6500 [7:32:40<4:34:20,  6.56s/it]                                                        61%|   | 3990/6500 [7:32:40<4:34:20,  6.56s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8721383213996887, 'eval_runtime': 1.4762, 'eval_samples_per_second': 8.129, 'eval_steps_per_second': 2.032, 'epoch': 0.61}
                                                        61%|   | 3990/6500 [7:32:42<4:34:20,  6.56s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-3990
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3990
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3990
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3990
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3990
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3990/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3990/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3990/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-3990/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.6669, 'learning_rate': 3.249076307605643e-05, 'epoch': 0.61}
{'loss': 0.4016, 'learning_rate': 3.2468122240362284e-05, 'epoch': 0.61}
{'loss': 0.3806, 'learning_rate': 3.2445485502637976e-05, 'epoch': 0.61}
{'loss': 0.401, 'learning_rate': 3.242285286817469e-05, 'epoch': 0.61}
{'loss': 0.3763, 'learning_rate': 3.240022434226268e-05, 'epoch': 0.61}
 61%|   | 3991/6500 [7:32:49<4:56:33,  7.09s/it]                                                        61%|   | 3991/6500 [7:32:49<4:56:33,  7.09s/it] 61%|   | 3992/6500 [7:32:56<5:00:46,  7.20s/it]                                                        61%|   | 3992/6500 [7:32:56<5:00:46,  7.20s/it] 61%|   | 3993/6500 [7:33:03<4:52:35,  7.00s/it]                                                        61%|   | 3993/6500 [7:33:03<4:52:35,  7.00s/it] 61%|   | 3994/6500 [7:33:09<4:46:37,  6.86s/it]                                                        61%|   | 3994/6500 [7:33:09<4:46:37,  6.86s/it] 61%|   | 3995/6500 [7:33:16<4:42:28,  6.77s/it]                                                        61%|   | 3995/6500 [7:33:16<4:42:28,  6.77s/it] 61%|   | 3996/65{'loss': 0.3861, 'learning_rate': 3.2377599930191224e-05, 'epoch': 0.61}
{'loss': 0.3923, 'learning_rate': 3.2354979637248636e-05, 'epoch': 0.61}
{'loss': 0.3955, 'learning_rate': 3.233236346872227e-05, 'epoch': 0.62}
{'loss': 0.3888, 'learning_rate': 3.230975142989853e-05, 'epoch': 0.62}
{'loss': 0.3908, 'learning_rate': 3.2287143526062825e-05, 'epoch': 0.62}
00 [7:33:22<4:39:38,  6.70s/it]                                                        61%|   | 3996/6500 [7:33:22<4:39:38,  6.70s/it] 61%|   | 3997/6500 [7:33:29<4:37:30,  6.65s/it]                                                        61%|   | 3997/6500 [7:33:29<4:37:30,  6.65s/it] 62%|   | 3998/6500 [7:33:35<4:35:57,  6.62s/it]                                                        62%|   | 3998/6500 [7:33:35<4:35:57,  6.62s/it] 62%|   | 3999/6500 [7:33:42<4:34:59,  6.60s/it]                                                        62%|   | 3999/6500 [7:33:42<4:34:59,  6.60s/it] 62%|   | 4000/6500 [7:33:49<4:34:05,  6.58s/it]                                                        62%|   | 4000/6500 [7:33:49<4:34:05,  6.58s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8738662004470825, 'eval_runtime': 1.4796, 'eval_samples_per_second': 8.11, 'eval_steps_per_second': 2.028, 'epoch': 0.62}
                                                        62%|   | 4000/6500 [7:33:50<4:34:05,  6.58s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-4000
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4000
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4000
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4000
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4000
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4000/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4000/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4000/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4000/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3863, 'learning_rate': 3.2264539762499644e-05, 'epoch': 0.62}
{'loss': 0.3843, 'learning_rate': 3.224194014449245e-05, 'epoch': 0.62}
{'loss': 0.3716, 'learning_rate': 3.221934467732377e-05, 'epoch': 0.62}
{'loss': 0.422, 'learning_rate': 3.219675336627516e-05, 'epoch': 0.62}
{'loss': 0.3844, 'learning_rate': 3.2174166216627214e-05, 'epoch': 0.62}
 62%|   | 4001/6500 [7:33:57<4:56:01,  7.11s/it]                                                        62%|   | 4001/6500 [7:33:57<4:56:01,  7.11s/it] 62%|   | 4002/6500 [7:34:03<4:48:48,  6.94s/it]                                                        62%|   | 4002/6500 [7:34:03<4:48:48,  6.94s/it] 62%|   | 4003/6500 [7:34:10<4:43:41,  6.82s/it]                                                        62%|   | 4003/6500 [7:34:10<4:43:41,  6.82s/it] 62%|   | 4004/6500 [7:34:16<4:40:01,  6.73s/it]                                                        62%|   | 4004/6500 [7:34:16<4:40:01,  6.73s/it] 62%|   | 4005/6500 [7:34:23<4:37:27,  6.67s/it]                                                        62%|   | 4005/6500 [7:34:23<4:37:27,  6.67s/it] 62%|   | 4006/65{'loss': 0.6599, 'learning_rate': 3.2151583233659526e-05, 'epoch': 0.62}
{'loss': 0.4007, 'learning_rate': 3.212900442265075e-05, 'epoch': 0.62}
{'loss': 0.3735, 'learning_rate': 3.2106429788878525e-05, 'epoch': 0.62}
{'loss': 0.397, 'learning_rate': 3.2083859337619534e-05, 'epoch': 0.62}
{'loss': 0.3751, 'learning_rate': 3.20612930741495e-05, 'epoch': 0.62}
00 [7:34:30<4:35:33,  6.63s/it]                                                        62%|   | 4006/6500 [7:34:30<4:35:33,  6.63s/it] 62%|   | 4007/6500 [7:34:36<4:34:35,  6.61s/it]                                                        62%|   | 4007/6500 [7:34:36<4:34:35,  6.61s/it] 62%|   | 4008/6500 [7:34:44<4:44:53,  6.86s/it]                                                        62%|   | 4008/6500 [7:34:44<4:44:53,  6.86s/it] 62%|   | 4009/6500 [7:34:50<4:40:51,  6.76s/it]                                                        62%|   | 4009/6500 [7:34:50<4:40:51,  6.76s/it] 62%|   | 4010/6500 [7:34:57<4:37:56,  6.70s/it]                                                        62%|   | 4010/6500 [7:34:57<4:37:56,  6.70s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8649641871452332, 'eval_runtime': 1.4841, 'eval_samples_per_second': 8.085, 'eval_steps_per_second': 2.021, 'epoch': 0.62}
                                                        62%|   | 4010/6500 [7:34:58<4:37:56,  6.70s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-4010
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4010I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4010

I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4010
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4010
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4010/pytorch_model.bin
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4010/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4010/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4010/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3812, 'learning_rate': 3.203873100374314e-05, 'epoch': 0.62}
{'loss': 0.3875, 'learning_rate': 3.201617313167421e-05, 'epoch': 0.62}
{'loss': 0.3947, 'learning_rate': 3.1993619463215454e-05, 'epoch': 0.62}
{'loss': 0.3844, 'learning_rate': 3.197107000363867e-05, 'epoch': 0.62}
{'loss': 0.3924, 'learning_rate': 3.194852475821465e-05, 'epoch': 0.62}
 62%|   | 4011/6500 [7:35:05<5:00:15,  7.24s/it]                                                        62%|   | 4011/6500 [7:35:05<5:00:15,  7.24s/it] 62%|   | 4012/6500 [7:35:12<4:51:33,  7.03s/it]                                                        62%|   | 4012/6500 [7:35:12<4:51:33,  7.03s/it] 62%|   | 4013/6500 [7:35:18<4:45:30,  6.89s/it]                                                        62%|   | 4013/6500 [7:35:18<4:45:30,  6.89s/it] 62%|   | 4014/6500 [7:35:25<4:41:33,  6.80s/it]                                                        62%|   | 4014/6500 [7:35:25<4:41:33,  6.80s/it] 62%|   | 4015/6500 [7:35:31<4:38:27,  6.72s/it]                                                        62%|   | 4015/6500 [7:35:31<4:38:27,  6.72s/it] 62%|   | 4016/65{'loss': 0.3857, 'learning_rate': 3.192598373221322e-05, 'epoch': 0.62}
{'loss': 0.3885, 'learning_rate': 3.1903446930903205e-05, 'epoch': 0.62}
{'loss': 0.3712, 'learning_rate': 3.188091435955244e-05, 'epoch': 0.62}
{'loss': 0.429, 'learning_rate': 3.1858386023427774e-05, 'epoch': 0.62}
{'loss': 0.3875, 'learning_rate': 3.183586192779507e-05, 'epoch': 0.62}
00 [7:35:38<4:36:23,  6.68s/it]                                                        62%|   | 4016/6500 [7:35:38<4:36:23,  6.68s/it] 62%|   | 4017/6500 [7:35:44<4:34:34,  6.63s/it]                                                        62%|   | 4017/6500 [7:35:44<4:34:34,  6.63s/it] 62%|   | 4018/6500 [7:35:51<4:33:26,  6.61s/it]                                                        62%|   | 4018/6500 [7:35:51<4:33:26,  6.61s/it] 62%|   | 4019/6500 [7:35:58<4:32:40,  6.59s/it]                                                        62%|   | 4019/6500 [7:35:58<4:32:40,  6.59s/it] 62%|   | 4020/6500 [7:36:04<4:32:03,  6.58s/it]                                                        62%|   | 4020/6500 [7:36:04<4:32:03,  6.58s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8707565665245056, 'eval_runtime': 1.5435, 'eval_samples_per_second': 7.774, 'eval_steps_per_second': 1.944, 'epoch': 0.62}
                                                        62%|   | 4020/6500 [7:36:06<4:32:03,  6.58s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-4020
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4020
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4020
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4020
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4020
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4020/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4020/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4020/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4020/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.6669, 'learning_rate': 3.18133420779192e-05, 'epoch': 0.62}
{'loss': 0.4009, 'learning_rate': 3.1790826479064046e-05, 'epoch': 0.62}
{'loss': 0.3847, 'learning_rate': 3.176831513649249e-05, 'epoch': 0.62}
{'loss': 0.3788, 'learning_rate': 3.174580805546642e-05, 'epoch': 0.62}
{'loss': 0.3776, 'learning_rate': 3.172330524124673e-05, 'epoch': 0.62}
 62%|   | 4021/6500 [7:36:13<4:55:46,  7.16s/it]                                                        62%|   | 4021/6500 [7:36:13<4:55:46,  7.16s/it] 62%|   | 4022/6500 [7:36:19<4:48:01,  6.97s/it]                                                        62%|   | 4022/6500 [7:36:19<4:48:01,  6.97s/it] 62%|   | 4023/6500 [7:36:26<4:42:36,  6.85s/it]                                                        62%|   | 4023/6500 [7:36:26<4:42:36,  6.85s/it] 62%|   | 4024/6500 [7:36:33<4:47:14,  6.96s/it]                                                        62%|   | 4024/6500 [7:36:33<4:47:14,  6.96s/it] 62%|   | 4025/6500 [7:36:40<4:42:04,  6.84s/it]                                                        62%|   | 4025/6500 [7:36:40<4:42:04,  6.84s/it] 62%|   | 4026/65{'loss': 0.386, 'learning_rate': 3.170080669909331e-05, 'epoch': 0.62}
{'loss': 0.3823, 'learning_rate': 3.167831243426507e-05, 'epoch': 0.62}
{'loss': 0.3994, 'learning_rate': 3.165582245201989e-05, 'epoch': 0.62}
{'loss': 0.395, 'learning_rate': 3.1633336757614694e-05, 'epoch': 0.62}
{'loss': 0.3914, 'learning_rate': 3.1610855356305354e-05, 'epoch': 0.62}
00 [7:36:46<4:38:34,  6.76s/it]                                                        62%|   | 4026/6500 [7:36:46<4:38:34,  6.76s/it] 62%|   | 4027/6500 [7:36:53<4:35:58,  6.70s/it]                                                        62%|   | 4027/6500 [7:36:53<4:35:58,  6.70s/it] 62%|   | 4028/6500 [7:36:59<4:34:17,  6.66s/it]                                                        62%|   | 4028/6500 [7:36:59<4:34:17,  6.66s/it] 62%|   | 4029/6500 [7:37:06<4:32:46,  6.62s/it]                                                        62%|   | 4029/6500 [7:37:06<4:32:46,  6.62s/it] 62%|   | 4030/6500 [7:37:12<4:31:52,  6.60s/it]                                                        62%|   | 4030/6500 [7:37:12<4:31:52,  6.60s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8745225667953491, 'eval_runtime': 1.4982, 'eval_samples_per_second': 8.01, 'eval_steps_per_second': 2.002, 'epoch': 0.62}
                                                        62%|   | 4030/6500 [7:37:14<4:31:52,  6.60s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-4030
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4030
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4030
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4030
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4030
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4030/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4030/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4030/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4030/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3957, 'learning_rate': 3.158837825334676e-05, 'epoch': 0.62}
{'loss': 0.3834, 'learning_rate': 3.15659054539928e-05, 'epoch': 0.62}
{'loss': 0.3822, 'learning_rate': 3.154343696349638e-05, 'epoch': 0.62}
{'loss': 0.4173, 'learning_rate': 3.152097278710933e-05, 'epoch': 0.62}
{'loss': 0.3912, 'learning_rate': 3.149851293008256e-05, 'epoch': 0.62}
 62%|   | 4031/6500 [7:37:21<4:54:28,  7.16s/it]                                                        62%|   | 4031/6500 [7:37:21<4:54:28,  7.16s/it] 62%|   | 4032/6500 [7:37:27<4:46:38,  6.97s/it]                                                        62%|   | 4032/6500 [7:37:27<4:46:38,  6.97s/it] 62%|   | 4033/6500 [7:37:34<4:41:17,  6.84s/it]                                                        62%|   | 4033/6500 [7:37:34<4:41:17,  6.84s/it] 62%|   | 4034/6500 [7:37:41<4:40:41,  6.83s/it]                                                        62%|   | 4034/6500 [7:37:41<4:40:41,  6.83s/it] 62%|   | 4035/6500 [7:37:47<4:37:13,  6.75s/it]                                                        62%|   | 4035/6500 [7:37:47<4:37:13,  6.75s/it] 62%|   | 4036/65{'loss': 0.6704, 'learning_rate': 3.147605739766588e-05, 'epoch': 0.62}
{'loss': 0.3894, 'learning_rate': 3.145360619510817e-05, 'epoch': 0.62}
{'loss': 0.3966, 'learning_rate': 3.143115932765723e-05, 'epoch': 0.62}
{'loss': 0.3771, 'learning_rate': 3.140871680055991e-05, 'epoch': 0.62}
{'loss': 0.3838, 'learning_rate': 3.1386278619062006e-05, 'epoch': 0.62}
00 [7:37:54<4:34:35,  6.69s/it]                                                        62%|   | 4036/6500 [7:37:54<4:34:35,  6.69s/it] 62%|   | 4037/6500 [7:38:00<4:32:49,  6.65s/it]                                                        62%|   | 4037/6500 [7:38:00<4:32:49,  6.65s/it] 62%|   | 4038/6500 [7:38:07<4:31:26,  6.62s/it]                                                        62%|   | 4038/6500 [7:38:07<4:31:26,  6.62s/it] 62%|   | 4039/6500 [7:38:13<4:30:36,  6.60s/it]                                                        62%|   | 4039/6500 [7:38:13<4:30:36,  6.60s/it] 62%|   | 4040/6500 [7:38:20<4:29:52,  6.58s/it]                                                        62%|   | 4040/6500 [7:38:20<4:29:52,  6.58s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8641484379768372, 'eval_runtime': 1.4923, 'eval_samples_per_second': 8.041, 'eval_steps_per_second': 2.01, 'epoch': 0.62}
                                                        62%|   | 4040/6500 [7:38:21<4:29:52,  6.58s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-4040
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4040
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4040
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4040
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4040
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4040/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4040/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4040/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4040/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3794, 'learning_rate': 3.13638447884083e-05, 'epoch': 0.62}
{'loss': 0.3949, 'learning_rate': 3.134141531384256e-05, 'epoch': 0.62}
{'loss': 0.4058, 'learning_rate': 3.131899020060754e-05, 'epoch': 0.62}
{'loss': 0.3974, 'learning_rate': 3.1296569453944977e-05, 'epoch': 0.62}
{'loss': 0.384, 'learning_rate': 3.127415307909558e-05, 'epoch': 0.62}
 62%|   | 4041/6500 [7:38:29<5:04:48,  7.44s/it]                                                        62%|   | 4041/6500 [7:38:29<5:04:48,  7.44s/it] 62%|   | 4042/6500 [7:38:36<4:53:34,  7.17s/it]                                                        62%|   | 4042/6500 [7:38:36<4:53:34,  7.17s/it] 62%|   | 4043/6500 [7:38:42<4:45:50,  6.98s/it]                                                        62%|   | 4043/6500 [7:38:42<4:45:50,  6.98s/it] 62%|   | 4044/6500 [7:38:49<4:40:18,  6.85s/it]                                                        62%|   | 4044/6500 [7:38:49<4:40:18,  6.85s/it] 62%|   | 4045/6500 [7:38:56<4:36:28,  6.76s/it]                                                        62%|   | 4045/6500 [7:38:56<4:36:28,  6.76s/it] 62%|   | 4046/65{'loss': 0.3989, 'learning_rate': 3.125174108129906e-05, 'epoch': 0.62}
{'loss': 0.372, 'learning_rate': 3.122933346579406e-05, 'epoch': 0.62}
{'loss': 0.388, 'learning_rate': 3.1206930237818245e-05, 'epoch': 0.62}
{'loss': 0.4092, 'learning_rate': 3.118453140260823e-05, 'epoch': 0.62}
{'loss': 0.3902, 'learning_rate': 3.116213696539959e-05, 'epoch': 0.62}
00 [7:39:02<4:33:30,  6.69s/it]                                                        62%|   | 4046/6500 [7:39:02<4:33:30,  6.69s/it] 62%|   | 4047/6500 [7:39:09<4:31:18,  6.64s/it]                                                        62%|   | 4047/6500 [7:39:09<4:31:18,  6.64s/it] 62%|   | 4048/6500 [7:39:15<4:29:59,  6.61s/it]                                                        62%|   | 4048/6500 [7:39:15<4:29:59,  6.61s/it] 62%|   | 4049/6500 [7:39:22<4:28:53,  6.58s/it]                                                        62%|   | 4049/6500 [7:39:22<4:28:53,  6.58s/it] 62%|   | 4050/6500 [7:39:28<4:28:09,  6.57s/it]                                                        62%|   | 4050/6500 [7:39:28<4:28:09,  6.57s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8717318773269653, 'eval_runtime': 1.4813, 'eval_samples_per_second': 8.101, 'eval_steps_per_second': 2.025, 'epoch': 0.62}
                                                        62%|   | 4050/6500 [7:39:30<4:28:09,  6.57s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-4050
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4050
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4050
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4050
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4050
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4050/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4050/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4050/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4050/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.665, 'learning_rate': 3.1139746931426894e-05, 'epoch': 0.62}
{'loss': 0.38, 'learning_rate': 3.1117361305923684e-05, 'epoch': 0.62}
{'loss': 0.4011, 'learning_rate': 3.109498009412246e-05, 'epoch': 0.62}
{'loss': 0.3724, 'learning_rate': 3.10726033012547e-05, 'epoch': 0.62}
{'loss': 0.3762, 'learning_rate': 3.105023093255084e-05, 'epoch': 0.62}
 62%|   | 4051/6500 [7:39:37<4:50:01,  7.11s/it]                                                        62%|   | 4051/6500 [7:39:37<4:50:01,  7.11s/it] 62%|   | 4052/6500 [7:39:43<4:42:56,  6.93s/it]                                                        62%|   | 4052/6500 [7:39:43<4:42:56,  6.93s/it] 62%|   | 4053/6500 [7:39:50<4:37:58,  6.82s/it]                                                        62%|   | 4053/6500 [7:39:50<4:37:58,  6.82s/it] 62%|   | 4054/6500 [7:39:56<4:34:27,  6.73s/it]                                                        62%|   | 4054/6500 [7:39:56<4:34:27,  6.73s/it] 62%|   | 4055/6500 [7:40:03<4:31:51,  6.67s/it]                                                        62%|   | 4055/6500 [7:40:03<4:31:51,  6.67s/it] 62%|   | 4056/65{'loss': 0.3882, 'learning_rate': 3.102786299324028e-05, 'epoch': 0.62}
{'loss': 0.3874, 'learning_rate': 3.100549948855138e-05, 'epoch': 0.62}
{'loss': 0.3927, 'learning_rate': 3.0983140423711495e-05, 'epoch': 0.62}
{'loss': 0.3981, 'learning_rate': 3.096078580394691e-05, 'epoch': 0.62}
{'loss': 0.3799, 'learning_rate': 3.09384356344829e-05, 'epoch': 0.62}
00 [7:40:09<4:30:03,  6.63s/it]                                                        62%|   | 4056/6500 [7:40:09<4:30:03,  6.63s/it] 62%|   | 4057/6500 [7:40:17<4:40:58,  6.90s/it]                                                        62%|   | 4057/6500 [7:40:17<4:40:58,  6.90s/it] 62%|   | 4058/6500 [7:40:23<4:36:25,  6.79s/it]                                                        62%|   | 4058/6500 [7:40:23<4:36:25,  6.79s/it] 62%|   | 4059/6500 [7:40:30<4:33:11,  6.72s/it]                                                        62%|   | 4059/6500 [7:40:30<4:33:11,  6.72s/it] 62%|   | 4060/6500 [7:40:36<4:30:49,  6.66s/it]                                                        62%|   | 4060/6500 [7:40:36<4:30:49,  6.66s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8762562870979309, 'eval_runtime': 1.4826, 'eval_samples_per_second': 8.094, 'eval_steps_per_second': 2.023, 'epoch': 0.62}
                                                        62%|   | 4060/6500 [7:40:38<4:30:49,  6.66s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-4060
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4060I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4060

I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4060
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4060
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4060/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4060/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4060/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4060/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3872, 'learning_rate': 3.091608992054365e-05, 'epoch': 0.62}
{'loss': 0.3721, 'learning_rate': 3.089374866735234e-05, 'epoch': 0.62}
{'loss': 0.4164, 'learning_rate': 3.0871411880131126e-05, 'epoch': 0.63}
{'loss': 0.3768, 'learning_rate': 3.084907956410107e-05, 'epoch': 0.63}
{'loss': 0.6588, 'learning_rate': 3.082675172448223e-05, 'epoch': 0.63}
 62%|   | 4061/6500 [7:40:45<4:53:21,  7.22s/it]                                                        62%|   | 4061/6500 [7:40:45<4:53:21,  7.22s/it] 62%|   | 4062/6500 [7:40:51<4:44:50,  7.01s/it]                                                        62%|   | 4062/6500 [7:40:51<4:44:50,  7.01s/it] 63%|   | 4063/6500 [7:40:58<4:38:56,  6.87s/it]                                                        63%|   | 4063/6500 [7:40:58<4:38:56,  6.87s/it] 63%|   | 4064/6500 [7:41:04<4:34:55,  6.77s/it]                                                        63%|   | 4064/6500 [7:41:04<4:34:55,  6.77s/it] 63%|   | 4065/6500 [7:41:11<4:32:51,  6.72s/it]                                                        63%|   | 4065/6500 [7:41:11<4:32:51,  6.72s/it] 63%|   | 4066/65{'loss': 0.4011, 'learning_rate': 3.080442836649361e-05, 'epoch': 0.63}
{'loss': 0.3818, 'learning_rate': 3.078210949535314e-05, 'epoch': 0.63}
{'loss': 0.3985, 'learning_rate': 3.0759795116277725e-05, 'epoch': 0.63}
{'loss': 0.3706, 'learning_rate': 3.0737485234483223e-05, 'epoch': 0.63}
{'loss': 0.3747, 'learning_rate': 3.071517985518442e-05, 'epoch': 0.63}
00 [7:41:18<4:32:58,  6.73s/it]                                                        63%|   | 4066/6500 [7:41:19<4:32:58,  6.73s/it] 63%|   | 4067/6500 [7:41:26<4:54:19,  7.26s/it]                                                        63%|   | 4067/6500 [7:41:26<4:54:19,  7.26s/it] 63%|   | 4068/6500 [7:41:33<4:46:59,  7.08s/it]                                                        63%|   | 4068/6500 [7:41:33<4:46:59,  7.08s/it] 63%|   | 4069/6500 [7:41:40<4:40:20,  6.92s/it]                                                        63%|   | 4069/6500 [7:41:40<4:40:20,  6.92s/it] 63%|   | 4070/6500 [7:41:46<4:35:28,  6.80s/it]                                                        63%|   | 4070/6500 [7:41:46<4:35:28,  6.80s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8633062243461609, 'eval_runtime': 1.8248, 'eval_samples_per_second': 6.576, 'eval_steps_per_second': 1.644, 'epoch': 0.63}
                                                        63%|   | 4070/6500 [7:41:48<4:35:28,  6.80s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-4070
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4070I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4070

I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4070
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4070
the pytorch model path isthe pytorch model path is  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4070/pytorch_model.bin/projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4070/pytorch_model.bin

the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4070/pytorch_model.binthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4070/pytorch_model.bin

/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3829, 'learning_rate': 3.069287898359509e-05, 'epoch': 0.63}
{'loss': 0.3798, 'learning_rate': 3.06705826249279e-05, 'epoch': 0.63}
{'loss': 0.395, 'learning_rate': 3.06482907843945e-05, 'epoch': 0.63}
{'loss': 0.3944, 'learning_rate': 3.062600346720545e-05, 'epoch': 0.63}
{'loss': 0.3842, 'learning_rate': 3.060372067857031e-05, 'epoch': 0.63}
 63%|   | 4071/6500 [7:41:55<4:59:28,  7.40s/it]                                                        63%|   | 4071/6500 [7:41:55<4:59:28,  7.40s/it] 63%|   | 4072/6500 [7:42:01<4:48:52,  7.14s/it]                                                        63%|   | 4072/6500 [7:42:01<4:48:52,  7.14s/it] 63%|   | 4073/6500 [7:42:10<5:02:29,  7.48s/it]                                                        63%|   | 4073/6500 [7:42:10<5:02:29,  7.48s/it] 63%|   | 4074/6500 [7:42:16<4:51:05,  7.20s/it]                                                        63%|   | 4074/6500 [7:42:16<4:51:05,  7.20s/it] 63%|   | 4075/6500 [7:42:23<4:42:59,  7.00s/it]                                                        63%|   | 4075/6500 [7:42:23<4:42:59,  7.00s/it] 63%|   | 4076/65{'loss': 0.3911, 'learning_rate': 3.058144242369753e-05, 'epoch': 0.63}
{'loss': 0.3755, 'learning_rate': 3.055916870779453e-05, 'epoch': 0.63}
{'loss': 0.4194, 'learning_rate': 3.053689953606762e-05, 'epoch': 0.63}
{'loss': 0.3836, 'learning_rate': 3.051463491372211e-05, 'epoch': 0.63}
{'loss': 0.6615, 'learning_rate': 3.0492374845962225e-05, 'epoch': 0.63}
00 [7:42:29<4:37:25,  6.87s/it]                                                        63%|   | 4076/6500 [7:42:29<4:37:25,  6.87s/it] 63%|   | 4077/6500 [7:42:36<4:33:20,  6.77s/it]                                                        63%|   | 4077/6500 [7:42:36<4:33:20,  6.77s/it] 63%|   | 4078/6500 [7:42:42<4:30:43,  6.71s/it]                                                        63%|   | 4078/6500 [7:42:42<4:30:43,  6.71s/it] 63%|   | 4079/6500 [7:42:49<4:28:43,  6.66s/it]                                                        63%|   | 4079/6500 [7:42:49<4:28:43,  6.66s/it] 63%|   | 4080/6500 [7:42:55<4:27:06,  6.62s/it]                                                        63%|   | 4080/6500 [7:42:55<4:27:06,  6.62s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8719143867492676, 'eval_runtime': 1.4878, 'eval_samples_per_second': 8.066, 'eval_steps_per_second': 2.016, 'epoch': 0.63}
                                                        63%|   | 4080/6500 [7:42:57<4:27:06,  6.62s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-4080
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4080I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4080

I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4080
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4080
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4080/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4080/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4080/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4080/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.4018, 'learning_rate': 3.04701193379911e-05, 'epoch': 0.63}
{'loss': 0.381, 'learning_rate': 3.0447868395010836e-05, 'epoch': 0.63}
{'loss': 0.3947, 'learning_rate': 3.0425622022222478e-05, 'epoch': 0.63}
{'loss': 0.3775, 'learning_rate': 3.040338022482594e-05, 'epoch': 0.63}
{'loss': 0.3823, 'learning_rate': 3.038114300802012e-05, 'epoch': 0.63}
 63%|   | 4081/6500 [7:43:04<4:48:22,  7.15s/it]                                                        63%|   | 4081/6500 [7:43:04<4:48:22,  7.15s/it] 63%|   | 4082/6500 [7:43:10<4:40:46,  6.97s/it]                                                        63%|   | 4082/6500 [7:43:10<4:40:46,  6.97s/it] 63%|   | 4083/6500 [7:43:17<4:35:36,  6.84s/it]                                                        63%|   | 4083/6500 [7:43:17<4:35:36,  6.84s/it] 63%|   | 4084/6500 [7:43:23<4:31:54,  6.75s/it]                                                        63%|   | 4084/6500 [7:43:23<4:31:54,  6.75s/it] 63%|   | 4085/6500 [7:43:30<4:29:25,  6.69s/it]                                                        63%|   | 4085/6500 [7:43:30<4:29:25,  6.69s/it] 63%|   | 4086/65{'loss': 0.389, 'learning_rate': 3.0358910377002848e-05, 'epoch': 0.63}
{'loss': 0.3928, 'learning_rate': 3.0336682336970846e-05, 'epoch': 0.63}
{'loss': 0.383, 'learning_rate': 3.0314458893119808e-05, 'epoch': 0.63}
{'loss': 0.3887, 'learning_rate': 3.0292240050644304e-05, 'epoch': 0.63}
{'loss': 0.3804, 'learning_rate': 3.0270025814737856e-05, 'epoch': 0.63}
00 [7:43:37<4:27:27,  6.65s/it]                                                        63%|   | 4086/6500 [7:43:37<4:27:27,  6.65s/it] 63%|   | 4087/6500 [7:43:43<4:26:08,  6.62s/it]                                                        63%|   | 4087/6500 [7:43:43<4:26:08,  6.62s/it] 63%|   | 4088/6500 [7:43:50<4:25:12,  6.60s/it]                                                        63%|   | 4088/6500 [7:43:50<4:25:12,  6.60s/it] 63%|   | 4089/6500 [7:43:57<4:31:50,  6.77s/it]                                                        63%|   | 4089/6500 [7:43:57<4:31:50,  6.77s/it] 63%|   | 4090/6500 [7:44:03<4:29:02,  6.70s/it]                                                        63%|   | 4090/6500 [7:44:03<4:29:02,  6.70s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.874717116355896, 'eval_runtime': 1.5066, 'eval_samples_per_second': 7.965, 'eval_steps_per_second': 1.991, 'epoch': 0.63}
                                                        63%|   | 4090/6500 [7:44:05<4:29:02,  6.70s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-4090
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4090
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4090
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4090
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4090
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4090/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4090/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4090/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4090/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3858, 'learning_rate': 3.024781619059292e-05, 'epoch': 0.63}
{'loss': 0.3726, 'learning_rate': 3.0225611183400855e-05, 'epoch': 0.63}
{'loss': 0.4182, 'learning_rate': 3.0203410798351945e-05, 'epoch': 0.63}
{'loss': 0.3855, 'learning_rate': 3.0181215040635402e-05, 'epoch': 0.63}
{'loss': 0.6557, 'learning_rate': 3.0159023915439338e-05, 'epoch': 0.63}
 63%|   | 4091/6500 [7:44:12<4:50:06,  7.23s/it]                                                        63%|   | 4091/6500 [7:44:12<4:50:06,  7.23s/it] 63%|   | 4092/6500 [7:44:18<4:41:51,  7.02s/it]                                                        63%|   | 4092/6500 [7:44:18<4:41:51,  7.02s/it] 63%|   | 4093/6500 [7:44:25<4:35:50,  6.88s/it]                                                        63%|   | 4093/6500 [7:44:25<4:35:50,  6.88s/it] 63%|   | 4094/6500 [7:44:31<4:31:45,  6.78s/it]                                                        63%|   | 4094/6500 [7:44:31<4:31:45,  6.78s/it] 63%|   | 4095/6500 [7:44:38<4:28:48,  6.71s/it]                                                        63%|   | 4095/6500 [7:44:38<4:28:48,  6.71s/it] 63%|   | 4096/65{'loss': 0.3971, 'learning_rate': 3.0136837427950793e-05, 'epoch': 0.63}
{'loss': 0.3852, 'learning_rate': 3.0114655583355733e-05, 'epoch': 0.63}
{'loss': 0.3807, 'learning_rate': 3.009247838683903e-05, 'epoch': 0.63}
{'loss': 0.384, 'learning_rate': 3.007030584358447e-05, 'epoch': 0.63}
{'loss': 0.3871, 'learning_rate': 3.004813795877473e-05, 'epoch': 0.63}
00 [7:44:45<4:26:51,  6.66s/it]                                                        63%|   | 4096/6500 [7:44:45<4:26:51,  6.66s/it] 63%|   | 4097/6500 [7:44:51<4:25:21,  6.63s/it]                                                        63%|   | 4097/6500 [7:44:51<4:25:21,  6.63s/it] 63%|   | 4098/6500 [7:44:58<4:24:16,  6.60s/it]                                                        63%|   | 4098/6500 [7:44:58<4:24:16,  6.60s/it] 63%|   | 4099/6500 [7:45:04<4:23:31,  6.59s/it]                                                        63%|   | 4099/6500 [7:45:04<4:23:31,  6.59s/it] 63%|   | 4100/6500 [7:45:11<4:22:58,  6.57s/it]                                                        63%|   | 4100/6500 [7:45:11<4:22:58,  6.57s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8647075891494751, 'eval_runtime': 1.7284, 'eval_samples_per_second': 6.943, 'eval_steps_per_second': 1.736, 'epoch': 0.63}
                                                        63%|   | 4100/6500 [7:45:12<4:22:58,  6.57s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-4100
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4100
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4100
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4100
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4100
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4100/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4100/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4100/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4100/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3878, 'learning_rate': 3.0025974737591434e-05, 'epoch': 0.63}
{'loss': 0.393, 'learning_rate': 3.0003816185215107e-05, 'epoch': 0.63}
{'loss': 0.3886, 'learning_rate': 2.9981662306825163e-05, 'epoch': 0.63}
{'loss': 0.3895, 'learning_rate': 2.995951310759994e-05, 'epoch': 0.63}
{'loss': 0.3925, 'learning_rate': 2.993736859271669e-05, 'epoch': 0.63}
 63%|   | 4101/6500 [7:45:19<4:48:21,  7.21s/it]                                                        63%|   | 4101/6500 [7:45:19<4:48:21,  7.21s/it] 63%|   | 4102/6500 [7:45:26<4:40:26,  7.02s/it]                                                        63%|   | 4102/6500 [7:45:26<4:40:26,  7.02s/it] 63%|   | 4103/6500 [7:45:33<4:34:41,  6.88s/it]                                                        63%|   | 4103/6500 [7:45:33<4:34:41,  6.88s/it] 63%|   | 4104/6500 [7:45:39<4:30:44,  6.78s/it]                                                        63%|   | 4104/6500 [7:45:39<4:30:44,  6.78s/it] 63%|   | 4105/6500 [7:45:46<4:35:21,  6.90s/it]                                                        63%|   | 4105/6500 [7:45:46<4:35:21,  6.90s/it] 63%|   | 4106/65{'loss': 0.3729, 'learning_rate': 2.991522876735154e-05, 'epoch': 0.63}
{'loss': 0.371, 'learning_rate': 2.9893093636679546e-05, 'epoch': 0.63}
{'loss': 0.4136, 'learning_rate': 2.987096320587467e-05, 'epoch': 0.63}
{'loss': 0.3951, 'learning_rate': 2.984883748010975e-05, 'epoch': 0.63}
{'loss': 0.6628, 'learning_rate': 2.982671646455655e-05, 'epoch': 0.63}
00 [7:45:53<4:31:05,  6.79s/it]                                                        63%|   | 4106/6500 [7:45:53<4:31:05,  6.79s/it] 63%|   | 4107/6500 [7:45:59<4:28:12,  6.72s/it]                                                        63%|   | 4107/6500 [7:45:59<4:28:12,  6.72s/it] 63%|   | 4108/6500 [7:46:06<4:25:56,  6.67s/it]                                                        63%|   | 4108/6500 [7:46:06<4:25:56,  6.67s/it] 63%|   | 4109/6500 [7:46:12<4:24:24,  6.63s/it]                                                        63%|   | 4109/6500 [7:46:12<4:24:24,  6.63s/it] 63%|   | 4110/6500 [7:46:19<4:23:11,  6.61s/it]                                                        63%|   | 4110/6500 [7:46:19<4:23:11,  6.61s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8721247315406799, 'eval_runtime': 1.4875, 'eval_samples_per_second': 8.067, 'eval_steps_per_second': 2.017, 'epoch': 0.63}
                                                        63%|   | 4110/6500 [7:46:21<4:23:11,  6.61s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-4110
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4110
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4110
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4110
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4110
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4110/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4110/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4110/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4110/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3909, 'learning_rate': 2.9804600164385733e-05, 'epoch': 0.63}
{'loss': 0.3857, 'learning_rate': 2.9782488584766822e-05, 'epoch': 0.63}
{'loss': 0.3805, 'learning_rate': 2.976038173086828e-05, 'epoch': 0.63}
{'loss': 0.3715, 'learning_rate': 2.9738279607857455e-05, 'epoch': 0.63}
{'loss': 0.3821, 'learning_rate': 2.9716182220900578e-05, 'epoch': 0.63}
 63%|   | 4111/6500 [7:46:27<4:45:04,  7.16s/it]                                                        63%|   | 4111/6500 [7:46:27<4:45:04,  7.16s/it] 63%|   | 4112/6500 [7:46:34<4:37:37,  6.98s/it]                                                        63%|   | 4112/6500 [7:46:34<4:37:37,  6.98s/it] 63%|   | 4113/6500 [7:46:41<4:32:32,  6.85s/it]                                                        63%|   | 4113/6500 [7:46:41<4:32:32,  6.85s/it] 63%|   | 4114/6500 [7:46:47<4:29:00,  6.76s/it]                                                        63%|   | 4114/6500 [7:46:47<4:29:00,  6.76s/it] 63%|   | 4115/6500 [7:46:54<4:26:40,  6.71s/it]                                                        63%|   | 4115/6500 [7:46:54<4:26:40,  6.71s/it] 63%|   | 4116/65{'loss': 0.3828, 'learning_rate': 2.9694089575162785e-05, 'epoch': 0.63}
{'loss': 0.3906, 'learning_rate': 2.9672001675808086e-05, 'epoch': 0.63}
{'loss': 0.3842, 'learning_rate': 2.96499185279994e-05, 'epoch': 0.63}
{'loss': 0.3807, 'learning_rate': 2.9627840136898523e-05, 'epoch': 0.63}
{'loss': 0.3985, 'learning_rate': 2.9605766507666145e-05, 'epoch': 0.63}
00 [7:47:00<4:24:44,  6.66s/it]                                                        63%|   | 4116/6500 [7:47:00<4:24:44,  6.66s/it] 63%|   | 4117/6500 [7:47:07<4:23:20,  6.63s/it]                                                        63%|   | 4117/6500 [7:47:07<4:23:20,  6.63s/it] 63%|   | 4118/6500 [7:47:13<4:22:22,  6.61s/it]                                                        63%|   | 4118/6500 [7:47:13<4:22:22,  6.61s/it] 63%|   | 4119/6500 [7:47:20<4:21:34,  6.59s/it]                                                        63%|   | 4119/6500 [7:47:20<4:21:34,  6.59s/it] 63%|   | 4120/6500 [7:47:26<4:20:58,  6.58s/it]                                                        63%|   | 4120/6500 [7:47:26<4:20:58,  6.58s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8753648996353149, 'eval_runtime': 2.0393, 'eval_samples_per_second': 5.885, 'eval_steps_per_second': 1.471, 'epoch': 0.63}
                                                        63%|   | 4120/6500 [7:47:29<4:20:58,  6.58s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-4120
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4120I AM HERE AND I AM SAVING THE MODEL

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4120
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4120
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4120
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4120/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4120/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4120/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4120/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3796, 'learning_rate': 2.9583697645461848e-05, 'epoch': 0.63}
{'loss': 0.3874, 'learning_rate': 2.95616335554441e-05, 'epoch': 0.63}
{'loss': 0.4113, 'learning_rate': 2.953957424277024e-05, 'epoch': 0.63}
{'loss': 0.3885, 'learning_rate': 2.9517519712596498e-05, 'epoch': 0.63}
{'loss': 0.6648, 'learning_rate': 2.9495469970078e-05, 'epoch': 0.63}
 63%|   | 4121/6500 [7:47:37<5:02:36,  7.63s/it]                                                        63%|   | 4121/6500 [7:47:37<5:02:36,  7.63s/it] 63%|   | 4122/6500 [7:47:43<4:49:38,  7.31s/it]                                                        63%|   | 4122/6500 [7:47:43<4:49:38,  7.31s/it] 63%|   | 4123/6500 [7:47:50<4:40:24,  7.08s/it]                                                        63%|   | 4123/6500 [7:47:50<4:40:24,  7.08s/it] 63%|   | 4124/6500 [7:47:56<4:33:49,  6.91s/it]                                                        63%|   | 4124/6500 [7:47:56<4:33:49,  6.91s/it] 63%|   | 4125/6500 [7:48:03<4:29:11,  6.80s/it]                                                        63%|   | 4125/6500 [7:48:03<4:29:11,  6.80s/it] 63%|   | 4126/65{'loss': 0.3764, 'learning_rate': 2.9473425020368716e-05, 'epoch': 0.63}
{'loss': 0.3962, 'learning_rate': 2.9451384868621523e-05, 'epoch': 0.63}
{'loss': 0.3764, 'learning_rate': 2.942934951998819e-05, 'epoch': 0.64}
{'loss': 0.3783, 'learning_rate': 2.940731897961933e-05, 'epoch': 0.64}
{'loss': 0.3817, 'learning_rate': 2.9385293252664452e-05, 'epoch': 0.64}
00 [7:48:09<4:26:02,  6.72s/it]                                                        63%|   | 4126/6500 [7:48:09<4:26:02,  6.72s/it] 63%|   | 4127/6500 [7:48:16<4:23:48,  6.67s/it]                                                        63%|   | 4127/6500 [7:48:16<4:23:48,  6.67s/it] 64%|   | 4128/6500 [7:48:22<4:22:11,  6.63s/it]                                                        64%|   | 4128/6500 [7:48:22<4:22:11,  6.63s/it] 64%|   | 4129/6500 [7:48:29<4:20:53,  6.60s/it]                                                        64%|   | 4129/6500 [7:48:29<4:20:53,  6.60s/it] 64%|   | 4130/6500 [7:48:36<4:21:15,  6.61s/it]                                                        64%|   | 4130/6500 [7:48:36<4:21:15,  6.61s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.864346444606781, 'eval_runtime': 1.642, 'eval_samples_per_second': 7.308, 'eval_steps_per_second': 1.827, 'epoch': 0.64}
                                                        64%|   | 4130/6500 [7:48:37<4:21:15,  6.61s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-4130
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4130
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4130
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4130
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4130
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4130/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4130/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4130/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4130/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3918, 'learning_rate': 2.936327234427195e-05, 'epoch': 0.64}
{'loss': 0.3961, 'learning_rate': 2.9341256259589052e-05, 'epoch': 0.64}
{'loss': 0.3937, 'learning_rate': 2.9319245003761895e-05, 'epoch': 0.64}
{'loss': 0.3838, 'learning_rate': 2.9297238581935482e-05, 'epoch': 0.64}
{'loss': 0.3893, 'learning_rate': 2.9275236999253674e-05, 'epoch': 0.64}
 64%|   | 4131/6500 [7:48:44<4:45:49,  7.24s/it]                                                        64%|   | 4131/6500 [7:48:44<4:45:49,  7.24s/it] 64%|   | 4132/6500 [7:48:51<4:37:27,  7.03s/it]                                                        64%|   | 4132/6500 [7:48:51<4:37:27,  7.03s/it] 64%|   | 4133/6500 [7:48:57<4:31:28,  6.88s/it]                                                        64%|   | 4133/6500 [7:48:57<4:31:28,  6.88s/it] 64%|   | 4134/6500 [7:49:04<4:27:17,  6.78s/it]                                                        64%|   | 4134/6500 [7:49:04<4:27:17,  6.78s/it] 64%|   | 4135/6500 [7:49:10<4:24:11,  6.70s/it]                                                        64%|   | 4135/6500 [7:49:10<4:24:11,  6.70s/it] 64%|   | 4136/65{'loss': 0.3707, 'learning_rate': 2.9253240260859215e-05, 'epoch': 0.64}
{'loss': 0.3885, 'learning_rate': 2.9231248371893695e-05, 'epoch': 0.64}
{'loss': 0.397, 'learning_rate': 2.920926133749759e-05, 'epoch': 0.64}
{'loss': 0.5151, 'learning_rate': 2.9187279162810243e-05, 'epoch': 0.64}
{'loss': 0.5446, 'learning_rate': 2.916530185296984e-05, 'epoch': 0.64}
00 [7:49:17<4:22:14,  6.66s/it]                                                        64%|   | 4136/6500 [7:49:17<4:22:14,  6.66s/it] 64%|   | 4137/6500 [7:49:23<4:20:52,  6.62s/it]                                                        64%|   | 4137/6500 [7:49:23<4:20:52,  6.62s/it] 64%|   | 4138/6500 [7:49:31<4:30:56,  6.88s/it]                                                        64%|   | 4138/6500 [7:49:31<4:30:56,  6.88s/it] 64%|   | 4139/6500 [7:49:38<4:26:48,  6.78s/it]                                                        64%|   | 4139/6500 [7:49:38<4:26:48,  6.78s/it] 64%|   | 4140/6500 [7:49:44<4:23:59,  6.71s/it]                                                        64%|   | 4140/6500 [7:49:44<4:23:59,  6.71s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8719119429588318, 'eval_runtime': 1.4755, 'eval_samples_per_second': 8.133, 'eval_steps_per_second': 2.033, 'epoch': 0.64}
                                                        64%|   | 4140/6500 [7:49:46<4:23:59,  6.71s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-4140
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4140
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4140
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4140
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4140
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4140/pytorch_model.bin
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4140/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4140/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4140/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3765, 'learning_rate': 2.9143329413113464e-05, 'epoch': 0.64}
{'loss': 0.3945, 'learning_rate': 2.9121361848377014e-05, 'epoch': 0.64}
{'loss': 0.3715, 'learning_rate': 2.909939916389529e-05, 'epoch': 0.64}
{'loss': 0.3718, 'learning_rate': 2.9077441364801938e-05, 'epoch': 0.64}
{'loss': 0.3829, 'learning_rate': 2.9055488456229473e-05, 'epoch': 0.64}
 64%|   | 4141/6500 [7:49:52<4:43:55,  7.22s/it]                                                        64%|   | 4141/6500 [7:49:52<4:43:55,  7.22s/it] 64%|   | 4142/6500 [7:49:59<4:35:49,  7.02s/it]                                                        64%|   | 4142/6500 [7:49:59<4:35:49,  7.02s/it] 64%|   | 4143/6500 [7:50:06<4:30:09,  6.88s/it]                                                        64%|   | 4143/6500 [7:50:06<4:30:09,  6.88s/it] 64%|   | 4144/6500 [7:50:12<4:26:01,  6.78s/it]                                                        64%|   | 4144/6500 [7:50:12<4:26:01,  6.78s/it] 64%|   | 4145/6500 [7:50:19<4:23:12,  6.71s/it]                                                        64%|   | 4145/6500 [7:50:19<4:23:12,  6.71s/it] 64%|   | 4146/65{'loss': 0.3787, 'learning_rate': 2.9033540443309227e-05, 'epoch': 0.64}
{'loss': 0.3983, 'learning_rate': 2.9011597331171414e-05, 'epoch': 0.64}
{'loss': 0.3934, 'learning_rate': 2.898965912494511e-05, 'epoch': 0.64}
{'loss': 0.3754, 'learning_rate': 2.8967725829758248e-05, 'epoch': 0.64}
{'loss': 0.3893, 'learning_rate': 2.8945797450737587e-05, 'epoch': 0.64}
00 [7:50:25<4:21:17,  6.66s/it]                                                        64%|   | 4146/6500 [7:50:25<4:21:17,  6.66s/it] 64%|   | 4147/6500 [7:50:32<4:19:49,  6.63s/it]                                                        64%|   | 4147/6500 [7:50:32<4:19:49,  6.63s/it] 64%|   | 4148/6500 [7:50:38<4:18:45,  6.60s/it]                                                        64%|   | 4148/6500 [7:50:38<4:18:45,  6.60s/it] 64%|   | 4149/6500 [7:50:45<4:17:51,  6.58s/it]                                                        64%|   | 4149/6500 [7:50:45<4:17:51,  6.58s/it] 64%|   | 4150/6500 [7:50:51<4:17:19,  6.57s/it]                                                        64%|   | 4150/6500 [7:50:51<4:17:19,  6.57s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8757866024971008, 'eval_runtime': 1.4729, 'eval_samples_per_second': 8.147, 'eval_steps_per_second': 2.037, 'epoch': 0.64}
                                                        64%|   | 4150/6500 [7:50:53<4:17:19,  6.57s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-4150
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4150
/projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4150
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4150
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4150
the pytorch model path isthe pytorch model path is  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4150/pytorch_model.bin/projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4150/pytorch_model.bin

the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4150/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4150/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3697, 'learning_rate': 2.892387399300877e-05, 'epoch': 0.64}
{'loss': 0.4154, 'learning_rate': 2.8901955461696262e-05, 'epoch': 0.64}
{'loss': 0.3703, 'learning_rate': 2.888004186192338e-05, 'epoch': 0.64}
{'loss': 0.6608, 'learning_rate': 2.8858133198812305e-05, 'epoch': 0.64}
{'loss': 0.3995, 'learning_rate': 2.8836229477484046e-05, 'epoch': 0.64}
 64%|   | 4151/6500 [7:51:00<4:39:29,  7.14s/it]                                                        64%|   | 4151/6500 [7:51:00<4:39:29,  7.14s/it] 64%|   | 4152/6500 [7:51:06<4:32:35,  6.97s/it]                                                        64%|   | 4152/6500 [7:51:06<4:32:35,  6.97s/it] 64%|   | 4153/6500 [7:51:13<4:27:21,  6.83s/it]                                                        64%|   | 4153/6500 [7:51:13<4:27:21,  6.83s/it] 64%|   | 4154/6500 [7:51:20<4:34:04,  7.01s/it]                                                        64%|   | 4154/6500 [7:51:20<4:34:04,  7.01s/it] 64%|   | 4155/6500 [7:51:27<4:28:45,  6.88s/it]                                                        64%|   | 4155/6500 [7:51:27<4:28:45,  6.88s/it] 64%|   | 4156/65{'loss': 0.3741, 'learning_rate': 2.881433070305849e-05, 'epoch': 0.64}
{'loss': 0.3886, 'learning_rate': 2.8792436880654305e-05, 'epoch': 0.64}
{'loss': 0.3701, 'learning_rate': 2.8770548015389054e-05, 'epoch': 0.64}
{'loss': 0.3769, 'learning_rate': 2.8748664112379127e-05, 'epoch': 0.64}
{'loss': 0.3857, 'learning_rate': 2.872678517673975e-05, 'epoch': 0.64}
00 [7:51:33<4:24:38,  6.77s/it]                                                        64%|   | 4156/6500 [7:51:33<4:24:38,  6.77s/it] 64%|   | 4157/6500 [7:51:40<4:21:58,  6.71s/it]                                                        64%|   | 4157/6500 [7:51:40<4:21:58,  6.71s/it] 64%|   | 4158/6500 [7:51:47<4:20:04,  6.66s/it]                                                        64%|   | 4158/6500 [7:51:47<4:20:04,  6.66s/it] 64%|   | 4159/6500 [7:51:53<4:18:29,  6.63s/it]                                                        64%|   | 4159/6500 [7:51:53<4:18:29,  6.63s/it] 64%|   | 4160/6500 [7:52:00<4:17:21,  6.60s/it]                                                        64%|   | 4160/6500 [7:52:00<4:17:21,  6.60s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8654614090919495, 'eval_runtime': 1.4772, 'eval_samples_per_second': 8.123, 'eval_steps_per_second': 2.031, 'epoch': 0.64}
                                                        64%|   | 4160/6500 [7:52:01<4:17:21,  6.60s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-4160
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4160I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4160

I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4160
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4160
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4160/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4160/pytorch_model.binthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4160/pytorch_model.bin

the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4160/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3845, 'learning_rate': 2.8704911213584996e-05, 'epoch': 0.64}
{'loss': 0.387, 'learning_rate': 2.8683042228027772e-05, 'epoch': 0.64}
{'loss': 0.3844, 'learning_rate': 2.866117822517982e-05, 'epoch': 0.64}
{'loss': 0.377, 'learning_rate': 2.863931921015171e-05, 'epoch': 0.64}
{'loss': 0.3809, 'learning_rate': 2.861746518805286e-05, 'epoch': 0.64}
 64%|   | 4161/6500 [7:52:08<4:37:35,  7.12s/it]                                                        64%|   | 4161/6500 [7:52:08<4:37:35,  7.12s/it] 64%|   | 4162/6500 [7:52:15<4:30:51,  6.95s/it]                                                        64%|   | 4162/6500 [7:52:15<4:30:51,  6.95s/it] 64%|   | 4163/6500 [7:52:21<4:26:03,  6.83s/it]                                                        64%|   | 4163/6500 [7:52:21<4:26:03,  6.83s/it] 64%|   | 4164/6500 [7:52:28<4:22:18,  6.74s/it]                                                        64%|   | 4164/6500 [7:52:28<4:22:18,  6.74s/it] 64%|   | 4165/6500 [7:52:34<4:20:09,  6.69s/it]                                                        64%|   | 4165/6500 [7:52:34<4:20:09,  6.69s/it] 64%|   | 4166/65{'loss': 0.3724, 'learning_rate': 2.8595616163991523e-05, 'epoch': 0.64}
{'loss': 0.4144, 'learning_rate': 2.857377214307478e-05, 'epoch': 0.64}
{'loss': 0.385, 'learning_rate': 2.8551933130408505e-05, 'epoch': 0.64}
{'loss': 0.6482, 'learning_rate': 2.8530099131097455e-05, 'epoch': 0.64}
{'loss': 0.3992, 'learning_rate': 2.850827015024519e-05, 'epoch': 0.64}
00 [7:52:41<4:18:19,  6.64s/it]                                                        64%|   | 4166/6500 [7:52:41<4:18:19,  6.64s/it] 64%|   | 4167/6500 [7:52:47<4:17:09,  6.61s/it]                                                        64%|   | 4167/6500 [7:52:47<4:17:09,  6.61s/it] 64%|   | 4168/6500 [7:52:54<4:16:11,  6.59s/it]                                                        64%|   | 4168/6500 [7:52:54<4:16:11,  6.59s/it] 64%|   | 4169/6500 [7:53:00<4:15:23,  6.57s/it]                                                        64%|   | 4169/6500 [7:53:00<4:15:23,  6.57s/it] 64%|   | 4170/6500 [7:53:08<4:22:22,  6.76s/it]                                                        64%|   | 4170/6500 [7:53:08<4:22:22,  6.76s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8747185468673706, 'eval_runtime': 1.4981, 'eval_samples_per_second': 8.01, 'eval_steps_per_second': 2.002, 'epoch': 0.64}
                                                        64%|   | 4170/6500 [7:53:09<4:22:22,  6.76s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-4170
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4170I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4170

I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4170
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4170
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4170/pytorch_model.bin
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4170/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4170/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4170/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.373, 'learning_rate': 2.8486446192954107e-05, 'epoch': 0.64}
{'loss': 0.3878, 'learning_rate': 2.8464627264325426e-05, 'epoch': 0.64}
{'loss': 0.3772, 'learning_rate': 2.8442813369459187e-05, 'epoch': 0.64}
{'loss': 0.386, 'learning_rate': 2.842100451345424e-05, 'epoch': 0.64}
{'loss': 0.3884, 'learning_rate': 2.8399200701408303e-05, 'epoch': 0.64}
 64%|   | 4171/6500 [7:53:16<4:41:33,  7.25s/it]                                                        64%|   | 4171/6500 [7:53:16<4:41:33,  7.25s/it] 64%|   | 4172/6500 [7:53:22<4:33:29,  7.05s/it]                                                        64%|   | 4172/6500 [7:53:22<4:33:29,  7.05s/it] 64%|   | 4173/6500 [7:53:29<4:27:38,  6.90s/it]                                                        64%|   | 4173/6500 [7:53:29<4:27:38,  6.90s/it] 64%|   | 4174/6500 [7:53:36<4:23:44,  6.80s/it]                                                        64%|   | 4174/6500 [7:53:36<4:23:44,  6.80s/it] 64%|   | 4175/6500 [7:53:42<4:20:49,  6.73s/it]                                                        64%|   | 4175/6500 [7:53:42<4:20:49,  6.73s/it] 64%|   | 4176/65{'loss': 0.3934, 'learning_rate': 2.8377401938417858e-05, 'epoch': 0.64}
{'loss': 0.3818, 'learning_rate': 2.835560822957824e-05, 'epoch': 0.64}
{'loss': 0.3849, 'learning_rate': 2.8333819579983623e-05, 'epoch': 0.64}
{'loss': 0.3848, 'learning_rate': 2.8312035994726926e-05, 'epoch': 0.64}
{'loss': 0.3754, 'learning_rate': 2.8290257478899945e-05, 'epoch': 0.64}
00 [7:53:49<4:19:03,  6.69s/it]                                                        64%|   | 4176/6500 [7:53:49<4:19:03,  6.69s/it] 64%|   | 4177/6500 [7:53:55<4:17:28,  6.65s/it]                                                        64%|   | 4177/6500 [7:53:55<4:17:28,  6.65s/it] 64%|   | 4178/6500 [7:54:02<4:16:24,  6.63s/it]                                                        64%|   | 4178/6500 [7:54:02<4:16:24,  6.63s/it] 64%|   | 4179/6500 [7:54:08<4:15:57,  6.62s/it]                                                        64%|   | 4179/6500 [7:54:08<4:15:57,  6.62s/it] 64%|   | 4180/6500 [7:54:15<4:15:21,  6.60s/it]                                                        64%|   | 4180/6500 [7:54:15<4:15:21,  6.60s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8771220445632935, 'eval_runtime': 1.7251, 'eval_samples_per_second': 6.956, 'eval_steps_per_second': 1.739, 'epoch': 0.64}
                                                        64%|   | 4180/6500 [7:54:17<4:15:21,  6.60s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-4180
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4180
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4180
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4180
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4180
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4180/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4180/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4180/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4180/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.368, 'learning_rate': 2.8268484037593286e-05, 'epoch': 0.64}
{'loss': 0.4168, 'learning_rate': 2.824671567589635e-05, 'epoch': 0.64}
{'loss': 0.3796, 'learning_rate': 2.8224952398897363e-05, 'epoch': 0.64}
{'loss': 0.6602, 'learning_rate': 2.820319421168336e-05, 'epoch': 0.64}
{'loss': 0.3933, 'learning_rate': 2.818144111934019e-05, 'epoch': 0.64}
 64%|   | 4181/6500 [7:54:24<4:38:53,  7.22s/it]                                                        64%|   | 4181/6500 [7:54:24<4:38:53,  7.22s/it] 64%|   | 4182/6500 [7:54:30<4:31:12,  7.02s/it]                                                        64%|   | 4182/6500 [7:54:30<4:31:12,  7.02s/it] 64%|   | 4183/6500 [7:54:37<4:25:45,  6.88s/it]                                                        64%|   | 4183/6500 [7:54:37<4:25:45,  6.88s/it] 64%|   | 4184/6500 [7:54:43<4:21:52,  6.78s/it]                                                        64%|   | 4184/6500 [7:54:43<4:21:52,  6.78s/it] 64%|   | 4185/6500 [7:54:50<4:19:27,  6.72s/it]                                                        64%|   | 4185/6500 [7:54:50<4:19:27,  6.72s/it] 64%|   | 4186/65{'loss': 0.3841, 'learning_rate': 2.815969312695249e-05, 'epoch': 0.64}
{'loss': 0.3685, 'learning_rate': 2.8137950239603734e-05, 'epoch': 0.64}
{'loss': 0.3704, 'learning_rate': 2.8116212462376183e-05, 'epoch': 0.64}
{'loss': 0.376, 'learning_rate': 2.8094479800350938e-05, 'epoch': 0.64}
{'loss': 0.3781, 'learning_rate': 2.8072752258607828e-05, 'epoch': 0.64}
00 [7:54:57<4:24:42,  6.86s/it]                                                        64%|   | 4186/6500 [7:54:57<4:24:42,  6.86s/it] 64%|   | 4187/6500 [7:55:04<4:21:20,  6.78s/it]                                                        64%|   | 4187/6500 [7:55:04<4:21:20,  6.78s/it] 64%|   | 4188/6500 [7:55:10<4:19:12,  6.73s/it]                                                        64%|   | 4188/6500 [7:55:10<4:19:12,  6.73s/it] 64%|   | 4189/6500 [7:55:17<4:17:13,  6.68s/it]                                                        64%|   | 4189/6500 [7:55:17<4:17:13,  6.68s/it] 64%|   | 4190/6500 [7:55:24<4:16:07,  6.65s/it]                                                        64%|   | 4190/6500 [7:55:24<4:16:07,  6.65s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8657858371734619, 'eval_runtime': 1.4909, 'eval_samples_per_second': 8.049, 'eval_steps_per_second': 2.012, 'epoch': 0.64}
                                                        64%|   | 4190/6500 [7:55:25<4:16:07,  6.65s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-4190
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4190
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4190
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4190
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4190
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4190/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4190/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4190/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4190/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3884, 'learning_rate': 2.8051029842225556e-05, 'epoch': 0.64}
{'loss': 0.3932, 'learning_rate': 2.8029312556281613e-05, 'epoch': 0.64}
{'loss': 0.3869, 'learning_rate': 2.8007600405852275e-05, 'epoch': 0.65}
{'loss': 0.3853, 'learning_rate': 2.798589339601262e-05, 'epoch': 0.65}
{'loss': 0.376, 'learning_rate': 2.7964191531836535e-05, 'epoch': 0.65}
 64%|   | 4191/6500 [7:55:32<4:36:39,  7.19s/it]                                                        64%|   | 4191/6500 [7:55:32<4:36:39,  7.19s/it] 64%|   | 4192/6500 [7:55:39<4:29:21,  7.00s/it]                                                        64%|   | 4192/6500 [7:55:39<4:29:21,  7.00s/it] 65%|   | 4193/6500 [7:55:45<4:24:14,  6.87s/it]                                                        65%|   | 4193/6500 [7:55:45<4:24:14,  6.87s/it] 65%|   | 4194/6500 [7:55:52<4:20:36,  6.78s/it]                                                        65%|   | 4194/6500 [7:55:52<4:20:36,  6.78s/it] 65%|   | 4195/6500 [7:55:58<4:18:29,  6.73s/it]                                                        65%|   | 4195/6500 [7:55:58<4:18:29,  6.73s/it] 65%|   | 4196/65{'loss': 0.3793, 'learning_rate': 2.794249481839669e-05, 'epoch': 0.65}
{'loss': 0.4083, 'learning_rate': 2.7920803260764582e-05, 'epoch': 0.65}
{'loss': 0.3843, 'learning_rate': 2.789911686401049e-05, 'epoch': 0.65}
{'loss': 0.6644, 'learning_rate': 2.787743563320343e-05, 'epoch': 0.65}
{'loss': 0.384, 'learning_rate': 2.785575957341129e-05, 'epoch': 0.65}
00 [7:56:05<4:16:52,  6.69s/it]                                                        65%|   | 4196/6500 [7:56:05<4:16:52,  6.69s/it] 65%|   | 4197/6500 [7:56:11<4:15:28,  6.66s/it]                                                        65%|   | 4197/6500 [7:56:11<4:15:28,  6.66s/it] 65%|   | 4198/6500 [7:56:18<4:14:14,  6.63s/it]                                                        65%|   | 4198/6500 [7:56:18<4:14:14,  6.63s/it] 65%|   | 4199/6500 [7:56:25<4:13:09,  6.60s/it]                                                        65%|   | 4199/6500 [7:56:25<4:13:09,  6.60s/it] 65%|   | 4200/6500 [7:56:31<4:12:25,  6.58s/it]                                                        65%|   | 4200/6500 [7:56:31<4:12:25,  6.58s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8732022047042847, 'eval_runtime': 1.5607, 'eval_samples_per_second': 7.689, 'eval_steps_per_second': 1.922, 'epoch': 0.65}
                                                        65%|   | 4200/6500 [7:56:33<4:12:25,  6.58s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-4200
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4200
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4200
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4200
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4200
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4200/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4200/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4200/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4200/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3906, 'learning_rate': 2.783408868970071e-05, 'epoch': 0.65}
{'loss': 0.3737, 'learning_rate': 2.7812422987137128e-05, 'epoch': 0.65}
{'loss': 0.38, 'learning_rate': 2.779076247078477e-05, 'epoch': 0.65}
{'loss': 0.3767, 'learning_rate': 2.7769107145706645e-05, 'epoch': 0.65}
{'loss': 0.388, 'learning_rate': 2.7747457016964563e-05, 'epoch': 0.65}
 65%|   | 4201/6500 [7:56:40<4:34:18,  7.16s/it]                                                        65%|   | 4201/6500 [7:56:40<4:34:18,  7.16s/it] 65%|   | 4202/6500 [7:56:47<4:37:48,  7.25s/it]                                                        65%|   | 4202/6500 [7:56:47<4:37:48,  7.25s/it] 65%|   | 4203/6500 [7:56:54<4:29:30,  7.04s/it]                                                        65%|   | 4203/6500 [7:56:54<4:29:30,  7.04s/it] 65%|   | 4204/6500 [7:57:00<4:23:47,  6.89s/it]                                                        65%|   | 4204/6500 [7:57:00<4:23:47,  6.89s/it] 65%|   | 4205/6500 [7:57:07<4:20:07,  6.80s/it]                                                        65%|   | 4205/6500 [7:57:07<4:20:07,  6.80s/it] 65%|   | 4206/65{'loss': 0.3936, 'learning_rate': 2.772581208961911e-05, 'epoch': 0.65}
{'loss': 0.3859, 'learning_rate': 2.7704172368729642e-05, 'epoch': 0.65}
{'loss': 0.3856, 'learning_rate': 2.7682537859354328e-05, 'epoch': 0.65}
{'loss': 0.3961, 'learning_rate': 2.7660908566550113e-05, 'epoch': 0.65}
{'loss': 0.3698, 'learning_rate': 2.7639284495372682e-05, 'epoch': 0.65}
00 [7:57:13<4:17:12,  6.73s/it]                                                        65%|   | 4206/6500 [7:57:13<4:17:12,  6.73s/it] 65%|   | 4207/6500 [7:57:20<4:15:00,  6.67s/it]                                                        65%|   | 4207/6500 [7:57:20<4:15:00,  6.67s/it] 65%|   | 4208/6500 [7:57:26<4:13:19,  6.63s/it]                                                        65%|   | 4208/6500 [7:57:26<4:13:19,  6.63s/it] 65%|   | 4209/6500 [7:57:33<4:12:19,  6.61s/it]                                                        65%|   | 4209/6500 [7:57:33<4:12:19,  6.61s/it] 65%|   | 4210/6500 [7:57:39<4:11:30,  6.59s/it]                                                        65%|   | 4210/6500 [7:57:39<4:11:30,  6.59s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8757606148719788, 'eval_runtime': 1.4942, 'eval_samples_per_second': 8.031, 'eval_steps_per_second': 2.008, 'epoch': 0.65}
                                                        65%|   | 4210/6500 [7:57:41<4:11:30,  6.59s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-4210
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4210
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4210
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4210
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4210
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4210/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4210/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4210/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4210/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3824, 'learning_rate': 2.7617665650876538e-05, 'epoch': 0.65}
{'loss': 0.4022, 'learning_rate': 2.759605203811497e-05, 'epoch': 0.65}
{'loss': 0.3948, 'learning_rate': 2.7574443662140016e-05, 'epoch': 0.65}
{'loss': 0.6612, 'learning_rate': 2.7552840528002498e-05, 'epoch': 0.65}
{'loss': 0.3817, 'learning_rate': 2.7531242640752035e-05, 'epoch': 0.65}
 65%|   | 4211/6500 [7:57:48<4:32:44,  7.15s/it]                                                        65%|   | 4211/6500 [7:57:48<4:32:44,  7.15s/it] 65%|   | 4212/6500 [7:57:54<4:25:45,  6.97s/it]                                                        65%|   | 4212/6500 [7:57:54<4:25:45,  6.97s/it] 65%|   | 4213/6500 [7:58:01<4:20:42,  6.84s/it]                                                        65%|   | 4213/6500 [7:58:01<4:20:42,  6.84s/it] 65%|   | 4214/6500 [7:58:08<4:17:09,  6.75s/it]                                                        65%|   | 4214/6500 [7:58:08<4:17:09,  6.75s/it] 65%|   | 4215/6500 [7:58:14<4:14:46,  6.69s/it]                                                        65%|   | 4215/6500 [7:58:14<4:14:46,  6.69s/it] 65%|   | 4216/65{'loss': 0.3972, 'learning_rate': 2.7509650005436994e-05, 'epoch': 0.65}
{'loss': 0.373, 'learning_rate': 2.7488062627104517e-05, 'epoch': 0.65}
{'loss': 0.3802, 'learning_rate': 2.7466480510800523e-05, 'epoch': 0.65}
{'loss': 0.3859, 'learning_rate': 2.744490366156971e-05, 'epoch': 0.65}
{'loss': 0.3806, 'learning_rate': 2.7423332084455544e-05, 'epoch': 0.65}
00 [7:58:21<4:13:03,  6.65s/it]                                                        65%|   | 4216/6500 [7:58:21<4:13:03,  6.65s/it] 65%|   | 4217/6500 [7:58:27<4:11:47,  6.62s/it]                                                        65%|   | 4217/6500 [7:58:27<4:11:47,  6.62s/it] 65%|   | 4218/6500 [7:58:35<4:20:48,  6.86s/it]                                                        65%|   | 4218/6500 [7:58:35<4:20:48,  6.86s/it] 65%|   | 4219/6500 [7:58:41<4:17:22,  6.77s/it]                                                        65%|   | 4219/6500 [7:58:41<4:17:22,  6.77s/it] 65%|   | 4220/6500 [7:58:48<4:14:59,  6.71s/it]                                                        65%|   | 4220/6500 [7:58:48<4:14:59,  6.71s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8651353716850281, 'eval_runtime': 1.4804, 'eval_samples_per_second': 8.106, 'eval_steps_per_second': 2.027, 'epoch': 0.65}
                                                        65%|   | 4220/6500 [7:58:49<4:14:59,  6.71s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-4220
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4220I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4220

I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4220
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4220
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4220/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4220/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4220/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4220/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.4006, 'learning_rate': 2.7401765784500215e-05, 'epoch': 0.65}
{'loss': 0.4013, 'learning_rate': 2.7380204766744737e-05, 'epoch': 0.65}
{'loss': 0.3733, 'learning_rate': 2.7358649036228866e-05, 'epoch': 0.65}
{'loss': 0.3898, 'learning_rate': 2.7337098597991116e-05, 'epoch': 0.65}
{'loss': 0.3698, 'learning_rate': 2.7315553457068778e-05, 'epoch': 0.65}
 65%|   | 4221/6500 [7:58:56<4:34:18,  7.22s/it]                                                        65%|   | 4221/6500 [7:58:56<4:34:18,  7.22s/it] 65%|   | 4222/6500 [7:59:03<4:26:31,  7.02s/it]                                                        65%|   | 4222/6500 [7:59:03<4:26:31,  7.02s/it] 65%|   | 4223/6500 [7:59:09<4:20:56,  6.88s/it]                                                        65%|   | 4223/6500 [7:59:09<4:20:56,  6.88s/it] 65%|   | 4224/6500 [7:59:16<4:17:09,  6.78s/it]                                                        65%|   | 4224/6500 [7:59:16<4:17:09,  6.78s/it] 65%|   | 4225/6500 [7:59:22<4:14:23,  6.71s/it]                                                        65%|   | 4225/6500 [7:59:22<4:14:23,  6.71s/it] 65%|   | 4226/65{'loss': 0.4228, 'learning_rate': 2.7294013618497894e-05, 'epoch': 0.65}
{'loss': 0.3734, 'learning_rate': 2.7272479087313274e-05, 'epoch': 0.65}
{'loss': 0.6617, 'learning_rate': 2.725094986854848e-05, 'epoch': 0.65}
{'loss': 0.3926, 'learning_rate': 2.7229425967235846e-05, 'epoch': 0.65}
{'loss': 0.3811, 'learning_rate': 2.720790738840644e-05, 'epoch': 0.65}
00 [7:59:29<4:12:21,  6.66s/it]                                                        65%|   | 4226/6500 [7:59:29<4:12:21,  6.66s/it] 65%|   | 4227/6500 [7:59:35<4:10:48,  6.62s/it]                                                        65%|   | 4227/6500 [7:59:35<4:10:48,  6.62s/it] 65%|   | 4228/6500 [7:59:42<4:09:36,  6.59s/it]                                                        65%|   | 4228/6500 [7:59:42<4:09:36,  6.59s/it] 65%|   | 4229/6500 [7:59:48<4:08:50,  6.57s/it]                                                        65%|   | 4229/6500 [7:59:48<4:08:50,  6.57s/it] 65%|   | 4230/6500 [7:59:55<4:08:11,  6.56s/it]                                                        65%|   | 4230/6500 [7:59:55<4:08:11,  6.56s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8724949955940247, 'eval_runtime': 1.4744, 'eval_samples_per_second': 8.139, 'eval_steps_per_second': 2.035, 'epoch': 0.65}
                                                        65%|   | 4230/6500 [7:59:56<4:08:11,  6.56s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-4230
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4230
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4230
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4230
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4230
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4230/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4230/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4230/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4230/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3863, 'learning_rate': 2.71863941370901e-05, 'epoch': 0.65}
{'loss': 0.3672, 'learning_rate': 2.7164886218315444e-05, 'epoch': 0.65}
{'loss': 0.3812, 'learning_rate': 2.7143383637109772e-05, 'epoch': 0.65}
{'loss': 0.3873, 'learning_rate': 2.7121886398499207e-05, 'epoch': 0.65}
{'loss': 0.3869, 'learning_rate': 2.71003945075086e-05, 'epoch': 0.65}
 65%|   | 4231/6500 [8:00:03<4:28:31,  7.10s/it]                                                        65%|   | 4231/6500 [8:00:03<4:28:31,  7.10s/it] 65%|   | 4232/6500 [8:00:10<4:22:02,  6.93s/it]                                                        65%|   | 4232/6500 [8:00:10<4:22:02,  6.93s/it] 65%|   | 4233/6500 [8:00:16<4:17:34,  6.82s/it]                                                        65%|   | 4233/6500 [8:00:16<4:17:34,  6.82s/it] 65%|   | 4234/6500 [8:00:24<4:24:08,  6.99s/it]                                                        65%|   | 4234/6500 [8:00:24<4:24:08,  6.99s/it] 65%|   | 4235/6500 [8:00:30<4:18:55,  6.86s/it]                                                        65%|   | 4235/6500 [8:00:30<4:18:55,  6.86s/it] 65%|   | 4236/65{'loss': 0.3831, 'learning_rate': 2.707890796916153e-05, 'epoch': 0.65}
{'loss': 0.3852, 'learning_rate': 2.7057426788480372e-05, 'epoch': 0.65}
{'loss': 0.3745, 'learning_rate': 2.7035950970486207e-05, 'epoch': 0.65}
{'loss': 0.3812, 'learning_rate': 2.701448052019888e-05, 'epoch': 0.65}
{'loss': 0.3632, 'learning_rate': 2.699301544263697e-05, 'epoch': 0.65}
00 [8:00:37<4:15:16,  6.77s/it]                                                        65%|   | 4236/6500 [8:00:37<4:15:16,  6.77s/it] 65%|   | 4237/6500 [8:00:43<4:12:33,  6.70s/it]                                                        65%|   | 4237/6500 [8:00:43<4:12:33,  6.70s/it] 65%|   | 4238/6500 [8:00:50<4:10:32,  6.65s/it]                                                        65%|   | 4238/6500 [8:00:50<4:10:32,  6.65s/it] 65%|   | 4239/6500 [8:00:57<4:09:23,  6.62s/it]                                                        65%|   | 4239/6500 [8:00:57<4:09:23,  6.62s/it] 65%|   | 4240/6500 [8:01:03<4:08:14,  6.59s/it]                                                        65%|   | 4240/6500 [8:01:03<4:08:14,  6.59s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8780330419540405, 'eval_runtime': 1.477, 'eval_samples_per_second': 8.124, 'eval_steps_per_second': 2.031, 'epoch': 0.65}
                                                        65%|   | 4240/6500 [8:01:05<4:08:14,  6.59s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-4240
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4240I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4240

I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4240
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4240
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4240/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4240/pytorch_model.binthe pytorch model path is
 /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4240/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4240/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.4132, 'learning_rate': 2.6971555742817823e-05, 'epoch': 0.65}
{'loss': 0.376, 'learning_rate': 2.6950101425757507e-05, 'epoch': 0.65}
{'loss': 0.6528, 'learning_rate': 2.6928652496470853e-05, 'epoch': 0.65}
{'loss': 0.3881, 'learning_rate': 2.690720895997138e-05, 'epoch': 0.65}
{'loss': 0.373, 'learning_rate': 2.688577082127141e-05, 'epoch': 0.65}
 65%|   | 4241/6500 [8:01:12<4:29:00,  7.14s/it]                                                        65%|   | 4241/6500 [8:01:12<4:29:00,  7.14s/it] 65%|   | 4242/6500 [8:01:18<4:22:00,  6.96s/it]                                                        65%|   | 4242/6500 [8:01:18<4:22:00,  6.96s/it] 65%|   | 4243/6500 [8:01:25<4:16:57,  6.83s/it]                                                        65%|   | 4243/6500 [8:01:25<4:16:57,  6.83s/it] 65%|   | 4244/6500 [8:01:31<4:13:40,  6.75s/it]                                                        65%|   | 4244/6500 [8:01:31<4:13:40,  6.75s/it] 65%|   | 4245/6500 [8:01:38<4:11:05,  6.68s/it]                                                        65%|   | 4245/6500 [8:01:38<4:11:05,  6.68s/it] 65%|   | 4246/65{'loss': 0.3925, 'learning_rate': 2.6864338085381975e-05, 'epoch': 0.65}
{'loss': 0.3671, 'learning_rate': 2.6842910757312843e-05, 'epoch': 0.65}
{'loss': 0.3779, 'learning_rate': 2.6821488842072524e-05, 'epoch': 0.65}
{'loss': 0.3799, 'learning_rate': 2.6800072344668258e-05, 'epoch': 0.65}
{'loss': 0.3821, 'learning_rate': 2.6778661270106025e-05, 'epoch': 0.65}
00 [8:01:44<4:09:20,  6.64s/it]                                                        65%|   | 4246/6500 [8:01:44<4:09:20,  6.64s/it] 65%|   | 4247/6500 [8:01:51<4:08:11,  6.61s/it]                                                        65%|   | 4247/6500 [8:01:51<4:08:11,  6.61s/it] 65%|   | 4248/6500 [8:01:57<4:07:23,  6.59s/it]                                                        65%|   | 4248/6500 [8:01:57<4:07:23,  6.59s/it] 65%|   | 4249/6500 [8:02:04<4:06:43,  6.58s/it]                                                        65%|   | 4249/6500 [8:02:04<4:06:43,  6.58s/it] 65%|   | 4250/6500 [8:02:10<4:06:03,  6.56s/it]                                                        65%|   | 4250/6500 [8:02:10<4:06:03,  6.56s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8659315705299377, 'eval_runtime': 1.4756, 'eval_samples_per_second': 8.132, 'eval_steps_per_second': 2.033, 'epoch': 0.65}
                                                        65%|   | 4250/6500 [8:02:12<4:06:03,  6.56s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-4250
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4250
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4250
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4250
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4250
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4250/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4250/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4250/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4250/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3848, 'learning_rate': 2.6757255623390532e-05, 'epoch': 0.65}
{'loss': 0.3894, 'learning_rate': 2.6735855409525224e-05, 'epoch': 0.65}
{'loss': 0.3798, 'learning_rate': 2.6714460633512262e-05, 'epoch': 0.65}
{'loss': 0.375, 'learning_rate': 2.6693071300352568e-05, 'epoch': 0.65}
{'loss': 0.3673, 'learning_rate': 2.6671687415045733e-05, 'epoch': 0.65}
 65%|   | 4251/6500 [8:02:20<4:37:36,  7.41s/it]                                                        65%|   | 4251/6500 [8:02:20<4:37:36,  7.41s/it] 65%|   | 4252/6500 [8:02:26<4:27:43,  7.15s/it]                                                        65%|   | 4252/6500 [8:02:26<4:27:43,  7.15s/it] 65%|   | 4253/6500 [8:02:33<4:20:49,  6.96s/it]                                                        65%|   | 4253/6500 [8:02:33<4:20:49,  6.96s/it] 65%|   | 4254/6500 [8:02:39<4:15:53,  6.84s/it]                                                        65%|   | 4254/6500 [8:02:39<4:15:53,  6.84s/it] 65%|   | 4255/6500 [8:02:46<4:12:28,  6.75s/it]                                                        65%|   | 4255/6500 [8:02:46<4:12:28,  6.75s/it] 65%|   | 4256/65{'loss': 0.4167, 'learning_rate': 2.665030898259012e-05, 'epoch': 0.65}
{'loss': 0.385, 'learning_rate': 2.6628936007982815e-05, 'epoch': 0.65}
{'loss': 0.6575, 'learning_rate': 2.660756849621962e-05, 'epoch': 0.66}
{'loss': 0.3932, 'learning_rate': 2.6586206452295058e-05, 'epoch': 0.66}
{'loss': 0.3775, 'learning_rate': 2.6564849881202393e-05, 'epoch': 0.66}
00 [8:02:52<4:10:01,  6.69s/it]                                                        65%|   | 4256/6500 [8:02:52<4:10:01,  6.69s/it] 65%|   | 4257/6500 [8:02:59<4:08:18,  6.64s/it]                                                        65%|   | 4257/6500 [8:02:59<4:08:18,  6.64s/it] 66%|   | 4258/6500 [8:03:06<4:07:01,  6.61s/it]                                                        66%|   | 4258/6500 [8:03:06<4:07:01,  6.61s/it] 66%|   | 4259/6500 [8:03:12<4:06:08,  6.59s/it]                                                        66%|   | 4259/6500 [8:03:12<4:06:08,  6.59s/it] 66%|   | 4260/6500 [8:03:19<4:05:20,  6.57s/it]                                                        66%|   | 4260/6500 [8:03:19<4:05:20,  6.57s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8720371127128601, 'eval_runtime': 1.4827, 'eval_samples_per_second': 8.093, 'eval_steps_per_second': 2.023, 'epoch': 0.66}
                                                        66%|   | 4260/6500 [8:03:20<4:05:20,  6.57s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-4260
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4260
/projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4260
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4260
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4260
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4260/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4260/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4260/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4260/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3785, 'learning_rate': 2.6543498787933563e-05, 'epoch': 0.66}
{'loss': 0.3761, 'learning_rate': 2.652215317747928e-05, 'epoch': 0.66}
{'loss': 0.3792, 'learning_rate': 2.650081305482895e-05, 'epoch': 0.66}
{'loss': 0.3788, 'learning_rate': 2.6479478424970683e-05, 'epoch': 0.66}
{'loss': 0.3946, 'learning_rate': 2.6458149292891353e-05, 'epoch': 0.66}
 66%|   | 4261/6500 [8:03:27<4:25:21,  7.11s/it]                                                        66%|   | 4261/6500 [8:03:27<4:25:21,  7.11s/it] 66%|   | 4262/6500 [8:03:34<4:19:00,  6.94s/it]                                                        66%|   | 4262/6500 [8:03:34<4:19:00,  6.94s/it] 66%|   | 4263/6500 [8:03:40<4:14:41,  6.83s/it]                                                        66%|   | 4263/6500 [8:03:40<4:14:41,  6.83s/it] 66%|   | 4264/6500 [8:03:47<4:11:24,  6.75s/it]                                                        66%|   | 4264/6500 [8:03:47<4:11:24,  6.75s/it] 66%|   | 4265/6500 [8:03:53<4:09:08,  6.69s/it]                                                        66%|   | 4265/6500 [8:03:53<4:09:08,  6.69s/it] 66%|   | 4266/65{'loss': 0.3888, 'learning_rate': 2.6436825663576466e-05, 'epoch': 0.66}
{'loss': 0.3785, 'learning_rate': 2.6415507542010315e-05, 'epoch': 0.66}
{'loss': 0.3802, 'learning_rate': 2.6394194933175875e-05, 'epoch': 0.66}
{'loss': 0.3738, 'learning_rate': 2.637288784205485e-05, 'epoch': 0.66}
{'loss': 0.3745, 'learning_rate': 2.6351586273627636e-05, 'epoch': 0.66}
00 [8:04:00<4:07:24,  6.64s/it]                                                        66%|   | 4266/6500 [8:04:00<4:07:24,  6.64s/it] 66%|   | 4267/6500 [8:04:07<4:13:09,  6.80s/it]                                                        66%|   | 4267/6500 [8:04:07<4:13:09,  6.80s/it] 66%|   | 4268/6500 [8:04:13<4:10:12,  6.73s/it]                                                        66%|   | 4268/6500 [8:04:13<4:10:12,  6.73s/it] 66%|   | 4269/6500 [8:04:20<4:08:13,  6.68s/it]                                                        66%|   | 4269/6500 [8:04:20<4:08:13,  6.68s/it] 66%|   | 4270/6500 [8:04:27<4:06:48,  6.64s/it]                                                        66%|   | 4270/6500 [8:04:27<4:06:48,  6.64s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8763472437858582, 'eval_runtime': 1.5, 'eval_samples_per_second': 8.0, 'eval_steps_per_second': 2.0, 'epoch': 0.66}
                                                        66%|   | 4270/6500 [8:04:28<4:06:48,  6.64s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-4270
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4270
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4270
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4270
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4270
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4270/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4270/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4270/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4270/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.4093, 'learning_rate': 2.6330290232873345e-05, 'epoch': 0.66}
{'loss': 0.3785, 'learning_rate': 2.6308999724769778e-05, 'epoch': 0.66}
{'loss': 0.6616, 'learning_rate': 2.6287714754293503e-05, 'epoch': 0.66}
{'loss': 0.389, 'learning_rate': 2.6266435326419747e-05, 'epoch': 0.66}
{'loss': 0.3832, 'learning_rate': 2.624516144612241e-05, 'epoch': 0.66}
 66%|   | 4271/6500 [8:04:35<4:26:49,  7.18s/it]                                                        66%|   | 4271/6500 [8:04:35<4:26:49,  7.18s/it] 66%|   | 4272/6500 [8:04:42<4:19:40,  6.99s/it]                                                        66%|   | 4272/6500 [8:04:42<4:19:40,  6.99s/it] 66%|   | 4273/6500 [8:04:48<4:14:35,  6.86s/it]                                                        66%|   | 4273/6500 [8:04:48<4:14:35,  6.86s/it] 66%|   | 4274/6500 [8:04:55<4:11:02,  6.77s/it]                                                        66%|   | 4274/6500 [8:04:55<4:11:02,  6.77s/it] 66%|   | 4275/6500 [8:05:01<4:08:29,  6.70s/it]                                                        66%|   | 4275/6500 [8:05:01<4:08:29,  6.70s/it] 66%|   | 4276/65{'loss': 0.3754, 'learning_rate': 2.6223893118374154e-05, 'epoch': 0.66}
{'loss': 0.3763, 'learning_rate': 2.6202630348146324e-05, 'epoch': 0.66}
{'loss': 0.3812, 'learning_rate': 2.618137314040896e-05, 'epoch': 0.66}
{'loss': 0.3844, 'learning_rate': 2.61601215001308e-05, 'epoch': 0.66}
{'loss': 0.3957, 'learning_rate': 2.6138875432279297e-05, 'epoch': 0.66}
00 [8:05:08<4:07:13,  6.67s/it]                                                        66%|   | 4276/6500 [8:05:08<4:07:13,  6.67s/it] 66%|   | 4277/6500 [8:05:14<4:05:51,  6.64s/it]                                                        66%|   | 4277/6500 [8:05:14<4:05:51,  6.64s/it] 66%|   | 4278/6500 [8:05:21<4:04:54,  6.61s/it]                                                        66%|   | 4278/6500 [8:05:21<4:04:54,  6.61s/it] 66%|   | 4279/6500 [8:05:27<4:04:02,  6.59s/it]                                                        66%|   | 4279/6500 [8:05:27<4:04:02,  6.59s/it] 66%|   | 4280/6500 [8:05:34<4:03:33,  6.58s/it]                                                        66%|   | 4280/6500 [8:05:34<4:03:33,  6.58s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8649828433990479, 'eval_runtime': 1.4788, 'eval_samples_per_second': 8.114, 'eval_steps_per_second': 2.029, 'epoch': 0.66}
                                                        66%|   | 4280/6500 [8:05:36<4:03:33,  6.58s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-4280
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4280
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4280
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4280
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4280
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4280/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4280/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4280/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4280/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3845, 'learning_rate': 2.6117634941820578e-05, 'epoch': 0.66}
{'loss': 0.3804, 'learning_rate': 2.6096400033719487e-05, 'epoch': 0.66}
{'loss': 0.3901, 'learning_rate': 2.607517071293955e-05, 'epoch': 0.66}
{'loss': 0.3617, 'learning_rate': 2.6053946984443e-05, 'epoch': 0.66}
{'loss': 0.379, 'learning_rate': 2.6032728853190757e-05, 'epoch': 0.66}
 66%|   | 4281/6500 [8:05:42<4:23:33,  7.13s/it]                                                        66%|   | 4281/6500 [8:05:42<4:23:33,  7.13s/it] 66%|   | 4282/6500 [8:05:49<4:17:04,  6.95s/it]                                                        66%|   | 4282/6500 [8:05:49<4:17:04,  6.95s/it] 66%|   | 4283/6500 [8:05:56<4:22:20,  7.10s/it]                                                        66%|   | 4283/6500 [8:05:56<4:22:20,  7.10s/it] 66%|   | 4284/6500 [8:06:03<4:16:13,  6.94s/it]                                                        66%|   | 4284/6500 [8:06:03<4:16:13,  6.94s/it] 66%|   | 4285/6500 [8:06:10<4:11:56,  6.82s/it]                                                        66%|   | 4285/6500 [8:06:10<4:11:56,  6.82s/it] 66%|   | 4286/65{'loss': 0.404, 'learning_rate': 2.601151632414241e-05, 'epoch': 0.66}
{'loss': 0.3832, 'learning_rate': 2.5990309402256264e-05, 'epoch': 0.66}
{'loss': 0.663, 'learning_rate': 2.596910809248932e-05, 'epoch': 0.66}
{'loss': 0.3739, 'learning_rate': 2.5947912399797246e-05, 'epoch': 0.66}
{'loss': 0.3921, 'learning_rate': 2.5926722329134412e-05, 'epoch': 0.66}
00 [8:06:16<4:08:46,  6.74s/it]                                                        66%|   | 4286/6500 [8:06:16<4:08:46,  6.74s/it] 66%|   | 4287/6500 [8:06:23<4:06:34,  6.69s/it]                                                        66%|   | 4287/6500 [8:06:23<4:06:34,  6.69s/it] 66%|   | 4288/6500 [8:06:29<4:04:58,  6.64s/it]                                                        66%|   | 4288/6500 [8:06:29<4:04:58,  6.64s/it] 66%|   | 4289/6500 [8:06:36<4:03:51,  6.62s/it]                                                        66%|   | 4289/6500 [8:06:36<4:03:51,  6.62s/it] 66%|   | 4290/6500 [8:06:42<4:03:05,  6.60s/it]                                                        66%|   | 4290/6500 [8:06:42<4:03:05,  6.60s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8742919564247131, 'eval_runtime': 1.4784, 'eval_samples_per_second': 8.117, 'eval_steps_per_second': 2.029, 'epoch': 0.66}
                                                        66%|   | 4290/6500 [8:06:44<4:03:05,  6.60s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-4290
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4290
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4290
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4290
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4290
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4290/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4290/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4290/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4290/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3672, 'learning_rate': 2.5905537885453856e-05, 'epoch': 0.66}
{'loss': 0.3684, 'learning_rate': 2.588435907370733e-05, 'epoch': 0.66}
{'loss': 0.3773, 'learning_rate': 2.5863185898845243e-05, 'epoch': 0.66}
{'loss': 0.3784, 'learning_rate': 2.58420183658167e-05, 'epoch': 0.66}
{'loss': 0.3916, 'learning_rate': 2.5820856479569487e-05, 'epoch': 0.66}
 66%|   | 4291/6500 [8:06:51<4:22:22,  7.13s/it]                                                        66%|   | 4291/6500 [8:06:51<4:22:22,  7.13s/it] 66%|   | 4292/6500 [8:06:57<4:15:55,  6.95s/it]                                                        66%|   | 4292/6500 [8:06:57<4:15:55,  6.95s/it] 66%|   | 4293/6500 [8:07:04<4:11:18,  6.83s/it]                                                        66%|   | 4293/6500 [8:07:04<4:11:18,  6.83s/it] 66%|   | 4294/6500 [8:07:10<4:08:04,  6.75s/it]                                                        66%|   | 4294/6500 [8:07:10<4:08:04,  6.75s/it] 66%|   | 4295/6500 [8:07:17<4:05:51,  6.69s/it]                                                        66%|   | 4295/6500 [8:07:17<4:05:51,  6.69s/it] 66%|   | 4296/65{'loss': 0.3853, 'learning_rate': 2.5799700245050074e-05, 'epoch': 0.66}
{'loss': 0.378, 'learning_rate': 2.5778549667203568e-05, 'epoch': 0.66}
{'loss': 0.3834, 'learning_rate': 2.5757404750973806e-05, 'epoch': 0.66}
{'loss': 0.3783, 'learning_rate': 2.573626550130329e-05, 'epoch': 0.66}
{'loss': 0.407, 'learning_rate': 2.5715131923133184e-05, 'epoch': 0.66}
00 [8:07:23<4:04:07,  6.65s/it]                                                        66%|   | 4296/6500 [8:07:23<4:04:07,  6.65s/it] 66%|   | 4297/6500 [8:07:30<4:03:03,  6.62s/it]                                                        66%|   | 4297/6500 [8:07:30<4:03:03,  6.62s/it] 66%|   | 4298/6500 [8:07:37<4:02:06,  6.60s/it]                                                        66%|   | 4298/6500 [8:07:37<4:02:06,  6.60s/it] 66%|   | 4299/6500 [8:07:44<4:11:24,  6.85s/it]                                                        66%|   | 4299/6500 [8:07:44<4:11:24,  6.85s/it] 66%|   | 4300/6500 [8:07:51<4:07:59,  6.76s/it]                                                        66%|   | 4300/6500 [8:07:51<4:07:59,  6.76s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8779260516166687, 'eval_runtime': 1.4781, 'eval_samples_per_second': 8.118, 'eval_steps_per_second': 2.03, 'epoch': 0.66}
                                                        66%|   | 4300/6500 [8:07:52<4:07:59,  6.76s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-4300
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4300
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4300
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4300
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4300
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4300/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4300/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4300/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4300/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.374, 'learning_rate': 2.569400402140334e-05, 'epoch': 0.66}
{'loss': 0.6467, 'learning_rate': 2.5672881801052273e-05, 'epoch': 0.66}
{'loss': 0.4067, 'learning_rate': 2.565176526701717e-05, 'epoch': 0.66}
{'loss': 0.3754, 'learning_rate': 2.5630654424233903e-05, 'epoch': 0.66}
{'loss': 0.3957, 'learning_rate': 2.560954927763699e-05, 'epoch': 0.66}
 66%|   | 4301/6500 [8:07:59<4:26:10,  7.26s/it]                                                        66%|   | 4301/6500 [8:07:59<4:26:10,  7.26s/it] 66%|   | 4302/6500 [8:08:05<4:18:09,  7.05s/it]                                                        66%|   | 4302/6500 [8:08:05<4:18:09,  7.05s/it] 66%|   | 4303/6500 [8:08:12<4:12:35,  6.90s/it]                                                        66%|   | 4303/6500 [8:08:12<4:12:35,  6.90s/it] 66%|   | 4304/6500 [8:08:19<4:08:41,  6.79s/it]                                                        66%|   | 4304/6500 [8:08:19<4:08:41,  6.79s/it] 66%|   | 4305/6500 [8:08:25<4:05:51,  6.72s/it]                                                        66%|   | 4305/6500 [8:08:25<4:05:51,  6.72s/it] 66%|   | 4306/65{'loss': 0.3676, 'learning_rate': 2.5588449832159633e-05, 'epoch': 0.66}
{'loss': 0.3793, 'learning_rate': 2.556735609273373e-05, 'epoch': 0.66}
{'loss': 0.3806, 'learning_rate': 2.554626806428977e-05, 'epoch': 0.66}
{'loss': 0.3913, 'learning_rate': 2.5525185751756963e-05, 'epoch': 0.66}
{'loss': 0.387, 'learning_rate': 2.5504109160063182e-05, 'epoch': 0.66}
00 [8:08:32<4:03:54,  6.67s/it]                                                        66%|   | 4306/6500 [8:08:32<4:03:54,  6.67s/it] 66%|   | 4307/6500 [8:08:38<4:02:36,  6.64s/it]                                                        66%|   | 4307/6500 [8:08:38<4:02:36,  6.64s/it] 66%|   | 4308/6500 [8:08:45<4:01:25,  6.61s/it]                                                        66%|   | 4308/6500 [8:08:45<4:01:25,  6.61s/it] 66%|   | 4309/6500 [8:08:51<4:00:42,  6.59s/it]                                                        66%|   | 4309/6500 [8:08:51<4:00:42,  6.59s/it] 66%|   | 4310/6500 [8:08:58<4:00:13,  6.58s/it]                                                        66%|   | 4310/6500 [8:08:58<4:00:13,  6.58s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8660479187965393, 'eval_runtime': 1.4781, 'eval_samples_per_second': 8.118, 'eval_steps_per_second': 2.03, 'epoch': 0.66}
                                                        66%|   | 4310/6500 [8:08:59<4:00:13,  6.58s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-4310
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4310
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4310
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4310
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4310
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4310/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4310/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4310/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4310/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3888, 'learning_rate': 2.5483038294134943e-05, 'epoch': 0.66}
{'loss': 0.3771, 'learning_rate': 2.5461973158897435e-05, 'epoch': 0.66}
{'loss': 0.3883, 'learning_rate': 2.5440913759274514e-05, 'epoch': 0.66}
{'loss': 0.3656, 'learning_rate': 2.5419860100188674e-05, 'epoch': 0.66}
{'loss': 0.4072, 'learning_rate': 2.5398812186561095e-05, 'epoch': 0.66}
 66%|   | 4311/6500 [8:09:06<4:20:13,  7.13s/it]                                                        66%|   | 4311/6500 [8:09:06<4:20:13,  7.13s/it] 66%|   | 4312/6500 [8:09:13<4:13:48,  6.96s/it]                                                        66%|   | 4312/6500 [8:09:13<4:13:48,  6.96s/it] 66%|   | 4313/6500 [8:09:19<4:09:15,  6.84s/it]                                                        66%|   | 4313/6500 [8:09:19<4:09:15,  6.84s/it] 66%|   | 4314/6500 [8:09:26<4:05:51,  6.75s/it]                                                        66%|   | 4314/6500 [8:09:26<4:05:51,  6.75s/it] 66%|   | 4315/6500 [8:09:33<4:13:33,  6.96s/it]                                                        66%|   | 4315/6500 [8:09:33<4:13:33,  6.96s/it] 66%|   | 4316/65{'loss': 0.3772, 'learning_rate': 2.537777002331158e-05, 'epoch': 0.66}
{'loss': 0.6577, 'learning_rate': 2.535673361535862e-05, 'epoch': 0.66}
{'loss': 0.3962, 'learning_rate': 2.5335702967619347e-05, 'epoch': 0.66}
{'loss': 0.3704, 'learning_rate': 2.5314678085009558e-05, 'epoch': 0.66}
{'loss': 0.3866, 'learning_rate': 2.5293658972443663e-05, 'epoch': 0.66}
00 [8:09:40<4:09:01,  6.84s/it]                                                        66%|   | 4316/6500 [8:09:40<4:09:01,  6.84s/it] 66%|   | 4317/6500 [8:09:47<4:05:38,  6.75s/it]                                                        66%|   | 4317/6500 [8:09:47<4:05:38,  6.75s/it] 66%|   | 4318/6500 [8:09:53<4:03:31,  6.70s/it]                                                        66%|   | 4318/6500 [8:09:53<4:03:31,  6.70s/it] 66%|   | 4319/6500 [8:10:00<4:01:47,  6.65s/it]                                                        66%|   | 4319/6500 [8:10:00<4:01:47,  6.65s/it] 66%|   | 4320/6500 [8:10:06<4:00:39,  6.62s/it]                                                        66%|   | 4320/6500 [8:10:06<4:00:39,  6.62s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8737769722938538, 'eval_runtime': 1.4759, 'eval_samples_per_second': 8.131, 'eval_steps_per_second': 2.033, 'epoch': 0.66}
                                                        66%|   | 4320/6500 [8:10:08<4:00:39,  6.62s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-4320
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  I AM HERE AND I AM SAVING THE MODEL/projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4320
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4320

I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4320
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4320
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4320/pytorch_model.bin
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4320/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4320/pytorch_model.binthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4320/pytorch_model.bin

/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3673, 'learning_rate': 2.5272645634834764e-05, 'epoch': 0.66}
{'loss': 0.3726, 'learning_rate': 2.5251638077094602e-05, 'epoch': 0.66}
{'loss': 0.3875, 'learning_rate': 2.523063630413357e-05, 'epoch': 0.67}
{'loss': 0.3779, 'learning_rate': 2.5209640320860694e-05, 'epoch': 0.67}
{'loss': 0.3876, 'learning_rate': 2.5188650132183677e-05, 'epoch': 0.67}
 66%|   | 4321/6500 [8:10:15<4:21:11,  7.19s/it]                                                        66%|   | 4321/6500 [8:10:15<4:21:11,  7.19s/it] 66%|   | 4322/6500 [8:10:21<4:14:07,  7.00s/it]                                                        66%|   | 4322/6500 [8:10:21<4:14:07,  7.00s/it] 67%|   | 4323/6500 [8:10:28<4:09:06,  6.87s/it]                                                        67%|   | 4323/6500 [8:10:28<4:09:06,  6.87s/it] 67%|   | 4324/6500 [8:10:34<4:05:31,  6.77s/it]                                                        67%|   | 4324/6500 [8:10:34<4:05:31,  6.77s/it] 67%|   | 4325/6500 [8:10:41<4:03:03,  6.71s/it]                                                        67%|   | 4325/6500 [8:10:41<4:03:03,  6.71s/it] 67%|   | 4326/65{'loss': 0.383, 'learning_rate': 2.5167665743008828e-05, 'epoch': 0.67}
{'loss': 0.3727, 'learning_rate': 2.5146687158241132e-05, 'epoch': 0.67}
{'loss': 0.3762, 'learning_rate': 2.5125714382784198e-05, 'epoch': 0.67}
{'loss': 0.3644, 'learning_rate': 2.5104747421540294e-05, 'epoch': 0.67}
{'loss': 0.4132, 'learning_rate': 2.5083786279410325e-05, 'epoch': 0.67}
00 [8:10:47<4:01:22,  6.66s/it]                                                        67%|   | 4326/6500 [8:10:47<4:01:22,  6.66s/it] 67%|   | 4327/6500 [8:10:54<4:00:01,  6.63s/it]                                                        67%|   | 4327/6500 [8:10:54<4:00:01,  6.63s/it] 67%|   | 4328/6500 [8:11:01<3:59:09,  6.61s/it]                                                        67%|   | 4328/6500 [8:11:01<3:59:09,  6.61s/it] 67%|   | 4329/6500 [8:11:07<3:58:26,  6.59s/it]                                                        67%|   | 4329/6500 [8:11:07<3:58:26,  6.59s/it] 67%|   | 4330/6500 [8:11:14<3:57:55,  6.58s/it]                                                        67%|   | 4330/6500 [8:11:14<3:57:55,  6.58s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8771070241928101, 'eval_runtime': 1.4708, 'eval_samples_per_second': 8.159, 'eval_steps_per_second': 2.04, 'epoch': 0.67}
                                                        67%|   | 4330/6500 [8:11:15<3:57:55,  6.58s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-4330
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4330
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4330
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4330
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4330
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4330/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4330/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4330/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4330/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3787, 'learning_rate': 2.5062830961293815e-05, 'epoch': 0.67}
{'loss': 0.6565, 'learning_rate': 2.5041881472088934e-05, 'epoch': 0.67}
{'loss': 0.3934, 'learning_rate': 2.502093781669252e-05, 'epoch': 0.67}
{'loss': 0.3785, 'learning_rate': 2.500000000000001e-05, 'epoch': 0.67}
{'loss': 0.3682, 'learning_rate': 2.49790680269055e-05, 'epoch': 0.67}
 67%|   | 4331/6500 [8:11:23<4:27:18,  7.39s/it]                                                        67%|   | 4331/6500 [8:11:23<4:27:18,  7.39s/it] 67%|   | 4332/6500 [8:11:30<4:17:54,  7.14s/it]                                                        67%|   | 4332/6500 [8:11:30<4:17:54,  7.14s/it] 67%|   | 4333/6500 [8:11:36<4:11:23,  6.96s/it]                                                        67%|   | 4333/6500 [8:11:36<4:11:23,  6.96s/it] 67%|   | 4334/6500 [8:11:43<4:06:47,  6.84s/it]                                                        67%|   | 4334/6500 [8:11:43<4:06:47,  6.84s/it] 67%|   | 4335/6500 [8:11:49<4:03:22,  6.74s/it]                                                        67%|   | 4335/6500 [8:11:49<4:03:22,  6.74s/it] 67%|   | 4336/65{'loss': 0.3683, 'learning_rate': 2.495814190230171e-05, 'epoch': 0.67}
{'loss': 0.3748, 'learning_rate': 2.4937221631079993e-05, 'epoch': 0.67}
{'loss': 0.3771, 'learning_rate': 2.4916307218130337e-05, 'epoch': 0.67}
{'loss': 0.3853, 'learning_rate': 2.4895398668341352e-05, 'epoch': 0.67}
{'loss': 0.3849, 'learning_rate': 2.4874495986600294e-05, 'epoch': 0.67}
00 [8:11:56<4:00:59,  6.68s/it]                                                        67%|   | 4336/6500 [8:11:56<4:00:59,  6.68s/it] 67%|   | 4337/6500 [8:12:02<3:59:19,  6.64s/it]                                                        67%|   | 4337/6500 [8:12:02<3:59:19,  6.64s/it] 67%|   | 4338/6500 [8:12:09<3:58:00,  6.61s/it]                                                        67%|   | 4338/6500 [8:12:09<3:58:00,  6.61s/it] 67%|   | 4339/6500 [8:12:15<3:57:04,  6.58s/it]                                                        67%|   | 4339/6500 [8:12:15<3:57:04,  6.58s/it] 67%|   | 4340/6500 [8:12:22<3:56:29,  6.57s/it]                                                        67%|   | 4340/6500 [8:12:22<3:56:29,  6.57s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8681525588035583, 'eval_runtime': 1.4844, 'eval_samples_per_second': 8.084, 'eval_steps_per_second': 2.021, 'epoch': 0.67}
                                                        67%|   | 4340/6500 [8:12:23<3:56:29,  6.57s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-4340
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4340
/projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4340
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4340
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4340
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4340/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4340/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4340/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4340/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3794, 'learning_rate': 2.4853599177793052e-05, 'epoch': 0.67}
{'loss': 0.3817, 'learning_rate': 2.483270824680409e-05, 'epoch': 0.67}
{'loss': 0.3674, 'learning_rate': 2.4811823198516555e-05, 'epoch': 0.67}
{'loss': 0.3661, 'learning_rate': 2.4790944037812202e-05, 'epoch': 0.67}
{'loss': 0.4085, 'learning_rate': 2.4770070769571408e-05, 'epoch': 0.67}
 67%|   | 4341/6500 [8:12:30<4:16:15,  7.12s/it]                                                        67%|   | 4341/6500 [8:12:30<4:16:15,  7.12s/it] 67%|   | 4342/6500 [8:12:37<4:09:49,  6.95s/it]                                                        67%|   | 4342/6500 [8:12:37<4:09:49,  6.95s/it] 67%|   | 4343/6500 [8:12:43<4:05:13,  6.82s/it]                                                        67%|   | 4343/6500 [8:12:43<4:05:13,  6.82s/it] 67%|   | 4344/6500 [8:12:50<4:01:54,  6.73s/it]                                                        67%|   | 4344/6500 [8:12:50<4:01:54,  6.73s/it] 67%|   | 4345/6500 [8:12:56<3:59:35,  6.67s/it]                                                        67%|   | 4345/6500 [8:12:56<3:59:35,  6.67s/it] 67%|   | 4346/65{'loss': 0.3798, 'learning_rate': 2.4749203398673172e-05, 'epoch': 0.67}
{'loss': 0.6548, 'learning_rate': 2.4728341929995092e-05, 'epoch': 0.67}
{'loss': 0.3876, 'learning_rate': 2.4707486368413445e-05, 'epoch': 0.67}
{'loss': 0.3805, 'learning_rate': 2.4686636718803086e-05, 'epoch': 0.67}
{'loss': 0.3758, 'learning_rate': 2.4665792986037507e-05, 'epoch': 0.67}
00 [8:13:03<3:58:02,  6.63s/it]                                                        67%|   | 4346/6500 [8:13:03<3:58:02,  6.63s/it] 67%|   | 4347/6500 [8:13:09<3:56:54,  6.60s/it]                                                        67%|   | 4347/6500 [8:13:09<3:56:54,  6.60s/it] 67%|   | 4348/6500 [8:13:17<4:02:55,  6.77s/it]                                                        67%|   | 4348/6500 [8:13:17<4:02:55,  6.77s/it] 67%|   | 4349/6500 [8:13:23<4:00:09,  6.70s/it]                                                        67%|   | 4349/6500 [8:13:23<4:00:09,  6.70s/it] 67%|   | 4350/6500 [8:13:30<3:58:21,  6.65s/it]                                                        67%|   | 4350/6500 [8:13:30<3:58:21,  6.65s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8744984269142151, 'eval_runtime': 1.5026, 'eval_samples_per_second': 7.986, 'eval_steps_per_second': 1.996, 'epoch': 0.67}
                                                        67%|   | 4350/6500 [8:13:31<3:58:21,  6.65s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-4350
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4350
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4350
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4350
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4350
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4350/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4350/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4350/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4350/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3706, 'learning_rate': 2.464495517498876e-05, 'epoch': 0.67}
{'loss': 0.374, 'learning_rate': 2.4624123290527578e-05, 'epoch': 0.67}
{'loss': 0.3854, 'learning_rate': 2.4603297337523294e-05, 'epoch': 0.67}
{'loss': 0.3883, 'learning_rate': 2.458247732084384e-05, 'epoch': 0.67}
{'loss': 0.3826, 'learning_rate': 2.456166324535577e-05, 'epoch': 0.67}
 67%|   | 4351/6500 [8:13:38<4:17:45,  7.20s/it]                                                        67%|   | 4351/6500 [8:13:38<4:17:45,  7.20s/it] 67%|   | 4352/6500 [8:13:45<4:10:39,  7.00s/it]                                                        67%|   | 4352/6500 [8:13:45<4:10:39,  7.00s/it] 67%|   | 4353/6500 [8:13:51<4:05:25,  6.86s/it]                                                        67%|   | 4353/6500 [8:13:51<4:05:25,  6.86s/it] 67%|   | 4354/6500 [8:13:58<4:01:47,  6.76s/it]                                                        67%|   | 4354/6500 [8:13:58<4:01:47,  6.76s/it] 67%|   | 4355/6500 [8:14:04<3:59:13,  6.69s/it]                                                        67%|   | 4355/6500 [8:14:04<3:59:13,  6.69s/it] 67%|   | 4356/65{'loss': 0.3825, 'learning_rate': 2.454085511592425e-05, 'epoch': 0.67}
{'loss': 0.3808, 'learning_rate': 2.4520052937413058e-05, 'epoch': 0.67}
{'loss': 0.3652, 'learning_rate': 2.4499256714684565e-05, 'epoch': 0.67}
{'loss': 0.375, 'learning_rate': 2.447846645259977e-05, 'epoch': 0.67}
{'loss': 0.4043, 'learning_rate': 2.4457682156018263e-05, 'epoch': 0.67}
00 [8:14:11<3:57:25,  6.64s/it]                                                        67%|   | 4356/6500 [8:14:11<3:57:25,  6.64s/it] 67%|   | 4357/6500 [8:14:17<3:56:02,  6.61s/it]                                                        67%|   | 4357/6500 [8:14:17<3:56:02,  6.61s/it] 67%|   | 4358/6500 [8:14:24<3:54:58,  6.58s/it]                                                        67%|   | 4358/6500 [8:14:24<3:54:58,  6.58s/it] 67%|   | 4359/6500 [8:14:30<3:54:17,  6.57s/it]                                                        67%|   | 4359/6500 [8:14:30<3:54:17,  6.57s/it] 67%|   | 4360/6500 [8:14:37<3:53:45,  6.55s/it]                                                        67%|   | 4360/6500 [8:14:37<3:53:45,  6.55s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.880280613899231, 'eval_runtime': 1.4728, 'eval_samples_per_second': 8.148, 'eval_steps_per_second': 2.037, 'epoch': 0.67}
                                                        67%|   | 4360/6500 [8:14:38<3:53:45,  6.55s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-4360
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4360I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4360

I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4360
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4360
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4360/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4360/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4360/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4360/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3792, 'learning_rate': 2.4436903829798274e-05, 'epoch': 0.67}
{'loss': 0.6562, 'learning_rate': 2.441613147879657e-05, 'epoch': 0.67}
{'loss': 0.3696, 'learning_rate': 2.4395365107868583e-05, 'epoch': 0.67}
{'loss': 0.3927, 'learning_rate': 2.437460472186832e-05, 'epoch': 0.67}
{'loss': 0.3671, 'learning_rate': 2.4353850325648404e-05, 'epoch': 0.67}
 67%|   | 4361/6500 [8:14:45<4:12:49,  7.09s/it]                                                        67%|   | 4361/6500 [8:14:45<4:12:49,  7.09s/it] 67%|   | 4362/6500 [8:14:52<4:06:45,  6.92s/it]                                                        67%|   | 4362/6500 [8:14:52<4:06:45,  6.92s/it] 67%|   | 4363/6500 [8:14:58<4:02:23,  6.81s/it]                                                        67%|   | 4363/6500 [8:14:58<4:02:23,  6.81s/it] 67%|   | 4364/6500 [8:15:06<4:08:48,  6.99s/it]                                                        67%|   | 4364/6500 [8:15:06<4:08:48,  6.99s/it] 67%|   | 4365/6500 [8:15:12<4:03:47,  6.85s/it]                                                        67%|   | 4365/6500 [8:15:12<4:03:47,  6.85s/it] 67%|   | 4366/65{'loss': 0.3664, 'learning_rate': 2.4333101924060035e-05, 'epoch': 0.67}
{'loss': 0.3775, 'learning_rate': 2.4312359521953045e-05, 'epoch': 0.67}
{'loss': 0.3782, 'learning_rate': 2.4291623124175822e-05, 'epoch': 0.67}
{'loss': 0.3862, 'learning_rate': 2.427089273557539e-05, 'epoch': 0.67}
{'loss': 0.3853, 'learning_rate': 2.4250168360997344e-05, 'epoch': 0.67}
00 [8:15:19<4:00:28,  6.76s/it]                                                        67%|   | 4366/6500 [8:15:19<4:00:28,  6.76s/it] 67%|   | 4367/6500 [8:15:25<3:57:50,  6.69s/it]                                                        67%|   | 4367/6500 [8:15:25<3:57:50,  6.69s/it] 67%|   | 4368/6500 [8:15:32<3:56:05,  6.64s/it]                                                        67%|   | 4368/6500 [8:15:32<3:56:05,  6.64s/it] 67%|   | 4369/6500 [8:15:38<3:54:40,  6.61s/it]                                                        67%|   | 4369/6500 [8:15:38<3:54:40,  6.61s/it] 67%|   | 4370/6500 [8:15:45<3:53:49,  6.59s/it]                                                        67%|   | 4370/6500 [8:15:45<3:53:49,  6.59s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8690184950828552, 'eval_runtime': 1.4844, 'eval_samples_per_second': 8.084, 'eval_steps_per_second': 2.021, 'epoch': 0.67}
                                                        67%|   | 4370/6500 [8:15:46<3:53:49,  6.59s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-4370
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4370I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4370

I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4370
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4370
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4370/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4370/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4370/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4370/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3772, 'learning_rate': 2.422945000528588e-05, 'epoch': 0.67}
{'loss': 0.3858, 'learning_rate': 2.4208737673283815e-05, 'epoch': 0.67}
{'loss': 0.3704, 'learning_rate': 2.4188031369832482e-05, 'epoch': 0.67}
{'loss': 0.3814, 'learning_rate': 2.416733109977188e-05, 'epoch': 0.67}
{'loss': 0.3941, 'learning_rate': 2.4146636867940565e-05, 'epoch': 0.67}
 67%|   | 4371/6500 [8:15:53<4:12:17,  7.11s/it]                                                        67%|   | 4371/6500 [8:15:53<4:12:17,  7.11s/it] 67%|   | 4372/6500 [8:16:00<4:05:59,  6.94s/it]                                                        67%|   | 4372/6500 [8:16:00<4:05:59,  6.94s/it] 67%|   | 4373/6500 [8:16:06<4:01:39,  6.82s/it]                                                        67%|   | 4373/6500 [8:16:06<4:01:39,  6.82s/it] 67%|   | 4374/6500 [8:16:13<3:58:28,  6.73s/it]                                                        67%|   | 4374/6500 [8:16:13<3:58:28,  6.73s/it] 67%|   | 4375/6500 [8:16:19<3:56:10,  6.67s/it]                                                        67%|   | 4375/6500 [8:16:19<3:56:10,  6.67s/it] 67%|   | 4376/65{'loss': 0.4057, 'learning_rate': 2.4125948679175686e-05, 'epoch': 0.67}
{'loss': 0.6405, 'learning_rate': 2.4105266538312994e-05, 'epoch': 0.67}
{'loss': 0.3774, 'learning_rate': 2.4084590450186806e-05, 'epoch': 0.67}
{'loss': 0.3896, 'learning_rate': 2.4063920419630025e-05, 'epoch': 0.67}
{'loss': 0.3616, 'learning_rate': 2.4043256451474162e-05, 'epoch': 0.67}
00 [8:16:26<3:54:25,  6.62s/it]                                                        67%|   | 4376/6500 [8:16:26<3:54:25,  6.62s/it] 67%|   | 4377/6500 [8:16:32<3:53:14,  6.59s/it]                                                        67%|   | 4377/6500 [8:16:32<3:53:14,  6.59s/it] 67%|   | 4378/6500 [8:16:39<3:52:34,  6.58s/it]                                                        67%|   | 4378/6500 [8:16:39<3:52:34,  6.58s/it] 67%|   | 4379/6500 [8:16:46<3:52:06,  6.57s/it]                                                        67%|   | 4379/6500 [8:16:46<3:52:06,  6.57s/it] 67%|   | 4380/6500 [8:16:53<4:01:04,  6.82s/it]                                                        67%|   | 4380/6500 [8:16:53<4:01:04,  6.82s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8749763369560242, 'eval_runtime': 1.4766, 'eval_samples_per_second': 8.127, 'eval_steps_per_second': 2.032, 'epoch': 0.67}
                                                        67%|   | 4380/6500 [8:16:54<4:01:04,  6.82s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-4380
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4380I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4380

I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4380
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4380
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4380/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4380/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4380/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4380/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3732, 'learning_rate': 2.402259855054928e-05, 'epoch': 0.67}
{'loss': 0.3805, 'learning_rate': 2.400194672168404e-05, 'epoch': 0.67}
{'loss': 0.3766, 'learning_rate': 2.39813009697057e-05, 'epoch': 0.67}
{'loss': 0.3886, 'learning_rate': 2.3960661299440047e-05, 'epoch': 0.67}
{'loss': 0.3924, 'learning_rate': 2.3940027715711495e-05, 'epoch': 0.67}
 67%|   | 4381/6500 [8:17:01<4:17:43,  7.30s/it]                                                        67%|   | 4381/6500 [8:17:01<4:17:43,  7.30s/it] 67%|   | 4382/6500 [8:17:08<4:09:52,  7.08s/it]                                                        67%|   | 4382/6500 [8:17:08<4:09:52,  7.08s/it] 67%|   | 4383/6500 [8:17:14<4:04:00,  6.92s/it]                                                        67%|   | 4383/6500 [8:17:14<4:04:00,  6.92s/it] 67%|   | 4384/6500 [8:17:21<3:59:58,  6.80s/it]                                                        67%|   | 4384/6500 [8:17:21<3:59:58,  6.80s/it] 67%|   | 4385/6500 [8:17:28<3:57:12,  6.73s/it]                                                        67%|   | 4385/6500 [8:17:28<3:57:12,  6.73s/it] 67%|   | 4386/65{'loss': 0.3782, 'learning_rate': 2.3919400223343015e-05, 'epoch': 0.67}
{'loss': 0.3834, 'learning_rate': 2.3898778827156156e-05, 'epoch': 0.67}
{'loss': 0.3659, 'learning_rate': 2.3878163531971053e-05, 'epoch': 0.68}
{'loss': 0.4137, 'learning_rate': 2.3857554342606397e-05, 'epoch': 0.68}
{'loss': 0.3699, 'learning_rate': 2.383695126387947e-05, 'epoch': 0.68}
00 [8:17:34<3:55:15,  6.68s/it]                                                        67%|   | 4386/6500 [8:17:34<3:55:15,  6.68s/it] 67%|   | 4387/6500 [8:17:41<3:53:47,  6.64s/it]                                                        67%|   | 4387/6500 [8:17:41<3:53:47,  6.64s/it] 68%|   | 4388/6500 [8:17:47<3:52:48,  6.61s/it]                                                        68%|   | 4388/6500 [8:17:47<3:52:48,  6.61s/it] 68%|   | 4389/6500 [8:17:54<3:52:09,  6.60s/it]                                                        68%|   | 4389/6500 [8:17:54<3:52:09,  6.60s/it] 68%|   | 4390/6500 [8:18:00<3:51:32,  6.58s/it]                                                        68%|   | 4390/6500 [8:18:00<3:51:32,  6.58s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8772830367088318, 'eval_runtime': 1.4827, 'eval_samples_per_second': 8.093, 'eval_steps_per_second': 2.023, 'epoch': 0.68}
                                                        68%|   | 4390/6500 [8:18:02<3:51:32,  6.58s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-4390
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4390
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4390
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4390
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4390
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4390/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4390/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4390/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4390/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.6567, 'learning_rate': 2.381635430060611e-05, 'epoch': 0.68}
{'loss': 0.3875, 'learning_rate': 2.379576345760073e-05, 'epoch': 0.68}
{'loss': 0.375, 'learning_rate': 2.3775178739676318e-05, 'epoch': 0.68}
{'loss': 0.3901, 'learning_rate': 2.3754600151644445e-05, 'epoch': 0.68}
{'loss': 0.3719, 'learning_rate': 2.37340276983152e-05, 'epoch': 0.68}
 68%|   | 4391/6500 [8:18:09<4:10:30,  7.13s/it]                                                        68%|   | 4391/6500 [8:18:09<4:10:30,  7.13s/it] 68%|   | 4392/6500 [8:18:15<4:04:16,  6.95s/it]                                                        68%|   | 4392/6500 [8:18:15<4:04:16,  6.95s/it] 68%|   | 4393/6500 [8:18:22<3:59:53,  6.83s/it]                                                        68%|   | 4393/6500 [8:18:22<3:59:53,  6.83s/it] 68%|   | 4394/6500 [8:18:28<3:56:39,  6.74s/it]                                                        68%|   | 4394/6500 [8:18:28<3:56:39,  6.74s/it] 68%|   | 4395/6500 [8:18:35<3:54:31,  6.68s/it]                                                        68%|   | 4395/6500 [8:18:35<3:54:31,  6.68s/it] 68%|   | 4396/65{'loss': 0.3802, 'learning_rate': 2.371346138449727e-05, 'epoch': 0.68}
{'loss': 0.3789, 'learning_rate': 2.369290121499792e-05, 'epoch': 0.68}
{'loss': 0.3891, 'learning_rate': 2.367234719462297e-05, 'epoch': 0.68}
{'loss': 0.3846, 'learning_rate': 2.3651799328176776e-05, 'epoch': 0.68}
{'loss': 0.3855, 'learning_rate': 2.3631257620462294e-05, 'epoch': 0.68}
00 [8:18:42<4:02:19,  6.91s/it]                                                        68%|   | 4396/6500 [8:18:42<4:02:19,  6.91s/it] 68%|   | 4397/6500 [8:18:49<3:58:31,  6.81s/it]                                                        68%|   | 4397/6500 [8:18:49<3:58:31,  6.81s/it] 68%|   | 4398/6500 [8:18:55<3:55:58,  6.74s/it]                                                        68%|   | 4398/6500 [8:18:55<3:55:58,  6.74s/it] 68%|   | 4399/6500 [8:19:02<3:54:06,  6.69s/it]                                                        68%|   | 4399/6500 [8:19:02<3:54:06,  6.69s/it] 68%|   | 4400/6500 [8:19:09<3:52:27,  6.64s/it]                                                        68%|   | 4400/6500 [8:19:09<3:52:27,  6.64s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.867677628993988, 'eval_runtime': 1.4882, 'eval_samples_per_second': 8.064, 'eval_steps_per_second': 2.016, 'epoch': 0.68}
                                                        68%|   | 4400/6500 [8:19:10<3:52:27,  6.64s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-4400
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4400I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4400

I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4400
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4400
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4400/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4400/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4400/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4400/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3789, 'learning_rate': 2.3610722076281023e-05, 'epoch': 0.68}
{'loss': 0.3749, 'learning_rate': 2.3590192700433013e-05, 'epoch': 0.68}
{'loss': 0.3681, 'learning_rate': 2.3569669497716883e-05, 'epoch': 0.68}
{'loss': 0.4161, 'learning_rate': 2.3549152472929808e-05, 'epoch': 0.68}
{'loss': 0.3855, 'learning_rate': 2.3528641630867526e-05, 'epoch': 0.68}
 68%|   | 4401/6500 [8:19:17<4:10:50,  7.17s/it]                                                        68%|   | 4401/6500 [8:19:17<4:10:50,  7.17s/it] 68%|   | 4402/6500 [8:19:24<4:04:17,  6.99s/it]                                                        68%|   | 4402/6500 [8:19:24<4:04:17,  6.99s/it] 68%|   | 4403/6500 [8:19:30<3:59:35,  6.86s/it]                                                        68%|   | 4403/6500 [8:19:30<3:59:35,  6.86s/it] 68%|   | 4404/6500 [8:19:37<3:56:12,  6.76s/it]                                                        68%|   | 4404/6500 [8:19:37<3:56:12,  6.76s/it] 68%|   | 4405/6500 [8:19:43<3:53:54,  6.70s/it]                                                        68%|   | 4405/6500 [8:19:43<3:53:54,  6.70s/it] 68%|   | 4406/65{'loss': 0.6451, 'learning_rate': 2.350813697632433e-05, 'epoch': 0.68}
{'loss': 0.3924, 'learning_rate': 2.348763851409302e-05, 'epoch': 0.68}
{'loss': 0.3714, 'learning_rate': 2.346714624896501e-05, 'epoch': 0.68}
{'loss': 0.3853, 'learning_rate': 2.3446660185730247e-05, 'epoch': 0.68}
{'loss': 0.3674, 'learning_rate': 2.3426180329177215e-05, 'epoch': 0.68}
00 [8:19:50<3:52:19,  6.66s/it]                                                        68%|   | 4406/6500 [8:19:50<3:52:19,  6.66s/it] 68%|   | 4407/6500 [8:19:56<3:51:04,  6.62s/it]                                                        68%|   | 4407/6500 [8:19:56<3:51:04,  6.62s/it] 68%|   | 4408/6500 [8:20:03<3:50:09,  6.60s/it]                                                        68%|   | 4408/6500 [8:20:03<3:50:09,  6.60s/it] 68%|   | 4409/6500 [8:20:09<3:49:26,  6.58s/it]                                                        68%|   | 4409/6500 [8:20:09<3:49:26,  6.58s/it] 68%|   | 4410/6500 [8:20:16<3:49:00,  6.57s/it]                                                        68%|   | 4410/6500 [8:20:16<3:49:00,  6.57s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8756656050682068, 'eval_runtime': 1.4885, 'eval_samples_per_second': 8.062, 'eval_steps_per_second': 2.015, 'epoch': 0.68}
                                                        68%|   | 4410/6500 [8:20:17<3:49:00,  6.57s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-4410
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4410
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4410
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4410
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4410
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4410/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4410/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4410/pytorch_model.binthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4410/pytorch_model.bin

/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3721, 'learning_rate': 2.3405706684092958e-05, 'epoch': 0.68}
{'loss': 0.3856, 'learning_rate': 2.3385239255263077e-05, 'epoch': 0.68}
{'loss': 0.3874, 'learning_rate': 2.336477804747169e-05, 'epoch': 0.68}
{'loss': 0.3743, 'learning_rate': 2.3344323065501494e-05, 'epoch': 0.68}
{'loss': 0.3797, 'learning_rate': 2.332387431413371e-05, 'epoch': 0.68}
 68%|   | 4411/6500 [8:20:24<4:07:56,  7.12s/it]                                                        68%|   | 4411/6500 [8:20:24<4:07:56,  7.12s/it] 68%|   | 4412/6500 [8:20:31<4:08:09,  7.13s/it]                                                        68%|   | 4412/6500 [8:20:31<4:08:09,  7.13s/it] 68%|   | 4413/6500 [8:20:38<4:01:54,  6.95s/it]                                                        68%|   | 4413/6500 [8:20:38<4:01:54,  6.95s/it] 68%|   | 4414/6500 [8:20:45<3:57:31,  6.83s/it]                                                        68%|   | 4414/6500 [8:20:45<3:57:31,  6.83s/it] 68%|   | 4415/6500 [8:20:51<3:54:21,  6.74s/it]                                                        68%|   | 4415/6500 [8:20:51<3:54:21,  6.74s/it] 68%|   | 4416/65{'loss': 0.3772, 'learning_rate': 2.330343179814811e-05, 'epoch': 0.68}
{'loss': 0.3678, 'learning_rate': 2.328299552232303e-05, 'epoch': 0.68}
{'loss': 0.3616, 'learning_rate': 2.326256549143529e-05, 'epoch': 0.68}
{'loss': 0.4128, 'learning_rate': 2.3242141710260295e-05, 'epoch': 0.68}
{'loss': 0.3772, 'learning_rate': 2.3221724183571986e-05, 'epoch': 0.68}
00 [8:20:58<3:52:26,  6.69s/it]                                                        68%|   | 4416/6500 [8:20:58<3:52:26,  6.69s/it] 68%|   | 4417/6500 [8:21:04<3:50:47,  6.65s/it]                                                        68%|   | 4417/6500 [8:21:04<3:50:47,  6.65s/it] 68%|   | 4418/6500 [8:21:11<3:49:31,  6.61s/it]                                                        68%|   | 4418/6500 [8:21:11<3:49:31,  6.61s/it] 68%|   | 4419/6500 [8:21:17<3:48:36,  6.59s/it]                                                        68%|   | 4419/6500 [8:21:17<3:48:36,  6.59s/it] 68%|   | 4420/6500 [8:21:24<3:48:00,  6.58s/it]                                                        68%|   | 4420/6500 [8:21:24<3:48:00,  6.58s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8808324933052063, 'eval_runtime': 1.7505, 'eval_samples_per_second': 6.855, 'eval_steps_per_second': 1.714, 'epoch': 0.68}
                                                        68%|   | 4420/6500 [8:21:26<3:48:00,  6.58s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-4420
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  I AM HERE AND I AM SAVING THE MODEL
/projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4420the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4420

I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4420
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4420
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4420/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4420/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4420/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4420/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.6505, 'learning_rate': 2.320131291614283e-05, 'epoch': 0.68}
{'loss': 0.3894, 'learning_rate': 2.318090791274385e-05, 'epoch': 0.68}
{'loss': 0.3751, 'learning_rate': 2.316050917814456e-05, 'epoch': 0.68}
{'loss': 0.3718, 'learning_rate': 2.314011671711308e-05, 'epoch': 0.68}
{'loss': 0.3622, 'learning_rate': 2.3119730534416028e-05, 'epoch': 0.68}
 68%|   | 4421/6500 [8:21:33<4:10:00,  7.22s/it]                                                        68%|   | 4421/6500 [8:21:33<4:10:00,  7.22s/it] 68%|   | 4422/6500 [8:21:39<4:02:48,  7.01s/it]                                                        68%|   | 4422/6500 [8:21:39<4:02:48,  7.01s/it] 68%|   | 4423/6500 [8:21:46<3:57:48,  6.87s/it]                                                        68%|   | 4423/6500 [8:21:46<3:57:48,  6.87s/it] 68%|   | 4424/6500 [8:21:52<3:54:26,  6.78s/it]                                                        68%|   | 4424/6500 [8:21:52<3:54:26,  6.78s/it] 68%|   | 4425/6500 [8:21:59<3:52:07,  6.71s/it]                                                        68%|   | 4425/6500 [8:21:59<3:52:07,  6.71s/it] 68%|   | 4426/65{'loss': 0.3772, 'learning_rate': 2.3099350634818506e-05, 'epoch': 0.68}
{'loss': 0.3672, 'learning_rate': 2.307897702308422e-05, 'epoch': 0.68}
{'loss': 0.3838, 'learning_rate': 2.305860970397537e-05, 'epoch': 0.68}
{'loss': 0.3841, 'learning_rate': 2.3038248682252693e-05, 'epoch': 0.68}
{'loss': 0.3812, 'learning_rate': 2.3017893962675458e-05, 'epoch': 0.68}
00 [8:22:05<3:50:29,  6.67s/it]                                                        68%|   | 4426/6500 [8:22:05<3:50:29,  6.67s/it] 68%|   | 4427/6500 [8:22:12<3:49:09,  6.63s/it]                                                        68%|   | 4427/6500 [8:22:12<3:49:09,  6.63s/it] 68%|   | 4428/6500 [8:22:19<3:55:58,  6.83s/it]                                                        68%|   | 4428/6500 [8:22:19<3:55:58,  6.83s/it] 68%|   | 4429/6500 [8:22:26<3:52:54,  6.75s/it]                                                        68%|   | 4429/6500 [8:22:26<3:52:54,  6.75s/it] 68%|   | 4430/6500 [8:22:32<3:50:44,  6.69s/it]                                                        68%|   | 4430/6500 [8:22:32<3:50:44,  6.69s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8681575059890747, 'eval_runtime': 1.5068, 'eval_samples_per_second': 7.964, 'eval_steps_per_second': 1.991, 'epoch': 0.68}
                                                        68%|   | 4430/6500 [8:22:34<3:50:44,  6.69s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-4430
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4430
/projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4430
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4430
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4430
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4430/pytorch_model.bin
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4430/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4430/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4430/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3815, 'learning_rate': 2.2997545550001455e-05, 'epoch': 0.68}
{'loss': 0.3705, 'learning_rate': 2.2977203448987007e-05, 'epoch': 0.68}
{'loss': 0.3729, 'learning_rate': 2.2956867664386945e-05, 'epoch': 0.68}
{'loss': 0.4101, 'learning_rate': 2.2936538200954644e-05, 'epoch': 0.68}
{'loss': 0.3801, 'learning_rate': 2.2916215063441985e-05, 'epoch': 0.68}
 68%|   | 4431/6500 [8:22:41<4:08:24,  7.20s/it]                                                        68%|   | 4431/6500 [8:22:41<4:08:24,  7.20s/it] 68%|   | 4432/6500 [8:22:47<4:01:31,  7.01s/it]                                                        68%|   | 4432/6500 [8:22:47<4:01:31,  7.01s/it] 68%|   | 4433/6500 [8:22:54<3:56:41,  6.87s/it]                                                        68%|   | 4433/6500 [8:22:54<3:56:41,  6.87s/it] 68%|   | 4434/6500 [8:23:00<3:53:10,  6.77s/it]                                                        68%|   | 4434/6500 [8:23:00<3:53:10,  6.77s/it] 68%|   | 4435/6500 [8:23:07<3:50:35,  6.70s/it]                                                        68%|   | 4435/6500 [8:23:07<3:50:35,  6.70s/it] 68%|   | 4436/65{'loss': 0.6576, 'learning_rate': 2.2895898256599392e-05, 'epoch': 0.68}
{'loss': 0.3751, 'learning_rate': 2.28755877851758e-05, 'epoch': 0.68}
{'loss': 0.3912, 'learning_rate': 2.2855283653918625e-05, 'epoch': 0.68}
{'loss': 0.37, 'learning_rate': 2.2834985867573855e-05, 'epoch': 0.68}
{'loss': 0.3733, 'learning_rate': 2.281469443088597e-05, 'epoch': 0.68}
00 [8:23:13<3:48:49,  6.65s/it]                                                        68%|   | 4436/6500 [8:23:13<3:48:49,  6.65s/it] 68%|   | 4437/6500 [8:23:20<3:47:38,  6.62s/it]                                                        68%|   | 4437/6500 [8:23:20<3:47:38,  6.62s/it] 68%|   | 4438/6500 [8:23:26<3:46:46,  6.60s/it]                                                        68%|   | 4438/6500 [8:23:26<3:46:46,  6.60s/it] 68%|   | 4439/6500 [8:23:33<3:46:01,  6.58s/it]                                                        68%|   | 4439/6500 [8:23:33<3:46:01,  6.58s/it] 68%|   | 4440/6500 [8:23:40<3:45:33,  6.57s/it]                                                        68%|   | 4440/6500 [8:23:40<3:45:33,  6.57s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8761435151100159, 'eval_runtime': 1.7095, 'eval_samples_per_second': 7.019, 'eval_steps_per_second': 1.755, 'epoch': 0.68}
                                                        68%|   | 4440/6500 [8:23:41<3:45:33,  6.57s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-4440
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4440
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4440
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4440
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4440
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4440/pytorch_model.binthe pytorch model path is
 /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4440/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4440/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4440/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.375, 'learning_rate': 2.2794409348597972e-05, 'epoch': 0.68}
{'loss': 0.3871, 'learning_rate': 2.277413062545138e-05, 'epoch': 0.68}
{'loss': 0.3859, 'learning_rate': 2.2753858266186213e-05, 'epoch': 0.68}
{'loss': 0.3795, 'learning_rate': 2.273359227554102e-05, 'epoch': 0.68}
{'loss': 0.3718, 'learning_rate': 2.271333265825285e-05, 'epoch': 0.68}
 68%|   | 4441/6500 [8:23:48<4:06:50,  7.19s/it]                                                        68%|   | 4441/6500 [8:23:48<4:06:50,  7.19s/it] 68%|   | 4442/6500 [8:23:55<4:00:05,  7.00s/it]                                                        68%|   | 4442/6500 [8:23:55<4:00:05,  7.00s/it] 68%|   | 4443/6500 [8:24:01<3:55:22,  6.87s/it]                                                        68%|   | 4443/6500 [8:24:01<3:55:22,  6.87s/it] 68%|   | 4444/6500 [8:24:08<3:52:17,  6.78s/it]                                                        68%|   | 4444/6500 [8:24:08<3:52:17,  6.78s/it] 68%|   | 4445/6500 [8:24:15<3:55:55,  6.89s/it]                                                        68%|   | 4445/6500 [8:24:15<3:55:55,  6.89s/it] 68%|   | 4446/65{'loss': 0.3897, 'learning_rate': 2.2693079419057266e-05, 'epoch': 0.68}
{'loss': 0.3614, 'learning_rate': 2.267283256268834e-05, 'epoch': 0.68}
{'loss': 0.3766, 'learning_rate': 2.2652592093878666e-05, 'epoch': 0.68}
{'loss': 0.3953, 'learning_rate': 2.2632358017359302e-05, 'epoch': 0.68}
{'loss': 0.3766, 'learning_rate': 2.261213033785985e-05, 'epoch': 0.68}
00 [8:24:22<3:52:12,  6.78s/it]                                                        68%|   | 4446/6500 [8:24:22<3:52:12,  6.78s/it] 68%|   | 4447/6500 [8:24:28<3:49:32,  6.71s/it]                                                        68%|   | 4447/6500 [8:24:28<3:49:32,  6.71s/it] 68%|   | 4448/6500 [8:24:35<3:47:40,  6.66s/it]                                                        68%|   | 4448/6500 [8:24:35<3:47:40,  6.66s/it] 68%|   | 4449/6500 [8:24:41<3:46:20,  6.62s/it]                                                        68%|   | 4449/6500 [8:24:41<3:46:20,  6.62s/it] 68%|   | 4450/6500 [8:24:48<3:45:12,  6.59s/it]                                                        68%|   | 4450/6500 [8:24:48<3:45:12,  6.59s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8786159157752991, 'eval_runtime': 1.4867, 'eval_samples_per_second': 8.072, 'eval_steps_per_second': 2.018, 'epoch': 0.68}
                                                        68%|   | 4450/6500 [8:24:49<3:45:12,  6.59s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-4450
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  I AM HERE AND I AM SAVING THE MODEL
/projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4450the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4450

I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4450
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4450
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4450/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4450/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4450/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4450/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.6568, 'learning_rate': 2.2591909060108407e-05, 'epoch': 0.68}
{'loss': 0.3756, 'learning_rate': 2.2571694188831582e-05, 'epoch': 0.68}
{'loss': 0.3947, 'learning_rate': 2.2551485728754467e-05, 'epoch': 0.69}
{'loss': 0.3657, 'learning_rate': 2.253128368460068e-05, 'epoch': 0.69}
{'loss': 0.3726, 'learning_rate': 2.2511088061092318e-05, 'epoch': 0.69}
 68%|   | 4451/6500 [8:24:56<4:03:13,  7.12s/it]                                                        68%|   | 4451/6500 [8:24:56<4:03:13,  7.12s/it] 68%|   | 4452/6500 [8:25:03<3:56:59,  6.94s/it]                                                        68%|   | 4452/6500 [8:25:03<3:56:59,  6.94s/it] 69%|   | 4453/6500 [8:25:09<3:52:35,  6.82s/it]                                                        69%|   | 4453/6500 [8:25:09<3:52:35,  6.82s/it] 69%|   | 4454/6500 [8:25:16<3:49:36,  6.73s/it]                                                        69%|   | 4454/6500 [8:25:16<3:49:36,  6.73s/it] 69%|   | 4455/6500 [8:25:22<3:47:23,  6.67s/it]                                                        69%|   | 4455/6500 [8:25:22<3:47:23,  6.67s/it] 69%|   | 4456/65{'loss': 0.3799, 'learning_rate': 2.2490898862949987e-05, 'epoch': 0.69}
{'loss': 0.374, 'learning_rate': 2.2470716094892785e-05, 'epoch': 0.69}
{'loss': 0.3895, 'learning_rate': 2.2450539761638316e-05, 'epoch': 0.69}
{'loss': 0.3919, 'learning_rate': 2.2430369867902694e-05, 'epoch': 0.69}
{'loss': 0.3696, 'learning_rate': 2.2410206418400477e-05, 'epoch': 0.69}
00 [8:25:29<3:45:46,  6.63s/it]                                                        69%|   | 4456/6500 [8:25:29<3:45:46,  6.63s/it] 69%|   | 4457/6500 [8:25:35<3:44:40,  6.60s/it]                                                        69%|   | 4457/6500 [8:25:35<3:44:40,  6.60s/it] 69%|   | 4458/6500 [8:25:42<3:43:47,  6.58s/it]                                                        69%|   | 4458/6500 [8:25:42<3:43:47,  6.58s/it] 69%|   | 4459/6500 [8:25:48<3:43:11,  6.56s/it]                                                        69%|   | 4459/6500 [8:25:48<3:43:11,  6.56s/it] 69%|   | 4460/6500 [8:25:55<3:42:48,  6.55s/it]                                                        69%|   | 4460/6500 [8:25:55<3:42:48,  6.55s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8694072961807251, 'eval_runtime': 1.4739, 'eval_samples_per_second': 8.142, 'eval_steps_per_second': 2.035, 'epoch': 0.69}
                                                        69%|   | 4460/6500 [8:25:56<3:42:48,  6.55s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-4460
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4460
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4460
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4460
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4460
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4460/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4460/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4460/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4460/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3818, 'learning_rate': 2.2390049417844756e-05, 'epoch': 0.69}
{'loss': 0.3599, 'learning_rate': 2.236989887094711e-05, 'epoch': 0.69}
{'loss': 0.4072, 'learning_rate': 2.234975478241761e-05, 'epoch': 0.69}
{'loss': 0.3694, 'learning_rate': 2.232961715696481e-05, 'epoch': 0.69}
{'loss': 0.6478, 'learning_rate': 2.2309485999295765e-05, 'epoch': 0.69}
 69%|   | 4461/6500 [8:26:04<4:10:13,  7.36s/it]                                                        69%|   | 4461/6500 [8:26:04<4:10:13,  7.36s/it] 69%|   | 4462/6500 [8:26:11<4:02:45,  7.15s/it]                                                        69%|   | 4462/6500 [8:26:11<4:02:45,  7.15s/it] 69%|   | 4463/6500 [8:26:17<3:56:16,  6.96s/it]                                                        69%|   | 4463/6500 [8:26:17<3:56:16,  6.96s/it] 69%|   | 4464/6500 [8:26:24<3:51:55,  6.83s/it]                                                        69%|   | 4464/6500 [8:26:24<3:51:55,  6.83s/it] 69%|   | 4465/6500 [8:26:30<3:48:41,  6.74s/it]                                                        69%|   | 4465/6500 [8:26:30<3:48:41,  6.74s/it] 69%|   | 4466/65{'loss': 0.3943, 'learning_rate': 2.228936131411601e-05, 'epoch': 0.69}
{'loss': 0.3702, 'learning_rate': 2.226924310612956e-05, 'epoch': 0.69}
{'loss': 0.3888, 'learning_rate': 2.2249131380038928e-05, 'epoch': 0.69}
{'loss': 0.364, 'learning_rate': 2.2229026140545112e-05, 'epoch': 0.69}
{'loss': 0.3724, 'learning_rate': 2.2208927392347596e-05, 'epoch': 0.69}
00 [8:26:37<3:46:29,  6.68s/it]                                                        69%|   | 4466/6500 [8:26:37<3:46:29,  6.68s/it] 69%|   | 4467/6500 [8:26:43<3:44:58,  6.64s/it]                                                        69%|   | 4467/6500 [8:26:43<3:44:58,  6.64s/it] 69%|   | 4468/6500 [8:26:50<3:43:40,  6.60s/it]                                                        69%|   | 4468/6500 [8:26:50<3:43:40,  6.60s/it] 69%|   | 4469/6500 [8:26:56<3:42:47,  6.58s/it]                                                        69%|   | 4469/6500 [8:26:56<3:42:47,  6.58s/it] 69%|   | 4470/6500 [8:27:03<3:42:15,  6.57s/it]                                                        69%|   | 4470/6500 [8:27:03<3:42:15,  6.57s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8770822286605835, 'eval_runtime': 1.4768, 'eval_samples_per_second': 8.126, 'eval_steps_per_second': 2.031, 'epoch': 0.69}
                                                        69%|   | 4470/6500 [8:27:04<3:42:15,  6.57s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-4470
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4470I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4470

I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4470
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4470
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4470/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4470/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4470/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4470/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.375, 'learning_rate': 2.2188835140144314e-05, 'epoch': 0.69}
{'loss': 0.3743, 'learning_rate': 2.2168749388631727e-05, 'epoch': 0.69}
{'loss': 0.3788, 'learning_rate': 2.214867014250475e-05, 'epoch': 0.69}
{'loss': 0.3827, 'learning_rate': 2.212859740645679e-05, 'epoch': 0.69}
{'loss': 0.37, 'learning_rate': 2.2108531185179727e-05, 'epoch': 0.69}
 69%|   | 4471/6500 [8:27:11<4:00:04,  7.10s/it]                                                        69%|   | 4471/6500 [8:27:11<4:00:04,  7.10s/it] 69%|   | 4472/6500 [8:27:18<3:54:12,  6.93s/it]                                                        69%|   | 4472/6500 [8:27:18<3:54:12,  6.93s/it] 69%|   | 4473/6500 [8:27:24<3:50:02,  6.81s/it]                                                        69%|   | 4473/6500 [8:27:24<3:50:02,  6.81s/it] 69%|   | 4474/6500 [8:27:31<3:47:06,  6.73s/it]                                                        69%|   | 4474/6500 [8:27:31<3:47:06,  6.73s/it] 69%|   | 4475/6500 [8:27:37<3:45:09,  6.67s/it]                                                        69%|   | 4475/6500 [8:27:37<3:45:09,  6.67s/it] 69%|   | 4476/65{'loss': 0.387, 'learning_rate': 2.2088471483363916e-05, 'epoch': 0.69}
{'loss': 0.3701, 'learning_rate': 2.206841830569819e-05, 'epoch': 0.69}
{'loss': 0.4198, 'learning_rate': 2.204837165686986e-05, 'epoch': 0.69}
{'loss': 0.3686, 'learning_rate': 2.20283315415647e-05, 'epoch': 0.69}
{'loss': 0.6498, 'learning_rate': 2.200829796446698e-05, 'epoch': 0.69}
00 [8:27:44<3:43:41,  6.63s/it]                                                        69%|   | 4476/6500 [8:27:44<3:43:41,  6.63s/it] 69%|   | 4477/6500 [8:27:51<3:51:14,  6.86s/it]                                                        69%|   | 4477/6500 [8:27:51<3:51:14,  6.86s/it] 69%|   | 4478/6500 [8:27:58<3:47:56,  6.76s/it]                                                        69%|   | 4478/6500 [8:27:58<3:47:56,  6.76s/it] 69%|   | 4479/6500 [8:28:04<3:45:27,  6.69s/it]                                                        69%|   | 4479/6500 [8:28:04<3:45:27,  6.69s/it] 69%|   | 4480/6500 [8:28:11<3:43:49,  6.65s/it]                                                        69%|   | 4480/6500 [8:28:11<3:43:49,  6.65s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8784850835800171, 'eval_runtime': 1.4762, 'eval_samples_per_second': 8.129, 'eval_steps_per_second': 2.032, 'epoch': 0.69}
                                                        69%|   | 4480/6500 [8:28:12<3:43:49,  6.65s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-4480
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4480
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4480
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4480
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4480
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4480/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4480/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4480/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4480/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3877, 'learning_rate': 2.198827093025943e-05, 'epoch': 0.69}
{'loss': 0.3691, 'learning_rate': 2.1968250443623224e-05, 'epoch': 0.69}
{'loss': 0.3822, 'learning_rate': 2.1948236509238034e-05, 'epoch': 0.69}
{'loss': 0.3714, 'learning_rate': 2.1928229131782007e-05, 'epoch': 0.69}
{'loss': 0.3771, 'learning_rate': 2.190822831593174e-05, 'epoch': 0.69}
 69%|   | 4481/6500 [8:28:19<4:01:00,  7.16s/it]                                                        69%|   | 4481/6500 [8:28:19<4:01:00,  7.16s/it] 69%|   | 4482/6500 [8:28:26<3:54:35,  6.97s/it]                                                        69%|   | 4482/6500 [8:28:26<3:54:35,  6.97s/it] 69%|   | 4483/6500 [8:28:32<3:50:06,  6.85s/it]                                                        69%|   | 4483/6500 [8:28:32<3:50:06,  6.85s/it] 69%|   | 4484/6500 [8:28:39<3:47:00,  6.76s/it]                                                        69%|   | 4484/6500 [8:28:39<3:47:00,  6.76s/it] 69%|   | 4485/6500 [8:28:46<3:44:44,  6.69s/it]                                                        69%|   | 4485/6500 [8:28:46<3:44:44,  6.69s/it] 69%|   | 4486/65{'loss': 0.381, 'learning_rate': 2.1888234066362302e-05, 'epoch': 0.69}
{'loss': 0.3851, 'learning_rate': 2.1868246387747232e-05, 'epoch': 0.69}
{'loss': 0.3778, 'learning_rate': 2.1848265284758524e-05, 'epoch': 0.69}
{'loss': 0.3877, 'learning_rate': 2.182829076206664e-05, 'epoch': 0.69}
{'loss': 0.3792, 'learning_rate': 2.18083228243405e-05, 'epoch': 0.69}
00 [8:28:52<3:43:06,  6.65s/it]                                                        69%|   | 4486/6500 [8:28:52<3:43:06,  6.65s/it] 69%|   | 4487/6500 [8:28:59<3:41:59,  6.62s/it]                                                        69%|   | 4487/6500 [8:28:59<3:41:59,  6.62s/it] 69%|   | 4488/6500 [8:29:05<3:41:06,  6.59s/it]                                                        69%|   | 4488/6500 [8:29:05<3:41:06,  6.59s/it] 69%|   | 4489/6500 [8:29:12<3:40:32,  6.58s/it]                                                        69%|   | 4489/6500 [8:29:12<3:40:32,  6.58s/it] 69%|   | 4490/6500 [8:29:18<3:40:12,  6.57s/it]                                                        69%|   | 4490/6500 [8:29:18<3:40:12,  6.57s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8694038391113281, 'eval_runtime': 1.4775, 'eval_samples_per_second': 8.122, 'eval_steps_per_second': 2.03, 'epoch': 0.69}
                                                        69%|   | 4490/6500 [8:29:20<3:40:12,  6.57s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-4490
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4490
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4490
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4490
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4490
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4490/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4490/pytorch_model.bin
the pytorch model path is the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4490/pytorch_model.bin
/projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4490/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3735, 'learning_rate': 2.17883614762475e-05, 'epoch': 0.69}
{'loss': 0.3609, 'learning_rate': 2.1768406722453465e-05, 'epoch': 0.69}
{'loss': 0.4099, 'learning_rate': 2.1748458567622733e-05, 'epoch': 0.69}
{'loss': 0.3789, 'learning_rate': 2.1728517016418016e-05, 'epoch': 0.69}
{'loss': 0.6489, 'learning_rate': 2.1708582073500554e-05, 'epoch': 0.69}
 69%|   | 4491/6500 [8:29:27<3:58:16,  7.12s/it]                                                        69%|   | 4491/6500 [8:29:27<3:58:16,  7.12s/it] 69%|   | 4492/6500 [8:29:33<3:52:19,  6.94s/it]                                                        69%|   | 4492/6500 [8:29:33<3:52:19,  6.94s/it] 69%|   | 4493/6500 [8:29:41<3:57:19,  7.09s/it]                                                        69%|   | 4493/6500 [8:29:41<3:57:19,  7.09s/it] 69%|   | 4494/6500 [8:29:47<3:51:39,  6.93s/it]                                                        69%|   | 4494/6500 [8:29:47<3:51:39,  6.93s/it] 69%|   | 4495/6500 [8:29:54<3:47:40,  6.81s/it]                                                        69%|   | 4495/6500 [8:29:54<3:47:40,  6.81s/it] 69%|   | 4496/65{'loss': 0.3921, 'learning_rate': 2.1688653743530023e-05, 'epoch': 0.69}
{'loss': 0.368, 'learning_rate': 2.166873203116454e-05, 'epoch': 0.69}
{'loss': 0.3665, 'learning_rate': 2.1648816941060668e-05, 'epoch': 0.69}
{'loss': 0.3682, 'learning_rate': 2.162890847787348e-05, 'epoch': 0.69}
{'loss': 0.3746, 'learning_rate': 2.160900664625643e-05, 'epoch': 0.69}
00 [8:30:00<3:44:50,  6.73s/it]                                                        69%|   | 4496/6500 [8:30:00<3:44:50,  6.73s/it] 69%|   | 4497/6500 [8:30:07<3:42:48,  6.67s/it]                                                        69%|   | 4497/6500 [8:30:07<3:42:48,  6.67s/it] 69%|   | 4498/6500 [8:30:13<3:41:25,  6.64s/it]                                                        69%|   | 4498/6500 [8:30:13<3:41:25,  6.64s/it] 69%|   | 4499/6500 [8:30:20<3:40:26,  6.61s/it]                                                        69%|   | 4499/6500 [8:30:20<3:40:26,  6.61s/it] 69%|   | 4500/6500 [8:30:26<3:39:35,  6.59s/it]                                                        69%|   | 4500/6500 [8:30:26<3:39:35,  6.59s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8793663382530212, 'eval_runtime': 1.4794, 'eval_samples_per_second': 8.112, 'eval_steps_per_second': 2.028, 'epoch': 0.69}
                                                        69%|   | 4500/6500 [8:30:28<3:39:35,  6.59s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-4500
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4500I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  
/projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4500
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4500
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4500
the pytorch model path isthe pytorch model path is the pytorch model path is/projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4500/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4500/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4500/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4500/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3721, 'learning_rate': 2.1589111450861475e-05, 'epoch': 0.69}
{'loss': 0.384, 'learning_rate': 2.1569222896338966e-05, 'epoch': 0.69}
{'loss': 0.3874, 'learning_rate': 2.154934098733774e-05, 'epoch': 0.69}
{'loss': 0.369, 'learning_rate': 2.1529465728505078e-05, 'epoch': 0.69}
{'loss': 0.3768, 'learning_rate': 2.150959712448669e-05, 'epoch': 0.69}
 69%|   | 4501/6500 [8:30:35<3:57:46,  7.14s/it]                                                        69%|   | 4501/6500 [8:30:35<3:57:46,  7.14s/it] 69%|   | 4502/6500 [8:30:41<3:51:40,  6.96s/it]                                                        69%|   | 4502/6500 [8:30:41<3:51:40,  6.96s/it] 69%|   | 4503/6500 [8:30:48<3:47:23,  6.83s/it]                                                        69%|   | 4503/6500 [8:30:48<3:47:23,  6.83s/it] 69%|   | 4504/6500 [8:30:54<3:44:22,  6.74s/it]                                                        69%|   | 4504/6500 [8:30:54<3:44:22,  6.74s/it] 69%|   | 4505/6500 [8:31:01<3:42:10,  6.68s/it]                                                        69%|   | 4505/6500 [8:31:01<3:42:10,  6.68s/it] 69%|   | 4506/65{'loss': 0.3676, 'learning_rate': 2.1489735179926757e-05, 'epoch': 0.69}
{'loss': 0.3685, 'learning_rate': 2.1469879899467877e-05, 'epoch': 0.69}
{'loss': 0.4011, 'learning_rate': 2.1450031287751103e-05, 'epoch': 0.69}
{'loss': 0.3802, 'learning_rate': 2.1430189349415934e-05, 'epoch': 0.69}
{'loss': 0.6522, 'learning_rate': 2.1410354089100292e-05, 'epoch': 0.69}
00 [8:31:08<3:40:41,  6.64s/it]                                                        69%|   | 4506/6500 [8:31:08<3:40:41,  6.64s/it] 69%|   | 4507/6500 [8:31:14<3:39:34,  6.61s/it]                                                        69%|   | 4507/6500 [8:31:14<3:39:34,  6.61s/it] 69%|   | 4508/6500 [8:31:21<3:38:49,  6.59s/it]                                                        69%|   | 4508/6500 [8:31:21<3:38:49,  6.59s/it] 69%|   | 4509/6500 [8:31:28<3:44:26,  6.76s/it]                                                        69%|   | 4509/6500 [8:31:28<3:44:26,  6.76s/it] 69%|   | 4510/6500 [8:31:34<3:42:03,  6.69s/it]                                                        69%|   | 4510/6500 [8:31:34<3:42:03,  6.69s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8786481022834778, 'eval_runtime': 1.5054, 'eval_samples_per_second': 7.971, 'eval_steps_per_second': 1.993, 'epoch': 0.69}
                                                        69%|   | 4510/6500 [8:31:36<3:42:03,  6.69s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-4510
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4510
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4510
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4510
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4510
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4510/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4510/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4510/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4510/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3834, 'learning_rate': 2.1390525511440563e-05, 'epoch': 0.69}
{'loss': 0.3738, 'learning_rate': 2.1370703621071557e-05, 'epoch': 0.69}
{'loss': 0.3636, 'learning_rate': 2.1350888422626492e-05, 'epoch': 0.69}
{'loss': 0.3655, 'learning_rate': 2.1331079920737074e-05, 'epoch': 0.69}
{'loss': 0.3692, 'learning_rate': 2.1311278120033412e-05, 'epoch': 0.69}
 69%|   | 4511/6500 [8:31:43<4:00:45,  7.26s/it]                                                        69%|   | 4511/6500 [8:31:43<4:00:45,  7.26s/it] 69%|   | 4512/6500 [8:31:49<3:53:27,  7.05s/it]                                                        69%|   | 4512/6500 [8:31:49<3:53:27,  7.05s/it] 69%|   | 4513/6500 [8:31:56<3:48:18,  6.89s/it]                                                        69%|   | 4513/6500 [8:31:56<3:48:18,  6.89s/it] 69%|   | 4514/6500 [8:32:03<3:44:42,  6.79s/it]                                                        69%|   | 4514/6500 [8:32:03<3:44:42,  6.79s/it] 69%|   | 4515/6500 [8:32:09<3:42:07,  6.71s/it]                                                        69%|   | 4515/6500 [8:32:09<3:42:07,  6.71s/it] 69%|   | 4516/65{'loss': 0.3733, 'learning_rate': 2.129148302514406e-05, 'epoch': 0.69}
{'loss': 0.3862, 'learning_rate': 2.1271694640696e-05, 'epoch': 0.69}
{'loss': 0.3799, 'learning_rate': 2.125191297131464e-05, 'epoch': 0.7}
{'loss': 0.3726, 'learning_rate': 2.1232138021623837e-05, 'epoch': 0.7}
{'loss': 0.3862, 'learning_rate': 2.1212369796245864e-05, 'epoch': 0.7}
00 [8:32:16<3:40:19,  6.66s/it]                                                        69%|   | 4516/6500 [8:32:16<3:40:19,  6.66s/it] 69%|   | 4517/6500 [8:32:22<3:39:04,  6.63s/it]                                                        69%|   | 4517/6500 [8:32:22<3:39:04,  6.63s/it] 70%|   | 4518/6500 [8:32:29<3:38:07,  6.60s/it]                                                        70%|   | 4518/6500 [8:32:29<3:38:07,  6.60s/it] 70%|   | 4519/6500 [8:32:35<3:37:19,  6.58s/it]                                                        70%|   | 4519/6500 [8:32:35<3:37:19,  6.58s/it] 70%|   | 4520/6500 [8:32:42<3:36:51,  6.57s/it]                                                        70%|   | 4520/6500 [8:32:42<3:36:51,  6.57s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8703510165214539, 'eval_runtime': 1.4809, 'eval_samples_per_second': 8.103, 'eval_steps_per_second': 2.026, 'epoch': 0.7}
                                                        70%|   | 4520/6500 [8:32:43<3:36:51,  6.57s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-4520
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4520
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4520
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4520
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4520
the pytorch model path isthe pytorch model path is  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4520/pytorch_model.bin/projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4520/pytorch_model.bin

the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4520/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4520/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3636, 'learning_rate': 2.1192608299801412e-05, 'epoch': 0.7}
{'loss': 0.3733, 'learning_rate': 2.1172853536909625e-05, 'epoch': 0.7}
{'loss': 0.3932, 'learning_rate': 2.115310551218805e-05, 'epoch': 0.7}
{'loss': 0.3754, 'learning_rate': 2.1133364230252688e-05, 'epoch': 0.7}
{'loss': 0.6557, 'learning_rate': 2.1113629695717907e-05, 'epoch': 0.7}
 70%|   | 4521/6500 [8:32:50<3:55:04,  7.13s/it]                                                        70%|   | 4521/6500 [8:32:50<3:55:04,  7.13s/it] 70%|   | 4522/6500 [8:32:57<3:49:05,  6.95s/it]                                                        70%|   | 4522/6500 [8:32:57<3:49:05,  6.95s/it] 70%|   | 4523/6500 [8:33:03<3:44:57,  6.83s/it]                                                        70%|   | 4523/6500 [8:33:03<3:44:57,  6.83s/it] 70%|   | 4524/6500 [8:33:10<3:42:01,  6.74s/it]                                                        70%|   | 4524/6500 [8:33:10<3:42:01,  6.74s/it] 70%|   | 4525/6500 [8:33:17<3:46:05,  6.87s/it]                                                        70%|   | 4525/6500 [8:33:17<3:46:05,  6.87s/it] 70%|   | 4526/65{'loss': 0.372, 'learning_rate': 2.109390191319655e-05, 'epoch': 0.7}
{'loss': 0.3892, 'learning_rate': 2.107418088729987e-05, 'epoch': 0.7}
{'loss': 0.3683, 'learning_rate': 2.1054466622637538e-05, 'epoch': 0.7}
{'loss': 0.3718, 'learning_rate': 2.1034759123817638e-05, 'epoch': 0.7}
{'loss': 0.3715, 'learning_rate': 2.101505839544668e-05, 'epoch': 0.7}
00 [8:33:24<3:42:45,  6.77s/it]                                                        70%|   | 4526/6500 [8:33:24<3:42:45,  6.77s/it] 70%|   | 4527/6500 [8:33:30<3:40:30,  6.71s/it]                                                        70%|   | 4527/6500 [8:33:30<3:40:30,  6.71s/it] 70%|   | 4528/6500 [8:33:37<3:38:44,  6.66s/it]                                                        70%|   | 4528/6500 [8:33:37<3:38:44,  6.66s/it] 70%|   | 4529/6500 [8:33:43<3:37:42,  6.63s/it]                                                        70%|   | 4529/6500 [8:33:43<3:37:42,  6.63s/it] 70%|   | 4530/6500 [8:33:50<3:36:48,  6.60s/it]                                                        70%|   | 4530/6500 [8:33:50<3:36:48,  6.60s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8782685399055481, 'eval_runtime': 1.4881, 'eval_samples_per_second': 8.064, 'eval_steps_per_second': 2.016, 'epoch': 0.7}
                                                        70%|   | 4530/6500 [8:33:51<3:36:48,  6.60s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-4530
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4530I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4530

I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4530
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4530
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4530/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4530/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4530/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4530/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3857, 'learning_rate': 2.0995364442129596e-05, 'epoch': 0.7}
{'loss': 0.3882, 'learning_rate': 2.097567726846972e-05, 'epoch': 0.7}
{'loss': 0.3813, 'learning_rate': 2.0955996879068807e-05, 'epoch': 0.7}
{'loss': 0.3745, 'learning_rate': 2.0936323278527036e-05, 'epoch': 0.7}
{'loss': 0.3769, 'learning_rate': 2.0916656471443013e-05, 'epoch': 0.7}
 70%|   | 4531/6500 [8:33:58<3:54:58,  7.16s/it]                                                        70%|   | 4531/6500 [8:33:58<3:54:58,  7.16s/it] 70%|   | 4532/6500 [8:34:05<3:48:49,  6.98s/it]                                                        70%|   | 4532/6500 [8:34:05<3:48:49,  6.98s/it] 70%|   | 4533/6500 [8:34:11<3:44:24,  6.85s/it]                                                        70%|   | 4533/6500 [8:34:11<3:44:24,  6.85s/it] 70%|   | 4534/6500 [8:34:18<3:41:32,  6.76s/it]                                                        70%|   | 4534/6500 [8:34:18<3:41:32,  6.76s/it] 70%|   | 4535/6500 [8:34:24<3:39:17,  6.70s/it]                                                        70%|   | 4535/6500 [8:34:24<3:39:17,  6.70s/it] 70%|   | 4536/65{'loss': 0.3648, 'learning_rate': 2.0896996462413686e-05, 'epoch': 0.7}
{'loss': 0.3985, 'learning_rate': 2.0877343256034487e-05, 'epoch': 0.7}
{'loss': 0.3706, 'learning_rate': 2.0857696856899232e-05, 'epoch': 0.7}
{'loss': 0.5942, 'learning_rate': 2.0838057269600154e-05, 'epoch': 0.7}
{'loss': 0.4366, 'learning_rate': 2.081842449872788e-05, 'epoch': 0.7}
00 [8:34:31<3:37:34,  6.65s/it]                                                        70%|   | 4536/6500 [8:34:31<3:37:34,  6.65s/it] 70%|   | 4537/6500 [8:34:37<3:36:22,  6.61s/it]                                                        70%|   | 4537/6500 [8:34:37<3:36:22,  6.61s/it] 70%|   | 4538/6500 [8:34:44<3:35:34,  6.59s/it]                                                        70%|   | 4538/6500 [8:34:44<3:35:34,  6.59s/it] 70%|   | 4539/6500 [8:34:51<3:34:59,  6.58s/it]                                                        70%|   | 4539/6500 [8:34:51<3:34:59,  6.58s/it] 70%|   | 4540/6500 [8:34:57<3:34:40,  6.57s/it]                                                        70%|   | 4540/6500 [8:34:57<3:34:40,  6.57s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8787055611610413, 'eval_runtime': 1.4699, 'eval_samples_per_second': 8.164, 'eval_steps_per_second': 2.041, 'epoch': 0.7}
                                                        70%|   | 4540/6500 [8:34:59<3:34:40,  6.57s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-4540
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4540
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4540
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4540
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4540
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4540/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4540/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4540/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4540/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3695, 'learning_rate': 2.079879854887145e-05, 'epoch': 0.7}
{'loss': 0.389, 'learning_rate': 2.0779179424618313e-05, 'epoch': 0.7}
{'loss': 0.3571, 'learning_rate': 2.075956713055432e-05, 'epoch': 0.7}
{'loss': 0.365, 'learning_rate': 2.0739961671263725e-05, 'epoch': 0.7}
{'loss': 0.3739, 'learning_rate': 2.0720363051329188e-05, 'epoch': 0.7}
 70%|   | 4541/6500 [8:35:06<3:52:56,  7.13s/it]                                                        70%|   | 4541/6500 [8:35:06<3:52:56,  7.13s/it] 70%|   | 4542/6500 [8:35:13<3:55:39,  7.22s/it]                                                        70%|   | 4542/6500 [8:35:13<3:55:39,  7.22s/it] 70%|   | 4543/6500 [8:35:20<3:48:51,  7.02s/it]                                                        70%|   | 4543/6500 [8:35:20<3:48:51,  7.02s/it] 70%|   | 4544/6500 [8:35:26<3:44:02,  6.87s/it]                                                        70%|   | 4544/6500 [8:35:26<3:44:02,  6.87s/it] 70%|   | 4545/6500 [8:35:33<3:40:45,  6.78s/it]                                                        70%|   | 4545/6500 [8:35:33<3:40:45,  6.78s/it] 70%|   | 4546/65{'loss': 0.3713, 'learning_rate': 2.070077127533178e-05, 'epoch': 0.7}
{'loss': 0.3847, 'learning_rate': 2.068118634785093e-05, 'epoch': 0.7}
{'loss': 0.3844, 'learning_rate': 2.0661608273464506e-05, 'epoch': 0.7}
{'loss': 0.3687, 'learning_rate': 2.064203705674877e-05, 'epoch': 0.7}
{'loss': 0.3838, 'learning_rate': 2.0622472702278372e-05, 'epoch': 0.7}
00 [8:35:39<3:38:24,  6.71s/it]                                                        70%|   | 4546/6500 [8:35:39<3:38:24,  6.71s/it] 70%|   | 4547/6500 [8:35:46<3:36:43,  6.66s/it]                                                        70%|   | 4547/6500 [8:35:46<3:36:43,  6.66s/it] 70%|   | 4548/6500 [8:35:52<3:35:30,  6.62s/it]                                                        70%|   | 4548/6500 [8:35:52<3:35:30,  6.62s/it] 70%|   | 4549/6500 [8:35:59<3:34:36,  6.60s/it]                                                        70%|   | 4549/6500 [8:35:59<3:34:36,  6.60s/it] 70%|   | 4550/6500 [8:36:05<3:34:18,  6.59s/it]                                                        70%|   | 4550/6500 [8:36:05<3:34:18,  6.59s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8692398071289062, 'eval_runtime': 1.5854, 'eval_samples_per_second': 7.569, 'eval_steps_per_second': 1.892, 'epoch': 0.7}
                                                        70%|   | 4550/6500 [8:36:07<3:34:18,  6.59s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-4550
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4550
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4550
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4550
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4550
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4550/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4550/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4550/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4550/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3591, 'learning_rate': 2.060291521462636e-05, 'epoch': 0.7}
{'loss': 0.4095, 'learning_rate': 2.0583364598364184e-05, 'epoch': 0.7}
{'loss': 0.3706, 'learning_rate': 2.056382085806167e-05, 'epoch': 0.7}
{'loss': 0.6489, 'learning_rate': 2.054428399828706e-05, 'epoch': 0.7}
{'loss': 0.3804, 'learning_rate': 2.0524754023606972e-05, 'epoch': 0.7}
 70%|   | 4551/6500 [8:36:14<3:52:36,  7.16s/it]                                                        70%|   | 4551/6500 [8:36:14<3:52:36,  7.16s/it] 70%|   | 4552/6500 [8:36:20<3:46:22,  6.97s/it]                                                        70%|   | 4552/6500 [8:36:20<3:46:22,  6.97s/it] 70%|   | 4553/6500 [8:36:27<3:41:56,  6.84s/it]                                                        70%|   | 4553/6500 [8:36:27<3:41:56,  6.84s/it] 70%|   | 4554/6500 [8:36:33<3:38:45,  6.74s/it]                                                        70%|   | 4554/6500 [8:36:33<3:38:45,  6.74s/it] 70%|   | 4555/6500 [8:36:40<3:36:25,  6.68s/it]                                                        70%|   | 4555/6500 [8:36:40<3:36:25,  6.68s/it] 70%|   | 4556/65{'loss': 0.3716, 'learning_rate': 2.0505230938586418e-05, 'epoch': 0.7}
{'loss': 0.3812, 'learning_rate': 2.048571474778882e-05, 'epoch': 0.7}
{'loss': 0.3669, 'learning_rate': 2.0466205455775934e-05, 'epoch': 0.7}
{'loss': 0.3738, 'learning_rate': 2.0446703067107947e-05, 'epoch': 0.7}
{'loss': 0.3798, 'learning_rate': 2.0427207586343432e-05, 'epoch': 0.7}
00 [8:36:46<3:34:58,  6.63s/it]                                                        70%|   | 4556/6500 [8:36:46<3:34:58,  6.63s/it] 70%|   | 4557/6500 [8:36:53<3:33:44,  6.60s/it]                                                        70%|   | 4557/6500 [8:36:53<3:33:44,  6.60s/it] 70%|   | 4558/6500 [8:37:00<3:41:20,  6.84s/it]                                                        70%|   | 4558/6500 [8:37:00<3:41:20,  6.84s/it] 70%|   | 4559/6500 [8:37:07<3:38:11,  6.74s/it]                                                        70%|   | 4559/6500 [8:37:07<3:38:11,  6.74s/it] 70%|   | 4560/6500 [8:37:13<3:36:02,  6.68s/it]                                                        70%|   | 4560/6500 [8:37:13<3:36:02,  6.68s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8781518340110779, 'eval_runtime': 1.4804, 'eval_samples_per_second': 8.106, 'eval_steps_per_second': 2.027, 'epoch': 0.7}
                                                        70%|   | 4560/6500 [8:37:15<3:36:02,  6.68s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-4560
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4560
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4560
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4560
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4560
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4560/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4560/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4560/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4560/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3807, 'learning_rate': 2.0407719018039345e-05, 'epoch': 0.7}
{'loss': 0.3771, 'learning_rate': 2.0388237366751006e-05, 'epoch': 0.7}
{'loss': 0.3815, 'learning_rate': 2.0368762637032144e-05, 'epoch': 0.7}
{'loss': 0.3811, 'learning_rate': 2.0349294833434855e-05, 'epoch': 0.7}
{'loss': 0.3743, 'learning_rate': 2.032983396050962e-05, 'epoch': 0.7}
 70%|   | 4561/6500 [8:37:22<3:52:22,  7.19s/it]                                                        70%|   | 4561/6500 [8:37:22<3:52:22,  7.19s/it] 70%|   | 4562/6500 [8:37:28<3:45:46,  6.99s/it]                                                        70%|   | 4562/6500 [8:37:28<3:45:46,  6.99s/it] 70%|   | 4563/6500 [8:37:35<3:41:16,  6.85s/it]                                                        70%|   | 4563/6500 [8:37:35<3:41:16,  6.85s/it] 70%|   | 4564/6500 [8:37:41<3:38:08,  6.76s/it]                                                        70%|   | 4564/6500 [8:37:41<3:38:08,  6.76s/it] 70%|   | 4565/6500 [8:37:48<3:35:49,  6.69s/it]                                                        70%|   | 4565/6500 [8:37:48<3:35:49,  6.69s/it] 70%|   | 4566/65{'loss': 0.3647, 'learning_rate': 2.0310380022805298e-05, 'epoch': 0.7}
{'loss': 0.411, 'learning_rate': 2.029093302486913e-05, 'epoch': 0.7}
{'loss': 0.382, 'learning_rate': 2.0271492971246753e-05, 'epoch': 0.7}
{'loss': 0.6427, 'learning_rate': 2.025205986648212e-05, 'epoch': 0.7}
{'loss': 0.3939, 'learning_rate': 2.0232633715117625e-05, 'epoch': 0.7}
00 [8:37:54<3:34:05,  6.64s/it]                                                        70%|   | 4566/6500 [8:37:54<3:34:05,  6.64s/it] 70%|   | 4567/6500 [8:38:01<3:32:54,  6.61s/it]                                                        70%|   | 4567/6500 [8:38:01<3:32:54,  6.61s/it] 70%|   | 4568/6500 [8:38:08<3:31:59,  6.58s/it]                                                        70%|   | 4568/6500 [8:38:08<3:31:59,  6.58s/it] 70%|   | 4569/6500 [8:38:14<3:31:19,  6.57s/it]                                                        70%|   | 4569/6500 [8:38:14<3:31:19,  6.57s/it] 70%|   | 4570/6500 [8:38:21<3:31:06,  6.56s/it]                                                        70%|   | 4570/6500 [8:38:21<3:31:06,  6.56s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.877440333366394, 'eval_runtime': 1.4798, 'eval_samples_per_second': 8.109, 'eval_steps_per_second': 2.027, 'epoch': 0.7}
                                                        70%|   | 4570/6500 [8:38:22<3:31:06,  6.56s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-4570
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4570
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4570
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4570
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4570
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4570/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4570/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4570/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4570/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3741, 'learning_rate': 2.0213214521694006e-05, 'epoch': 0.7}
{'loss': 0.3691, 'learning_rate': 2.019380229075039e-05, 'epoch': 0.7}
{'loss': 0.3724, 'learning_rate': 2.0174397026824266e-05, 'epoch': 0.7}
{'loss': 0.3749, 'learning_rate': 2.0154998734451474e-05, 'epoch': 0.7}
{'loss': 0.3745, 'learning_rate': 2.0135607418166285e-05, 'epoch': 0.7}
 70%|   | 4571/6500 [8:38:29<3:48:15,  7.10s/it]                                                        70%|   | 4571/6500 [8:38:29<3:48:15,  7.10s/it] 70%|   | 4572/6500 [8:38:36<3:42:43,  6.93s/it]                                                        70%|   | 4572/6500 [8:38:36<3:42:43,  6.93s/it] 70%|   | 4573/6500 [8:38:42<3:38:52,  6.82s/it]                                                        70%|   | 4573/6500 [8:38:42<3:38:52,  6.82s/it] 70%|   | 4574/6500 [8:38:49<3:44:49,  7.00s/it]                                                        70%|   | 4574/6500 [8:38:50<3:44:49,  7.00s/it] 70%|   | 4575/6500 [8:38:56<3:40:13,  6.86s/it]                                                        70%|   | 4575/6500 [8:38:56<3:40:13,  6.86s/it] 70%|   | 4576/65{'loss': 0.3931, 'learning_rate': 2.0116223082501285e-05, 'epoch': 0.7}
{'loss': 0.3879, 'learning_rate': 2.0096845731987462e-05, 'epoch': 0.7}
{'loss': 0.3777, 'learning_rate': 2.0077475371154114e-05, 'epoch': 0.7}
{'loss': 0.3798, 'learning_rate': 2.0058112004528966e-05, 'epoch': 0.7}
{'loss': 0.3681, 'learning_rate': 2.003875563663809e-05, 'epoch': 0.7}
00 [8:39:03<3:37:02,  6.77s/it]                                                        70%|   | 4576/6500 [8:39:03<3:37:02,  6.77s/it] 70%|   | 4577/6500 [8:39:09<3:34:47,  6.70s/it]                                                        70%|   | 4577/6500 [8:39:09<3:34:47,  6.70s/it] 70%|   | 4578/6500 [8:39:16<3:33:10,  6.66s/it]                                                        70%|   | 4578/6500 [8:39:16<3:33:10,  6.66s/it] 70%|   | 4579/6500 [8:39:22<3:32:04,  6.62s/it]                                                        70%|   | 4579/6500 [8:39:22<3:32:04,  6.62s/it] 70%|   | 4580/6500 [8:39:29<3:31:08,  6.60s/it]                                                        70%|   | 4580/6500 [8:39:29<3:31:08,  6.60s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8696514368057251, 'eval_runtime': 1.4801, 'eval_samples_per_second': 8.107, 'eval_steps_per_second': 2.027, 'epoch': 0.7}
                                                        70%|   | 4580/6500 [8:39:30<3:31:08,  6.60s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-4580
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4580
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4580
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4580
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4580
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4580/pytorch_model.bin
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4580/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4580/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4580/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3667, 'learning_rate': 2.0019406272005915e-05, 'epoch': 0.7}
{'loss': 0.4105, 'learning_rate': 2.0000063915155237e-05, 'epoch': 0.7}
{'loss': 0.3778, 'learning_rate': 1.998072857060722e-05, 'epoch': 0.71}
{'loss': 0.652, 'learning_rate': 1.996140024288138e-05, 'epoch': 0.71}
{'loss': 0.3854, 'learning_rate': 1.994207893649559e-05, 'epoch': 0.71}
 70%|   | 4581/6500 [8:39:37<3:48:29,  7.14s/it]                                                        70%|   | 4581/6500 [8:39:37<3:48:29,  7.14s/it] 70%|   | 4582/6500 [8:39:44<3:42:43,  6.97s/it]                                                        70%|   | 4582/6500 [8:39:44<3:42:43,  6.97s/it] 71%|   | 4583/6500 [8:39:50<3:38:32,  6.84s/it]                                                        71%|   | 4583/6500 [8:39:50<3:38:32,  6.84s/it] 71%|   | 4584/6500 [8:39:57<3:35:32,  6.75s/it]                                                        71%|   | 4584/6500 [8:39:57<3:35:32,  6.75s/it] 71%|   | 4585/6500 [8:40:03<3:33:22,  6.69s/it]                                                        71%|   | 4585/6500 [8:40:03<3:33:22,  6.69s/it] 71%|   | 4586/65{'loss': 0.3755, 'learning_rate': 1.99227646559661e-05, 'epoch': 0.71}
{'loss': 0.3661, 'learning_rate': 1.990345740580749e-05, 'epoch': 0.71}
{'loss': 0.3606, 'learning_rate': 1.9884157190532747e-05, 'epoch': 0.71}
{'loss': 0.3713, 'learning_rate': 1.9864864014653133e-05, 'epoch': 0.71}
{'loss': 0.3744, 'learning_rate': 1.9845577882678336e-05, 'epoch': 0.71}
00 [8:40:10<3:31:47,  6.64s/it]                                                        71%|   | 4586/6500 [8:40:10<3:31:47,  6.64s/it] 71%|   | 4587/6500 [8:40:16<3:30:39,  6.61s/it]                                                        71%|   | 4587/6500 [8:40:16<3:30:39,  6.61s/it] 71%|   | 4588/6500 [8:40:23<3:29:53,  6.59s/it]                                                        71%|   | 4588/6500 [8:40:23<3:29:53,  6.59s/it] 71%|   | 4589/6500 [8:40:29<3:29:20,  6.57s/it]                                                        71%|   | 4589/6500 [8:40:29<3:29:20,  6.57s/it] 71%|   | 4590/6500 [8:40:37<3:34:47,  6.75s/it]                                                        71%|   | 4590/6500 [8:40:37<3:34:47,  6.75s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8792387843132019, 'eval_runtime': 1.7355, 'eval_samples_per_second': 6.914, 'eval_steps_per_second': 1.729, 'epoch': 0.71}
                                                        71%|   | 4590/6500 [8:40:38<3:34:47,  6.75s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-4590
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4590
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4590
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4590
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4590
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4590/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4590/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4590/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4590/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3842, 'learning_rate': 1.982629879911636e-05, 'epoch': 0.71}
{'loss': 0.3774, 'learning_rate': 1.980702676847358e-05, 'epoch': 0.71}
{'loss': 0.378, 'learning_rate': 1.978776179525472e-05, 'epoch': 0.71}
{'loss': 0.3717, 'learning_rate': 1.9768503883962846e-05, 'epoch': 0.71}
{'loss': 0.362, 'learning_rate': 1.974925303909938e-05, 'epoch': 0.71}
 71%|   | 4591/6500 [8:40:46<3:55:17,  7.40s/it]                                                        71%|   | 4591/6500 [8:40:46<3:55:17,  7.40s/it] 71%|   | 4592/6500 [8:40:52<3:47:04,  7.14s/it]                                                        71%|   | 4592/6500 [8:40:52<3:47:04,  7.14s/it] 71%|   | 4593/6500 [8:40:59<3:41:16,  6.96s/it]                                                        71%|   | 4593/6500 [8:40:59<3:41:16,  6.96s/it] 71%|   | 4594/6500 [8:41:05<3:37:16,  6.84s/it]                                                        71%|   | 4594/6500 [8:41:05<3:37:16,  6.84s/it] 71%|   | 4595/6500 [8:41:12<3:34:18,  6.75s/it]                                                        71%|   | 4595/6500 [8:41:12<3:34:18,  6.75s/it] 71%|   | 4596/65{'loss': 0.3702, 'learning_rate': 1.973000926516409e-05, 'epoch': 0.71}
{'loss': 0.3992, 'learning_rate': 1.971077256665509e-05, 'epoch': 0.71}
{'loss': 0.3781, 'learning_rate': 1.9691542948068837e-05, 'epoch': 0.71}
{'loss': 0.6464, 'learning_rate': 1.967232041390016e-05, 'epoch': 0.71}
{'loss': 0.369, 'learning_rate': 1.9653104968642173e-05, 'epoch': 0.71}
00 [8:41:18<3:32:09,  6.69s/it]                                                        71%|   | 4596/6500 [8:41:18<3:32:09,  6.69s/it] 71%|   | 4597/6500 [8:41:25<3:30:46,  6.65s/it]                                                        71%|   | 4597/6500 [8:41:25<3:30:46,  6.65s/it] 71%|   | 4598/6500 [8:41:31<3:29:44,  6.62s/it]                                                        71%|   | 4598/6500 [8:41:31<3:29:44,  6.62s/it] 71%|   | 4599/6500 [8:41:38<3:28:59,  6.60s/it]                                                        71%|   | 4599/6500 [8:41:38<3:28:59,  6.60s/it] 71%|   | 4600/6500 [8:41:44<3:28:24,  6.58s/it]                                                        71%|   | 4600/6500 [8:41:44<3:28:24,  6.58s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.878618597984314, 'eval_runtime': 1.4968, 'eval_samples_per_second': 8.017, 'eval_steps_per_second': 2.004, 'epoch': 0.71}
                                                        71%|   | 4600/6500 [8:41:46<3:28:24,  6.58s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-4600
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4600I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4600

I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4600
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4600
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4600/pytorch_model.bin 
/projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4600/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4600/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4600/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3802, 'learning_rate': 1.963389661678639e-05, 'epoch': 0.71}
{'loss': 0.3693, 'learning_rate': 1.961469536282264e-05, 'epoch': 0.71}
{'loss': 0.361, 'learning_rate': 1.9595501211239102e-05, 'epoch': 0.71}
{'loss': 0.3686, 'learning_rate': 1.9576314166522303e-05, 'epoch': 0.71}
{'loss': 0.3725, 'learning_rate': 1.955713423315708e-05, 'epoch': 0.71}
 71%|   | 4601/6500 [8:41:53<3:45:26,  7.12s/it]                                                        71%|   | 4601/6500 [8:41:53<3:45:26,  7.12s/it] 71%|   | 4602/6500 [8:41:59<3:39:46,  6.95s/it]                                                        71%|   | 4602/6500 [8:41:59<3:39:46,  6.95s/it] 71%|   | 4603/6500 [8:42:06<3:35:49,  6.83s/it]                                                        71%|   | 4603/6500 [8:42:06<3:35:49,  6.83s/it] 71%|   | 4604/6500 [8:42:12<3:33:01,  6.74s/it]                                                        71%|   | 4604/6500 [8:42:12<3:33:01,  6.74s/it] 71%|   | 4605/6500 [8:42:19<3:31:02,  6.68s/it]                                                        71%|   | 4605/6500 [8:42:19<3:31:02,  6.68s/it] 71%|   | 4606/65{'loss': 0.3813, 'learning_rate': 1.9537961415626638e-05, 'epoch': 0.71}
{'loss': 0.3795, 'learning_rate': 1.9518795718412502e-05, 'epoch': 0.71}
{'loss': 0.3783, 'learning_rate': 1.9499637145994538e-05, 'epoch': 0.71}
{'loss': 0.3807, 'learning_rate': 1.948048570285094e-05, 'epoch': 0.71}
{'loss': 0.3657, 'learning_rate': 1.9461341393458254e-05, 'epoch': 0.71}
00 [8:42:26<3:36:59,  6.87s/it]                                                        71%|   | 4606/6500 [8:42:26<3:36:59,  6.87s/it] 71%|   | 4607/6500 [8:42:33<3:33:39,  6.77s/it]                                                        71%|   | 4607/6500 [8:42:33<3:33:39,  6.77s/it] 71%|   | 4608/6500 [8:42:39<3:31:21,  6.70s/it]                                                        71%|   | 4608/6500 [8:42:39<3:31:21,  6.70s/it] 71%|   | 4609/6500 [8:42:46<3:29:51,  6.66s/it]                                                        71%|   | 4609/6500 [8:42:46<3:29:51,  6.66s/it] 71%|   | 4610/6500 [8:42:53<3:28:42,  6.63s/it]                                                        71%|   | 4610/6500 [8:42:53<3:28:42,  6.63s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8718702793121338, 'eval_runtime': 1.7294, 'eval_samples_per_second': 6.939, 'eval_steps_per_second': 1.735, 'epoch': 0.71}
                                                        71%|   | 4610/6500 [8:42:54<3:28:42,  6.63s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-4610
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4610
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4610
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4610
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4610
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4610/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4610/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4610/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4610/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3779, 'learning_rate': 1.9442204222291362e-05, 'epoch': 0.71}
{'loss': 0.3954, 'learning_rate': 1.942307419382341e-05, 'epoch': 0.71}
{'loss': 0.3852, 'learning_rate': 1.9403951312525957e-05, 'epoch': 0.71}
{'loss': 0.653, 'learning_rate': 1.938483558286886e-05, 'epoch': 0.71}
{'loss': 0.3755, 'learning_rate': 1.9365727009320307e-05, 'epoch': 0.71}
 71%|   | 4611/6500 [8:43:01<3:48:33,  7.26s/it]                                                        71%|   | 4611/6500 [8:43:01<3:48:33,  7.26s/it] 71%|   | 4612/6500 [8:43:08<3:41:38,  7.04s/it]                                                        71%|   | 4612/6500 [8:43:08<3:41:38,  7.04s/it] 71%|   | 4613/6500 [8:43:14<3:36:48,  6.89s/it]                                                        71%|   | 4613/6500 [8:43:14<3:36:48,  6.89s/it] 71%|   | 4614/6500 [8:43:21<3:33:25,  6.79s/it]                                                        71%|   | 4614/6500 [8:43:21<3:33:25,  6.79s/it] 71%|   | 4615/6500 [8:43:27<3:30:59,  6.72s/it]                                                        71%|   | 4615/6500 [8:43:27<3:30:59,  6.72s/it] 71%|   | 4616/65{'loss': 0.3901, 'learning_rate': 1.9346625596346794e-05, 'epoch': 0.71}
{'loss': 0.3564, 'learning_rate': 1.9327531348413185e-05, 'epoch': 0.71}
{'loss': 0.3702, 'learning_rate': 1.9308444269982624e-05, 'epoch': 0.71}
{'loss': 0.3781, 'learning_rate': 1.928936436551661e-05, 'epoch': 0.71}
{'loss': 0.3694, 'learning_rate': 1.9270291639474953e-05, 'epoch': 0.71}
00 [8:43:34<3:29:07,  6.66s/it]                                                        71%|   | 4616/6500 [8:43:34<3:29:07,  6.66s/it] 71%|   | 4617/6500 [8:43:41<3:27:53,  6.62s/it]                                                        71%|   | 4617/6500 [8:43:41<3:27:53,  6.62s/it] 71%|   | 4618/6500 [8:43:47<3:26:57,  6.60s/it]                                                        71%|   | 4618/6500 [8:43:47<3:26:57,  6.60s/it] 71%|   | 4619/6500 [8:43:54<3:26:23,  6.58s/it]                                                        71%|   | 4619/6500 [8:43:54<3:26:23,  6.58s/it] 71%|   | 4620/6500 [8:44:00<3:26:10,  6.58s/it]                                                        71%|   | 4620/6500 [8:44:00<3:26:10,  6.58s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8781840205192566, 'eval_runtime': 1.4832, 'eval_samples_per_second': 8.09, 'eval_steps_per_second': 2.023, 'epoch': 0.71}
                                                        71%|   | 4620/6500 [8:44:02<3:26:10,  6.58s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-4620
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4620
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4620
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4620
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4620
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4620/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4620/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4620/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4620/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3849, 'learning_rate': 1.9251226096315782e-05, 'epoch': 0.71}
{'loss': 0.3863, 'learning_rate': 1.923216774049557e-05, 'epoch': 0.71}
{'loss': 0.3623, 'learning_rate': 1.921311657646907e-05, 'epoch': 0.71}
{'loss': 0.3781, 'learning_rate': 1.919407260868937e-05, 'epoch': 0.71}
{'loss': 0.3593, 'learning_rate': 1.917503584160791e-05, 'epoch': 0.71}
 71%|   | 4621/6500 [8:44:09<3:43:07,  7.12s/it]                                                        71%|   | 4621/6500 [8:44:09<3:43:07,  7.12s/it] 71%|   | 4622/6500 [8:44:16<3:43:33,  7.14s/it]                                                        71%|   | 4622/6500 [8:44:16<3:43:33,  7.14s/it] 71%|   | 4623/6500 [8:44:22<3:37:52,  6.96s/it]                                                        71%|   | 4623/6500 [8:44:22<3:37:52,  6.96s/it] 71%|   | 4624/6500 [8:44:29<3:33:49,  6.84s/it]                                                        71%|   | 4624/6500 [8:44:29<3:33:49,  6.84s/it] 71%|   | 4625/6500 [8:44:35<3:30:53,  6.75s/it]                                                        71%|   | 4625/6500 [8:44:35<3:30:53,  6.75s/it] 71%|   | 4626/65{'loss': 0.4084, 'learning_rate': 1.9156006279674393e-05, 'epoch': 0.71}
{'loss': 0.3622, 'learning_rate': 1.913698392733687e-05, 'epoch': 0.71}
{'loss': 0.649, 'learning_rate': 1.9117968789041712e-05, 'epoch': 0.71}
{'loss': 0.3861, 'learning_rate': 1.9098960869233584e-05, 'epoch': 0.71}
{'loss': 0.3715, 'learning_rate': 1.9079960172355464e-05, 'epoch': 0.71}
00 [8:44:42<3:28:51,  6.69s/it]                                                        71%|   | 4626/6500 [8:44:42<3:28:51,  6.69s/it] 71%|   | 4627/6500 [8:44:48<3:27:27,  6.65s/it]                                                        71%|   | 4627/6500 [8:44:48<3:27:27,  6.65s/it] 71%|   | 4628/6500 [8:44:55<3:26:20,  6.61s/it]                                                        71%|   | 4628/6500 [8:44:55<3:26:20,  6.61s/it] 71%|   | 4629/6500 [8:45:02<3:25:38,  6.59s/it]                                                        71%|   | 4629/6500 [8:45:02<3:25:38,  6.59s/it] 71%|   | 4630/6500 [8:45:08<3:25:11,  6.58s/it]                                                        71%|   | 4630/6500 [8:45:08<3:25:11,  6.58s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8787116408348083, 'eval_runtime': 1.4886, 'eval_samples_per_second': 8.061, 'eval_steps_per_second': 2.015, 'epoch': 0.71}
                                                        71%|   | 4630/6500 [8:45:10<3:25:11,  6.58s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-4630
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4630
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4630
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4630
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4630
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4630/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4630/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4630/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4630/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3805, 'learning_rate': 1.9060966702848664e-05, 'epoch': 0.71}
{'loss': 0.3694, 'learning_rate': 1.904198046515278e-05, 'epoch': 0.71}
{'loss': 0.3748, 'learning_rate': 1.9023001463705754e-05, 'epoch': 0.71}
{'loss': 0.3792, 'learning_rate': 1.9004029702943775e-05, 'epoch': 0.71}
{'loss': 0.3764, 'learning_rate': 1.8985065187301394e-05, 'epoch': 0.71}
 71%|   | 4631/6500 [8:45:17<3:42:16,  7.14s/it]                                                        71%|   | 4631/6500 [8:45:17<3:42:16,  7.14s/it] 71%|  | 4632/6500 [8:45:23<3:36:36,  6.96s/it]                                                        71%|  | 4632/6500 [8:45:23<3:36:36,  6.96s/it] 71%|  | 4633/6500 [8:45:30<3:32:40,  6.83s/it]                                                        71%|  | 4633/6500 [8:45:30<3:32:40,  6.83s/it] 71%|  | 4634/6500 [8:45:36<3:29:47,  6.75s/it]                                                        71%|  | 4634/6500 [8:45:36<3:29:47,  6.75s/it] 71%|  | 4635/6500 [8:45:43<3:27:38,  6.68s/it]                                                        71%|  | 4635/6500 [8:45:43<3:27:38,  6.68s/it] 71%|{'loss': 0.3816, 'learning_rate': 1.896610792121145e-05, 'epoch': 0.71}
{'loss': 0.3775, 'learning_rate': 1.8947157909105097e-05, 'epoch': 0.71}
{'loss': 0.3773, 'learning_rate': 1.8928215155411773e-05, 'epoch': 0.71}
{'loss': 0.3703, 'learning_rate': 1.8909279664559236e-05, 'epoch': 0.71}
{'loss': 0.3567, 'learning_rate': 1.8890351440973542e-05, 'epoch': 0.71}
  | 4636/6500 [8:45:49<3:26:17,  6.64s/it]                                                        71%|  | 4636/6500 [8:45:49<3:26:17,  6.64s/it] 71%|  | 4637/6500 [8:45:56<3:25:19,  6.61s/it]                                                        71%|  | 4637/6500 [8:45:56<3:25:19,  6.61s/it] 71%|  | 4638/6500 [8:46:02<3:24:31,  6.59s/it]                                                        71%|  | 4638/6500 [8:46:02<3:24:31,  6.59s/it] 71%|  | 4639/6500 [8:46:10<3:34:57,  6.93s/it]                                                        71%|  | 4639/6500 [8:46:10<3:34:57,  6.93s/it] 71%|  | 4640/6500 [8:46:17<3:31:15,  6.82s/it]                                                        71%|  | 4640/6500 [8:46:17<3:31:15,  6.82s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8697941899299622, 'eval_runtime': 1.5246, 'eval_samples_per_second': 7.871, 'eval_steps_per_second': 1.968, 'epoch': 0.71}
                                                        71%|  | 4640/6500 [8:46:18<3:31:15,  6.82s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-4640
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4640
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4640
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4640
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4640
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4640/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4640/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4640/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4640/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.4076, 'learning_rate': 1.8871430489079052e-05, 'epoch': 0.71}
{'loss': 0.3718, 'learning_rate': 1.885251681329842e-05, 'epoch': 0.71}
{'loss': 0.6545, 'learning_rate': 1.8833610418052606e-05, 'epoch': 0.71}
{'loss': 0.3847, 'learning_rate': 1.8814711307760875e-05, 'epoch': 0.71}
{'loss': 0.3613, 'learning_rate': 1.8795819486840747e-05, 'epoch': 0.71}
 71%|  | 4641/6500 [8:46:25<3:47:00,  7.33s/it]                                                        71%|  | 4641/6500 [8:46:25<3:47:00,  7.33s/it] 71%|  | 4642/6500 [8:46:32<3:39:32,  7.09s/it]                                                        71%|  | 4642/6500 [8:46:32<3:39:32,  7.09s/it] 71%|  | 4643/6500 [8:46:38<3:34:18,  6.92s/it]                                                        71%|  | 4643/6500 [8:46:38<3:34:18,  6.92s/it] 71%|  | 4644/6500 [8:46:45<3:30:45,  6.81s/it]                                                        71%|  | 4644/6500 [8:46:45<3:30:45,  6.81s/it] 71%|  | 4645/6500 [8:46:51<3:27:59,  6.73s/it]                                                        71%|  | 4645/6500 [8:46:51<3:27:59,  6.73s/it] 71%|{'loss': 0.3816, 'learning_rate': 1.877693495970809e-05, 'epoch': 0.71}
{'loss': 0.3629, 'learning_rate': 1.875805773077705e-05, 'epoch': 0.71}
{'loss': 0.3686, 'learning_rate': 1.873918780446006e-05, 'epoch': 0.72}
{'loss': 0.3739, 'learning_rate': 1.8720325185167836e-05, 'epoch': 0.72}
{'loss': 0.3763, 'learning_rate': 1.870146987730943e-05, 'epoch': 0.72}
  | 4646/6500 [8:46:58<3:26:09,  6.67s/it]                                                        71%|  | 4646/6500 [8:46:58<3:26:09,  6.67s/it] 71%|  | 4647/6500 [8:47:04<3:24:51,  6.63s/it]                                                        71%|  | 4647/6500 [8:47:04<3:24:51,  6.63s/it] 72%|  | 4648/6500 [8:47:13<3:38:43,  7.09s/it]                                                        72%|  | 4648/6500 [8:47:13<3:38:43,  7.09s/it] 72%|  | 4649/6500 [8:47:19<3:34:05,  6.94s/it]                                                        72%|  | 4649/6500 [8:47:19<3:34:05,  6.94s/it] 72%|  | 4650/6500 [8:47:26<3:30:18,  6.82s/it]                                                        72%|  | 4650/6500 [8:47:26<3:30:18,  6.82s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8794369101524353, 'eval_runtime': 1.7117, 'eval_samples_per_second': 7.01, 'eval_steps_per_second': 1.753, 'epoch': 0.72}
                                                        72%|  | 4650/6500 [8:47:27<3:30:18,  6.82s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-4650
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4650
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4650
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4650
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4650
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4650/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4650/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4650/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4650/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3745, 'learning_rate': 1.8682621885292145e-05, 'epoch': 0.72}
{'loss': 0.3724, 'learning_rate': 1.866378121352158e-05, 'epoch': 0.72}
{'loss': 0.3745, 'learning_rate': 1.8644947866401653e-05, 'epoch': 0.72}
{'loss': 0.377, 'learning_rate': 1.8626121848334493e-05, 'epoch': 0.72}
{'loss': 0.3692, 'learning_rate': 1.8607303163720602e-05, 'epoch': 0.72}
 72%|  | 4651/6500 [8:47:34<3:47:04,  7.37s/it]                                                        72%|  | 4651/6500 [8:47:34<3:47:04,  7.37s/it] 72%|  | 4652/6500 [8:47:41<3:39:25,  7.12s/it]                                                        72%|  | 4652/6500 [8:47:41<3:39:25,  7.12s/it] 72%|  | 4653/6500 [8:47:47<3:33:54,  6.95s/it]                                                        72%|  | 4653/6500 [8:47:47<3:33:54,  6.95s/it] 72%|  | 4654/6500 [8:47:54<3:32:19,  6.90s/it]                                                        72%|  | 4654/6500 [8:47:54<3:32:19,  6.90s/it] 72%|  | 4655/6500 [8:48:02<3:42:25,  7.23s/it]                                                        72%|  | 4655/6500 [8:48:02<3:42:25,  7.23s/it] 72%|{'loss': 0.413, 'learning_rate': 1.858849181695872e-05, 'epoch': 0.72}
{'loss': 0.3723, 'learning_rate': 1.8569687812445896e-05, 'epoch': 0.72}
{'loss': 0.6484, 'learning_rate': 1.8550891154577442e-05, 'epoch': 0.72}
{'loss': 0.3856, 'learning_rate': 1.853210184774697e-05, 'epoch': 0.72}
{'loss': 0.3704, 'learning_rate': 1.8513319896346358e-05, 'epoch': 0.72}
  | 4656/6500 [8:48:09<3:36:07,  7.03s/it]                                                        72%|  | 4656/6500 [8:48:09<3:36:07,  7.03s/it] 72%|  | 4657/6500 [8:48:15<3:31:35,  6.89s/it]                                                        72%|  | 4657/6500 [8:48:15<3:31:35,  6.89s/it] 72%|  | 4658/6500 [8:48:22<3:28:25,  6.79s/it]                                                        72%|  | 4658/6500 [8:48:22<3:28:25,  6.79s/it] 72%|  | 4659/6500 [8:48:28<3:26:06,  6.72s/it]                                                        72%|  | 4659/6500 [8:48:28<3:26:06,  6.72s/it] 72%|  | 4660/6500 [8:48:35<3:24:24,  6.67s/it]                                                        72%|  | 4660/6500 [8:48:35<3:24:24,  6.67s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8757951855659485, 'eval_runtime': 1.4815, 'eval_samples_per_second': 8.1, 'eval_steps_per_second': 2.025, 'epoch': 0.72}
                                                        72%|  | 4660/6500 [8:48:36<3:24:24,  6.67s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-4660
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4660
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4660
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4660
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4660
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4660/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4660/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4660/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4660/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3679, 'learning_rate': 1.8494545304765785e-05, 'epoch': 0.72}
{'loss': 0.367, 'learning_rate': 1.8475778077393685e-05, 'epoch': 0.72}
{'loss': 0.3759, 'learning_rate': 1.8457018218616794e-05, 'epoch': 0.72}
{'loss': 0.3784, 'learning_rate': 1.8438265732820126e-05, 'epoch': 0.72}
{'loss': 0.3877, 'learning_rate': 1.8419520624386925e-05, 'epoch': 0.72}
 72%|  | 4661/6500 [8:48:43<3:40:00,  7.18s/it]                                                        72%|  | 4661/6500 [8:48:43<3:40:00,  7.18s/it] 72%|  | 4662/6500 [8:48:50<3:34:13,  6.99s/it]                                                        72%|  | 4662/6500 [8:48:50<3:34:13,  6.99s/it] 72%|  | 4663/6500 [8:48:56<3:30:00,  6.86s/it]                                                        72%|  | 4663/6500 [8:48:56<3:30:00,  6.86s/it] 72%|  | 4664/6500 [8:49:03<3:27:02,  6.77s/it]                                                        72%|  | 4664/6500 [8:49:03<3:27:02,  6.77s/it] 72%|  | 4665/6500 [8:49:10<3:24:57,  6.70s/it]                                                        72%|  | 4665/6500 [8:49:10<3:24:57,  6.70s/it] 72%|{'loss': 0.3783, 'learning_rate': 1.8400782897698764e-05, 'epoch': 0.72}
{'loss': 0.3738, 'learning_rate': 1.838205255713548e-05, 'epoch': 0.72}
{'loss': 0.3773, 'learning_rate': 1.8363329607075168e-05, 'epoch': 0.72}
{'loss': 0.3681, 'learning_rate': 1.8344614051894204e-05, 'epoch': 0.72}
{'loss': 0.3661, 'learning_rate': 1.8325905895967237e-05, 'epoch': 0.72}
  | 4666/6500 [8:49:16<3:23:26,  6.66s/it]                                                        72%|  | 4666/6500 [8:49:16<3:23:26,  6.66s/it] 72%|  | 4667/6500 [8:49:23<3:22:21,  6.62s/it]                                                        72%|  | 4667/6500 [8:49:23<3:22:21,  6.62s/it] 72%|  | 4668/6500 [8:49:29<3:21:24,  6.60s/it]                                                        72%|  | 4668/6500 [8:49:29<3:21:24,  6.60s/it] 72%|  | 4669/6500 [8:49:36<3:20:44,  6.58s/it]                                                        72%|  | 4669/6500 [8:49:36<3:20:44,  6.58s/it] 72%|  | 4670/6500 [8:49:42<3:20:19,  6.57s/it]                                                        72%|  | 4670/6500 [8:49:42<3:20:19,  6.57s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8735804557800293, 'eval_runtime': 1.4829, 'eval_samples_per_second': 8.092, 'eval_steps_per_second': 2.023, 'epoch': 0.72}
                                                        72%|  | 4670/6500 [8:49:44<3:20:19,  6.57s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-4670
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  I AM HERE AND I AM SAVING THE MODEL
/projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4670the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4670

I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4670
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4670
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4670/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4670/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4670/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4670/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3974, 'learning_rate': 1.830720514366719e-05, 'epoch': 0.72}
{'loss': 0.3786, 'learning_rate': 1.8288511799365243e-05, 'epoch': 0.72}
{'loss': 0.6478, 'learning_rate': 1.826982586743085e-05, 'epoch': 0.72}
{'loss': 0.3745, 'learning_rate': 1.8251147352231747e-05, 'epoch': 0.72}
{'loss': 0.3789, 'learning_rate': 1.8232476258133924e-05, 'epoch': 0.72}
 72%|  | 4671/6500 [8:49:52<3:45:12,  7.39s/it]                                                        72%|  | 4671/6500 [8:49:52<3:45:12,  7.39s/it] 72%|  | 4672/6500 [8:49:58<3:37:33,  7.14s/it]                                                        72%|  | 4672/6500 [8:49:58<3:37:33,  7.14s/it] 72%|  | 4673/6500 [8:50:05<3:32:09,  6.97s/it]                                                        72%|  | 4673/6500 [8:50:05<3:32:09,  6.97s/it] 72%|  | 4674/6500 [8:50:11<3:28:15,  6.84s/it]                                                        72%|  | 4674/6500 [8:50:11<3:28:15,  6.84s/it] 72%|  | 4675/6500 [8:50:18<3:25:27,  6.75s/it]                                                        72%|  | 4675/6500 [8:50:18<3:25:27,  6.75s/it] 72%|{'loss': 0.3617, 'learning_rate': 1.821381258950161e-05, 'epoch': 0.72}
{'loss': 0.3613, 'learning_rate': 1.819515635069734e-05, 'epoch': 0.72}
{'loss': 0.3675, 'learning_rate': 1.8176507546081894e-05, 'epoch': 0.72}
{'loss': 0.3784, 'learning_rate': 1.8157866180014327e-05, 'epoch': 0.72}
{'loss': 0.3824, 'learning_rate': 1.8139232256851928e-05, 'epoch': 0.72}
  | 4676/6500 [8:50:24<3:23:28,  6.69s/it]                                                        72%|  | 4676/6500 [8:50:24<3:23:28,  6.69s/it] 72%|  | 4677/6500 [8:50:31<3:22:06,  6.65s/it]                                                        72%|  | 4677/6500 [8:50:31<3:22:06,  6.65s/it] 72%|  | 4678/6500 [8:50:37<3:21:07,  6.62s/it]                                                        72%|  | 4678/6500 [8:50:37<3:21:07,  6.62s/it] 72%|  | 4679/6500 [8:50:44<3:20:21,  6.60s/it]                                                        72%|  | 4679/6500 [8:50:44<3:20:21,  6.60s/it] 72%|  | 4680/6500 [8:50:51<3:19:47,  6.59s/it]                                                        72%|  | 4680/6500 [8:50:51<3:19:47,  6.59s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8779721260070801, 'eval_runtime': 1.4786, 'eval_samples_per_second': 8.116, 'eval_steps_per_second': 2.029, 'epoch': 0.72}
                                                        72%|  | 4680/6500 [8:50:52<3:19:47,  6.59s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-4680
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4680
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4680
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4680
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4680
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4680/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4680/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4680/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4680/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3816, 'learning_rate': 1.8120605780950277e-05, 'epoch': 0.72}
{'loss': 0.366, 'learning_rate': 1.8101986756663197e-05, 'epoch': 0.72}
{'loss': 0.3812, 'learning_rate': 1.8083375188342765e-05, 'epoch': 0.72}
{'loss': 0.3566, 'learning_rate': 1.8064771080339328e-05, 'epoch': 0.72}
{'loss': 0.3692, 'learning_rate': 1.8046174437001484e-05, 'epoch': 0.72}
 72%|  | 4681/6500 [8:50:59<3:36:04,  7.13s/it]                                                        72%|  | 4681/6500 [8:50:59<3:36:04,  7.13s/it] 72%|  | 4682/6500 [8:51:06<3:30:47,  6.96s/it]                                                        72%|  | 4682/6500 [8:51:06<3:30:47,  6.96s/it] 72%|  | 4683/6500 [8:51:12<3:26:56,  6.83s/it]                                                        72%|  | 4683/6500 [8:51:12<3:26:56,  6.83s/it] 72%|  | 4684/6500 [8:51:19<3:24:14,  6.75s/it]                                                        72%|  | 4684/6500 [8:51:19<3:24:14,  6.75s/it] 72%|  | 4685/6500 [8:51:25<3:22:16,  6.69s/it]                                                        72%|  | 4685/6500 [8:51:25<3:22:16,  6.69s/it] 72%|{'loss': 0.391, 'learning_rate': 1.8027585262676093e-05, 'epoch': 0.72}
{'loss': 0.3811, 'learning_rate': 1.8009003561708244e-05, 'epoch': 0.72}
{'loss': 0.6491, 'learning_rate': 1.7990429338441293e-05, 'epoch': 0.72}
{'loss': 0.373, 'learning_rate': 1.7971862597216872e-05, 'epoch': 0.72}
{'loss': 0.3823, 'learning_rate': 1.7953303342374832e-05, 'epoch': 0.72}
  | 4686/6500 [8:51:32<3:21:02,  6.65s/it]                                                        72%|  | 4686/6500 [8:51:32<3:21:02,  6.65s/it] 72%|  | 4687/6500 [8:51:39<3:28:04,  6.89s/it]                                                        72%|  | 4687/6500 [8:51:39<3:28:04,  6.89s/it] 72%|  | 4688/6500 [8:51:46<3:25:04,  6.79s/it]                                                        72%|  | 4688/6500 [8:51:46<3:25:04,  6.79s/it] 72%|  | 4689/6500 [8:51:53<3:30:44,  6.98s/it]                                                        72%|  | 4689/6500 [8:51:53<3:30:44,  6.98s/it] 72%|  | 4690/6500 [8:52:00<3:26:43,  6.85s/it]                                                        72%|  | 4690/6500 [8:52:00<3:26:43,  6.85s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8784645795822144, 'eval_runtime': 1.8456, 'eval_samples_per_second': 6.502, 'eval_steps_per_second': 1.625, 'epoch': 0.72}
                                                        72%|  | 4690/6500 [8:52:02<3:26:43,  6.85s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-4690
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4690
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4690
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4690
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4690
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4690/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4690/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4690/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4690/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3532, 'learning_rate': 1.793475157825329e-05, 'epoch': 0.72}
{'loss': 0.363, 'learning_rate': 1.7916207309188604e-05, 'epoch': 0.72}
{'loss': 0.37, 'learning_rate': 1.7897670539515387e-05, 'epoch': 0.72}
{'loss': 0.3667, 'learning_rate': 1.7879141273566497e-05, 'epoch': 0.72}
{'loss': 0.3815, 'learning_rate': 1.7860619515673033e-05, 'epoch': 0.72}
 72%|  | 4691/6500 [8:52:09<3:44:43,  7.45s/it]                                                        72%|  | 4691/6500 [8:52:09<3:44:43,  7.45s/it] 72%|  | 4692/6500 [8:52:15<3:36:28,  7.18s/it]                                                        72%|  | 4692/6500 [8:52:15<3:36:28,  7.18s/it] 72%|  | 4693/6500 [8:52:22<3:30:38,  6.99s/it]                                                        72%|  | 4693/6500 [8:52:22<3:30:38,  6.99s/it] 72%|  | 4694/6500 [8:52:28<3:26:26,  6.86s/it]                                                        72%|  | 4694/6500 [8:52:28<3:26:26,  6.86s/it] 72%|  | 4695/6500 [8:52:35<3:23:29,  6.76s/it]                                                        72%|  | 4695/6500 [8:52:35<3:23:29,  6.76s/it] 72%|{'loss': 0.3863, 'learning_rate': 1.784210527016435e-05, 'epoch': 0.72}
{'loss': 0.3594, 'learning_rate': 1.7823598541368035e-05, 'epoch': 0.72}
{'loss': 0.3769, 'learning_rate': 1.7805099333609944e-05, 'epoch': 0.72}
{'loss': 0.3595, 'learning_rate': 1.778660765121412e-05, 'epoch': 0.72}
{'loss': 0.4015, 'learning_rate': 1.776812349850289e-05, 'epoch': 0.72}
  | 4696/6500 [8:52:41<3:21:24,  6.70s/it]                                                        72%|  | 4696/6500 [8:52:41<3:21:24,  6.70s/it] 72%|  | 4697/6500 [8:52:48<3:20:03,  6.66s/it]                                                        72%|  | 4697/6500 [8:52:48<3:20:03,  6.66s/it] 72%|  | 4698/6500 [8:52:54<3:18:53,  6.62s/it]                                                        72%|  | 4698/6500 [8:52:54<3:18:53,  6.62s/it] 72%|  | 4699/6500 [8:53:01<3:18:03,  6.60s/it]                                                        72%|  | 4699/6500 [8:53:01<3:18:03,  6.60s/it] 72%|  | 4700/6500 [8:53:07<3:17:28,  6.58s/it]                                                        72%|  | 4700/6500 [8:53:07<3:17:28,  6.58s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8730146288871765, 'eval_runtime': 1.4747, 'eval_samples_per_second': 8.137, 'eval_steps_per_second': 2.034, 'epoch': 0.72}
                                                        72%|  | 4700/6500 [8:53:09<3:17:28,  6.58s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-4700
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4700
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4700
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4700
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4700
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4700/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4700/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4700/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4700/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.357, 'learning_rate': 1.7749646879796828e-05, 'epoch': 0.72}
{'loss': 0.644, 'learning_rate': 1.7731177799414718e-05, 'epoch': 0.72}
{'loss': 0.3924, 'learning_rate': 1.77127162616736e-05, 'epoch': 0.72}
{'loss': 0.365, 'learning_rate': 1.7694262270888747e-05, 'epoch': 0.72}
{'loss': 0.3839, 'learning_rate': 1.7675815831373665e-05, 'epoch': 0.72}
 72%|  | 4701/6500 [8:53:16<3:34:08,  7.14s/it]                                                        72%|  | 4701/6500 [8:53:16<3:34:08,  7.14s/it] 72%|  | 4702/6500 [8:53:22<3:28:35,  6.96s/it]                                                        72%|  | 4702/6500 [8:53:22<3:28:35,  6.96s/it] 72%|  | 4703/6500 [8:53:30<3:30:21,  7.02s/it]                                                        72%|  | 4703/6500 [8:53:30<3:30:21,  7.02s/it] 72%|  | 4704/6500 [8:53:36<3:25:53,  6.88s/it]                                                        72%|  | 4704/6500 [8:53:36<3:25:53,  6.88s/it] 72%|  | 4705/6500 [8:53:43<3:22:43,  6.78s/it]                                                        72%|  | 4705/6500 [8:53:43<3:22:43,  6.78s/it] 72%|{'loss': 0.3601, 'learning_rate': 1.7657376947440103e-05, 'epoch': 0.72}
{'loss': 0.3728, 'learning_rate': 1.7638945623398022e-05, 'epoch': 0.72}
{'loss': 0.3772, 'learning_rate': 1.7620521863555656e-05, 'epoch': 0.72}
{'loss': 0.3806, 'learning_rate': 1.7602105672219444e-05, 'epoch': 0.72}
{'loss': 0.375, 'learning_rate': 1.7583697053694033e-05, 'epoch': 0.72}
  | 4706/6500 [8:53:49<3:20:34,  6.71s/it]                                                        72%|  | 4706/6500 [8:53:49<3:20:34,  6.71s/it] 72%|  | 4707/6500 [8:53:56<3:19:16,  6.67s/it]                                                        72%|  | 4707/6500 [8:53:56<3:19:16,  6.67s/it] 72%|  | 4708/6500 [8:54:02<3:18:08,  6.63s/it]                                                        72%|  | 4708/6500 [8:54:02<3:18:08,  6.63s/it] 72%|  | 4709/6500 [8:54:09<3:17:30,  6.62s/it]                                                        72%|  | 4709/6500 [8:54:09<3:17:30,  6.62s/it] 72%|  | 4710/6500 [8:54:16<3:16:54,  6.60s/it]                                                        72%|  | 4710/6500 [8:54:16<3:16:54,  6.60s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8794975876808167, 'eval_runtime': 1.492, 'eval_samples_per_second': 8.043, 'eval_steps_per_second': 2.011, 'epoch': 0.72}
                                                        72%|  | 4710/6500 [8:54:17<3:16:54,  6.60s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-4710
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4710
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4710
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4710
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4710
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4710/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4710/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4710/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4710/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.377, 'learning_rate': 1.756529601228234e-05, 'epoch': 0.72}
{'loss': 0.3684, 'learning_rate': 1.75469025522855e-05, 'epoch': 0.72}
{'loss': 0.3754, 'learning_rate': 1.7528516678002864e-05, 'epoch': 0.73}
{'loss': 0.3607, 'learning_rate': 1.7510138393732028e-05, 'epoch': 0.73}
{'loss': 0.4039, 'learning_rate': 1.7491767703768807e-05, 'epoch': 0.73}
 72%|  | 4711/6500 [8:54:24<3:33:10,  7.15s/it]                                                        72%|  | 4711/6500 [8:54:24<3:33:10,  7.15s/it] 72%|  | 4712/6500 [8:54:31<3:29:07,  7.02s/it]                                                        72%|  | 4712/6500 [8:54:31<3:29:07,  7.02s/it] 73%|  | 4713/6500 [8:54:37<3:24:54,  6.88s/it]                                                        73%|  | 4713/6500 [8:54:37<3:24:54,  6.88s/it] 73%|  | 4714/6500 [8:54:44<3:21:50,  6.78s/it]                                                        73%|  | 4714/6500 [8:54:44<3:21:50,  6.78s/it] 73%|  | 4715/6500 [8:54:50<3:19:41,  6.71s/it]                                                        73%|  | 4715/6500 [8:54:50<3:19:41,  6.71s/it] 73%|{'loss': 0.3666, 'learning_rate': 1.7473404612407225e-05, 'epoch': 0.73}
{'loss': 0.6444, 'learning_rate': 1.7455049123939554e-05, 'epoch': 0.73}
{'loss': 0.3858, 'learning_rate': 1.7436701242656272e-05, 'epoch': 0.73}
{'loss': 0.3596, 'learning_rate': 1.7418360972846088e-05, 'epoch': 0.73}
{'loss': 0.3784, 'learning_rate': 1.740002831879594e-05, 'epoch': 0.73}
  | 4716/6500 [8:54:57<3:18:07,  6.66s/it]                                                        73%|  | 4716/6500 [8:54:57<3:18:07,  6.66s/it] 73%|  | 4717/6500 [8:55:03<3:16:58,  6.63s/it]                                                        73%|  | 4717/6500 [8:55:03<3:16:58,  6.63s/it] 73%|  | 4718/6500 [8:55:10<3:16:18,  6.61s/it]                                                        73%|  | 4718/6500 [8:55:10<3:16:18,  6.61s/it] 73%|  | 4719/6500 [8:55:18<3:26:01,  6.94s/it]                                                        73%|  | 4719/6500 [8:55:18<3:26:01,  6.94s/it] 73%|  | 4720/6500 [8:55:24<3:22:25,  6.82s/it]                                                        73%|  | 4720/6500 [8:55:24<3:22:25,  6.82s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8775689601898193, 'eval_runtime': 1.4802, 'eval_samples_per_second': 8.107, 'eval_steps_per_second': 2.027, 'epoch': 0.73}
                                                        73%|  | 4720/6500 [8:55:26<3:22:25,  6.82s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-4720
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4720I AM HERE AND I AM SAVING THE MODEL

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4720
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4720
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4720
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4720/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4720/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4720/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4720/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.357, 'learning_rate': 1.7381703284790947e-05, 'epoch': 0.73}
{'loss': 0.3674, 'learning_rate': 1.736338587511449e-05, 'epoch': 0.73}
{'loss': 0.3708, 'learning_rate': 1.7345076094048147e-05, 'epoch': 0.73}
{'loss': 0.3754, 'learning_rate': 1.732677394587173e-05, 'epoch': 0.73}
{'loss': 0.3719, 'learning_rate': 1.7308479434863216e-05, 'epoch': 0.73}
 73%|  | 4721/6500 [8:55:33<3:36:04,  7.29s/it]                                                        73%|  | 4721/6500 [8:55:33<3:36:04,  7.29s/it] 73%|  | 4722/6500 [8:55:39<3:29:31,  7.07s/it]                                                        73%|  | 4722/6500 [8:55:39<3:29:31,  7.07s/it] 73%|  | 4723/6500 [8:55:46<3:24:46,  6.91s/it]                                                        73%|  | 4723/6500 [8:55:46<3:24:46,  6.91s/it] 73%|  | 4724/6500 [8:55:52<3:21:24,  6.80s/it]                                                        73%|  | 4724/6500 [8:55:52<3:21:24,  6.80s/it] 73%|  | 4725/6500 [8:55:59<3:19:09,  6.73s/it]                                                        73%|  | 4725/6500 [8:55:59<3:19:09,  6.73s/it] 73%|{'loss': 0.3797, 'learning_rate': 1.7290192565298897e-05, 'epoch': 0.73}
{'loss': 0.3716, 'learning_rate': 1.7271913341453176e-05, 'epoch': 0.73}
{'loss': 0.3745, 'learning_rate': 1.725364176759873e-05, 'epoch': 0.73}
{'loss': 0.3576, 'learning_rate': 1.7235377848006434e-05, 'epoch': 0.73}
{'loss': 0.41, 'learning_rate': 1.7217121586945334e-05, 'epoch': 0.73}
  | 4726/6500 [8:56:05<3:17:37,  6.68s/it]                                                        73%|  | 4726/6500 [8:56:05<3:17:37,  6.68s/it] 73%|  | 4727/6500 [8:56:12<3:16:22,  6.65s/it]                                                        73%|  | 4727/6500 [8:56:12<3:16:22,  6.65s/it] 73%|  | 4728/6500 [8:56:19<3:15:26,  6.62s/it]                                                        73%|  | 4728/6500 [8:56:19<3:15:26,  6.62s/it] 73%|  | 4729/6500 [8:56:25<3:14:43,  6.60s/it]                                                        73%|  | 4729/6500 [8:56:25<3:14:43,  6.60s/it] 73%|  | 4730/6500 [8:56:32<3:14:14,  6.58s/it]                                                        73%|  | 4730/6500 [8:56:32<3:14:14,  6.58s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8735260367393494, 'eval_runtime': 1.4765, 'eval_samples_per_second': 8.127, 'eval_steps_per_second': 2.032, 'epoch': 0.73}
                                                        73%|  | 4730/6500 [8:56:33<3:14:14,  6.58s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-4730
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4730
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4730
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4730
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4730
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4730/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4730/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4730/pytorch_model.binthe pytorch model path is
 /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4730/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3736, 'learning_rate': 1.719887298868274e-05, 'epoch': 0.73}
{'loss': 0.6477, 'learning_rate': 1.718063205748416e-05, 'epoch': 0.73}
{'loss': 0.3816, 'learning_rate': 1.7162398797613282e-05, 'epoch': 0.73}
{'loss': 0.3687, 'learning_rate': 1.714417321333203e-05, 'epoch': 0.73}
{'loss': 0.3627, 'learning_rate': 1.7125955308900527e-05, 'epoch': 0.73}
 73%|  | 4731/6500 [8:56:40<3:30:41,  7.15s/it]                                                        73%|  | 4731/6500 [8:56:40<3:30:41,  7.15s/it] 73%|  | 4732/6500 [8:56:47<3:25:36,  6.98s/it]                                                        73%|  | 4732/6500 [8:56:47<3:25:36,  6.98s/it] 73%|  | 4733/6500 [8:56:53<3:22:01,  6.86s/it]                                                        73%|  | 4733/6500 [8:56:53<3:22:01,  6.86s/it] 73%|  | 4734/6500 [8:57:00<3:19:20,  6.77s/it]                                                        73%|  | 4734/6500 [8:57:00<3:19:20,  6.77s/it] 73%|  | 4735/6500 [8:57:07<3:25:24,  6.98s/it]                                                        73%|  | 4735/6500 [8:57:07<3:25:24,  6.98s/it] 73%|{'loss': 0.3659, 'learning_rate': 1.7107745088577087e-05, 'epoch': 0.73}
{'loss': 0.3783, 'learning_rate': 1.708954255661825e-05, 'epoch': 0.73}
{'loss': 0.3639, 'learning_rate': 1.7071347717278734e-05, 'epoch': 0.73}
{'loss': 0.3862, 'learning_rate': 1.7053160574811488e-05, 'epoch': 0.73}
{'loss': 0.3716, 'learning_rate': 1.7034981133467652e-05, 'epoch': 0.73}
  | 4736/6500 [8:57:14<3:22:09,  6.88s/it]                                                        73%|  | 4736/6500 [8:57:14<3:22:09,  6.88s/it] 73%|  | 4737/6500 [8:57:21<3:19:40,  6.80s/it]                                                        73%|  | 4737/6500 [8:57:21<3:19:40,  6.80s/it] 73%|  | 4738/6500 [8:57:27<3:17:44,  6.73s/it]                                                        73%|  | 4738/6500 [8:57:27<3:17:44,  6.73s/it] 73%|  | 4739/6500 [8:57:34<3:16:18,  6.69s/it]                                                        73%|  | 4739/6500 [8:57:34<3:16:18,  6.69s/it] 73%|  | 4740/6500 [8:57:40<3:15:20,  6.66s/it]                                                        73%|  | 4740/6500 [8:57:40<3:15:20,  6.66s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8787803053855896, 'eval_runtime': 1.481, 'eval_samples_per_second': 8.103, 'eval_steps_per_second': 2.026, 'epoch': 0.73}
                                                        73%|  | 4740/6500 [8:57:42<3:15:20,  6.66s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-4740
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4740
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4740
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4740
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4740
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4740/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4740/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4740/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4740/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3778, 'learning_rate': 1.7016809397496523e-05, 'epoch': 0.73}
{'loss': 0.3833, 'learning_rate': 1.6998645371145666e-05, 'epoch': 0.73}
{'loss': 0.3682, 'learning_rate': 1.6980489058660797e-05, 'epoch': 0.73}
{'loss': 0.3659, 'learning_rate': 1.6962340464285848e-05, 'epoch': 0.73}
{'loss': 0.3975, 'learning_rate': 1.6944199592262943e-05, 'epoch': 0.73}
 73%|  | 4741/6500 [8:57:49<3:30:45,  7.19s/it]                                                        73%|  | 4741/6500 [8:57:49<3:30:45,  7.19s/it] 73%|  | 4742/6500 [8:57:55<3:25:18,  7.01s/it]                                                        73%|  | 4742/6500 [8:57:55<3:25:18,  7.01s/it] 73%|  | 4743/6500 [8:58:02<3:21:28,  6.88s/it]                                                        73%|  | 4743/6500 [8:58:02<3:21:28,  6.88s/it] 73%|  | 4744/6500 [8:58:09<3:19:05,  6.80s/it]                                                        73%|  | 4744/6500 [8:58:09<3:19:05,  6.80s/it] 73%|  | 4745/6500 [8:58:15<3:17:04,  6.74s/it]                                                        73%|  | 4745/6500 [8:58:15<3:17:04,  6.74s/it] 73%|{'loss': 0.3758, 'learning_rate': 1.69260664468324e-05, 'epoch': 0.73}
{'loss': 0.6475, 'learning_rate': 1.6907941032232738e-05, 'epoch': 0.73}
{'loss': 0.3784, 'learning_rate': 1.6889823352700652e-05, 'epoch': 0.73}
{'loss': 0.3748, 'learning_rate': 1.687171341247104e-05, 'epoch': 0.73}
{'loss': 0.3652, 'learning_rate': 1.6853611215777006e-05, 'epoch': 0.73}
  | 4746/6500 [8:58:22<3:15:47,  6.70s/it]                                                        73%|  | 4746/6500 [8:58:22<3:15:47,  6.70s/it] 73%|  | 4747/6500 [8:58:28<3:14:37,  6.66s/it]                                                        73%|  | 4747/6500 [8:58:28<3:14:37,  6.66s/it] 73%|  | 4748/6500 [8:58:35<3:14:01,  6.64s/it]                                                        73%|  | 4748/6500 [8:58:35<3:14:01,  6.64s/it] 73%|  | 4749/6500 [8:58:41<3:13:22,  6.63s/it]                                                        73%|  | 4749/6500 [8:58:41<3:13:22,  6.63s/it] 73%|  | 4750/6500 [8:58:48<3:12:57,  6.62s/it]                                                        73%|  | 4750/6500 [8:58:48<3:12:57,  6.62s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8791275024414062, 'eval_runtime': 1.483, 'eval_samples_per_second': 8.092, 'eval_steps_per_second': 2.023, 'epoch': 0.73}
                                                        73%|  | 4750/6500 [8:58:50<3:12:57,  6.62s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-4750
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4750
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4750
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4750
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4750
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4750/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4750/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4750/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4750/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3686, 'learning_rate': 1.6835516766849822e-05, 'epoch': 0.73}
{'loss': 0.3696, 'learning_rate': 1.681743006991894e-05, 'epoch': 0.73}
{'loss': 0.3789, 'learning_rate': 1.679935112921202e-05, 'epoch': 0.73}
{'loss': 0.3861, 'learning_rate': 1.678127994895492e-05, 'epoch': 0.73}
{'loss': 0.3848, 'learning_rate': 1.6763216533371652e-05, 'epoch': 0.73}
 73%|  | 4751/6500 [8:58:57<3:28:52,  7.17s/it]                                                        73%|  | 4751/6500 [8:58:57<3:28:52,  7.17s/it] 73%|  | 4752/6500 [8:59:04<3:31:46,  7.27s/it]                                                        73%|  | 4752/6500 [8:59:04<3:31:46,  7.27s/it] 73%|  | 4753/6500 [8:59:11<3:25:40,  7.06s/it]                                                        73%|  | 4753/6500 [8:59:11<3:25:40,  7.06s/it] 73%|  | 4754/6500 [8:59:17<3:21:24,  6.92s/it]                                                        73%|  | 4754/6500 [8:59:17<3:21:24,  6.92s/it] 73%|  | 4755/6500 [8:59:24<3:18:29,  6.82s/it]                                                        73%|  | 4755/6500 [8:59:24<3:18:29,  6.82s/it] 73%|{'loss': 0.372, 'learning_rate': 1.6745160886684434e-05, 'epoch': 0.73}
{'loss': 0.3817, 'learning_rate': 1.6727113013113673e-05, 'epoch': 0.73}
{'loss': 0.3609, 'learning_rate': 1.6709072916877938e-05, 'epoch': 0.73}
{'loss': 0.3698, 'learning_rate': 1.6691040602194004e-05, 'epoch': 0.73}
{'loss': 0.4, 'learning_rate': 1.6673016073276798e-05, 'epoch': 0.73}
  | 4756/6500 [8:59:30<3:16:21,  6.76s/it]                                                        73%|  | 4756/6500 [8:59:30<3:16:21,  6.76s/it] 73%|  | 4757/6500 [8:59:37<3:14:41,  6.70s/it]                                                        73%|  | 4757/6500 [8:59:37<3:14:41,  6.70s/it] 73%|  | 4758/6500 [8:59:44<3:13:32,  6.67s/it]                                                        73%|  | 4758/6500 [8:59:44<3:13:32,  6.67s/it] 73%|  | 4759/6500 [8:59:50<3:12:52,  6.65s/it]                                                        73%|  | 4759/6500 [8:59:50<3:12:52,  6.65s/it] 73%|  | 4760/6500 [8:59:57<3:12:18,  6.63s/it]                                                        73%|  | 4760/6500 [8:59:57<3:12:18,  6.63s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.872576117515564, 'eval_runtime': 1.4804, 'eval_samples_per_second': 8.106, 'eval_steps_per_second': 2.026, 'epoch': 0.73}
                                                        73%|  | 4760/6500 [8:59:58<3:12:18,  6.63s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-4760
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4760I AM HERE AND I AM SAVING THE MODEL

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4760
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4760
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4760
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4760/pytorch_model.bin
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4760/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4760/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4760/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3804, 'learning_rate': 1.6654999334339455e-05, 'epoch': 0.73}
{'loss': 0.6517, 'learning_rate': 1.6636990389593298e-05, 'epoch': 0.73}
{'loss': 0.363, 'learning_rate': 1.661898924324777e-05, 'epoch': 0.73}
{'loss': 0.3846, 'learning_rate': 1.660099589951054e-05, 'epoch': 0.73}
{'loss': 0.3593, 'learning_rate': 1.6583010362587453e-05, 'epoch': 0.73}
 73%|  | 4761/6500 [9:00:05<3:28:05,  7.18s/it]                                                        73%|  | 4761/6500 [9:00:05<3:28:05,  7.18s/it] 73%|  | 4762/6500 [9:00:12<3:22:46,  7.00s/it]                                                        73%|  | 4762/6500 [9:00:12<3:22:46,  7.00s/it] 73%|  | 4763/6500 [9:00:18<3:19:06,  6.88s/it]                                                        73%|  | 4763/6500 [9:00:18<3:19:06,  6.88s/it] 73%|  | 4764/6500 [9:00:25<3:16:31,  6.79s/it]                                                        73%|  | 4764/6500 [9:00:25<3:16:31,  6.79s/it] 73%|  | 4765/6500 [9:00:32<3:14:48,  6.74s/it]                                                        73%|  | 4765/6500 [9:00:32<3:14:48,  6.74s/it] 73%|{'loss': 0.3615, 'learning_rate': 1.6565032636682515e-05, 'epoch': 0.73}
{'loss': 0.3699, 'learning_rate': 1.6547062725997915e-05, 'epoch': 0.73}
{'loss': 0.3772, 'learning_rate': 1.6529100634733996e-05, 'epoch': 0.73}
{'loss': 0.3838, 'learning_rate': 1.65111463670893e-05, 'epoch': 0.73}
{'loss': 0.3741, 'learning_rate': 1.6493199927260534e-05, 'epoch': 0.73}
  | 4766/6500 [9:00:38<3:13:28,  6.69s/it]                                                        73%|  | 4766/6500 [9:00:38<3:13:28,  6.69s/it] 73%|  | 4767/6500 [9:00:45<3:12:28,  6.66s/it]                                                        73%|  | 4767/6500 [9:00:45<3:12:28,  6.66s/it] 73%|  | 4768/6500 [9:00:52<3:16:53,  6.82s/it]                                                        73%|  | 4768/6500 [9:00:52<3:16:53,  6.82s/it] 73%|  | 4769/6500 [9:00:59<3:14:54,  6.76s/it]                                                        73%|  | 4769/6500 [9:00:59<3:14:54,  6.76s/it] 73%|  | 4770/6500 [9:01:05<3:13:42,  6.72s/it]                                                        73%|  | 4770/6500 [9:01:05<3:13:42,  6.72s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8787074685096741, 'eval_runtime': 1.5308, 'eval_samples_per_second': 7.839, 'eval_steps_per_second': 1.96, 'epoch': 0.73}
                                                        73%|  | 4770/6500 [9:01:07<3:13:42,  6.72s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-4770
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4770
/projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4770
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4770
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4770
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4770/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4770/pytorch_model.bin
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4770/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4770/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3682, 'learning_rate': 1.6475261319442553e-05, 'epoch': 0.73}
{'loss': 0.3735, 'learning_rate': 1.6457330547828402e-05, 'epoch': 0.73}
{'loss': 0.3539, 'learning_rate': 1.6439407616609316e-05, 'epoch': 0.73}
{'loss': 0.3991, 'learning_rate': 1.642149252997463e-05, 'epoch': 0.73}
{'loss': 0.3638, 'learning_rate': 1.64035852921119e-05, 'epoch': 0.73}
 73%|  | 4771/6500 [9:01:14<3:28:57,  7.25s/it]                                                        73%|  | 4771/6500 [9:01:14<3:28:57,  7.25s/it] 73%|  | 4772/6500 [9:01:20<3:23:09,  7.05s/it]                                                        73%|  | 4772/6500 [9:01:20<3:23:09,  7.05s/it] 73%|  | 4773/6500 [9:01:27<3:19:04,  6.92s/it]                                                        73%|  | 4773/6500 [9:01:27<3:19:04,  6.92s/it] 73%|  | 4774/6500 [9:01:33<3:16:16,  6.82s/it]                                                        73%|  | 4774/6500 [9:01:33<3:16:16,  6.82s/it] 73%|  | 4775/6500 [9:01:40<3:14:14,  6.76s/it]                                                        73%|  | 4775/6500 [9:01:40<3:14:14,  6.76s/it] 73%|{'loss': 0.5104, 'learning_rate': 1.6385685907206842e-05, 'epoch': 0.73}
{'loss': 0.5181, 'learning_rate': 1.6367794379443323e-05, 'epoch': 0.73}
{'loss': 0.3639, 'learning_rate': 1.6349910713003385e-05, 'epoch': 0.74}
{'loss': 0.3832, 'learning_rate': 1.6332034912067217e-05, 'epoch': 0.74}
{'loss': 0.3575, 'learning_rate': 1.6314166980813183e-05, 'epoch': 0.74}
  | 4776/6500 [9:01:47<3:12:43,  6.71s/it]                                                        73%|  | 4776/6500 [9:01:47<3:12:43,  6.71s/it] 73%|  | 4777/6500 [9:01:53<3:11:37,  6.67s/it]                                                        73%|  | 4777/6500 [9:01:53<3:11:37,  6.67s/it] 74%|  | 4778/6500 [9:02:00<3:10:49,  6.65s/it]                                                        74%|  | 4778/6500 [9:02:00<3:10:49,  6.65s/it] 74%|  | 4779/6500 [9:02:06<3:10:13,  6.63s/it]                                                        74%|  | 4779/6500 [9:02:06<3:10:13,  6.63s/it] 74%|  | 4780/6500 [9:02:13<3:09:52,  6.62s/it]                                                        74%|  | 4780/6500 [9:02:13<3:09:52,  6.62s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8780741691589355, 'eval_runtime': 1.7975, 'eval_samples_per_second': 6.676, 'eval_steps_per_second': 1.669, 'epoch': 0.74}
                                                        74%|  | 4780/6500 [9:02:15<3:09:52,  6.62s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-4780
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4780I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4780

I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4780
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4780
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4780/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4780/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4780/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4780/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3538, 'learning_rate': 1.6296306923417807e-05, 'epoch': 0.74}
{'loss': 0.3749, 'learning_rate': 1.6278454744055765e-05, 'epoch': 0.74}
{'loss': 0.3625, 'learning_rate': 1.6260610446899898e-05, 'epoch': 0.74}
{'loss': 0.3843, 'learning_rate': 1.62427740361212e-05, 'epoch': 0.74}
{'loss': 0.3834, 'learning_rate': 1.622494551588884e-05, 'epoch': 0.74}
 74%|  | 4781/6500 [9:02:22<3:28:12,  7.27s/it]                                                        74%|  | 4781/6500 [9:02:22<3:28:12,  7.27s/it] 74%|  | 4782/6500 [9:02:28<3:22:14,  7.06s/it]                                                        74%|  | 4782/6500 [9:02:28<3:22:14,  7.06s/it] 74%|  | 4783/6500 [9:02:35<3:18:03,  6.92s/it]                                                        74%|  | 4783/6500 [9:02:35<3:18:03,  6.92s/it] 74%|  | 4784/6500 [9:02:42<3:21:40,  7.05s/it]                                                        74%|  | 4784/6500 [9:02:42<3:21:40,  7.05s/it] 74%|  | 4785/6500 [9:02:49<3:17:24,  6.91s/it]                                                        74%|  | 4785/6500 [9:02:49<3:17:24,  6.91s/it] 74%|{'loss': 0.3701, 'learning_rate': 1.620712489037009e-05, 'epoch': 0.74}
{'loss': 0.3739, 'learning_rate': 1.6189312163730436e-05, 'epoch': 0.74}
{'loss': 0.3592, 'learning_rate': 1.617150734013349e-05, 'epoch': 0.74}
{'loss': 0.4034, 'learning_rate': 1.615371042374102e-05, 'epoch': 0.74}
{'loss': 0.3647, 'learning_rate': 1.6135921418712956e-05, 'epoch': 0.74}
  | 4786/6500 [9:02:55<3:14:17,  6.80s/it]                                                        74%|  | 4786/6500 [9:02:55<3:14:17,  6.80s/it] 74%|  | 4787/6500 [9:03:02<3:12:23,  6.74s/it]                                                        74%|  | 4787/6500 [9:03:02<3:12:23,  6.74s/it] 74%|  | 4788/6500 [9:03:09<3:10:50,  6.69s/it]                                                        74%|  | 4788/6500 [9:03:09<3:10:50,  6.69s/it] 74%|  | 4789/6500 [9:03:15<3:09:47,  6.66s/it]                                                        74%|  | 4789/6500 [9:03:15<3:09:47,  6.66s/it] 74%|  | 4790/6500 [9:03:22<3:09:01,  6.63s/it]                                                        74%|  | 4790/6500 [9:03:22<3:09:01,  6.63s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8734906315803528, 'eval_runtime': 1.4957, 'eval_samples_per_second': 8.023, 'eval_steps_per_second': 2.006, 'epoch': 0.74}
                                                        74%|  | 4790/6500 [9:03:23<3:09:01,  6.63s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-4790
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4790
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4790
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4790
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4790
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4790/pytorch_model.binthe pytorch model path is
 /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4790/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4790/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4790/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.6477, 'learning_rate': 1.611814032920736e-05, 'epoch': 0.74}
{'loss': 0.3845, 'learning_rate': 1.610036715938046e-05, 'epoch': 0.74}
{'loss': 0.3698, 'learning_rate': 1.608260191338662e-05, 'epoch': 0.74}
{'loss': 0.3817, 'learning_rate': 1.606484459537836e-05, 'epoch': 0.74}
{'loss': 0.3606, 'learning_rate': 1.6047095209506346e-05, 'epoch': 0.74}
 74%|  | 4791/6500 [9:03:30<3:24:58,  7.20s/it]                                                        74%|  | 4791/6500 [9:03:30<3:24:58,  7.20s/it] 74%|  | 4792/6500 [9:03:37<3:19:35,  7.01s/it]                                                        74%|  | 4792/6500 [9:03:37<3:19:35,  7.01s/it] 74%|  | 4793/6500 [9:03:43<3:15:42,  6.88s/it]                                                        74%|  | 4793/6500 [9:03:43<3:15:42,  6.88s/it] 74%|  | 4794/6500 [9:03:50<3:13:01,  6.79s/it]                                                        74%|  | 4794/6500 [9:03:50<3:13:01,  6.79s/it] 74%|  | 4795/6500 [9:03:57<3:11:02,  6.72s/it]                                                        74%|  | 4795/6500 [9:03:57<3:11:02,  6.72s/it] 74%|{'loss': 0.3723, 'learning_rate': 1.6029353759919408e-05, 'epoch': 0.74}
{'loss': 0.3746, 'learning_rate': 1.601162025076447e-05, 'epoch': 0.74}
{'loss': 0.3782, 'learning_rate': 1.5993894686186645e-05, 'epoch': 0.74}
{'loss': 0.3678, 'learning_rate': 1.5976177070329174e-05, 'epoch': 0.74}
{'loss': 0.373, 'learning_rate': 1.5958467407333428e-05, 'epoch': 0.74}
  | 4796/6500 [9:04:03<3:09:38,  6.68s/it]                                                        74%|  | 4796/6500 [9:04:03<3:09:38,  6.68s/it] 74%|  | 4797/6500 [9:04:10<3:08:31,  6.64s/it]                                                        74%|  | 4797/6500 [9:04:10<3:08:31,  6.64s/it] 74%|  | 4798/6500 [9:04:16<3:07:49,  6.62s/it]                                                        74%|  | 4798/6500 [9:04:16<3:07:49,  6.62s/it] 74%|  | 4799/6500 [9:04:23<3:07:18,  6.61s/it]                                                        74%|  | 4799/6500 [9:04:23<3:07:18,  6.61s/it] 74%|  | 4800/6500 [9:04:30<3:14:14,  6.86s/it]                                                        74%|  | 4800/6500 [9:04:30<3:14:14,  6.86s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8767449855804443, 'eval_runtime': 1.4848, 'eval_samples_per_second': 8.082, 'eval_steps_per_second': 2.02, 'epoch': 0.74}
                                                        74%|  | 4800/6500 [9:04:32<3:14:14,  6.86s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-4800
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4800
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4800
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4800
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4800
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4800/pytorch_model.bin
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4800/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4800/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4800/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3654, 'learning_rate': 1.5940765701338966e-05, 'epoch': 0.74}
{'loss': 0.3684, 'learning_rate': 1.5923071956483442e-05, 'epoch': 0.74}
{'loss': 0.3587, 'learning_rate': 1.5905386176902655e-05, 'epoch': 0.74}
{'loss': 0.407, 'learning_rate': 1.5887708366730554e-05, 'epoch': 0.74}
{'loss': 0.3716, 'learning_rate': 1.5870038530099225e-05, 'epoch': 0.74}
 74%|  | 4801/6500 [9:04:39<3:28:33,  7.37s/it]                                                        74%|  | 4801/6500 [9:04:39<3:28:33,  7.37s/it] 74%|  | 4802/6500 [9:04:45<3:21:46,  7.13s/it]                                                        74%|  | 4802/6500 [9:04:45<3:21:46,  7.13s/it] 74%|  | 4803/6500 [9:04:52<3:16:57,  6.96s/it]                                                        74%|  | 4803/6500 [9:04:52<3:16:57,  6.96s/it] 74%|  | 4804/6500 [9:04:59<3:13:35,  6.85s/it]                                                        74%|  | 4804/6500 [9:04:59<3:13:35,  6.85s/it] 74%|  | 4805/6500 [9:05:05<3:11:14,  6.77s/it]                                                        74%|  | 4805/6500 [9:05:05<3:11:14,  6.77s/it] 74%|{'loss': 0.6348, 'learning_rate': 1.5852376671138863e-05, 'epoch': 0.74}
{'loss': 0.389, 'learning_rate': 1.5834722793977835e-05, 'epoch': 0.74}
{'loss': 0.3662, 'learning_rate': 1.5817076902742622e-05, 'epoch': 0.74}
{'loss': 0.3708, 'learning_rate': 1.5799439001557846e-05, 'epoch': 0.74}
{'loss': 0.3723, 'learning_rate': 1.5781809094546268e-05, 'epoch': 0.74}
  | 4806/6500 [9:05:12<3:09:35,  6.72s/it]                                                        74%|  | 4806/6500 [9:05:12<3:09:35,  6.72s/it] 74%|  | 4807/6500 [9:05:18<3:08:14,  6.67s/it]                                                        74%|  | 4807/6500 [9:05:18<3:08:14,  6.67s/it] 74%|  | 4808/6500 [9:05:25<3:07:19,  6.64s/it]                                                        74%|  | 4808/6500 [9:05:25<3:07:19,  6.64s/it] 74%|  | 4809/6500 [9:05:32<3:06:39,  6.62s/it]                                                        74%|  | 4809/6500 [9:05:32<3:06:39,  6.62s/it] 74%|  | 4810/6500 [9:05:38<3:06:08,  6.61s/it]                                                        74%|  | 4810/6500 [9:05:38<3:06:08,  6.61s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8782071471214294, 'eval_runtime': 1.4834, 'eval_samples_per_second': 8.089, 'eval_steps_per_second': 2.022, 'epoch': 0.74}
                                                        74%|  | 4810/6500 [9:05:40<3:06:08,  6.61s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-4810
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4810I AM HERE AND I AM SAVING THE MODEL

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4810
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4810
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4810
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4810/pytorch_model.bin
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4810/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4810/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4810/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3701, 'learning_rate': 1.576418718582876e-05, 'epoch': 0.74}
{'loss': 0.3744, 'learning_rate': 1.574657327952434e-05, 'epoch': 0.74}
{'loss': 0.3815, 'learning_rate': 1.5728967379750147e-05, 'epoch': 0.74}
{'loss': 0.3723, 'learning_rate': 1.5711369490621465e-05, 'epoch': 0.74}
{'loss': 0.374, 'learning_rate': 1.5693779616251687e-05, 'epoch': 0.74}
 74%|  | 4811/6500 [9:05:46<3:21:15,  7.15s/it]                                                        74%|  | 4811/6500 [9:05:46<3:21:15,  7.15s/it] 74%|  | 4812/6500 [9:05:53<3:16:08,  6.97s/it]                                                        74%|  | 4812/6500 [9:05:53<3:16:08,  6.97s/it] 74%|  | 4813/6500 [9:06:00<3:12:36,  6.85s/it]                                                        74%|  | 4813/6500 [9:06:00<3:12:36,  6.85s/it] 74%|  | 4814/6500 [9:06:06<3:10:06,  6.77s/it]                                                        74%|  | 4814/6500 [9:06:06<3:10:06,  6.77s/it] 74%|  | 4815/6500 [9:06:13<3:08:19,  6.71s/it]                                                        74%|  | 4815/6500 [9:06:13<3:08:19,  6.71s/it] 74%|{'loss': 0.377, 'learning_rate': 1.5676197760752348e-05, 'epoch': 0.74}
{'loss': 0.3611, 'learning_rate': 1.565862392823308e-05, 'epoch': 0.74}
{'loss': 0.3553, 'learning_rate': 1.5641058122801665e-05, 'epoch': 0.74}
{'loss': 0.4039, 'learning_rate': 1.5623500348564013e-05, 'epoch': 0.74}
{'loss': 0.3745, 'learning_rate': 1.5605950609624136e-05, 'epoch': 0.74}
  | 4816/6500 [9:06:20<3:14:23,  6.93s/it]                                                        74%|  | 4816/6500 [9:06:20<3:14:23,  6.93s/it] 74%|  | 4817/6500 [9:06:27<3:11:16,  6.82s/it]                                                        74%|  | 4817/6500 [9:06:27<3:11:16,  6.82s/it] 74%|  | 4818/6500 [9:06:33<3:08:55,  6.74s/it]                                                        74%|  | 4818/6500 [9:06:33<3:08:55,  6.74s/it] 74%|  | 4819/6500 [9:06:40<3:07:19,  6.69s/it]                                                        74%|  | 4819/6500 [9:06:40<3:07:19,  6.69s/it] 74%|  | 4820/6500 [9:06:46<3:06:09,  6.65s/it]                                                        74%|  | 4820/6500 [9:06:46<3:06:09,  6.65s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8748061060905457, 'eval_runtime': 1.4841, 'eval_samples_per_second': 8.086, 'eval_steps_per_second': 2.021, 'epoch': 0.74}
                                                        74%|  | 4820/6500 [9:06:48<3:06:09,  6.65s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-4820
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4820
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4820
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4820
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4820
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4820/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4820/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4820/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4820/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.648, 'learning_rate': 1.558840891008419e-05, 'epoch': 0.74}
{'loss': 0.3804, 'learning_rate': 1.557087525404443e-05, 'epoch': 0.74}
{'loss': 0.3728, 'learning_rate': 1.5553349645603254e-05, 'epoch': 0.74}
{'loss': 0.3623, 'learning_rate': 1.5535832088857157e-05, 'epoch': 0.74}
{'loss': 0.3583, 'learning_rate': 1.5518322587900758e-05, 'epoch': 0.74}
 74%|  | 4821/6500 [9:06:55<3:20:37,  7.17s/it]                                                        74%|  | 4821/6500 [9:06:55<3:20:37,  7.17s/it] 74%|  | 4822/6500 [9:07:01<3:15:22,  6.99s/it]                                                        74%|  | 4822/6500 [9:07:01<3:15:22,  6.99s/it] 74%|  | 4823/6500 [9:07:08<3:11:39,  6.86s/it]                                                        74%|  | 4823/6500 [9:07:08<3:11:39,  6.86s/it] 74%|  | 4824/6500 [9:07:14<3:08:59,  6.77s/it]                                                        74%|  | 4824/6500 [9:07:14<3:08:59,  6.77s/it] 74%|  | 4825/6500 [9:07:21<3:07:13,  6.71s/it]                                                        74%|  | 4825/6500 [9:07:21<3:07:13,  6.71s/it] 74%|{'loss': 0.3679, 'learning_rate': 1.5500821146826805e-05, 'epoch': 0.74}
{'loss': 0.3643, 'learning_rate': 1.548332776972617e-05, 'epoch': 0.74}
{'loss': 0.3755, 'learning_rate': 1.5465842460687784e-05, 'epoch': 0.74}
{'loss': 0.3769, 'learning_rate': 1.5448365223798756e-05, 'epoch': 0.74}
{'loss': 0.3662, 'learning_rate': 1.5430896063144274e-05, 'epoch': 0.74}
  | 4826/6500 [9:07:28<3:05:56,  6.66s/it]                                                        74%|  | 4826/6500 [9:07:28<3:05:56,  6.66s/it] 74%|  | 4827/6500 [9:07:34<3:04:55,  6.63s/it]                                                        74%|  | 4827/6500 [9:07:34<3:04:55,  6.63s/it] 74%|  | 4828/6500 [9:07:41<3:04:12,  6.61s/it]                                                        74%|  | 4828/6500 [9:07:41<3:04:12,  6.61s/it] 74%|  | 4829/6500 [9:07:47<3:03:39,  6.59s/it]                                                        74%|  | 4829/6500 [9:07:47<3:03:39,  6.59s/it] 74%|  | 4830/6500 [9:07:54<3:03:20,  6.59s/it]                                                        74%|  | 4830/6500 [9:07:54<3:03:20,  6.59s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.876616358757019, 'eval_runtime': 1.4819, 'eval_samples_per_second': 8.098, 'eval_steps_per_second': 2.024, 'epoch': 0.74}
                                                        74%|  | 4830/6500 [9:07:55<3:03:20,  6.59s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-4830
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4830I AM HERE AND I AM SAVING THE MODEL

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4830
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4830
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4830
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4830/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4830/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4830/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4830/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3776, 'learning_rate': 1.5413434982807657e-05, 'epoch': 0.74}
{'loss': 0.3726, 'learning_rate': 1.5395981986870322e-05, 'epoch': 0.74}
{'loss': 0.3709, 'learning_rate': 1.5378537079411802e-05, 'epoch': 0.74}
{'loss': 0.3956, 'learning_rate': 1.5361100264509735e-05, 'epoch': 0.74}
{'loss': 0.374, 'learning_rate': 1.534367154623988e-05, 'epoch': 0.74}
 74%|  | 4831/6500 [9:08:02<3:18:44,  7.14s/it]                                                        74%|  | 4831/6500 [9:08:02<3:18:44,  7.14s/it] 74%|  | 4832/6500 [9:08:10<3:21:23,  7.24s/it]                                                        74%|  | 4832/6500 [9:08:10<3:21:23,  7.24s/it] 74%|  | 4833/6500 [9:08:16<3:15:45,  7.05s/it]                                                        74%|  | 4833/6500 [9:08:16<3:15:45,  7.05s/it] 74%|  | 4834/6500 [9:08:23<3:11:31,  6.90s/it]                                                        74%|  | 4834/6500 [9:08:23<3:11:31,  6.90s/it] 74%|  | 4835/6500 [9:08:29<3:08:32,  6.79s/it]                                                        74%|  | 4835/6500 [9:08:29<3:08:32,  6.79s/it] 74%|{'loss': 0.6494, 'learning_rate': 1.5326250928676084e-05, 'epoch': 0.74}
{'loss': 0.3692, 'learning_rate': 1.5308838415890313e-05, 'epoch': 0.74}
{'loss': 0.3768, 'learning_rate': 1.5291434011952655e-05, 'epoch': 0.74}
{'loss': 0.3684, 'learning_rate': 1.5274037720931244e-05, 'epoch': 0.74}
{'loss': 0.3637, 'learning_rate': 1.5256649546892382e-05, 'epoch': 0.74}
  | 4836/6500 [9:08:36<3:06:24,  6.72s/it]                                                        74%|  | 4836/6500 [9:08:36<3:06:24,  6.72s/it] 74%|  | 4837/6500 [9:08:43<3:04:57,  6.67s/it]                                                        74%|  | 4837/6500 [9:08:43<3:04:57,  6.67s/it] 74%|  | 4838/6500 [9:08:49<3:03:55,  6.64s/it]                                                        74%|  | 4838/6500 [9:08:49<3:03:55,  6.64s/it] 74%|  | 4839/6500 [9:08:56<3:03:11,  6.62s/it]                                                        74%|  | 4839/6500 [9:08:56<3:03:11,  6.62s/it] 74%|  | 4840/6500 [9:09:02<3:02:37,  6.60s/it]                                                        74%|  | 4840/6500 [9:09:02<3:02:37,  6.60s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8793131113052368, 'eval_runtime': 1.4808, 'eval_samples_per_second': 8.104, 'eval_steps_per_second': 2.026, 'epoch': 0.74}
                                                        74%|  | 4840/6500 [9:09:04<3:02:37,  6.60s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-4840
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4840
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4840
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4840
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4840
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4840/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4840/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4840/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4840/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3661, 'learning_rate': 1.5239269493900443e-05, 'epoch': 0.74}
{'loss': 0.3826, 'learning_rate': 1.5221897566017911e-05, 'epoch': 0.74}
{'loss': 0.3851, 'learning_rate': 1.5204533767305373e-05, 'epoch': 0.75}
{'loss': 0.3748, 'learning_rate': 1.5187178101821503e-05, 'epoch': 0.75}
{'loss': 0.3721, 'learning_rate': 1.5169830573623089e-05, 'epoch': 0.75}
 74%|  | 4841/6500 [9:09:11<3:17:18,  7.14s/it]                                                        74%|  | 4841/6500 [9:09:11<3:17:18,  7.14s/it] 74%|  | 4842/6500 [9:09:17<3:12:25,  6.96s/it]                                                        74%|  | 4842/6500 [9:09:17<3:12:25,  6.96s/it] 75%|  | 4843/6500 [9:09:24<3:09:00,  6.84s/it]                                                        75%|  | 4843/6500 [9:09:24<3:09:00,  6.84s/it] 75%|  | 4844/6500 [9:09:30<3:06:38,  6.76s/it]                                                        75%|  | 4844/6500 [9:09:30<3:06:38,  6.76s/it] 75%|  | 4845/6500 [9:09:37<3:04:49,  6.70s/it]                                                        75%|  | 4845/6500 [9:09:37<3:04:49,  6.70s/it] 75%|{'loss': 0.3817, 'learning_rate': 1.5152491186765005e-05, 'epoch': 0.75}
{'loss': 0.3564, 'learning_rate': 1.5135159945300231e-05, 'epoch': 0.75}
{'loss': 0.3749, 'learning_rate': 1.5117836853279837e-05, 'epoch': 0.75}
{'loss': 0.3855, 'learning_rate': 1.5100521914753007e-05, 'epoch': 0.75}
{'loss': 0.3815, 'learning_rate': 1.5083215133766971e-05, 'epoch': 0.75}
  | 4846/6500 [9:09:43<3:03:35,  6.66s/it]                                                        75%|  | 4846/6500 [9:09:43<3:03:35,  6.66s/it] 75%|  | 4847/6500 [9:09:50<3:02:34,  6.63s/it]                                                        75%|  | 4847/6500 [9:09:50<3:02:34,  6.63s/it] 75%|  | 4848/6500 [9:09:57<3:01:59,  6.61s/it]                                                        75%|  | 4848/6500 [9:09:57<3:01:59,  6.61s/it] 75%|  | 4849/6500 [9:10:04<3:06:38,  6.78s/it]                                                        75%|  | 4849/6500 [9:10:04<3:06:38,  6.78s/it] 75%|  | 4850/6500 [9:10:10<3:04:39,  6.71s/it]                                                        75%|  | 4850/6500 [9:10:10<3:04:39,  6.71s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8759210109710693, 'eval_runtime': 1.7318, 'eval_samples_per_second': 6.929, 'eval_steps_per_second': 1.732, 'epoch': 0.75}
                                                        75%|  | 4850/6500 [9:10:12<3:04:39,  6.71s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-4850
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4850
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4850
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4850
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4850
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4850/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4850/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4850/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4850/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.6428, 'learning_rate': 1.5065916514367101e-05, 'epoch': 0.75}
{'loss': 0.3714, 'learning_rate': 1.5048626060596843e-05, 'epoch': 0.75}
{'loss': 0.3805, 'learning_rate': 1.5031343776497736e-05, 'epoch': 0.75}
{'loss': 0.3528, 'learning_rate': 1.501406966610941e-05, 'epoch': 0.75}
{'loss': 0.3634, 'learning_rate': 1.4996803733469578e-05, 'epoch': 0.75}
 75%|  | 4851/6500 [9:10:19<3:21:15,  7.32s/it]                                                        75%|  | 4851/6500 [9:10:19<3:21:15,  7.32s/it] 75%|  | 4852/6500 [9:10:26<3:14:48,  7.09s/it]                                                        75%|  | 4852/6500 [9:10:26<3:14:48,  7.09s/it] 75%|  | 4853/6500 [9:10:32<3:10:22,  6.94s/it]                                                        75%|  | 4853/6500 [9:10:32<3:10:22,  6.94s/it] 75%|  | 4854/6500 [9:10:39<3:07:11,  6.82s/it]                                                        75%|  | 4854/6500 [9:10:39<3:07:11,  6.82s/it] 75%|  | 4855/6500 [9:10:45<3:04:56,  6.75s/it]                                                        75%|  | 4855/6500 [9:10:45<3:04:56,  6.75s/it] 75%|{'loss': 0.3719, 'learning_rate': 1.4979545982614051e-05, 'epoch': 0.75}
{'loss': 0.3583, 'learning_rate': 1.4962296417576726e-05, 'epoch': 0.75}
{'loss': 0.3823, 'learning_rate': 1.4945055042389578e-05, 'epoch': 0.75}
{'loss': 0.3839, 'learning_rate': 1.492782186108268e-05, 'epoch': 0.75}
{'loss': 0.3556, 'learning_rate': 1.4910596877684191e-05, 'epoch': 0.75}
  | 4856/6500 [9:10:52<3:03:18,  6.69s/it]                                                        75%|  | 4856/6500 [9:10:52<3:03:18,  6.69s/it] 75%|  | 4857/6500 [9:10:58<3:02:08,  6.65s/it]                                                        75%|  | 4857/6500 [9:10:58<3:02:08,  6.65s/it] 75%|  | 4858/6500 [9:11:05<3:01:21,  6.63s/it]                                                        75%|  | 4858/6500 [9:11:05<3:01:21,  6.63s/it] 75%|  | 4859/6500 [9:11:12<3:00:41,  6.61s/it]                                                        75%|  | 4859/6500 [9:11:12<3:00:41,  6.61s/it] 75%|  | 4860/6500 [9:11:18<3:00:10,  6.59s/it]                                                        75%|  | 4860/6500 [9:11:18<3:00:10,  6.59s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8789358735084534, 'eval_runtime': 1.4805, 'eval_samples_per_second': 8.105, 'eval_steps_per_second': 2.026, 'epoch': 0.75}
                                                        75%|  | 4860/6500 [9:11:20<3:00:10,  6.59s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-4860
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4860
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4860
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4860
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4860
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4860/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4860/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4860/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4860/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3747, 'learning_rate': 1.4893380096220321e-05, 'epoch': 0.75}
{'loss': 0.354, 'learning_rate': 1.4876171520715399e-05, 'epoch': 0.75}
{'loss': 0.4046, 'learning_rate': 1.485897115519183e-05, 'epoch': 0.75}
{'loss': 0.3593, 'learning_rate': 1.4841779003670092e-05, 'epoch': 0.75}
{'loss': 0.6462, 'learning_rate': 1.4824595070168745e-05, 'epoch': 0.75}
 75%|  | 4861/6500 [9:11:27<3:14:46,  7.13s/it]                                                        75%|  | 4861/6500 [9:11:27<3:14:46,  7.13s/it] 75%|  | 4862/6500 [9:11:33<3:09:57,  6.96s/it]                                                        75%|  | 4862/6500 [9:11:33<3:09:57,  6.96s/it] 75%|  | 4863/6500 [9:11:40<3:06:30,  6.84s/it]                                                        75%|  | 4863/6500 [9:11:40<3:06:30,  6.84s/it] 75%|  | 4864/6500 [9:11:46<3:04:06,  6.75s/it]                                                        75%|  | 4864/6500 [9:11:46<3:04:06,  6.75s/it] 75%|  | 4865/6500 [9:11:53<3:07:26,  6.88s/it]                                                        75%|  | 4865/6500 [9:11:53<3:07:26,  6.88s/it] 75%|{'loss': 0.3842, 'learning_rate': 1.4807419358704433e-05, 'epoch': 0.75}
{'loss': 0.3669, 'learning_rate': 1.4790251873291872e-05, 'epoch': 0.75}
{'loss': 0.3729, 'learning_rate': 1.4773092617943851e-05, 'epoch': 0.75}
{'loss': 0.3559, 'learning_rate': 1.4755941596671253e-05, 'epoch': 0.75}
{'loss': 0.3595, 'learning_rate': 1.4738798813483017e-05, 'epoch': 0.75}
  | 4866/6500 [9:12:00<3:04:41,  6.78s/it]                                                        75%|  | 4866/6500 [9:12:00<3:04:41,  6.78s/it] 75%|  | 4867/6500 [9:12:06<3:02:43,  6.71s/it]                                                        75%|  | 4867/6500 [9:12:06<3:02:43,  6.71s/it] 75%|  | 4868/6500 [9:12:13<3:01:24,  6.67s/it]                                                        75%|  | 4868/6500 [9:12:13<3:01:24,  6.67s/it] 75%|  | 4869/6500 [9:12:20<3:00:32,  6.64s/it]                                                        75%|  | 4869/6500 [9:12:20<3:00:32,  6.64s/it] 75%|  | 4870/6500 [9:12:26<2:59:47,  6.62s/it]                                                        75%|  | 4870/6500 [9:12:26<2:59:47,  6.62s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8787919878959656, 'eval_runtime': 1.4998, 'eval_samples_per_second': 8.001, 'eval_steps_per_second': 2.0, 'epoch': 0.75}
                                                        75%|  | 4870/6500 [9:12:28<2:59:47,  6.62s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-4870
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4870
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4870
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4870
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4870
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4870/pytorch_model.bin 
/projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4870/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4870/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4870/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3694, 'learning_rate': 1.4721664272386172e-05, 'epoch': 0.75}
{'loss': 0.3672, 'learning_rate': 1.4704537977385818e-05, 'epoch': 0.75}
{'loss': 0.3761, 'learning_rate': 1.4687419932485097e-05, 'epoch': 0.75}
{'loss': 0.3724, 'learning_rate': 1.4670310141685263e-05, 'epoch': 0.75}
{'loss': 0.3649, 'learning_rate': 1.4653208608985624e-05, 'epoch': 0.75}
 75%|  | 4871/6500 [9:12:35<3:14:53,  7.18s/it]                                                        75%|  | 4871/6500 [9:12:35<3:14:53,  7.18s/it] 75%|  | 4872/6500 [9:12:41<3:09:43,  6.99s/it]                                                        75%|  | 4872/6500 [9:12:41<3:09:43,  6.99s/it] 75%|  | 4873/6500 [9:12:48<3:06:06,  6.86s/it]                                                        75%|  | 4873/6500 [9:12:48<3:06:06,  6.86s/it] 75%|  | 4874/6500 [9:12:54<3:03:30,  6.77s/it]                                                        75%|  | 4874/6500 [9:12:54<3:03:30,  6.77s/it] 75%|  | 4875/6500 [9:13:01<3:01:36,  6.71s/it]                                                        75%|  | 4875/6500 [9:13:01<3:01:36,  6.71s/it] 75%|{'loss': 0.3677, 'learning_rate': 1.463611533838355e-05, 'epoch': 0.75}
{'loss': 0.3551, 'learning_rate': 1.4619030333874506e-05, 'epoch': 0.75}
{'loss': 0.4031, 'learning_rate': 1.4601953599452012e-05, 'epoch': 0.75}
{'loss': 0.3619, 'learning_rate': 1.4584885139107635e-05, 'epoch': 0.75}
{'loss': 0.6472, 'learning_rate': 1.4567824956831043e-05, 'epoch': 0.75}
  | 4876/6500 [9:13:07<3:00:19,  6.66s/it]                                                        75%|  | 4876/6500 [9:13:07<3:00:19,  6.66s/it] 75%|  | 4877/6500 [9:13:14<2:59:17,  6.63s/it]                                                        75%|  | 4877/6500 [9:13:14<2:59:17,  6.63s/it] 75%|  | 4878/6500 [9:13:21<2:58:40,  6.61s/it]                                                        75%|  | 4878/6500 [9:13:21<2:58:40,  6.61s/it] 75%|  | 4879/6500 [9:13:27<2:58:05,  6.59s/it]                                                        75%|  | 4879/6500 [9:13:27<2:58:05,  6.59s/it] 75%|  | 4880/6500 [9:13:34<2:57:39,  6.58s/it]                                                        75%|  | 4880/6500 [9:13:34<2:57:39,  6.58s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8767402768135071, 'eval_runtime': 1.7279, 'eval_samples_per_second': 6.945, 'eval_steps_per_second': 1.736, 'epoch': 0.75}
                                                        75%|  | 4880/6500 [9:13:35<2:57:39,  6.58s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-4880
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4880
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4880
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4880
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4880
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4880/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4880/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4880/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4880/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.383, 'learning_rate': 1.4550773056609923e-05, 'epoch': 0.75}
{'loss': 0.3581, 'learning_rate': 1.4533729442430066e-05, 'epoch': 0.75}
{'loss': 0.3822, 'learning_rate': 1.4516694118275315e-05, 'epoch': 0.75}
{'loss': 0.363, 'learning_rate': 1.4499667088127572e-05, 'epoch': 0.75}
{'loss': 0.3735, 'learning_rate': 1.44826483559668e-05, 'epoch': 0.75}
 75%|  | 4881/6500 [9:13:43<3:19:44,  7.40s/it]                                                        75%|  | 4881/6500 [9:13:43<3:19:44,  7.40s/it] 75%|  | 4882/6500 [9:13:50<3:12:47,  7.15s/it]                                                        75%|  | 4882/6500 [9:13:50<3:12:47,  7.15s/it] 75%|  | 4883/6500 [9:13:56<3:07:54,  6.97s/it]                                                        75%|  | 4883/6500 [9:13:56<3:07:54,  6.97s/it] 75%|  | 4884/6500 [9:14:03<3:04:30,  6.85s/it]                                                        75%|  | 4884/6500 [9:14:03<3:04:30,  6.85s/it] 75%|  | 4885/6500 [9:14:09<3:02:04,  6.76s/it]                                                        75%|  | 4885/6500 [9:14:09<3:02:04,  6.76s/it] 75%|{'loss': 0.374, 'learning_rate': 1.4465637925771025e-05, 'epoch': 0.75}
{'loss': 0.3773, 'learning_rate': 1.444863580151633e-05, 'epoch': 0.75}
{'loss': 0.3712, 'learning_rate': 1.4431641987176869e-05, 'epoch': 0.75}
{'loss': 0.3743, 'learning_rate': 1.4414656486724826e-05, 'epoch': 0.75}
{'loss': 0.3679, 'learning_rate': 1.4397679304130468e-05, 'epoch': 0.75}
  | 4886/6500 [9:14:16<3:00:13,  6.70s/it]                                                        75%|  | 4886/6500 [9:14:16<3:00:13,  6.70s/it] 75%|  | 4887/6500 [9:14:22<2:59:04,  6.66s/it]                                                        75%|  | 4887/6500 [9:14:22<2:59:04,  6.66s/it] 75%|  | 4888/6500 [9:14:29<2:58:08,  6.63s/it]                                                        75%|  | 4888/6500 [9:14:29<2:58:08,  6.63s/it] 75%|  | 4889/6500 [9:14:35<2:57:28,  6.61s/it]                                                        75%|  | 4889/6500 [9:14:35<2:57:28,  6.61s/it] 75%|  | 4890/6500 [9:14:42<2:56:59,  6.60s/it]                                                        75%|  | 4890/6500 [9:14:42<2:56:59,  6.60s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8763361573219299, 'eval_runtime': 1.4803, 'eval_samples_per_second': 8.107, 'eval_steps_per_second': 2.027, 'epoch': 0.75}
                                                        75%|  | 4890/6500 [9:14:44<2:56:59,  6.60s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-4890
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4890
/projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4890
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4890
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4890
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4890/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4890/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4890/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4890/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3639, 'learning_rate': 1.4380710443362112e-05, 'epoch': 0.75}
{'loss': 0.354, 'learning_rate': 1.4363749908386132e-05, 'epoch': 0.75}
{'loss': 0.4042, 'learning_rate': 1.4346797703166925e-05, 'epoch': 0.75}
{'loss': 0.3706, 'learning_rate': 1.4329853831666978e-05, 'epoch': 0.75}
{'loss': 0.6409, 'learning_rate': 1.4312918297846822e-05, 'epoch': 0.75}
 75%|  | 4891/6500 [9:14:50<3:11:42,  7.15s/it]                                                        75%|  | 4891/6500 [9:14:50<3:11:42,  7.15s/it] 75%|  | 4892/6500 [9:14:57<3:06:43,  6.97s/it]                                                        75%|  | 4892/6500 [9:14:57<3:06:43,  6.97s/it] 75%|  | 4893/6500 [9:15:04<3:03:20,  6.85s/it]                                                        75%|  | 4893/6500 [9:15:04<3:03:20,  6.85s/it] 75%|  | 4894/6500 [9:15:10<3:00:51,  6.76s/it]                                                        75%|  | 4894/6500 [9:15:10<3:00:51,  6.76s/it] 75%|  | 4895/6500 [9:15:17<2:59:14,  6.70s/it]                                                        75%|  | 4895/6500 [9:15:17<2:59:14,  6.70s/it] 75%|{'loss': 0.3781, 'learning_rate': 1.4295991105665035e-05, 'epoch': 0.75}
{'loss': 0.3656, 'learning_rate': 1.4279072259078241e-05, 'epoch': 0.75}
{'loss': 0.3586, 'learning_rate': 1.4262161762041121e-05, 'epoch': 0.75}
{'loss': 0.3579, 'learning_rate': 1.4245259618506396e-05, 'epoch': 0.75}
{'loss': 0.3669, 'learning_rate': 1.4228365832424844e-05, 'epoch': 0.75}
  | 4896/6500 [9:15:23<2:57:59,  6.66s/it]                                                        75%|  | 4896/6500 [9:15:23<2:57:59,  6.66s/it] 75%|  | 4897/6500 [9:15:31<3:04:00,  6.89s/it]                                                        75%|  | 4897/6500 [9:15:31<3:04:00,  6.89s/it] 75%|  | 4898/6500 [9:15:37<3:01:15,  6.79s/it]                                                        75%|  | 4898/6500 [9:15:37<3:01:15,  6.79s/it] 75%|  | 4899/6500 [9:15:44<2:59:14,  6.72s/it]                                                        75%|  | 4899/6500 [9:15:44<2:59:14,  6.72s/it] 75%|  | 4900/6500 [9:15:50<2:57:49,  6.67s/it]                                                        75%|  | 4900/6500 [9:15:50<2:57:49,  6.67s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8793348073959351, 'eval_runtime': 1.4806, 'eval_samples_per_second': 8.105, 'eval_steps_per_second': 2.026, 'epoch': 0.75}
                                                        75%|  | 4900/6500 [9:15:52<2:57:49,  6.67s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-4900
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4900
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4900
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4900
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4900
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4900/pytorch_model.bin
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4900/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4900/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4900/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3646, 'learning_rate': 1.421148040774528e-05, 'epoch': 0.75}
{'loss': 0.375, 'learning_rate': 1.4194603348414581e-05, 'epoch': 0.75}
{'loss': 0.3767, 'learning_rate': 1.4177734658377662e-05, 'epoch': 0.75}
{'loss': 0.3705, 'learning_rate': 1.4160874341577446e-05, 'epoch': 0.75}
{'loss': 0.3753, 'learning_rate': 1.4144022401954949e-05, 'epoch': 0.75}
 75%|  | 4901/6500 [9:15:59<3:11:38,  7.19s/it]                                                        75%|  | 4901/6500 [9:15:59<3:11:38,  7.19s/it] 75%|  | 4902/6500 [9:16:05<3:06:29,  7.00s/it]                                                        75%|  | 4902/6500 [9:16:05<3:06:29,  7.00s/it] 75%|  | 4903/6500 [9:16:12<3:03:15,  6.89s/it]                                                        75%|  | 4903/6500 [9:16:12<3:03:15,  6.89s/it] 75%|  | 4904/6500 [9:16:19<3:00:36,  6.79s/it]                                                        75%|  | 4904/6500 [9:16:19<3:00:36,  6.79s/it] 75%|  | 4905/6500 [9:16:25<2:58:38,  6.72s/it]                                                        75%|  | 4905/6500 [9:16:25<2:58:38,  6.72s/it] 75%|{'loss': 0.3657, 'learning_rate': 1.41271788434492e-05, 'epoch': 0.75}
{'loss': 0.3624, 'learning_rate': 1.4110343669997295e-05, 'epoch': 0.75}
{'loss': 0.3973, 'learning_rate': 1.409351688553434e-05, 'epoch': 0.76}
{'loss': 0.3725, 'learning_rate': 1.40766984939935e-05, 'epoch': 0.76}
{'loss': 0.6469, 'learning_rate': 1.4059888499305973e-05, 'epoch': 0.76}
  | 4906/6500 [9:16:32<2:57:13,  6.67s/it]                                                        75%|  | 4906/6500 [9:16:32<2:57:13,  6.67s/it] 75%|  | 4907/6500 [9:16:38<2:56:11,  6.64s/it]                                                        75%|  | 4907/6500 [9:16:38<2:56:11,  6.64s/it] 76%|  | 4908/6500 [9:16:45<2:55:24,  6.61s/it]                                                        76%|  | 4908/6500 [9:16:45<2:55:24,  6.61s/it] 76%|  | 4909/6500 [9:16:51<2:54:53,  6.60s/it]                                                        76%|  | 4909/6500 [9:16:51<2:54:53,  6.60s/it] 76%|  | 4910/6500 [9:16:58<2:54:27,  6.58s/it]                                                        76%|  | 4910/6500 [9:16:58<2:54:27,  6.58s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8753919005393982, 'eval_runtime': 1.4774, 'eval_samples_per_second': 8.122, 'eval_steps_per_second': 2.031, 'epoch': 0.76}
                                                        76%|  | 4910/6500 [9:16:59<2:54:27,  6.58s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-4910
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4910
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4910
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4910
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4910
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4910/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4910/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4910/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4910/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3762, 'learning_rate': 1.4043086905400988e-05, 'epoch': 0.76}
{'loss': 0.3708, 'learning_rate': 1.4026293716205812e-05, 'epoch': 0.76}
{'loss': 0.3518, 'learning_rate': 1.400950893564576e-05, 'epoch': 0.76}
{'loss': 0.3659, 'learning_rate': 1.3992732567644185e-05, 'epoch': 0.76}
{'loss': 0.3668, 'learning_rate': 1.3975964616122428e-05, 'epoch': 0.76}
 76%|  | 4911/6500 [9:17:06<3:08:46,  7.13s/it]                                                        76%|  | 4911/6500 [9:17:06<3:08:46,  7.13s/it] 76%|  | 4912/6500 [9:17:13<3:04:07,  6.96s/it]                                                        76%|  | 4912/6500 [9:17:13<3:04:07,  6.96s/it] 76%|  | 4913/6500 [9:17:20<3:07:49,  7.10s/it]                                                        76%|  | 4913/6500 [9:17:20<3:07:49,  7.10s/it] 76%|  | 4914/6500 [9:17:27<3:03:19,  6.94s/it]                                                        76%|  | 4914/6500 [9:17:27<3:03:19,  6.94s/it] 76%|  | 4915/6500 [9:17:33<3:00:14,  6.82s/it]                                                        76%|  | 4915/6500 [9:17:33<3:00:14,  6.82s/it] 76%|{'loss': 0.3731, 'learning_rate': 1.3959205084999911e-05, 'epoch': 0.76}
{'loss': 0.3855, 'learning_rate': 1.3942453978194075e-05, 'epoch': 0.76}
{'loss': 0.3724, 'learning_rate': 1.3925711299620386e-05, 'epoch': 0.76}
{'loss': 0.3708, 'learning_rate': 1.3908977053192352e-05, 'epoch': 0.76}
{'loss': 0.3871, 'learning_rate': 1.3892251242821491e-05, 'epoch': 0.76}
  | 4916/6500 [9:17:40<2:57:59,  6.74s/it]                                                        76%|  | 4916/6500 [9:17:40<2:57:59,  6.74s/it] 76%|  | 4917/6500 [9:17:46<2:56:25,  6.69s/it]                                                        76%|  | 4917/6500 [9:17:46<2:56:25,  6.69s/it] 76%|  | 4918/6500 [9:17:53<2:55:17,  6.65s/it]                                                        76%|  | 4918/6500 [9:17:53<2:55:17,  6.65s/it] 76%|  | 4919/6500 [9:18:00<2:54:25,  6.62s/it]                                                        76%|  | 4919/6500 [9:18:00<2:54:25,  6.62s/it] 76%|  | 4920/6500 [9:18:06<2:53:44,  6.60s/it]                                                        76%|  | 4920/6500 [9:18:06<2:53:44,  6.60s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8764024972915649, 'eval_runtime': 1.4848, 'eval_samples_per_second': 8.082, 'eval_steps_per_second': 2.021, 'epoch': 0.76}
                                                        76%|  | 4920/6500 [9:18:08<2:53:44,  6.60s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-4920
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in I AM HERE AND I AM SAVING THE MODEL
 the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4920
/projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4920
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4920
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4920
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4920/pytorch_model.bin
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4920/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4920/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4920/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3591, 'learning_rate': 1.3875533872417362e-05, 'epoch': 0.76}
{'loss': 0.3686, 'learning_rate': 1.385882494588755e-05, 'epoch': 0.76}
{'loss': 0.394, 'learning_rate': 1.3842124467137662e-05, 'epoch': 0.76}
{'loss': 0.3736, 'learning_rate': 1.382543244007134e-05, 'epoch': 0.76}
{'loss': 0.653, 'learning_rate': 1.3808748868590254e-05, 'epoch': 0.76}
 76%|  | 4921/6500 [9:18:15<3:08:02,  7.15s/it]                                                        76%|  | 4921/6500 [9:18:15<3:08:02,  7.15s/it] 76%|  | 4922/6500 [9:18:21<3:03:13,  6.97s/it]                                                        76%|  | 4922/6500 [9:18:21<3:03:13,  6.97s/it] 76%|  | 4923/6500 [9:18:28<2:59:47,  6.84s/it]                                                        76%|  | 4923/6500 [9:18:28<2:59:47,  6.84s/it] 76%|  | 4924/6500 [9:18:34<2:57:23,  6.75s/it]                                                        76%|  | 4924/6500 [9:18:34<2:57:23,  6.75s/it] 76%|  | 4925/6500 [9:18:41<2:55:35,  6.69s/it]                                                        76%|  | 4925/6500 [9:18:41<2:55:35,  6.69s/it] 76%|{'loss': 0.3659, 'learning_rate': 1.3792073756594065e-05, 'epoch': 0.76}
{'loss': 0.3806, 'learning_rate': 1.3775407107980481e-05, 'epoch': 0.76}
{'loss': 0.3586, 'learning_rate': 1.3758748926645237e-05, 'epoch': 0.76}
{'loss': 0.3623, 'learning_rate': 1.374209921648208e-05, 'epoch': 0.76}
{'loss': 0.3664, 'learning_rate': 1.3725457981382783e-05, 'epoch': 0.76}
  | 4926/6500 [9:18:47<2:54:25,  6.65s/it]                                                        76%|  | 4926/6500 [9:18:47<2:54:25,  6.65s/it] 76%|  | 4927/6500 [9:18:54<2:53:26,  6.62s/it]                                                        76%|  | 4927/6500 [9:18:54<2:53:26,  6.62s/it] 76%|  | 4928/6500 [9:19:00<2:52:47,  6.60s/it]                                                        76%|  | 4928/6500 [9:19:00<2:52:47,  6.60s/it] 76%|  | 4929/6500 [9:19:08<2:59:11,  6.84s/it]                                                        76%|  | 4929/6500 [9:19:08<2:59:11,  6.84s/it] 76%|  | 4930/6500 [9:19:14<2:56:45,  6.76s/it]                                                        76%|  | 4930/6500 [9:19:14<2:56:45,  6.76s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8785226941108704, 'eval_runtime': 1.4887, 'eval_samples_per_second': 8.061, 'eval_steps_per_second': 2.015, 'epoch': 0.76}
                                                        76%|  | 4930/6500 [9:19:16<2:56:45,  6.76s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-4930
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4930
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4930
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4930
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4930
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4930/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4930/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4930/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4930/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3741, 'learning_rate': 1.3708825225237126e-05, 'epoch': 0.76}
{'loss': 0.3933, 'learning_rate': 1.369220095193292e-05, 'epoch': 0.76}
{'loss': 0.3863, 'learning_rate': 1.3675585165355987e-05, 'epoch': 0.76}
{'loss': 0.3657, 'learning_rate': 1.3658977869390166e-05, 'epoch': 0.76}
{'loss': 0.3738, 'learning_rate': 1.364237906791731e-05, 'epoch': 0.76}
 76%|  | 4931/6500 [9:19:23<3:09:21,  7.24s/it]                                                        76%|  | 4931/6500 [9:19:23<3:09:21,  7.24s/it] 76%|  | 4932/6500 [9:19:29<3:03:45,  7.03s/it]                                                        76%|  | 4932/6500 [9:19:29<3:03:45,  7.03s/it] 76%|  | 4933/6500 [9:19:36<2:59:49,  6.89s/it]                                                        76%|  | 4933/6500 [9:19:36<2:59:49,  6.89s/it] 76%|  | 4934/6500 [9:19:42<2:57:05,  6.79s/it]                                                        76%|  | 4934/6500 [9:19:42<2:57:05,  6.79s/it] 76%|  | 4935/6500 [9:19:49<2:55:11,  6.72s/it]                                                        76%|  | 4935/6500 [9:19:49<2:55:11,  6.72s/it] 76%|{'loss': 0.359, 'learning_rate': 1.3625788764817305e-05, 'epoch': 0.76}
{'loss': 0.3989, 'learning_rate': 1.3609206963968002e-05, 'epoch': 0.76}
{'loss': 0.3635, 'learning_rate': 1.3592633669245309e-05, 'epoch': 0.76}
{'loss': 0.6327, 'learning_rate': 1.3576068884523142e-05, 'epoch': 0.76}
{'loss': 0.3983, 'learning_rate': 1.3559512613673402e-05, 'epoch': 0.76}
  | 4936/6500 [9:19:55<2:53:43,  6.66s/it]                                                        76%|  | 4936/6500 [9:19:55<2:53:43,  6.66s/it] 76%|  | 4937/6500 [9:20:02<2:52:42,  6.63s/it]                                                        76%|  | 4937/6500 [9:20:02<2:52:42,  6.63s/it] 76%|  | 4938/6500 [9:20:09<2:51:54,  6.60s/it]                                                        76%|  | 4938/6500 [9:20:09<2:51:54,  6.60s/it] 76%|  | 4939/6500 [9:20:15<2:51:22,  6.59s/it]                                                        76%|  | 4939/6500 [9:20:15<2:51:22,  6.59s/it] 76%|  | 4940/6500 [9:20:22<2:50:58,  6.58s/it]                                                        76%|  | 4940/6500 [9:20:22<2:50:58,  6.58s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.87592613697052, 'eval_runtime': 1.4767, 'eval_samples_per_second': 8.126, 'eval_steps_per_second': 2.032, 'epoch': 0.76}
                                                        76%|  | 4940/6500 [9:20:23<2:50:58,  6.58s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-4940
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4940
/projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4940
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4940
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4940
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4940/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4940/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4940/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4940/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3673, 'learning_rate': 1.3542964860566037e-05, 'epoch': 0.76}
{'loss': 0.3791, 'learning_rate': 1.3526425629068967e-05, 'epoch': 0.76}
{'loss': 0.3515, 'learning_rate': 1.3509894923048145e-05, 'epoch': 0.76}
{'loss': 0.3639, 'learning_rate': 1.3493372746367522e-05, 'epoch': 0.76}
{'loss': 0.371, 'learning_rate': 1.3476859102889056e-05, 'epoch': 0.76}
 76%|  | 4941/6500 [9:20:30<3:05:00,  7.12s/it]                                                        76%|  | 4941/6500 [9:20:30<3:05:00,  7.12s/it] 76%|  | 4942/6500 [9:20:37<3:00:23,  6.95s/it]                                                        76%|  | 4942/6500 [9:20:37<3:00:23,  6.95s/it] 76%|  | 4943/6500 [9:20:43<2:57:10,  6.83s/it]                                                        76%|  | 4943/6500 [9:20:43<2:57:10,  6.83s/it] 76%|  | 4944/6500 [9:20:50<2:54:55,  6.75s/it]                                                        76%|  | 4944/6500 [9:20:50<2:54:55,  6.75s/it] 76%|  | 4945/6500 [9:20:56<2:53:16,  6.69s/it]                                                        76%|  | 4945/6500 [9:20:56<2:53:16,  6.69s/it] 76%|{'loss': 0.3741, 'learning_rate': 1.3460353996472707e-05, 'epoch': 0.76}
{'loss': 0.3706, 'learning_rate': 1.3443857430976465e-05, 'epoch': 0.76}
{'loss': 0.3745, 'learning_rate': 1.3427369410256269e-05, 'epoch': 0.76}
{'loss': 0.3603, 'learning_rate': 1.3410889938166105e-05, 'epoch': 0.76}
{'loss': 0.37, 'learning_rate': 1.3394419018557957e-05, 'epoch': 0.76}
  | 4946/6500 [9:21:03<2:56:52,  6.83s/it]                                                        76%|  | 4946/6500 [9:21:03<2:56:52,  6.83s/it] 76%|  | 4947/6500 [9:21:10<2:54:34,  6.74s/it]                                                        76%|  | 4947/6500 [9:21:10<2:54:34,  6.74s/it] 76%|  | 4948/6500 [9:21:16<2:52:55,  6.69s/it]                                                        76%|  | 4948/6500 [9:21:16<2:52:55,  6.69s/it] 76%|  | 4949/6500 [9:21:23<2:51:47,  6.65s/it]                                                        76%|  | 4949/6500 [9:21:23<2:51:47,  6.65s/it] 76%|  | 4950/6500 [9:21:30<2:50:54,  6.62s/it]                                                        76%|  | 4950/6500 [9:21:30<2:50:54,  6.62s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8789377808570862, 'eval_runtime': 1.4883, 'eval_samples_per_second': 8.063, 'eval_steps_per_second': 2.016, 'epoch': 0.76}
                                                        76%|  | 4950/6500 [9:21:31<2:50:54,  6.62s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-4950
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4950
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4950
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4950
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4950
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4950/pytorch_model.bin
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4950/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4950/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4950/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3564, 'learning_rate': 1.3377956655281792e-05, 'epoch': 0.76}
{'loss': 0.3982, 'learning_rate': 1.336150285218558e-05, 'epoch': 0.76}
{'loss': 0.3618, 'learning_rate': 1.334505761311533e-05, 'epoch': 0.76}
{'loss': 0.6388, 'learning_rate': 1.3328620941914994e-05, 'epoch': 0.76}
{'loss': 0.3815, 'learning_rate': 1.3312192842426546e-05, 'epoch': 0.76}
 76%|  | 4951/6500 [9:21:38<3:04:52,  7.16s/it]                                                        76%|  | 4951/6500 [9:21:38<3:04:52,  7.16s/it] 76%|  | 4952/6500 [9:21:45<3:00:05,  6.98s/it]                                                        76%|  | 4952/6500 [9:21:45<3:00:05,  6.98s/it] 76%|  | 4953/6500 [9:21:51<2:56:42,  6.85s/it]                                                        76%|  | 4953/6500 [9:21:51<2:56:42,  6.85s/it] 76%|  | 4954/6500 [9:21:58<2:54:18,  6.76s/it]                                                        76%|  | 4954/6500 [9:21:58<2:54:18,  6.76s/it] 76%|  | 4955/6500 [9:22:04<2:52:35,  6.70s/it]                                                        76%|  | 4955/6500 [9:22:04<2:52:35,  6.70s/it] 76%|{'loss': 0.3618, 'learning_rate': 1.3295773318489974e-05, 'epoch': 0.76}
{'loss': 0.3776, 'learning_rate': 1.3279362373943204e-05, 'epoch': 0.76}
{'loss': 0.3571, 'learning_rate': 1.3262960012622216e-05, 'epoch': 0.76}
{'loss': 0.3624, 'learning_rate': 1.3246566238360963e-05, 'epoch': 0.76}
{'loss': 0.3678, 'learning_rate': 1.323018105499138e-05, 'epoch': 0.76}
  | 4956/6500 [9:22:11<2:51:16,  6.66s/it]                                                        76%|  | 4956/6500 [9:22:11<2:51:16,  6.66s/it] 76%|  | 4957/6500 [9:22:17<2:50:18,  6.62s/it]                                                        76%|  | 4957/6500 [9:22:17<2:50:18,  6.62s/it] 76%|  | 4958/6500 [9:22:24<2:49:41,  6.60s/it]                                                        76%|  | 4958/6500 [9:22:24<2:49:41,  6.60s/it] 76%|  | 4959/6500 [9:22:30<2:49:20,  6.59s/it]                                                        76%|  | 4959/6500 [9:22:30<2:49:20,  6.59s/it] 76%|  | 4960/6500 [9:22:37<2:48:55,  6.58s/it]                                                        76%|  | 4960/6500 [9:22:37<2:48:55,  6.58s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8789323568344116, 'eval_runtime': 1.4843, 'eval_samples_per_second': 8.084, 'eval_steps_per_second': 2.021, 'epoch': 0.76}
                                                        76%|  | 4960/6500 [9:22:39<2:48:55,  6.58s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-4960
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4960
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4960
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4960
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4960
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4960/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4960/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4960/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4960/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3689, 'learning_rate': 1.3213804466343421e-05, 'epoch': 0.76}
{'loss': 0.3731, 'learning_rate': 1.3197436476244995e-05, 'epoch': 0.76}
{'loss': 0.3763, 'learning_rate': 1.3181077088522036e-05, 'epoch': 0.76}
{'loss': 0.371, 'learning_rate': 1.3164726306998442e-05, 'epoch': 0.76}
{'loss': 0.3653, 'learning_rate': 1.314838413549611e-05, 'epoch': 0.76}
 76%|  | 4961/6500 [9:22:46<3:03:27,  7.15s/it]                                                        76%|  | 4961/6500 [9:22:46<3:03:27,  7.15s/it] 76%|  | 4962/6500 [9:22:53<3:06:29,  7.28s/it]                                                        76%|  | 4962/6500 [9:22:53<3:06:29,  7.28s/it] 76%|  | 4963/6500 [9:23:00<3:00:50,  7.06s/it]                                                        76%|  | 4963/6500 [9:23:00<3:00:50,  7.06s/it] 76%|  | 4964/6500 [9:23:06<2:56:48,  6.91s/it]                                                        76%|  | 4964/6500 [9:23:06<2:56:48,  6.91s/it] 76%|  | 4965/6500 [9:23:13<2:53:54,  6.80s/it]                                                        76%|  | 4965/6500 [9:23:13<2:53:54,  6.80s/it] 76%|{'loss': 0.36, 'learning_rate': 1.3132050577834925e-05, 'epoch': 0.76}
{'loss': 0.4055, 'learning_rate': 1.3115725637832776e-05, 'epoch': 0.76}
{'loss': 0.3735, 'learning_rate': 1.3099409319305483e-05, 'epoch': 0.76}
{'loss': 0.6464, 'learning_rate': 1.3083101626066901e-05, 'epoch': 0.76}
{'loss': 0.3788, 'learning_rate': 1.3066802561928854e-05, 'epoch': 0.76}
  | 4966/6500 [9:23:19<2:51:54,  6.72s/it]                                                        76%|  | 4966/6500 [9:23:19<2:51:54,  6.72s/it] 76%|  | 4967/6500 [9:23:26<2:50:26,  6.67s/it]                                                        76%|  | 4967/6500 [9:23:26<2:50:26,  6.67s/it] 76%|  | 4968/6500 [9:23:32<2:49:24,  6.63s/it]                                                        76%|  | 4968/6500 [9:23:32<2:49:24,  6.63s/it] 76%|  | 4969/6500 [9:23:39<2:48:38,  6.61s/it]                                                        76%|  | 4969/6500 [9:23:39<2:48:38,  6.61s/it] 76%|  | 4970/6500 [9:23:45<2:48:04,  6.59s/it]                                                        76%|  | 4970/6500 [9:23:45<2:48:04,  6.59s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8760703802108765, 'eval_runtime': 1.4837, 'eval_samples_per_second': 8.088, 'eval_steps_per_second': 2.022, 'epoch': 0.76}
                                                        76%|  | 4970/6500 [9:23:47<2:48:04,  6.59s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-4970
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4970
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4970
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4970
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4970
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4970/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4970/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4970/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4970/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.371, 'learning_rate': 1.3050512130701154e-05, 'epoch': 0.76}
{'loss': 0.3674, 'learning_rate': 1.3034230336191583e-05, 'epoch': 0.76}
{'loss': 0.3628, 'learning_rate': 1.3017957182205915e-05, 'epoch': 0.77}
{'loss': 0.3718, 'learning_rate': 1.3001692672547905e-05, 'epoch': 0.77}
{'loss': 0.363, 'learning_rate': 1.2985436811019274e-05, 'epoch': 0.77}
 76%|  | 4971/6500 [9:23:54<3:02:07,  7.15s/it]                                                        76%|  | 4971/6500 [9:23:54<3:02:07,  7.15s/it] 76%|  | 4972/6500 [9:24:00<2:57:26,  6.97s/it]                                                        76%|  | 4972/6500 [9:24:00<2:57:26,  6.97s/it] 77%|  | 4973/6500 [9:24:07<2:54:16,  6.85s/it]                                                        77%|  | 4973/6500 [9:24:07<2:54:16,  6.85s/it] 77%|  | 4974/6500 [9:24:14<2:51:58,  6.76s/it]                                                        77%|  | 4974/6500 [9:24:14<2:51:58,  6.76s/it] 77%|  | 4975/6500 [9:24:20<2:50:17,  6.70s/it]                                                        77%|  | 4975/6500 [9:24:20<2:50:17,  6.70s/it] 77%|{'loss': 0.3841, 'learning_rate': 1.2969189601419745e-05, 'epoch': 0.77}
{'loss': 0.3716, 'learning_rate': 1.295295104754699e-05, 'epoch': 0.77}
{'loss': 0.369, 'learning_rate': 1.293672115319668e-05, 'epoch': 0.77}
{'loss': 0.3717, 'learning_rate': 1.292049992216246e-05, 'epoch': 0.77}
{'loss': 0.3586, 'learning_rate': 1.2904287358235928e-05, 'epoch': 0.77}
  | 4976/6500 [9:24:27<2:49:03,  6.66s/it]                                                        77%|  | 4976/6500 [9:24:27<2:49:03,  6.66s/it] 77%|  | 4977/6500 [9:24:33<2:48:12,  6.63s/it]                                                        77%|  | 4977/6500 [9:24:33<2:48:12,  6.63s/it] 77%|  | 4978/6500 [9:24:41<2:54:12,  6.87s/it]                                                        77%|  | 4978/6500 [9:24:41<2:54:12,  6.87s/it] 77%|  | 4979/6500 [9:24:47<2:51:45,  6.78s/it]                                                        77%|  | 4979/6500 [9:24:47<2:51:45,  6.78s/it] 77%|  | 4980/6500 [9:24:54<2:50:03,  6.71s/it]                                                        77%|  | 4980/6500 [9:24:54<2:50:03,  6.71s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8773736357688904, 'eval_runtime': 1.4731, 'eval_samples_per_second': 8.146, 'eval_steps_per_second': 2.037, 'epoch': 0.77}
                                                        77%|  | 4980/6500 [9:24:55<2:50:03,  6.71s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-4980
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4980
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4980
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4980
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4980
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4980/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4980/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4980/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4980/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3532, 'learning_rate': 1.288808346520668e-05, 'epoch': 0.77}
{'loss': 0.4066, 'learning_rate': 1.2871888246862274e-05, 'epoch': 0.77}
{'loss': 0.3708, 'learning_rate': 1.2855701706988254e-05, 'epoch': 0.77}
{'loss': 0.6436, 'learning_rate': 1.2839523849368113e-05, 'epoch': 0.77}
{'loss': 0.3794, 'learning_rate': 1.2823354677783334e-05, 'epoch': 0.77}
 77%|  | 4981/6500 [9:25:02<3:02:46,  7.22s/it]                                                        77%|  | 4981/6500 [9:25:02<3:02:46,  7.22s/it] 77%|  | 4982/6500 [9:25:09<2:57:36,  7.02s/it]                                                        77%|  | 4982/6500 [9:25:09<2:57:36,  7.02s/it] 77%|  | 4983/6500 [9:25:15<2:53:56,  6.88s/it]                                                        77%|  | 4983/6500 [9:25:15<2:53:56,  6.88s/it] 77%|  | 4984/6500 [9:25:22<2:51:23,  6.78s/it]                                                        77%|  | 4984/6500 [9:25:22<2:51:23,  6.78s/it] 77%|  | 4985/6500 [9:25:28<2:49:29,  6.71s/it]                                                        77%|  | 4985/6500 [9:25:28<2:49:29,  6.71s/it] 77%|{'loss': 0.3783, 'learning_rate': 1.2807194196013367e-05, 'epoch': 0.77}
{'loss': 0.3637, 'learning_rate': 1.2791042407835613e-05, 'epoch': 0.77}
{'loss': 0.3699, 'learning_rate': 1.2774899317025468e-05, 'epoch': 0.77}
{'loss': 0.3692, 'learning_rate': 1.275876492735627e-05, 'epoch': 0.77}
{'loss': 0.3736, 'learning_rate': 1.2742639242599358e-05, 'epoch': 0.77}
  | 4986/6500 [9:25:35<2:48:09,  6.66s/it]                                                        77%|  | 4986/6500 [9:25:35<2:48:09,  6.66s/it] 77%|  | 4987/6500 [9:25:42<2:47:15,  6.63s/it]                                                        77%|  | 4987/6500 [9:25:42<2:47:15,  6.63s/it] 77%|  | 4988/6500 [9:25:48<2:46:34,  6.61s/it]                                                        77%|  | 4988/6500 [9:25:48<2:46:34,  6.61s/it] 77%|  | 4989/6500 [9:25:55<2:46:05,  6.60s/it]                                                        77%|  | 4989/6500 [9:25:55<2:46:05,  6.60s/it] 77%|  | 4990/6500 [9:26:01<2:45:38,  6.58s/it]                                                        77%|  | 4990/6500 [9:26:01<2:45:38,  6.58s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8783887028694153, 'eval_runtime': 1.4703, 'eval_samples_per_second': 8.161, 'eval_steps_per_second': 2.04, 'epoch': 0.77}
                                                        77%|  | 4990/6500 [9:26:03<2:45:38,  6.58s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-4990
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4990I AM HERE AND I AM SAVING THE MODEL

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4990
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4990
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4990
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4990/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4990/pytorch_model.binthe pytorch model path is
 /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4990/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-4990/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3803, 'learning_rate': 1.2726522266523976e-05, 'epoch': 0.77}
{'loss': 0.3764, 'learning_rate': 1.2710414002897393e-05, 'epoch': 0.77}
{'loss': 0.3727, 'learning_rate': 1.2694314455484813e-05, 'epoch': 0.77}
{'loss': 0.3714, 'learning_rate': 1.2678223628049402e-05, 'epoch': 0.77}
{'loss': 0.3508, 'learning_rate': 1.26621415243523e-05, 'epoch': 0.77}
 77%|  | 4991/6500 [9:26:10<2:59:36,  7.14s/it]                                                        77%|  | 4991/6500 [9:26:10<2:59:36,  7.14s/it] 77%|  | 4992/6500 [9:26:16<2:55:03,  6.97s/it]                                                        77%|  | 4992/6500 [9:26:16<2:55:03,  6.97s/it] 77%|  | 4993/6500 [9:26:23<2:51:54,  6.84s/it]                                                        77%|  | 4993/6500 [9:26:23<2:51:54,  6.84s/it] 77%|  | 4994/6500 [9:26:30<2:56:13,  7.02s/it]                                                        77%|  | 4994/6500 [9:26:30<2:56:13,  7.02s/it] 77%|  | 4995/6500 [9:26:37<2:52:34,  6.88s/it]                                                        77%|  | 4995/6500 [9:26:37<2:52:34,  6.88s/it] 77%|{'loss': 0.3683, 'learning_rate': 1.2646068148152596e-05, 'epoch': 0.77}
{'loss': 0.3968, 'learning_rate': 1.263000350320735e-05, 'epoch': 0.77}
{'loss': 0.3721, 'learning_rate': 1.261394759327157e-05, 'epoch': 0.77}
{'loss': 0.6505, 'learning_rate': 1.259790042209823e-05, 'epoch': 0.77}
{'loss': 0.3651, 'learning_rate': 1.258186199343826e-05, 'epoch': 0.77}
  | 4996/6500 [9:26:43<2:49:58,  6.78s/it]                                                        77%|  | 4996/6500 [9:26:43<2:49:58,  6.78s/it] 77%|  | 4997/6500 [9:26:50<2:48:11,  6.71s/it]                                                        77%|  | 4997/6500 [9:26:50<2:48:11,  6.71s/it] 77%|  | 4998/6500 [9:26:56<2:46:55,  6.67s/it]                                                        77%|  | 4998/6500 [9:26:56<2:46:55,  6.67s/it] 77%|  | 4999/6500 [9:27:03<2:46:01,  6.64s/it]                                                        77%|  | 4999/6500 [9:27:03<2:46:01,  6.64s/it] 77%|  | 5000/6500 [9:27:10<2:45:22,  6.62s/it]                                                        77%|  | 5000/6500 [9:27:10<2:45:22,  6.62s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8758484125137329, 'eval_runtime': 1.4802, 'eval_samples_per_second': 8.107, 'eval_steps_per_second': 2.027, 'epoch': 0.77}
                                                        77%|  | 5000/6500 [9:27:11<2:45:22,  6.62s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-5000
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5000
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5000
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5000
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5000
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5000/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5000/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5000/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5000/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3799, 'learning_rate': 1.2565832311040554e-05, 'epoch': 0.77}
{'loss': 0.3614, 'learning_rate': 1.2549811378651932e-05, 'epoch': 0.77}
{'loss': 0.3608, 'learning_rate': 1.2533799200017194e-05, 'epoch': 0.77}
{'loss': 0.3639, 'learning_rate': 1.25177957788791e-05, 'epoch': 0.77}
{'loss': 0.3658, 'learning_rate': 1.250180111897834e-05, 'epoch': 0.77}
 77%|  | 5001/6500 [9:27:18<2:59:13,  7.17s/it]                                                        77%|  | 5001/6500 [9:27:18<2:59:13,  7.17s/it] 77%|  | 5002/6500 [9:27:25<2:54:25,  6.99s/it]                                                        77%|  | 5002/6500 [9:27:25<2:54:25,  6.99s/it] 77%|  | 5003/6500 [9:27:31<2:51:04,  6.86s/it]                                                        77%|  | 5003/6500 [9:27:31<2:51:04,  6.86s/it] 77%|  | 5004/6500 [9:27:38<2:48:42,  6.77s/it]                                                        77%|  | 5004/6500 [9:27:38<2:48:42,  6.77s/it] 77%|  | 5005/6500 [9:27:44<2:47:00,  6.70s/it]                                                        77%|  | 5005/6500 [9:27:44<2:47:00,  6.70s/it] 77%|{'loss': 0.3742, 'learning_rate': 1.2485815224053582e-05, 'epoch': 0.77}
{'loss': 0.3722, 'learning_rate': 1.2469838097841424e-05, 'epoch': 0.77}
{'loss': 0.3649, 'learning_rate': 1.2453869744076419e-05, 'epoch': 0.77}
{'loss': 0.3774, 'learning_rate': 1.2437910166491084e-05, 'epoch': 0.77}
{'loss': 0.3666, 'learning_rate': 1.242195936881586e-05, 'epoch': 0.77}
  | 5006/6500 [9:27:51<2:45:48,  6.66s/it]                                                        77%|  | 5006/6500 [9:27:51<2:45:48,  6.66s/it] 77%|  | 5007/6500 [9:27:57<2:44:57,  6.63s/it]                                                        77%|  | 5007/6500 [9:27:57<2:44:57,  6.63s/it] 77%|  | 5008/6500 [9:28:04<2:44:19,  6.61s/it]                                                        77%|  | 5008/6500 [9:28:04<2:44:19,  6.61s/it] 77%|  | 5009/6500 [9:28:10<2:43:49,  6.59s/it]                                                        77%|  | 5009/6500 [9:28:10<2:43:49,  6.59s/it] 77%|  | 5010/6500 [9:28:18<2:50:09,  6.85s/it]                                                        77%|  | 5010/6500 [9:28:18<2:50:09,  6.85s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8761147260665894, 'eval_runtime': 1.48, 'eval_samples_per_second': 8.108, 'eval_steps_per_second': 2.027, 'epoch': 0.77}
                                                        77%|  | 5010/6500 [9:28:19<2:50:09,  6.85s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-5010
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5010
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5010
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5010
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5010
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5010/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5010/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5010/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5010/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3721, 'learning_rate': 1.240601735477916e-05, 'epoch': 0.77}
{'loss': 0.3845, 'learning_rate': 1.2390084128107343e-05, 'epoch': 0.77}
{'loss': 0.395, 'learning_rate': 1.2374159692524673e-05, 'epoch': 0.77}
{'loss': 0.6316, 'learning_rate': 1.2358244051753403e-05, 'epoch': 0.77}
{'loss': 0.363, 'learning_rate': 1.2342337209513721e-05, 'epoch': 0.77}
 77%|  | 5011/6500 [9:28:26<3:02:15,  7.34s/it]                                                        77%|  | 5011/6500 [9:28:26<3:02:15,  7.34s/it] 77%|  | 5012/6500 [9:28:33<2:56:17,  7.11s/it]                                                        77%|  | 5012/6500 [9:28:33<2:56:17,  7.11s/it] 77%|  | 5013/6500 [9:28:40<2:52:03,  6.94s/it]                                                        77%|  | 5013/6500 [9:28:40<2:52:03,  6.94s/it] 77%|  | 5014/6500 [9:28:46<2:49:03,  6.83s/it]                                                        77%|  | 5014/6500 [9:28:46<2:49:03,  6.83s/it] 77%|  | 5015/6500 [9:28:53<2:46:57,  6.75s/it]                                                        77%|  | 5015/6500 [9:28:53<2:46:57,  6.75s/it] 77%|{'loss': 0.3798, 'learning_rate': 1.2326439169523757e-05, 'epoch': 0.77}
{'loss': 0.3563, 'learning_rate': 1.2310549935499576e-05, 'epoch': 0.77}
{'loss': 0.3653, 'learning_rate': 1.2294669511155193e-05, 'epoch': 0.77}
{'loss': 0.3724, 'learning_rate': 1.2278797900202559e-05, 'epoch': 0.77}
{'loss': 0.3692, 'learning_rate': 1.226293510635157e-05, 'epoch': 0.77}
  | 5016/6500 [9:28:59<2:45:27,  6.69s/it]                                                        77%|  | 5016/6500 [9:28:59<2:45:27,  6.69s/it] 77%|  | 5017/6500 [9:29:06<2:44:22,  6.65s/it]                                                        77%|  | 5017/6500 [9:29:06<2:44:22,  6.65s/it] 77%|  | 5018/6500 [9:29:12<2:43:36,  6.62s/it]                                                        77%|  | 5018/6500 [9:29:12<2:43:36,  6.62s/it] 77%|  | 5019/6500 [9:29:19<2:43:02,  6.61s/it]                                                        77%|  | 5019/6500 [9:29:19<2:43:02,  6.61s/it] 77%|  | 5020/6500 [9:29:25<2:42:32,  6.59s/it]                                                        77%|  | 5020/6500 [9:29:25<2:42:32,  6.59s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8795086145401001, 'eval_runtime': 1.4875, 'eval_samples_per_second': 8.067, 'eval_steps_per_second': 2.017, 'epoch': 0.77}
                                                        77%|  | 5020/6500 [9:29:27<2:42:32,  6.59s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-5020
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  I AM HERE AND I AM SAVING THE MODEL
/projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5020the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5020

I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5020
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5020
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5020/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5020/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5020/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5020/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3751, 'learning_rate': 1.2247081133310051e-05, 'epoch': 0.77}
{'loss': 0.3821, 'learning_rate': 1.2231235984783779e-05, 'epoch': 0.77}
{'loss': 0.363, 'learning_rate': 1.2215399664476468e-05, 'epoch': 0.77}
{'loss': 0.3731, 'learning_rate': 1.2199572176089741e-05, 'epoch': 0.77}
{'loss': 0.3569, 'learning_rate': 1.2183753523323182e-05, 'epoch': 0.77}
 77%|  | 5021/6500 [9:29:34<2:56:02,  7.14s/it]                                                        77%|  | 5021/6500 [9:29:34<2:56:02,  7.14s/it] 77%|  | 5022/6500 [9:29:40<2:51:40,  6.97s/it]                                                        77%|  | 5022/6500 [9:29:40<2:51:40,  6.97s/it] 77%|  | 5023/6500 [9:29:47<2:48:35,  6.85s/it]                                                        77%|  | 5023/6500 [9:29:47<2:48:35,  6.85s/it] 77%|  | 5024/6500 [9:29:54<2:46:18,  6.76s/it]                                                        77%|  | 5024/6500 [9:29:54<2:46:18,  6.76s/it] 77%|  | 5025/6500 [9:30:00<2:44:43,  6.70s/it]                                                        77%|  | 5025/6500 [9:30:00<2:44:43,  6.70s/it] 77%|{'loss': 0.3956, 'learning_rate': 1.2167943709874313e-05, 'epoch': 0.77}
{'loss': 0.3583, 'learning_rate': 1.2152142739438565e-05, 'epoch': 0.77}
{'loss': 0.6447, 'learning_rate': 1.2136350615709352e-05, 'epoch': 0.77}
{'loss': 0.3789, 'learning_rate': 1.2120567342377965e-05, 'epoch': 0.77}
{'loss': 0.3651, 'learning_rate': 1.2104792923133646e-05, 'epoch': 0.77}
  | 5026/6500 [9:30:07<2:48:11,  6.85s/it]                                                        77%|  | 5026/6500 [9:30:07<2:48:11,  6.85s/it] 77%|  | 5027/6500 [9:30:14<2:45:56,  6.76s/it]                                                        77%|  | 5027/6500 [9:30:14<2:45:56,  6.76s/it] 77%|  | 5028/6500 [9:30:20<2:44:19,  6.70s/it]                                                        77%|  | 5028/6500 [9:30:20<2:44:19,  6.70s/it] 77%|  | 5029/6500 [9:30:27<2:43:12,  6.66s/it]                                                        77%|  | 5029/6500 [9:30:27<2:43:12,  6.66s/it] 77%|  | 5030/6500 [9:30:34<2:42:26,  6.63s/it]                                                        77%|  | 5030/6500 [9:30:34<2:42:26,  6.63s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8790796399116516, 'eval_runtime': 1.4978, 'eval_samples_per_second': 8.012, 'eval_steps_per_second': 2.003, 'epoch': 0.77}
                                                        77%|  | 5030/6500 [9:30:35<2:42:26,  6.63s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-5030
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5030
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5030
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5030
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5030
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5030/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5030/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5030/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5030/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3728, 'learning_rate': 1.208902736166358e-05, 'epoch': 0.77}
{'loss': 0.3565, 'learning_rate': 1.2073270661652875e-05, 'epoch': 0.77}
{'loss': 0.3633, 'learning_rate': 1.2057522826784545e-05, 'epoch': 0.77}
{'loss': 0.3719, 'learning_rate': 1.2041783860739559e-05, 'epoch': 0.77}
{'loss': 0.3684, 'learning_rate': 1.2026053767196803e-05, 'epoch': 0.77}
 77%|  | 5031/6500 [9:30:42<2:55:29,  7.17s/it]                                                        77%|  | 5031/6500 [9:30:42<2:55:29,  7.17s/it] 77%|  | 5032/6500 [9:30:49<2:50:54,  6.99s/it]                                                        77%|  | 5032/6500 [9:30:49<2:50:54,  6.99s/it] 77%|  | 5033/6500 [9:30:55<2:47:43,  6.86s/it]                                                        77%|  | 5033/6500 [9:30:55<2:47:43,  6.86s/it] 77%|  | 5034/6500 [9:31:02<2:45:22,  6.77s/it]                                                        77%|  | 5034/6500 [9:31:02<2:45:22,  6.77s/it] 77%|  | 5035/6500 [9:31:08<2:43:42,  6.70s/it]                                                        77%|  | 5035/6500 [9:31:08<2:43:42,  6.70s/it] 77%|{'loss': 0.3724, 'learning_rate': 1.2010332549833098e-05, 'epoch': 0.77}
{'loss': 0.3732, 'learning_rate': 1.1994620212323177e-05, 'epoch': 0.77}
{'loss': 0.3608, 'learning_rate': 1.1978916758339704e-05, 'epoch': 0.78}
{'loss': 0.3646, 'learning_rate': 1.196322219155327e-05, 'epoch': 0.78}
{'loss': 0.3552, 'learning_rate': 1.1947536515632374e-05, 'epoch': 0.78}
  | 5036/6500 [9:31:15<2:42:30,  6.66s/it]                                                        77%|  | 5036/6500 [9:31:15<2:42:30,  6.66s/it] 77%|  | 5037/6500 [9:31:21<2:41:40,  6.63s/it]                                                        77%|  | 5037/6500 [9:31:21<2:41:40,  6.63s/it] 78%|  | 5038/6500 [9:31:28<2:41:05,  6.61s/it]                                                        78%|  | 5038/6500 [9:31:28<2:41:05,  6.61s/it] 78%|  | 5039/6500 [9:31:34<2:40:38,  6.60s/it]                                                        78%|  | 5039/6500 [9:31:34<2:40:38,  6.60s/it] 78%|  | 5040/6500 [9:31:41<2:40:14,  6.58s/it]                                                        78%|  | 5040/6500 [9:31:41<2:40:14,  6.58s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8786345720291138, 'eval_runtime': 1.4803, 'eval_samples_per_second': 8.106, 'eval_steps_per_second': 2.027, 'epoch': 0.78}
                                                        78%|  | 5040/6500 [9:31:42<2:40:14,  6.58s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-5040
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5040
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5040
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5040
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5040
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5040/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5040/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5040/pytorch_model.binthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5040/pytorch_model.bin

/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.4002, 'learning_rate': 1.1931859734243461e-05, 'epoch': 0.78}
{'loss': 0.3677, 'learning_rate': 1.1916191851050873e-05, 'epoch': 0.78}
{'loss': 0.6352, 'learning_rate': 1.1900532869716908e-05, 'epoch': 0.78}
{'loss': 0.3846, 'learning_rate': 1.1884882793901714e-05, 'epoch': 0.78}
{'loss': 0.3557, 'learning_rate': 1.1869241627263427e-05, 'epoch': 0.78}
 78%|  | 5041/6500 [9:31:49<2:53:29,  7.13s/it]                                                        78%|  | 5041/6500 [9:31:49<2:53:29,  7.13s/it] 78%|  | 5042/6500 [9:31:57<2:55:26,  7.22s/it]                                                        78%|  | 5042/6500 [9:31:57<2:55:26,  7.22s/it] 78%|  | 5043/6500 [9:32:03<2:50:36,  7.03s/it]                                                        78%|  | 5043/6500 [9:32:03<2:50:36,  7.03s/it] 78%|  | 5044/6500 [9:32:10<2:47:06,  6.89s/it]                                                        78%|  | 5044/6500 [9:32:10<2:47:06,  6.89s/it] 78%|  | 5045/6500 [9:32:17<2:44:37,  6.79s/it]                                                        78%|  | 5045/6500 [9:32:17<2:44:37,  6.79s/it] 78%|{'loss': 0.3676, 'learning_rate': 1.1853609373458069e-05, 'epoch': 0.78}
{'loss': 0.3586, 'learning_rate': 1.1837986036139587e-05, 'epoch': 0.78}
{'loss': 0.3625, 'learning_rate': 1.1822371618959837e-05, 'epoch': 0.78}
{'loss': 0.362, 'learning_rate': 1.1806766125568603e-05, 'epoch': 0.78}
{'loss': 0.3752, 'learning_rate': 1.1791169559613564e-05, 'epoch': 0.78}
  | 5046/6500 [9:32:23<2:42:45,  6.72s/it]                                                        78%|  | 5046/6500 [9:32:23<2:42:45,  6.72s/it] 78%|  | 5047/6500 [9:32:30<2:41:34,  6.67s/it]                                                        78%|  | 5047/6500 [9:32:30<2:41:34,  6.67s/it] 78%|  | 5048/6500 [9:32:36<2:40:44,  6.64s/it]                                                        78%|  | 5048/6500 [9:32:36<2:40:44,  6.64s/it] 78%|  | 5049/6500 [9:32:43<2:39:58,  6.62s/it]                                                        78%|  | 5049/6500 [9:32:43<2:39:58,  6.62s/it] 78%|  | 5050/6500 [9:32:49<2:39:27,  6.60s/it]                                                        78%|  | 5050/6500 [9:32:49<2:39:27,  6.60s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8802762031555176, 'eval_runtime': 1.4829, 'eval_samples_per_second': 8.092, 'eval_steps_per_second': 2.023, 'epoch': 0.78}
                                                        78%|  | 5050/6500 [9:32:51<2:39:27,  6.60s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-5050
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5050
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5050
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5050
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5050
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5050/pytorch_model.bin
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5050/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5050/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5050/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3703, 'learning_rate': 1.177558192474033e-05, 'epoch': 0.78}
{'loss': 0.3687, 'learning_rate': 1.1760003224592415e-05, 'epoch': 0.78}
{'loss': 0.3664, 'learning_rate': 1.174443346281124e-05, 'epoch': 0.78}
{'loss': 0.3631, 'learning_rate': 1.1728872643036154e-05, 'epoch': 0.78}
{'loss': 0.3571, 'learning_rate': 1.171332076890439e-05, 'epoch': 0.78}
 78%|  | 5051/6500 [9:32:58<2:53:22,  7.18s/it]                                                        78%|  | 5051/6500 [9:32:58<2:53:22,  7.18s/it] 78%|  | 5052/6500 [9:33:04<2:48:49,  7.00s/it]                                                        78%|  | 5052/6500 [9:33:04<2:48:49,  7.00s/it] 78%|  | 5053/6500 [9:33:11<2:45:39,  6.87s/it]                                                        78%|  | 5053/6500 [9:33:11<2:45:39,  6.87s/it] 78%|  | 5054/6500 [9:33:18<2:43:20,  6.78s/it]                                                        78%|  | 5054/6500 [9:33:18<2:43:20,  6.78s/it] 78%|  | 5055/6500 [9:33:24<2:41:44,  6.72s/it]                                                        78%|  | 5055/6500 [9:33:24<2:41:44,  6.72s/it] 78%|{'loss': 0.4026, 'learning_rate': 1.1697777844051105e-05, 'epoch': 0.78}
{'loss': 0.3658, 'learning_rate': 1.1682243872109367e-05, 'epoch': 0.78}
{'loss': 0.6421, 'learning_rate': 1.1666718856710152e-05, 'epoch': 0.78}
{'loss': 0.3781, 'learning_rate': 1.1651202801482331e-05, 'epoch': 0.78}
{'loss': 0.3654, 'learning_rate': 1.163569571005269e-05, 'epoch': 0.78}
  | 5056/6500 [9:33:31<2:40:28,  6.67s/it]                                                        78%|  | 5056/6500 [9:33:31<2:40:28,  6.67s/it] 78%|  | 5057/6500 [9:33:37<2:39:34,  6.64s/it]                                                        78%|  | 5057/6500 [9:33:37<2:39:34,  6.64s/it] 78%|  | 5058/6500 [9:33:44<2:38:55,  6.61s/it]                                                        78%|  | 5058/6500 [9:33:44<2:38:55,  6.61s/it] 78%|  | 5059/6500 [9:33:51<2:44:50,  6.86s/it]                                                        78%|  | 5059/6500 [9:33:51<2:44:50,  6.86s/it] 78%|  | 5060/6500 [9:33:58<2:42:38,  6.78s/it]                                                        78%|  | 5060/6500 [9:33:58<2:42:38,  6.78s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8789575099945068, 'eval_runtime': 1.4815, 'eval_samples_per_second': 8.1, 'eval_steps_per_second': 2.025, 'epoch': 0.78}
                                                        78%|  | 5060/6500 [9:33:59<2:42:38,  6.78s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-5060
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5060
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5060
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5060
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5060
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5060/pytorch_model.bin
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5060/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5060/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5060/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3609, 'learning_rate': 1.1620197586045922e-05, 'epoch': 0.78}
{'loss': 0.36, 'learning_rate': 1.160470843308461e-05, 'epoch': 0.78}
{'loss': 0.3642, 'learning_rate': 1.1589228254789258e-05, 'epoch': 0.78}
{'loss': 0.3742, 'learning_rate': 1.1573757054778261e-05, 'epoch': 0.78}
{'loss': 0.3797, 'learning_rate': 1.1558294836667916e-05, 'epoch': 0.78}
 78%|  | 5061/6500 [9:34:06<2:54:25,  7.27s/it]                                                        78%|  | 5061/6500 [9:34:06<2:54:25,  7.27s/it] 78%|  | 5062/6500 [9:34:13<2:49:14,  7.06s/it]                                                        78%|  | 5062/6500 [9:34:13<2:49:14,  7.06s/it] 78%|  | 5063/6500 [9:34:19<2:45:33,  6.91s/it]                                                        78%|  | 5063/6500 [9:34:19<2:45:33,  6.91s/it] 78%|  | 5064/6500 [9:34:26<2:42:53,  6.81s/it]                                                        78%|  | 5064/6500 [9:34:26<2:42:53,  6.81s/it] 78%|  | 5065/6500 [9:34:33<2:40:58,  6.73s/it]                                                        78%|  | 5065/6500 [9:34:33<2:40:58,  6.73s/it] 78%|{'loss': 0.3732, 'learning_rate': 1.1542841604072435e-05, 'epoch': 0.78}
{'loss': 0.3696, 'learning_rate': 1.1527397360603897e-05, 'epoch': 0.78}
{'loss': 0.3698, 'learning_rate': 1.1511962109872305e-05, 'epoch': 0.78}
{'loss': 0.3583, 'learning_rate': 1.1496535855485557e-05, 'epoch': 0.78}
{'loss': 0.3611, 'learning_rate': 1.1481118601049452e-05, 'epoch': 0.78}
  | 5066/6500 [9:34:39<2:39:39,  6.68s/it]                                                        78%|  | 5066/6500 [9:34:39<2:39:39,  6.68s/it] 78%|  | 5067/6500 [9:34:46<2:38:43,  6.65s/it]                                                        78%|  | 5067/6500 [9:34:46<2:38:43,  6.65s/it] 78%|  | 5068/6500 [9:34:52<2:38:02,  6.62s/it]                                                        78%|  | 5068/6500 [9:34:52<2:38:02,  6.62s/it] 78%|  | 5069/6500 [9:34:59<2:37:32,  6.61s/it]                                                        78%|  | 5069/6500 [9:34:59<2:37:32,  6.61s/it] 78%|  | 5070/6500 [9:35:05<2:37:10,  6.59s/it]                                                        78%|  | 5070/6500 [9:35:05<2:37:10,  6.59s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8789026737213135, 'eval_runtime': 1.5364, 'eval_samples_per_second': 7.81, 'eval_steps_per_second': 1.953, 'epoch': 0.78}
                                                        78%|  | 5070/6500 [9:35:07<2:37:10,  6.59s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-5070
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5070
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5070
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5070
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5070
the pytorch model path is the pytorch model path is/projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5070/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5070/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5070/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5070/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3956, 'learning_rate': 1.1465710350167674e-05, 'epoch': 0.78}
{'loss': 0.3659, 'learning_rate': 1.1450311106441807e-05, 'epoch': 0.78}
{'loss': 0.6419, 'learning_rate': 1.1434920873471333e-05, 'epoch': 0.78}
{'loss': 0.3632, 'learning_rate': 1.1419539654853628e-05, 'epoch': 0.78}
{'loss': 0.3768, 'learning_rate': 1.1404167454183957e-05, 'epoch': 0.78}
 78%|  | 5071/6500 [9:35:14<2:51:12,  7.19s/it]                                                        78%|  | 5071/6500 [9:35:14<2:51:12,  7.19s/it] 78%|  | 5072/6500 [9:35:21<2:46:40,  7.00s/it]                                                        78%|  | 5072/6500 [9:35:21<2:46:40,  7.00s/it] 78%|  | 5073/6500 [9:35:27<2:43:21,  6.87s/it]                                                        78%|  | 5073/6500 [9:35:27<2:43:21,  6.87s/it] 78%|  | 5074/6500 [9:35:34<2:41:00,  6.77s/it]                                                        78%|  | 5074/6500 [9:35:34<2:41:00,  6.77s/it] 78%|  | 5075/6500 [9:35:41<2:45:43,  6.98s/it]                                                        78%|  | 5075/6500 [9:35:41<2:45:43,  6.98s/it] 78%|{'loss': 0.3569, 'learning_rate': 1.138880427505547e-05, 'epoch': 0.78}
{'loss': 0.3591, 'learning_rate': 1.1373450121059242e-05, 'epoch': 0.78}
{'loss': 0.3561, 'learning_rate': 1.1358104995784186e-05, 'epoch': 0.78}
{'loss': 0.3693, 'learning_rate': 1.1342768902817136e-05, 'epoch': 0.78}
{'loss': 0.3718, 'learning_rate': 1.1327441845742814e-05, 'epoch': 0.78}
  | 5076/6500 [9:35:48<2:42:41,  6.86s/it]                                                        78%|  | 5076/6500 [9:35:48<2:42:41,  6.86s/it] 78%|  | 5077/6500 [9:35:54<2:40:33,  6.77s/it]                                                        78%|  | 5077/6500 [9:35:54<2:40:33,  6.77s/it] 78%|  | 5078/6500 [9:36:01<2:38:54,  6.70s/it]                                                        78%|  | 5078/6500 [9:36:01<2:38:54,  6.70s/it] 78%|  | 5079/6500 [9:36:07<2:37:46,  6.66s/it]                                                        78%|  | 5079/6500 [9:36:07<2:37:46,  6.66s/it] 78%|  | 5080/6500 [9:36:14<2:36:53,  6.63s/it]                                                        78%|  | 5080/6500 [9:36:14<2:36:53,  6.63s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8794615268707275, 'eval_runtime': 1.483, 'eval_samples_per_second': 8.092, 'eval_steps_per_second': 2.023, 'epoch': 0.78}
                                                        78%|  | 5080/6500 [9:36:15<2:36:53,  6.63s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-5080
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5080
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5080
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5080
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5080
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5080/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5080/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5080/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5080/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3707, 'learning_rate': 1.1312123828143827e-05, 'epoch': 0.78}
{'loss': 0.3709, 'learning_rate': 1.1296814853600673e-05, 'epoch': 0.78}
{'loss': 0.3759, 'learning_rate': 1.1281514925691722e-05, 'epoch': 0.78}
{'loss': 0.352, 'learning_rate': 1.126622404799325e-05, 'epoch': 0.78}
{'loss': 0.3709, 'learning_rate': 1.1250942224079403e-05, 'epoch': 0.78}
 78%|  | 5081/6500 [9:36:22<2:49:48,  7.18s/it]                                                        78%|  | 5081/6500 [9:36:22<2:49:48,  7.18s/it] 78%|  | 5082/6500 [9:36:29<2:45:18,  6.99s/it]                                                        78%|  | 5082/6500 [9:36:29<2:45:18,  6.99s/it] 78%|  | 5083/6500 [9:36:35<2:42:07,  6.87s/it]                                                        78%|  | 5083/6500 [9:36:35<2:42:07,  6.87s/it] 78%|  | 5084/6500 [9:36:42<2:39:49,  6.77s/it]                                                        78%|  | 5084/6500 [9:36:42<2:39:49,  6.77s/it] 78%|  | 5085/6500 [9:36:49<2:38:10,  6.71s/it]                                                        78%|  | 5085/6500 [9:36:49<2:38:10,  6.71s/it] 78%|{'loss': 0.3883, 'learning_rate': 1.1235669457522207e-05, 'epoch': 0.78}
{'loss': 0.3735, 'learning_rate': 1.1220405751891588e-05, 'epoch': 0.78}
{'loss': 0.6474, 'learning_rate': 1.1205151110755352e-05, 'epoch': 0.78}
{'loss': 0.3678, 'learning_rate': 1.1189905537679157e-05, 'epoch': 0.78}
{'loss': 0.3799, 'learning_rate': 1.1174669036226571e-05, 'epoch': 0.78}
  | 5086/6500 [9:36:55<2:37:02,  6.66s/it]                                                        78%|  | 5086/6500 [9:36:55<2:37:02,  6.66s/it] 78%|  | 5087/6500 [9:37:02<2:36:08,  6.63s/it]                                                        78%|  | 5087/6500 [9:37:02<2:36:08,  6.63s/it] 78%|  | 5088/6500 [9:37:08<2:35:30,  6.61s/it]                                                        78%|  | 5088/6500 [9:37:08<2:35:30,  6.61s/it] 78%|  | 5089/6500 [9:37:15<2:35:04,  6.59s/it]                                                        78%|  | 5089/6500 [9:37:15<2:35:04,  6.59s/it] 78%|  | 5090/6500 [9:37:21<2:34:41,  6.58s/it]                                                        78%|  | 5090/6500 [9:37:21<2:34:41,  6.58s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8770508170127869, 'eval_runtime': 1.4786, 'eval_samples_per_second': 8.116, 'eval_steps_per_second': 2.029, 'epoch': 0.78}
                                                        78%|  | 5090/6500 [9:37:23<2:34:41,  6.58s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-5090
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5090I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5090

I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5090
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5090
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5090/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5090/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5090/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5090/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.351, 'learning_rate': 1.1159441609959038e-05, 'epoch': 0.78}
{'loss': 0.3636, 'learning_rate': 1.1144223262435871e-05, 'epoch': 0.78}
{'loss': 0.37, 'learning_rate': 1.1129013997214271e-05, 'epoch': 0.78}
{'loss': 0.3647, 'learning_rate': 1.111381381784931e-05, 'epoch': 0.78}
{'loss': 0.3785, 'learning_rate': 1.1098622727893937e-05, 'epoch': 0.78}
 78%|  | 5091/6500 [9:37:31<2:54:54,  7.45s/it]                                                        78%|  | 5091/6500 [9:37:31<2:54:54,  7.45s/it] 78%|  | 5092/6500 [9:37:37<2:48:41,  7.19s/it]                                                        78%|  | 5092/6500 [9:37:37<2:48:41,  7.19s/it] 78%|  | 5093/6500 [9:37:44<2:44:13,  7.00s/it]                                                        78%|  | 5093/6500 [9:37:44<2:44:13,  7.00s/it] 78%|  | 5094/6500 [9:37:51<2:41:08,  6.88s/it]                                                        78%|  | 5094/6500 [9:37:51<2:41:08,  6.88s/it] 78%|  | 5095/6500 [9:37:57<2:38:54,  6.79s/it]                                                        78%|  | 5095/6500 [9:37:57<2:38:54,  6.79s/it] 78%|{'loss': 0.3825, 'learning_rate': 1.1083440730898974e-05, 'epoch': 0.78}
{'loss': 0.3679, 'learning_rate': 1.1068267830413126e-05, 'epoch': 0.78}
{'loss': 0.3756, 'learning_rate': 1.105310402998297e-05, 'epoch': 0.78}
{'loss': 0.3563, 'learning_rate': 1.1037949333152953e-05, 'epoch': 0.78}
{'loss': 0.3993, 'learning_rate': 1.102280374346537e-05, 'epoch': 0.78}
  | 5096/6500 [9:38:04<2:37:19,  6.72s/it]                                                        78%|  | 5096/6500 [9:38:04<2:37:19,  6.72s/it] 78%|  | 5097/6500 [9:38:10<2:36:39,  6.70s/it]                                                        78%|  | 5097/6500 [9:38:10<2:36:39,  6.70s/it] 78%|  | 5098/6500 [9:38:17<2:35:39,  6.66s/it]                                                        78%|  | 5098/6500 [9:38:17<2:35:39,  6.66s/it] 78%|  | 5099/6500 [9:38:24<2:34:56,  6.64s/it]                                                        78%|  | 5099/6500 [9:38:24<2:34:56,  6.64s/it] 78%|  | 5100/6500 [9:38:30<2:34:24,  6.62s/it]                                                        78%|  | 5100/6500 [9:38:30<2:34:24,  6.62s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8777597546577454, 'eval_runtime': 1.4856, 'eval_samples_per_second': 8.078, 'eval_steps_per_second': 2.019, 'epoch': 0.78}
                                                        78%|  | 5100/6500 [9:38:32<2:34:24,  6.62s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-5100
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5100
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5100
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5100
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5100
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5100/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5100/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5100/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5100/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3592, 'learning_rate': 1.1007667264460431e-05, 'epoch': 0.78}
{'loss': 0.6449, 'learning_rate': 1.099253989967619e-05, 'epoch': 0.78}
{'loss': 0.3784, 'learning_rate': 1.0977421652648568e-05, 'epoch': 0.79}
{'loss': 0.3663, 'learning_rate': 1.0962312526911383e-05, 'epoch': 0.79}
{'loss': 0.3801, 'learning_rate': 1.0947212525996292e-05, 'epoch': 0.79}
 78%|  | 5101/6500 [9:38:39<2:47:03,  7.17s/it]                                                        78%|  | 5101/6500 [9:38:39<2:47:03,  7.17s/it] 78%|  | 5102/6500 [9:38:45<2:42:45,  6.99s/it]                                                        78%|  | 5102/6500 [9:38:45<2:42:45,  6.99s/it] 79%|  | 5103/6500 [9:38:52<2:39:43,  6.86s/it]                                                        79%|  | 5103/6500 [9:38:52<2:39:43,  6.86s/it] 79%|  | 5104/6500 [9:38:58<2:37:32,  6.77s/it]                                                        79%|  | 5104/6500 [9:38:58<2:37:32,  6.77s/it] 79%|  | 5105/6500 [9:39:05<2:36:01,  6.71s/it]                                                        79%|  | 5105/6500 [9:39:05<2:36:01,  6.71s/it] 79%|{'loss': 0.3607, 'learning_rate': 1.0932121653432831e-05, 'epoch': 0.79}
{'loss': 0.3701, 'learning_rate': 1.0917039912748395e-05, 'epoch': 0.79}
{'loss': 0.3684, 'learning_rate': 1.0901967307468269e-05, 'epoch': 0.79}
{'loss': 0.3724, 'learning_rate': 1.0886903841115547e-05, 'epoch': 0.79}
{'loss': 0.3783, 'learning_rate': 1.087184951721124e-05, 'epoch': 0.79}
  | 5106/6500 [9:39:11<2:34:53,  6.67s/it]                                                        79%|  | 5106/6500 [9:39:11<2:34:53,  6.67s/it] 79%|  | 5107/6500 [9:39:19<2:38:17,  6.82s/it]                                                        79%|  | 5107/6500 [9:39:19<2:38:17,  6.82s/it] 79%|  | 5108/6500 [9:39:25<2:36:27,  6.74s/it]                                                        79%|  | 5108/6500 [9:39:25<2:36:27,  6.74s/it] 79%|  | 5109/6500 [9:39:32<2:35:09,  6.69s/it]                                                        79%|  | 5109/6500 [9:39:32<2:35:09,  6.69s/it] 79%|  | 5110/6500 [9:39:38<2:34:14,  6.66s/it]                                                        79%|  | 5110/6500 [9:39:38<2:34:14,  6.66s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8793156743049622, 'eval_runtime': 1.507, 'eval_samples_per_second': 7.963, 'eval_steps_per_second': 1.991, 'epoch': 0.79}
                                                        79%|  | 5110/6500 [9:39:40<2:34:14,  6.66s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-5110
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5110
/projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5110
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5110
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5110
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5110/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5110/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5110/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5110/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3792, 'learning_rate': 1.0856804339274207e-05, 'epoch': 0.79}
{'loss': 0.3626, 'learning_rate': 1.0841768310821165e-05, 'epoch': 0.79}
{'loss': 0.3716, 'learning_rate': 1.0826741435366695e-05, 'epoch': 0.79}
{'loss': 0.3551, 'learning_rate': 1.0811723716423233e-05, 'epoch': 0.79}
{'loss': 0.4086, 'learning_rate': 1.0796715157501086e-05, 'epoch': 0.79}
 79%|  | 5111/6500 [9:39:47<2:46:28,  7.19s/it]                                                        79%|  | 5111/6500 [9:39:47<2:46:28,  7.19s/it] 79%|  | 5112/6500 [9:39:53<2:42:05,  7.01s/it]                                                        79%|  | 5112/6500 [9:39:53<2:42:05,  7.01s/it] 79%|  | 5113/6500 [9:40:00<2:38:53,  6.87s/it]                                                        79%|  | 5113/6500 [9:40:00<2:38:53,  6.87s/it] 79%|  | 5114/6500 [9:40:06<2:36:54,  6.79s/it]                                                        79%|  | 5114/6500 [9:40:06<2:36:54,  6.79s/it] 79%|  | 5115/6500 [9:40:13<2:35:15,  6.73s/it]                                                        79%|  | 5115/6500 [9:40:13<2:35:15,  6.73s/it] 79%|{'loss': 0.367, 'learning_rate': 1.0781715762108412e-05, 'epoch': 0.79}
{'loss': 0.6388, 'learning_rate': 1.0766725533751232e-05, 'epoch': 0.79}
{'loss': 0.3788, 'learning_rate': 1.0751744475933411e-05, 'epoch': 0.79}
{'loss': 0.3608, 'learning_rate': 1.0736772592156697e-05, 'epoch': 0.79}
{'loss': 0.3737, 'learning_rate': 1.0721809885920653e-05, 'epoch': 0.79}
  | 5116/6500 [9:40:20<2:34:03,  6.68s/it]                                                        79%|  | 5116/6500 [9:40:20<2:34:03,  6.68s/it] 79%|  | 5117/6500 [9:40:26<2:33:11,  6.65s/it]                                                        79%|  | 5117/6500 [9:40:26<2:33:11,  6.65s/it] 79%|  | 5118/6500 [9:40:33<2:32:34,  6.62s/it]                                                        79%|  | 5118/6500 [9:40:33<2:32:34,  6.62s/it] 79%|  | 5119/6500 [9:40:39<2:32:02,  6.61s/it]                                                        79%|  | 5119/6500 [9:40:39<2:32:02,  6.61s/it] 79%|  | 5120/6500 [9:40:46<2:31:41,  6.60s/it]                                                        79%|  | 5120/6500 [9:40:46<2:31:41,  6.60s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8767977356910706, 'eval_runtime': 1.4826, 'eval_samples_per_second': 8.094, 'eval_steps_per_second': 2.024, 'epoch': 0.79}
                                                        79%|  | 5120/6500 [9:40:47<2:31:41,  6.60s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-5120
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5120
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5120
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5120
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5120
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5120/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5120/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5120/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5120/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3586, 'learning_rate': 1.0706856360722739e-05, 'epoch': 0.79}
{'loss': 0.3629, 'learning_rate': 1.0691912020058236e-05, 'epoch': 0.79}
{'loss': 0.3719, 'learning_rate': 1.0676976867420308e-05, 'epoch': 0.79}
{'loss': 0.3764, 'learning_rate': 1.0662050906299942e-05, 'epoch': 0.79}
{'loss': 0.3651, 'learning_rate': 1.0647134140186004e-05, 'epoch': 0.79}
 79%|  | 5121/6500 [9:40:54<2:44:32,  7.16s/it]                                                        79%|  | 5121/6500 [9:40:54<2:44:32,  7.16s/it] 79%|  | 5122/6500 [9:41:01<2:40:23,  6.98s/it]                                                        79%|  | 5122/6500 [9:41:01<2:40:23,  6.98s/it] 79%|  | 5123/6500 [9:41:08<2:43:24,  7.12s/it]                                                        79%|  | 5123/6500 [9:41:08<2:43:24,  7.12s/it] 79%|  | 5124/6500 [9:41:15<2:39:35,  6.96s/it]                                                        79%|  | 5124/6500 [9:41:15<2:39:35,  6.96s/it] 79%|  | 5125/6500 [9:41:21<2:36:51,  6.85s/it]                                                        79%|  | 5125/6500 [9:41:21<2:36:51,  6.85s/it] 79%|{'loss': 0.37, 'learning_rate': 1.0632226572565191e-05, 'epoch': 0.79}
{'loss': 0.3628, 'learning_rate': 1.0617328206922056e-05, 'epoch': 0.79}
{'loss': 0.3631, 'learning_rate': 1.0602439046738999e-05, 'epoch': 0.79}
{'loss': 0.3519, 'learning_rate': 1.058755909549628e-05, 'epoch': 0.79}
{'loss': 0.3989, 'learning_rate': 1.0572688356672e-05, 'epoch': 0.79}
  | 5126/6500 [9:41:28<2:34:51,  6.76s/it]                                                        79%|  | 5126/6500 [9:41:28<2:34:51,  6.76s/it] 79%|  | 5127/6500 [9:41:35<2:33:27,  6.71s/it]                                                        79%|  | 5127/6500 [9:41:35<2:33:27,  6.71s/it] 79%|  | 5128/6500 [9:41:41<2:32:22,  6.66s/it]                                                        79%|  | 5128/6500 [9:41:41<2:32:22,  6.66s/it] 79%|  | 5129/6500 [9:41:48<2:31:32,  6.63s/it]                                                        79%|  | 5129/6500 [9:41:48<2:31:32,  6.63s/it] 79%|  | 5130/6500 [9:41:54<2:31:04,  6.62s/it]                                                        79%|  | 5130/6500 [9:41:54<2:31:04,  6.62s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.879808783531189, 'eval_runtime': 1.4814, 'eval_samples_per_second': 8.1, 'eval_steps_per_second': 2.025, 'epoch': 0.79}
                                                        79%|  | 5130/6500 [9:41:56<2:31:04,  6.62s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-5130
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5130
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5130
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5130
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5130
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5130/pytorch_model.bin
the pytorch model path isthe pytorch model path is  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5130/pytorch_model.bin/projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5130/pytorch_model.bin

the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5130/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3659, 'learning_rate': 1.0557826833742079e-05, 'epoch': 0.79}
{'loss': 0.6379, 'learning_rate': 1.0542974530180327e-05, 'epoch': 0.79}
{'loss': 0.3783, 'learning_rate': 1.0528131449458372e-05, 'epoch': 0.79}
{'loss': 0.3597, 'learning_rate': 1.0513297595045702e-05, 'epoch': 0.79}
{'loss': 0.3584, 'learning_rate': 1.0498472970409635e-05, 'epoch': 0.79}
 79%|  | 5131/6500 [9:42:03<2:43:23,  7.16s/it]                                                        79%|  | 5131/6500 [9:42:03<2:43:23,  7.16s/it] 79%|  | 5132/6500 [9:42:09<2:39:40,  7.00s/it]                                                        79%|  | 5132/6500 [9:42:09<2:39:40,  7.00s/it] 79%|  | 5133/6500 [9:42:16<2:36:36,  6.87s/it]                                                        79%|  | 5133/6500 [9:42:16<2:36:36,  6.87s/it] 79%|  | 5134/6500 [9:42:23<2:34:19,  6.78s/it]                                                        79%|  | 5134/6500 [9:42:23<2:34:19,  6.78s/it] 79%|  | 5135/6500 [9:42:29<2:32:49,  6.72s/it]                                                        79%|  | 5135/6500 [9:42:29<2:32:49,  6.72s/it] 79%|{'loss': 0.3546, 'learning_rate': 1.0483657579015338e-05, 'epoch': 0.79}
{'loss': 0.3691, 'learning_rate': 1.046885142432582e-05, 'epoch': 0.79}
{'loss': 0.3592, 'learning_rate': 1.0454054509801926e-05, 'epoch': 0.79}
{'loss': 0.3758, 'learning_rate': 1.0439266838902345e-05, 'epoch': 0.79}
{'loss': 0.3751, 'learning_rate': 1.042448841508361e-05, 'epoch': 0.79}
  | 5136/6500 [9:42:36<2:31:47,  6.68s/it]                                                        79%|  | 5136/6500 [9:42:36<2:31:47,  6.68s/it] 79%|  | 5137/6500 [9:42:42<2:31:02,  6.65s/it]                                                        79%|  | 5137/6500 [9:42:42<2:31:02,  6.65s/it] 79%|  | 5138/6500 [9:42:49<2:30:26,  6.63s/it]                                                        79%|  | 5138/6500 [9:42:49<2:30:26,  6.63s/it] 79%|  | 5139/6500 [9:42:56<2:36:52,  6.92s/it]                                                        79%|  | 5139/6500 [9:42:56<2:36:52,  6.92s/it] 79%|  | 5140/6500 [9:43:03<2:34:30,  6.82s/it]                                                        79%|  | 5140/6500 [9:43:03<2:34:30,  6.82s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8793613314628601, 'eval_runtime': 1.4896, 'eval_samples_per_second': 8.056, 'eval_steps_per_second': 2.014, 'epoch': 0.79}
                                                        79%|  | 5140/6500 [9:43:05<2:34:30,  6.82s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-5140
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5140I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5140

I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5140
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5140
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5140/pytorch_model.bin
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5140/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5140/pytorch_model.binthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5140/pytorch_model.bin

/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3665, 'learning_rate': 1.0409719241800098e-05, 'epoch': 0.79}
{'loss': 0.3708, 'learning_rate': 1.0394959322503989e-05, 'epoch': 0.79}
{'loss': 0.3645, 'learning_rate': 1.038020866064533e-05, 'epoch': 0.79}
{'loss': 0.3585, 'learning_rate': 1.0365467259672013e-05, 'epoch': 0.79}
{'loss': 0.397, 'learning_rate': 1.0350735123029736e-05, 'epoch': 0.79}
 79%|  | 5141/6500 [9:43:12<2:45:45,  7.32s/it]                                                        79%|  | 5141/6500 [9:43:12<2:45:45,  7.32s/it] 79%|  | 5142/6500 [9:43:18<2:40:17,  7.08s/it]                                                        79%|  | 5142/6500 [9:43:18<2:40:17,  7.08s/it] 79%|  | 5143/6500 [9:43:25<2:36:27,  6.92s/it]                                                        79%|  | 5143/6500 [9:43:25<2:36:27,  6.92s/it] 79%|  | 5144/6500 [9:43:31<2:33:41,  6.80s/it]                                                        79%|  | 5144/6500 [9:43:31<2:33:41,  6.80s/it] 79%|  | 5145/6500 [9:43:38<2:31:48,  6.72s/it]                                                        79%|  | 5145/6500 [9:43:38<2:31:48,  6.72s/it] 79%|{'loss': 0.3715, 'learning_rate': 1.0336012254162053e-05, 'epoch': 0.79}
{'loss': 0.647, 'learning_rate': 1.0321298656510342e-05, 'epoch': 0.79}
{'loss': 0.3739, 'learning_rate': 1.0306594333513825e-05, 'epoch': 0.79}
{'loss': 0.3764, 'learning_rate': 1.029189928860954e-05, 'epoch': 0.79}
{'loss': 0.3548, 'learning_rate': 1.027721352523237e-05, 'epoch': 0.79}
  | 5146/6500 [9:43:44<2:30:28,  6.67s/it]                                                        79%|  | 5146/6500 [9:43:44<2:30:28,  6.67s/it] 79%|  | 5147/6500 [9:43:51<2:29:24,  6.63s/it]                                                        79%|  | 5147/6500 [9:43:51<2:29:24,  6.63s/it] 79%|  | 5148/6500 [9:43:57<2:28:43,  6.60s/it]                                                        79%|  | 5148/6500 [9:43:57<2:28:43,  6.60s/it] 79%|  | 5149/6500 [9:44:04<2:28:09,  6.58s/it]                                                        79%|  | 5149/6500 [9:44:04<2:28:09,  6.58s/it] 79%|  | 5150/6500 [9:44:10<2:27:50,  6.57s/it]                                                        79%|  | 5150/6500 [9:44:10<2:27:50,  6.57s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8784011602401733, 'eval_runtime': 1.4753, 'eval_samples_per_second': 8.134, 'eval_steps_per_second': 2.033, 'epoch': 0.79}
                                                        79%|  | 5150/6500 [9:44:12<2:27:50,  6.57s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-5150
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5150
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5150
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5150
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5150
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5150/pytorch_model.bin
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5150/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5150/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5150/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3623, 'learning_rate': 1.0262537046815018e-05, 'epoch': 0.79}
{'loss': 0.3672, 'learning_rate': 1.0247869856788023e-05, 'epoch': 0.79}
{'loss': 0.3716, 'learning_rate': 1.0233211958579764e-05, 'epoch': 0.79}
{'loss': 0.3781, 'learning_rate': 1.0218563355616411e-05, 'epoch': 0.79}
{'loss': 0.3722, 'learning_rate': 1.0203924051322e-05, 'epoch': 0.79}
 79%|  | 5151/6500 [9:44:19<2:40:13,  7.13s/it]                                                        79%|  | 5151/6500 [9:44:19<2:40:13,  7.13s/it] 79%|  | 5152/6500 [9:44:25<2:36:15,  6.95s/it]                                                        79%|  | 5152/6500 [9:44:25<2:36:15,  6.95s/it] 79%|  | 5153/6500 [9:44:32<2:33:25,  6.83s/it]                                                        79%|  | 5153/6500 [9:44:32<2:33:25,  6.83s/it] 79%|  | 5154/6500 [9:44:38<2:31:26,  6.75s/it]                                                        79%|  | 5154/6500 [9:44:38<2:31:26,  6.75s/it] 79%|  | 5155/6500 [9:44:45<2:29:52,  6.69s/it]                                                        79%|  | 5155/6500 [9:44:45<2:29:52,  6.69s/it] 79%|{'loss': 0.3642, 'learning_rate': 1.0189294049118374e-05, 'epoch': 0.79}
{'loss': 0.3758, 'learning_rate': 1.0174673352425217e-05, 'epoch': 0.79}
{'loss': 0.3542, 'learning_rate': 1.0160061964660017e-05, 'epoch': 0.79}
{'loss': 0.3645, 'learning_rate': 1.0145459889238106e-05, 'epoch': 0.79}
{'loss': 0.3852, 'learning_rate': 1.0130867129572625e-05, 'epoch': 0.79}
  | 5156/6500 [9:44:52<2:34:51,  6.91s/it]                                                        79%|  | 5156/6500 [9:44:52<2:34:51,  6.91s/it] 79%|  | 5157/6500 [9:44:59<2:32:18,  6.80s/it]                                                        79%|  | 5157/6500 [9:44:59<2:32:18,  6.80s/it] 79%|  | 5158/6500 [9:45:06<2:30:29,  6.73s/it]                                                        79%|  | 5158/6500 [9:45:06<2:30:29,  6.73s/it] 79%|  | 5159/6500 [9:45:12<2:29:07,  6.67s/it]                                                        79%|  | 5159/6500 [9:45:12<2:29:07,  6.67s/it] 79%|  | 5160/6500 [9:45:19<2:28:16,  6.64s/it]                                                        79%|  | 5160/6500 [9:45:19<2:28:16,  6.64s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8776611089706421, 'eval_runtime': 1.4723, 'eval_samples_per_second': 8.151, 'eval_steps_per_second': 2.038, 'epoch': 0.79}
                                                        79%|  | 5160/6500 [9:45:20<2:28:16,  6.64s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-5160
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5160
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5160
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5160
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5160
the pytorch model path is the pytorch model path is/projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5160/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5160/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5160/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5160/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3682, 'learning_rate': 1.011628368907454e-05, 'epoch': 0.79}
{'loss': 0.6471, 'learning_rate': 1.0101709571152651e-05, 'epoch': 0.79}
{'loss': 0.3667, 'learning_rate': 1.0087144779213564e-05, 'epoch': 0.79}
{'loss': 0.3798, 'learning_rate': 1.0072589316661723e-05, 'epoch': 0.79}
{'loss': 0.3646, 'learning_rate': 1.0058043186899351e-05, 'epoch': 0.79}
 79%|  | 5161/6500 [9:45:27<2:39:54,  7.17s/it]                                                        79%|  | 5161/6500 [9:45:27<2:39:54,  7.17s/it] 79%|  | 5162/6500 [9:45:34<2:35:36,  6.98s/it]                                                        79%|  | 5162/6500 [9:45:34<2:35:36,  6.98s/it] 79%|  | 5163/6500 [9:45:40<2:32:39,  6.85s/it]                                                        79%|  | 5163/6500 [9:45:40<2:32:39,  6.85s/it] 79%|  | 5164/6500 [9:45:47<2:31:32,  6.81s/it]                                                        79%|  | 5164/6500 [9:45:47<2:31:32,  6.81s/it] 79%|  | 5165/6500 [9:45:53<2:29:40,  6.73s/it]                                                        79%|  | 5165/6500 [9:45:53<2:29:40,  6.73s/it] 79%|{'loss': 0.3642, 'learning_rate': 1.0043506393326535e-05, 'epoch': 0.79}
{'loss': 0.365, 'learning_rate': 1.0028978939341161e-05, 'epoch': 0.79}
{'loss': 0.3728, 'learning_rate': 1.0014460828338928e-05, 'epoch': 0.8}
{'loss': 0.3813, 'learning_rate': 9.999952063713364e-06, 'epoch': 0.8}
{'loss': 0.3765, 'learning_rate': 9.985452648855803e-06, 'epoch': 0.8}
  | 5166/6500 [9:46:00<2:28:20,  6.67s/it]                                                        79%|  | 5166/6500 [9:46:00<2:28:20,  6.67s/it] 79%|  | 5167/6500 [9:46:06<2:27:24,  6.64s/it]                                                        79%|  | 5167/6500 [9:46:06<2:27:24,  6.64s/it] 80%|  | 5168/6500 [9:46:13<2:26:41,  6.61s/it]                                                        80%|  | 5168/6500 [9:46:13<2:26:41,  6.61s/it] 80%|  | 5169/6500 [9:46:20<2:26:06,  6.59s/it]                                                        80%|  | 5169/6500 [9:46:20<2:26:06,  6.59s/it] 80%|  | 5170/6500 [9:46:26<2:25:43,  6.57s/it]                                                        80%|  | 5170/6500 [9:46:26<2:25:43,  6.57s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8784283399581909, 'eval_runtime': 1.4966, 'eval_samples_per_second': 8.018, 'eval_steps_per_second': 2.005, 'epoch': 0.8}
                                                        80%|  | 5170/6500 [9:46:28<2:25:43,  6.57s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-5170
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5170I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5170

I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5170
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5170
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5170/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5170/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5170/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5170/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3691, 'learning_rate': 9.970962587155386e-06, 'epoch': 0.8}
{'loss': 0.3667, 'learning_rate': 9.95648188199909e-06, 'epoch': 0.8}
{'loss': 0.3503, 'learning_rate': 9.942010536771685e-06, 'epoch': 0.8}
{'loss': 0.393, 'learning_rate': 9.927548554855758e-06, 'epoch': 0.8}
{'loss': 0.3637, 'learning_rate': 9.913095939631722e-06, 'epoch': 0.8}
 80%|  | 5171/6500 [9:46:34<2:37:55,  7.13s/it]                                                        80%|  | 5171/6500 [9:46:34<2:37:55,  7.13s/it] 80%|  | 5172/6500 [9:46:42<2:38:25,  7.16s/it]                                                        80%|  | 5172/6500 [9:46:42<2:38:25,  7.16s/it] 80%|  | 5173/6500 [9:46:48<2:34:05,  6.97s/it]                                                        80%|  | 5173/6500 [9:46:48<2:34:05,  6.97s/it] 80%|  | 5174/6500 [9:46:55<2:31:07,  6.84s/it]                                                        80%|  | 5174/6500 [9:46:55<2:31:07,  6.84s/it] 80%|  | 5175/6500 [9:47:01<2:29:11,  6.76s/it]                                                        80%|  | 5175/6500 [9:47:01<2:29:11,  6.76s/it] 80%|{'loss': 0.587, 'learning_rate': 9.898652694477773e-06, 'epoch': 0.8}
{'loss': 0.4311, 'learning_rate': 9.884218822769931e-06, 'epoch': 0.8}
{'loss': 0.3626, 'learning_rate': 9.869794327882016e-06, 'epoch': 0.8}
{'loss': 0.3807, 'learning_rate': 9.8553792131857e-06, 'epoch': 0.8}
{'loss': 0.3511, 'learning_rate': 9.840973482050403e-06, 'epoch': 0.8}
  | 5176/6500 [9:47:08<2:27:42,  6.69s/it]                                                        80%|  | 5176/6500 [9:47:08<2:27:42,  6.69s/it] 80%|  | 5177/6500 [9:47:14<2:26:43,  6.65s/it]                                                        80%|  | 5177/6500 [9:47:14<2:26:43,  6.65s/it] 80%|  | 5178/6500 [9:47:21<2:26:47,  6.66s/it]                                                        80%|  | 5178/6500 [9:47:21<2:26:47,  6.66s/it] 80%|  | 5179/6500 [9:47:28<2:25:56,  6.63s/it]                                                        80%|  | 5179/6500 [9:47:28<2:25:56,  6.63s/it] 80%|  | 5180/6500 [9:47:34<2:25:16,  6.60s/it]                                                        80%|  | 5180/6500 [9:47:34<2:25:16,  6.60s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8789715766906738, 'eval_runtime': 1.7764, 'eval_samples_per_second': 6.755, 'eval_steps_per_second': 1.689, 'epoch': 0.8}
                                                        80%|  | 5180/6500 [9:47:36<2:25:16,  6.60s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-5180
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5180
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5180
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5180
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5180
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5180/pytorch_model.bin
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5180/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5180/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5180/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3581, 'learning_rate': 9.82657713784339e-06, 'epoch': 0.8}
{'loss': 0.3645, 'learning_rate': 9.81219018392971e-06, 'epoch': 0.8}
{'loss': 0.3596, 'learning_rate': 9.797812623672225e-06, 'epoch': 0.8}
{'loss': 0.3752, 'learning_rate': 9.783444460431624e-06, 'epoch': 0.8}
{'loss': 0.3747, 'learning_rate': 9.769085697566343e-06, 'epoch': 0.8}
 80%|  | 5181/6500 [9:47:43<2:39:33,  7.26s/it]                                                        80%|  | 5181/6500 [9:47:43<2:39:33,  7.26s/it] 80%|  | 5182/6500 [9:47:50<2:34:50,  7.05s/it]                                                        80%|  | 5182/6500 [9:47:50<2:34:50,  7.05s/it] 80%|  | 5183/6500 [9:47:56<2:31:23,  6.90s/it]                                                        80%|  | 5183/6500 [9:47:56<2:31:23,  6.90s/it] 80%|  | 5184/6500 [9:48:03<2:29:03,  6.80s/it]                                                        80%|  | 5184/6500 [9:48:03<2:29:03,  6.80s/it] 80%|  | 5185/6500 [9:48:09<2:27:20,  6.72s/it]                                                        80%|  | 5185/6500 [9:48:09<2:27:20,  6.72s/it] 80%|{'loss': 0.3571, 'learning_rate': 9.75473633843268e-06, 'epoch': 0.8}
{'loss': 0.3735, 'learning_rate': 9.740396386384692e-06, 'epoch': 0.8}
{'loss': 0.3597, 'learning_rate': 9.726065844774274e-06, 'epoch': 0.8}
{'loss': 0.4015, 'learning_rate': 9.711744716951093e-06, 'epoch': 0.8}
{'loss': 0.362, 'learning_rate': 9.697433006262624e-06, 'epoch': 0.8}
  | 5186/6500 [9:48:16<2:26:05,  6.67s/it]                                                        80%|  | 5186/6500 [9:48:16<2:26:05,  6.67s/it] 80%|  | 5187/6500 [9:48:22<2:25:11,  6.63s/it]                                                        80%|  | 5187/6500 [9:48:22<2:25:11,  6.63s/it] 80%|  | 5188/6500 [9:48:29<2:28:34,  6.79s/it]                                                        80%|  | 5188/6500 [9:48:29<2:28:34,  6.79s/it] 80%|  | 5189/6500 [9:48:36<2:26:53,  6.72s/it]                                                        80%|  | 5189/6500 [9:48:36<2:26:53,  6.72s/it] 80%|  | 5190/6500 [9:48:43<2:25:38,  6.67s/it]                                                        80%|  | 5190/6500 [9:48:43<2:25:38,  6.67s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8781107068061829, 'eval_runtime': 1.5014, 'eval_samples_per_second': 7.993, 'eval_steps_per_second': 1.998, 'epoch': 0.8}
                                                        80%|  | 5190/6500 [9:48:44<2:25:38,  6.67s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-5190
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5190
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5190
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5190
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5190
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5190/pytorch_model.bin
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5190/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5190/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5190/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.6401, 'learning_rate': 9.683130716054151e-06, 'epoch': 0.8}
{'loss': 0.3734, 'learning_rate': 9.668837849668744e-06, 'epoch': 0.8}
{'loss': 0.3588, 'learning_rate': 9.65455441044727e-06, 'epoch': 0.8}
{'loss': 0.3726, 'learning_rate': 9.640280401728396e-06, 'epoch': 0.8}
{'loss': 0.3576, 'learning_rate': 9.6260158268486e-06, 'epoch': 0.8}
 80%|  | 5191/6500 [9:48:51<2:37:06,  7.20s/it]                                                        80%|  | 5191/6500 [9:48:51<2:37:06,  7.20s/it] 80%|  | 5192/6500 [9:48:58<2:32:40,  7.00s/it]                                                        80%|  | 5192/6500 [9:48:58<2:32:40,  7.00s/it] 80%|  | 5193/6500 [9:49:04<2:29:40,  6.87s/it]                                                        80%|  | 5193/6500 [9:49:04<2:29:40,  6.87s/it] 80%|  | 5194/6500 [9:49:11<2:27:23,  6.77s/it]                                                        80%|  | 5194/6500 [9:49:11<2:27:23,  6.77s/it] 80%|  | 5195/6500 [9:49:17<2:25:54,  6.71s/it]                                                        80%|  | 5195/6500 [9:49:17<2:25:54,  6.71s/it] 80%|{'loss': 0.3697, 'learning_rate': 9.611760689142114e-06, 'epoch': 0.8}
{'loss': 0.3751, 'learning_rate': 9.597514991941003e-06, 'epoch': 0.8}
{'loss': 0.3758, 'learning_rate': 9.583278738575113e-06, 'epoch': 0.8}
{'loss': 0.3689, 'learning_rate': 9.569051932372081e-06, 'epoch': 0.8}
{'loss': 0.3729, 'learning_rate': 9.554834576657334e-06, 'epoch': 0.8}
  | 5196/6500 [9:49:24<2:24:42,  6.66s/it]                                                        80%|  | 5196/6500 [9:49:24<2:24:42,  6.66s/it] 80%|  | 5197/6500 [9:49:30<2:23:52,  6.63s/it]                                                        80%|  | 5197/6500 [9:49:30<2:23:52,  6.63s/it] 80%|  | 5198/6500 [9:49:37<2:23:15,  6.60s/it]                                                        80%|  | 5198/6500 [9:49:37<2:23:15,  6.60s/it] 80%|  | 5199/6500 [9:49:43<2:22:43,  6.58s/it]                                                        80%|  | 5199/6500 [9:49:43<2:22:43,  6.58s/it] 80%|  | 5200/6500 [9:49:50<2:22:25,  6.57s/it]                                                        80%|  | 5200/6500 [9:49:50<2:22:25,  6.57s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8804014325141907, 'eval_runtime': 1.469, 'eval_samples_per_second': 8.169, 'eval_steps_per_second': 2.042, 'epoch': 0.8}
                                                        80%|  | 5200/6500 [9:49:51<2:22:25,  6.57s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-5200
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5200
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5200
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5200
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5200
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5200/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5200/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5200/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5200/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3648, 'learning_rate': 9.540626674754094e-06, 'epoch': 0.8}
{'loss': 0.3636, 'learning_rate': 9.52642822998337e-06, 'epoch': 0.8}
{'loss': 0.3551, 'learning_rate': 9.512239245663968e-06, 'epoch': 0.8}
{'loss': 0.398, 'learning_rate': 9.498059725112467e-06, 'epoch': 0.8}
{'loss': 0.3695, 'learning_rate': 9.483889671643253e-06, 'epoch': 0.8}
 80%|  | 5201/6500 [9:49:58<2:34:14,  7.12s/it]                                                        80%|  | 5201/6500 [9:49:58<2:34:14,  7.12s/it] 80%|  | 5202/6500 [9:50:05<2:30:20,  6.95s/it]                                                        80%|  | 5202/6500 [9:50:05<2:30:20,  6.95s/it] 80%|  | 5203/6500 [9:50:11<2:27:36,  6.83s/it]                                                        80%|  | 5203/6500 [9:50:11<2:27:36,  6.83s/it] 80%|  | 5204/6500 [9:50:19<2:31:20,  7.01s/it]                                                        80%|  | 5204/6500 [9:50:19<2:31:20,  7.01s/it] 80%|  | 5205/6500 [9:50:25<2:28:19,  6.87s/it]                                                        80%|  | 5205/6500 [9:50:25<2:28:19,  6.87s/it] 80%|{'loss': 0.6344, 'learning_rate': 9.469729088568497e-06, 'epoch': 0.8}
{'loss': 0.3847, 'learning_rate': 9.455577979198127e-06, 'epoch': 0.8}
{'loss': 0.3629, 'learning_rate': 9.441436346839894e-06, 'epoch': 0.8}
{'loss': 0.3559, 'learning_rate': 9.427304194799309e-06, 'epoch': 0.8}
{'loss': 0.3581, 'learning_rate': 9.413181526379683e-06, 'epoch': 0.8}
  | 5206/6500 [9:50:32<2:26:11,  6.78s/it]                                                        80%|  | 5206/6500 [9:50:32<2:26:11,  6.78s/it] 80%|  | 5207/6500 [9:50:39<2:24:32,  6.71s/it]                                                        80%|  | 5207/6500 [9:50:39<2:24:32,  6.71s/it] 80%|  | 5208/6500 [9:50:45<2:23:27,  6.66s/it]                                                        80%|  | 5208/6500 [9:50:45<2:23:27,  6.66s/it] 80%|  | 5209/6500 [9:50:52<2:22:39,  6.63s/it]                                                        80%|  | 5209/6500 [9:50:52<2:22:39,  6.63s/it] 80%|  | 5210/6500 [9:50:58<2:22:00,  6.60s/it]                                                        80%|  | 5210/6500 [9:50:58<2:22:00,  6.60s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8791475892066956, 'eval_runtime': 1.4805, 'eval_samples_per_second': 8.105, 'eval_steps_per_second': 2.026, 'epoch': 0.8}
                                                        80%|  | 5210/6500 [9:51:00<2:22:00,  6.60s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-5210
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5210I AM HERE AND I AM SAVING THE MODEL

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5210
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5210
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5210
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5210/pytorch_model.bin
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5210/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5210/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5210/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.363, 'learning_rate': 9.399068344882106e-06, 'epoch': 0.8}
{'loss': 0.3631, 'learning_rate': 9.384964653605444e-06, 'epoch': 0.8}
{'loss': 0.3721, 'learning_rate': 9.370870455846354e-06, 'epoch': 0.8}
{'loss': 0.3761, 'learning_rate': 9.356785754899262e-06, 'epoch': 0.8}
{'loss': 0.3654, 'learning_rate': 9.342710554056389e-06, 'epoch': 0.8}
 80%|  | 5211/6500 [9:51:07<2:33:26,  7.14s/it]                                                        80%|  | 5211/6500 [9:51:07<2:33:26,  7.14s/it] 80%|  | 5212/6500 [9:51:13<2:30:08,  6.99s/it]                                                        80%|  | 5212/6500 [9:51:13<2:30:08,  6.99s/it] 80%|  | 5213/6500 [9:51:20<2:27:10,  6.86s/it]                                                        80%|  | 5213/6500 [9:51:20<2:27:10,  6.86s/it] 80%|  | 5214/6500 [9:51:26<2:25:07,  6.77s/it]                                                        80%|  | 5214/6500 [9:51:26<2:25:07,  6.77s/it] 80%|  | 5215/6500 [9:51:33<2:23:37,  6.71s/it]                                                        80%|  | 5215/6500 [9:51:33<2:23:37,  6.71s/it] 80%|{'loss': 0.3683, 'learning_rate': 9.32864485660772e-06, 'epoch': 0.8}
{'loss': 0.354, 'learning_rate': 9.314588665841039e-06, 'epoch': 0.8}
{'loss': 0.3523, 'learning_rate': 9.300541985041883e-06, 'epoch': 0.8}
{'loss': 0.3973, 'learning_rate': 9.286504817493574e-06, 'epoch': 0.8}
{'loss': 0.3683, 'learning_rate': 9.272477166477223e-06, 'epoch': 0.8}
  | 5216/6500 [9:51:39<2:22:25,  6.66s/it]                                                        80%|  | 5216/6500 [9:51:39<2:22:25,  6.66s/it] 80%|  | 5217/6500 [9:51:46<2:21:33,  6.62s/it]                                                        80%|  | 5217/6500 [9:51:46<2:21:33,  6.62s/it] 80%|  | 5218/6500 [9:51:53<2:21:02,  6.60s/it]                                                        80%|  | 5218/6500 [9:51:53<2:21:02,  6.60s/it] 80%|  | 5219/6500 [9:51:59<2:20:37,  6.59s/it]                                                        80%|  | 5219/6500 [9:51:59<2:20:37,  6.59s/it] 80%|  | 5220/6500 [9:52:07<2:26:46,  6.88s/it]                                                        80%|  | 5220/6500 [9:52:07<2:26:46,  6.88s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8788020014762878, 'eval_runtime': 1.5289, 'eval_samples_per_second': 7.849, 'eval_steps_per_second': 1.962, 'epoch': 0.8}
                                                        80%|  | 5220/6500 [9:52:08<2:26:46,  6.88s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-5220
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5220
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5220
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5220
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5220
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5220/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5220/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5220/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5220/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.6419, 'learning_rate': 9.25845903527171e-06, 'epoch': 0.8}
{'loss': 0.3767, 'learning_rate': 9.244450427153683e-06, 'epoch': 0.8}
{'loss': 0.3638, 'learning_rate': 9.230451345397568e-06, 'epoch': 0.8}
{'loss': 0.3545, 'learning_rate': 9.216461793275572e-06, 'epoch': 0.8}
{'loss': 0.3556, 'learning_rate': 9.202481774057659e-06, 'epoch': 0.8}
 80%|  | 5221/6500 [9:52:15<2:36:37,  7.35s/it]                                                        80%|  | 5221/6500 [9:52:15<2:36:37,  7.35s/it] 80%|  | 5222/6500 [9:52:22<2:31:20,  7.11s/it]                                                        80%|  | 5222/6500 [9:52:22<2:31:20,  7.11s/it] 80%|  | 5223/6500 [9:52:28<2:27:35,  6.93s/it]                                                        80%|  | 5223/6500 [9:52:28<2:27:35,  6.93s/it] 80%|  | 5224/6500 [9:52:35<2:24:57,  6.82s/it]                                                        80%|  | 5224/6500 [9:52:35<2:24:57,  6.82s/it] 80%|  | 5225/6500 [9:52:41<2:23:05,  6.73s/it]                                                        80%|  | 5225/6500 [9:52:41<2:23:05,  6.73s/it] 80%|{'loss': 0.359, 'learning_rate': 9.188511291011581e-06, 'epoch': 0.8}
{'loss': 0.3607, 'learning_rate': 9.174550347402855e-06, 'epoch': 0.8}
{'loss': 0.3743, 'learning_rate': 9.160598946494769e-06, 'epoch': 0.8}
{'loss': 0.3749, 'learning_rate': 9.14665709154836e-06, 'epoch': 0.8}
{'loss': 0.3684, 'learning_rate': 9.132724785822466e-06, 'epoch': 0.8}
  | 5226/6500 [9:52:48<2:21:47,  6.68s/it]                                                        80%|  | 5226/6500 [9:52:48<2:21:47,  6.68s/it] 80%|  | 5227/6500 [9:52:54<2:20:52,  6.64s/it]                                                        80%|  | 5227/6500 [9:52:54<2:20:52,  6.64s/it] 80%|  | 5228/6500 [9:53:01<2:20:19,  6.62s/it]                                                        80%|  | 5228/6500 [9:53:01<2:20:19,  6.62s/it] 80%|  | 5229/6500 [9:53:08<2:20:19,  6.62s/it]                                                        80%|  | 5229/6500 [9:53:08<2:20:19,  6.62s/it] 80%|  | 5230/6500 [9:53:14<2:19:52,  6.61s/it]                                                        80%|  | 5230/6500 [9:53:14<2:19:52,  6.61s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8808568120002747, 'eval_runtime': 1.5396, 'eval_samples_per_second': 7.794, 'eval_steps_per_second': 1.949, 'epoch': 0.8}
                                                        80%|  | 5230/6500 [9:53:16<2:19:52,  6.61s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-5230
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5230
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5230
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5230
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5230
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5230/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5230/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5230/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5230/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3673, 'learning_rate': 9.118802032573676e-06, 'epoch': 0.8}
{'loss': 0.3567, 'learning_rate': 9.104888835056352e-06, 'epoch': 0.8}
{'loss': 0.365, 'learning_rate': 9.090985196522612e-06, 'epoch': 0.81}
{'loss': 0.3893, 'learning_rate': 9.077091120222353e-06, 'epoch': 0.81}
{'loss': 0.3669, 'learning_rate': 9.063206609403224e-06, 'epoch': 0.81}
 80%|  | 5231/6500 [9:53:23<2:31:44,  7.17s/it]                                                        80%|  | 5231/6500 [9:53:23<2:31:44,  7.17s/it] 80%|  | 5232/6500 [9:53:29<2:27:39,  6.99s/it]                                                        80%|  | 5232/6500 [9:53:29<2:27:39,  6.99s/it] 81%|  | 5233/6500 [9:53:36<2:24:41,  6.85s/it]                                                        81%|  | 5233/6500 [9:53:36<2:24:41,  6.85s/it] 81%|  | 5234/6500 [9:53:42<2:22:42,  6.76s/it]                                                        81%|  | 5234/6500 [9:53:42<2:22:42,  6.76s/it] 81%|  | 5235/6500 [9:53:49<2:21:06,  6.69s/it]                                                        81%|  | 5235/6500 [9:53:49<2:21:06,  6.69s/it] 81%|{'loss': 0.6438, 'learning_rate': 9.049331667310657e-06, 'epoch': 0.81}
{'loss': 0.3628, 'learning_rate': 9.035466297187827e-06, 'epoch': 0.81}
{'loss': 0.3734, 'learning_rate': 9.02161050227568e-06, 'epoch': 0.81}
{'loss': 0.3609, 'learning_rate': 9.007764285812925e-06, 'epoch': 0.81}
{'loss': 0.3597, 'learning_rate': 8.99392765103605e-06, 'epoch': 0.81}
  | 5236/6500 [9:53:56<2:26:51,  6.97s/it]                                                        81%|  | 5236/6500 [9:53:56<2:26:51,  6.97s/it] 81%|  | 5237/6500 [9:54:03<2:24:02,  6.84s/it]                                                        81%|  | 5237/6500 [9:54:03<2:24:02,  6.84s/it] 81%|  | 5238/6500 [9:54:10<2:22:00,  6.75s/it]                                                        81%|  | 5238/6500 [9:54:10<2:22:00,  6.75s/it] 81%|  | 5239/6500 [9:54:16<2:20:37,  6.69s/it]                                                        81%|  | 5239/6500 [9:54:16<2:20:37,  6.69s/it] 81%|  | 5240/6500 [9:54:23<2:19:35,  6.65s/it]                                                        81%|  | 5240/6500 [9:54:23<2:19:35,  6.65s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8800419569015503, 'eval_runtime': 1.4713, 'eval_samples_per_second': 8.156, 'eval_steps_per_second': 2.039, 'epoch': 0.81}
                                                        81%|  | 5240/6500 [9:54:24<2:19:35,  6.65s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-5240
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5240
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5240
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5240
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5240
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5240/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5240/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5240/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5240/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3616, 'learning_rate': 8.980100601179248e-06, 'epoch': 0.81}
{'loss': 0.3747, 'learning_rate': 8.966283139474525e-06, 'epoch': 0.81}
{'loss': 0.3771, 'learning_rate': 8.952475269151628e-06, 'epoch': 0.81}
{'loss': 0.3682, 'learning_rate': 8.938676993438066e-06, 'epoch': 0.81}
{'loss': 0.3655, 'learning_rate': 8.9248883155591e-06, 'epoch': 0.81}
 81%|  | 5241/6500 [9:54:31<2:30:31,  7.17s/it]                                                        81%|  | 5241/6500 [9:54:31<2:30:31,  7.17s/it] 81%|  | 5242/6500 [9:54:38<2:26:24,  6.98s/it]                                                        81%|  | 5242/6500 [9:54:38<2:26:24,  6.98s/it] 81%|  | 5243/6500 [9:54:44<2:23:30,  6.85s/it]                                                        81%|  | 5243/6500 [9:54:44<2:23:30,  6.85s/it] 81%|  | 5244/6500 [9:54:51<2:21:24,  6.76s/it]                                                        81%|  | 5244/6500 [9:54:51<2:21:24,  6.76s/it] 81%|  | 5245/6500 [9:54:57<2:20:01,  6.69s/it]                                                        81%|  | 5245/6500 [9:54:57<2:20:01,  6.69s/it] 81%|{'loss': 0.3727, 'learning_rate': 8.911109238737747e-06, 'epoch': 0.81}
{'loss': 0.3523, 'learning_rate': 8.897339766194785e-06, 'epoch': 0.81}
{'loss': 0.3664, 'learning_rate': 8.883579901148747e-06, 'epoch': 0.81}
{'loss': 0.383, 'learning_rate': 8.869829646815914e-06, 'epoch': 0.81}
{'loss': 0.37, 'learning_rate': 8.856089006410328e-06, 'epoch': 0.81}
  | 5246/6500 [9:55:04<2:18:59,  6.65s/it]                                                        81%|  | 5246/6500 [9:55:04<2:18:59,  6.65s/it] 81%|  | 5247/6500 [9:55:10<2:18:11,  6.62s/it]                                                        81%|  | 5247/6500 [9:55:10<2:18:11,  6.62s/it] 81%|  | 5248/6500 [9:55:17<2:17:33,  6.59s/it]                                                        81%|  | 5248/6500 [9:55:17<2:17:33,  6.59s/it] 81%|  | 5249/6500 [9:55:23<2:17:15,  6.58s/it]                                                        81%|  | 5249/6500 [9:55:23<2:17:15,  6.58s/it] 81%|  | 5250/6500 [9:55:30<2:16:52,  6.57s/it]                                                        81%|  | 5250/6500 [9:55:30<2:16:52,  6.57s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8773261308670044, 'eval_runtime': 1.4709, 'eval_samples_per_second': 8.158, 'eval_steps_per_second': 2.04, 'epoch': 0.81}
                                                        81%|  | 5250/6500 [9:55:31<2:16:52,  6.57s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-5250
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5250
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5250
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5250
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5250
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5250/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5250/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5250/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5250/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.6399, 'learning_rate': 8.842357983143784e-06, 'epoch': 0.81}
{'loss': 0.3655, 'learning_rate': 8.828636580225813e-06, 'epoch': 0.81}
{'loss': 0.3762, 'learning_rate': 8.81492480086371e-06, 'epoch': 0.81}
{'loss': 0.3469, 'learning_rate': 8.801222648262519e-06, 'epoch': 0.81}
{'loss': 0.3559, 'learning_rate': 8.78753012562505e-06, 'epoch': 0.81}
 81%|  | 5251/6500 [9:55:38<2:28:12,  7.12s/it]                                                        81%|  | 5251/6500 [9:55:38<2:28:12,  7.12s/it] 81%|  | 5252/6500 [9:55:45<2:24:30,  6.95s/it]                                                        81%|  | 5252/6500 [9:55:45<2:24:30,  6.95s/it] 81%|  | 5253/6500 [9:55:52<2:25:41,  7.01s/it]                                                        81%|  | 5253/6500 [9:55:52<2:25:41,  7.01s/it] 81%|  | 5254/6500 [9:55:59<2:22:39,  6.87s/it]                                                        81%|  | 5254/6500 [9:55:59<2:22:39,  6.87s/it] 81%|  | 5255/6500 [9:56:05<2:20:28,  6.77s/it]                                                        81%|  | 5255/6500 [9:56:05<2:20:28,  6.77s/it] 81%|{'loss': 0.364, 'learning_rate': 8.773847236151838e-06, 'epoch': 0.81}
{'loss': 0.3605, 'learning_rate': 8.760173983041176e-06, 'epoch': 0.81}
{'loss': 0.3716, 'learning_rate': 8.746510369489103e-06, 'epoch': 0.81}
{'loss': 0.3786, 'learning_rate': 8.73285639868942e-06, 'epoch': 0.81}
{'loss': 0.3632, 'learning_rate': 8.719212073833633e-06, 'epoch': 0.81}
  | 5256/6500 [9:56:12<2:19:05,  6.71s/it]                                                        81%|  | 5256/6500 [9:56:12<2:19:05,  6.71s/it] 81%|  | 5257/6500 [9:56:18<2:17:57,  6.66s/it]                                                        81%|  | 5257/6500 [9:56:18<2:17:57,  6.66s/it] 81%|  | 5258/6500 [9:56:25<2:17:08,  6.63s/it]                                                        81%|  | 5258/6500 [9:56:25<2:17:08,  6.63s/it] 81%|  | 5259/6500 [9:56:31<2:16:27,  6.60s/it]                                                        81%|  | 5259/6500 [9:56:31<2:16:27,  6.60s/it] 81%|  | 5260/6500 [9:56:38<2:16:04,  6.58s/it]                                                        81%|  | 5260/6500 [9:56:38<2:16:04,  6.58s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.880485475063324, 'eval_runtime': 1.7263, 'eval_samples_per_second': 6.951, 'eval_steps_per_second': 1.738, 'epoch': 0.81}
                                                        81%|  | 5260/6500 [9:56:40<2:16:04,  6.58s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-5260
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5260I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5260

I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5260
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5260
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5260/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5260/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5260/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5260/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3696, 'learning_rate': 8.705577398111025e-06, 'epoch': 0.81}
{'loss': 0.3518, 'learning_rate': 8.691952374708634e-06, 'epoch': 0.81}
{'loss': 0.3997, 'learning_rate': 8.678337006811216e-06, 'epoch': 0.81}
{'loss': 0.3555, 'learning_rate': 8.664731297601286e-06, 'epoch': 0.81}
{'loss': 0.6436, 'learning_rate': 8.651135250259091e-06, 'epoch': 0.81}
 81%|  | 5261/6500 [9:56:47<2:29:01,  7.22s/it]                                                        81%|  | 5261/6500 [9:56:47<2:29:01,  7.22s/it] 81%|  | 5262/6500 [9:56:53<2:24:38,  7.01s/it]                                                        81%|  | 5262/6500 [9:56:53<2:24:38,  7.01s/it] 81%|  | 5263/6500 [9:57:00<2:21:39,  6.87s/it]                                                        81%|  | 5263/6500 [9:57:00<2:21:39,  6.87s/it] 81%|  | 5264/6500 [9:57:06<2:19:32,  6.77s/it]                                                        81%|  | 5264/6500 [9:57:06<2:19:32,  6.77s/it] 81%|  | 5265/6500 [9:57:13<2:18:02,  6.71s/it]                                                        81%|  | 5265/6500 [9:57:13<2:18:02,  6.71s/it] 81%|{'loss': 0.3756, 'learning_rate': 8.63754886796262e-06, 'epoch': 0.81}
{'loss': 0.3646, 'learning_rate': 8.623972153887622e-06, 'epoch': 0.81}
{'loss': 0.3695, 'learning_rate': 8.610405111207559e-06, 'epoch': 0.81}
{'loss': 0.3552, 'learning_rate': 8.596847743093645e-06, 'epoch': 0.81}
{'loss': 0.3656, 'learning_rate': 8.583300052714838e-06, 'epoch': 0.81}
  | 5266/6500 [9:57:19<2:16:52,  6.66s/it]                                                        81%|  | 5266/6500 [9:57:19<2:16:52,  6.66s/it] 81%|  | 5267/6500 [9:57:26<2:16:06,  6.62s/it]                                                        81%|  | 5267/6500 [9:57:26<2:16:06,  6.62s/it] 81%|  | 5268/6500 [9:57:32<2:15:30,  6.60s/it]                                                        81%|  | 5268/6500 [9:57:32<2:15:30,  6.60s/it] 81%|  | 5269/6500 [9:57:39<2:18:51,  6.77s/it]                                                        81%|  | 5269/6500 [9:57:39<2:18:51,  6.77s/it] 81%|  | 5270/6500 [9:57:46<2:17:20,  6.70s/it]                                                        81%|  | 5270/6500 [9:57:46<2:17:20,  6.70s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8800567388534546, 'eval_runtime': 1.5016, 'eval_samples_per_second': 7.991, 'eval_steps_per_second': 1.998, 'epoch': 0.81}
                                                        81%|  | 5270/6500 [9:57:48<2:17:20,  6.70s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-5270
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5270
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5270
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5270
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5270
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5270/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5270/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5270/pytorch_model.binthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5270/pytorch_model.bin

/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3642, 'learning_rate': 8.569762043237839e-06, 'epoch': 0.81}
{'loss': 0.3762, 'learning_rate': 8.55623371782705e-06, 'epoch': 0.81}
{'loss': 0.3671, 'learning_rate': 8.542715079644648e-06, 'epoch': 0.81}
{'loss': 0.3716, 'learning_rate': 8.529206131850532e-06, 'epoch': 0.81}
{'loss': 0.3711, 'learning_rate': 8.515706877602336e-06, 'epoch': 0.81}
 81%|  | 5271/6500 [9:57:54<2:28:12,  7.24s/it]                                                        81%|  | 5271/6500 [9:57:54<2:28:12,  7.24s/it] 81%|  | 5272/6500 [9:58:01<2:23:53,  7.03s/it]                                                        81%|  | 5272/6500 [9:58:01<2:23:53,  7.03s/it] 81%|  | 5273/6500 [9:58:08<2:20:44,  6.88s/it]                                                        81%|  | 5273/6500 [9:58:08<2:20:44,  6.88s/it] 81%|  | 5274/6500 [9:58:14<2:18:29,  6.78s/it]                                                        81%|  | 5274/6500 [9:58:14<2:18:29,  6.78s/it] 81%|  | 5275/6500 [9:58:21<2:16:59,  6.71s/it]                                                        81%|  | 5275/6500 [9:58:21<2:16:59,  6.71s/it] 81%|{'loss': 0.3648, 'learning_rate': 8.502217320055427e-06, 'epoch': 0.81}
{'loss': 0.3541, 'learning_rate': 8.48873746236291e-06, 'epoch': 0.81}
{'loss': 0.4061, 'learning_rate': 8.475267307675616e-06, 'epoch': 0.81}
{'loss': 0.3565, 'learning_rate': 8.461806859142119e-06, 'epoch': 0.81}
{'loss': 0.6435, 'learning_rate': 8.448356119908713e-06, 'epoch': 0.81}
  | 5276/6500 [9:58:27<2:15:55,  6.66s/it]                                                        81%|  | 5276/6500 [9:58:27<2:15:55,  6.66s/it] 81%|  | 5277/6500 [9:58:34<2:15:04,  6.63s/it]                                                        81%|  | 5277/6500 [9:58:34<2:15:04,  6.63s/it] 81%|  | 5278/6500 [9:58:40<2:14:28,  6.60s/it]                                                        81%|  | 5278/6500 [9:58:40<2:14:28,  6.60s/it] 81%|  | 5279/6500 [9:58:47<2:14:01,  6.59s/it]                                                        81%|  | 5279/6500 [9:58:47<2:14:01,  6.59s/it] 81%|  | 5280/6500 [9:58:53<2:13:44,  6.58s/it]                                                        81%|  | 5280/6500 [9:58:53<2:13:44,  6.58s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8787552118301392, 'eval_runtime': 1.4755, 'eval_samples_per_second': 8.133, 'eval_steps_per_second': 2.033, 'epoch': 0.81}
                                                        81%|  | 5280/6500 [9:58:55<2:13:44,  6.58s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-5280
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5280
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5280
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5280
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5280
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5280/pytorch_model.bin
the pytorch model path is the pytorch model path is/projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5280/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5280/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5280/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3836, 'learning_rate': 8.434915093119421e-06, 'epoch': 0.81}
{'loss': 0.3549, 'learning_rate': 8.421483781916018e-06, 'epoch': 0.81}
{'loss': 0.3724, 'learning_rate': 8.408062189437971e-06, 'epoch': 0.81}
{'loss': 0.3621, 'learning_rate': 8.3946503188225e-06, 'epoch': 0.81}
{'loss': 0.3593, 'learning_rate': 8.381248173204558e-06, 'epoch': 0.81}
 81%|  | 5281/6500 [9:59:02<2:25:11,  7.15s/it]                                                        81%|  | 5281/6500 [9:59:02<2:25:11,  7.15s/it] 81%| | 5282/6500 [9:59:08<2:21:29,  6.97s/it]                                                        81%| | 5282/6500 [9:59:08<2:21:29,  6.97s/it] 81%| | 5283/6500 [9:59:15<2:18:51,  6.85s/it]                                                        81%| | 5283/6500 [9:59:15<2:18:51,  6.85s/it] 81%| | 5284/6500 [9:59:22<2:16:55,  6.76s/it]                                                        81%| | 5284/6500 [9:59:22<2:16:55,  6.76s/it] 81%| | 5285/6500 [9:59:29<2:20:51,  6.96s/it]                                                        81%| | 5285/6500 [9:59:29<2:20:51,  6.96s/it] 8{'loss': 0.3719, 'learning_rate': 8.367855755716802e-06, 'epoch': 0.81}
{'loss': 0.3804, 'learning_rate': 8.354473069489643e-06, 'epoch': 0.81}
{'loss': 0.3729, 'learning_rate': 8.341100117651191e-06, 'epoch': 0.81}
{'loss': 0.3702, 'learning_rate': 8.327736903327299e-06, 'epoch': 0.81}
{'loss': 0.3731, 'learning_rate': 8.314383429641531e-06, 'epoch': 0.81}
1%| | 5286/6500 [9:59:36<2:18:15,  6.83s/it]                                                        81%| | 5286/6500 [9:59:36<2:18:15,  6.83s/it] 81%| | 5287/6500 [9:59:42<2:16:19,  6.74s/it]                                                        81%| | 5287/6500 [9:59:42<2:16:19,  6.74s/it] 81%| | 5288/6500 [9:59:49<2:15:02,  6.69s/it]                                                        81%| | 5288/6500 [9:59:49<2:15:02,  6.69s/it] 81%| | 5289/6500 [9:59:55<2:14:06,  6.64s/it]                                                        81%| | 5289/6500 [9:59:55<2:14:06,  6.64s/it] 81%| | 5290/6500 [10:00:02<2:13:25,  6.62s/it]                                                         81%| | 5290/6500 [10:00:02<2:13:25,  6.62s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.878642201423645, 'eval_runtime': 1.4815, 'eval_samples_per_second': 8.1, 'eval_steps_per_second': 2.025, 'epoch': 0.81}
                                                         81%| | 5290/6500 [10:00:03<2:13:25,  6.62s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-5290
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5290I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5290

I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5290
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5290
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5290/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5290/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5290/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5290/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3628, 'learning_rate': 8.301039699715185e-06, 'epoch': 0.81}
{'loss': 0.3595, 'learning_rate': 8.287705716667276e-06, 'epoch': 0.81}
{'loss': 0.3994, 'learning_rate': 8.274381483614552e-06, 'epoch': 0.81}
{'loss': 0.3654, 'learning_rate': 8.261067003671447e-06, 'epoch': 0.81}
{'loss': 0.6404, 'learning_rate': 8.247762279950156e-06, 'epoch': 0.81}
 81%| | 5291/6500 [10:00:10<2:24:10,  7.16s/it]                                                         81%| | 5291/6500 [10:00:10<2:24:10,  7.16s/it] 81%| | 5292/6500 [10:00:17<2:21:29,  7.03s/it]                                                         81%| | 5292/6500 [10:00:17<2:21:29,  7.03s/it] 81%| | 5293/6500 [10:00:23<2:18:33,  6.89s/it]                                                         81%| | 5293/6500 [10:00:23<2:18:33,  6.89s/it] 81%| | 5294/6500 [10:00:30<2:16:41,  6.80s/it]                                                         81%| | 5294/6500 [10:00:30<2:16:41,  6.80s/it] 81%| | 5295/6500 [10:00:37<2:15:08,  6.73s/it]                                                         81%| | 5295/6500 [10:00:37<2:1{'loss': 0.3778, 'learning_rate': 8.234467315560573e-06, 'epoch': 0.81}
{'loss': 0.364, 'learning_rate': 8.221182113610314e-06, 'epoch': 0.81}
{'loss': 0.3577, 'learning_rate': 8.207906677204718e-06, 'epoch': 0.82}
{'loss': 0.3555, 'learning_rate': 8.194641009446835e-06, 'epoch': 0.82}
{'loss': 0.3679, 'learning_rate': 8.181385113437439e-06, 'epoch': 0.82}
5:08,  6.73s/it] 81%| | 5296/6500 [10:00:43<2:13:55,  6.67s/it]                                                         81%| | 5296/6500 [10:00:43<2:13:55,  6.67s/it] 81%| | 5297/6500 [10:00:50<2:12:59,  6.63s/it]                                                         81%| | 5297/6500 [10:00:50<2:12:59,  6.63s/it] 82%| | 5298/6500 [10:00:56<2:12:18,  6.60s/it]                                                         82%| | 5298/6500 [10:00:56<2:12:18,  6.60s/it] 82%| | 5299/6500 [10:01:03<2:11:50,  6.59s/it]                                                         82%| | 5299/6500 [10:01:03<2:11:50,  6.59s/it] 82%| | 5300/6500 [10:01:09<2:11:28,  6.57s/it]                                                         82%| | 5300/6500 [10:01:09<2:11:28,  6.57s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8801945447921753, 'eval_runtime': 2.3339, 'eval_samples_per_second': 5.142, 'eval_steps_per_second': 1.285, 'epoch': 0.82}
                                                         82%| | 5300/6500 [10:01:12<2:11:28,  6.57s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-5300
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5300
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5300
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5300
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5300
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5300/pytorch_model.bin
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5300/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5300/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5300/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3626, 'learning_rate': 8.16813899227501e-06, 'epoch': 0.82}
{'loss': 0.3771, 'learning_rate': 8.154902649055746e-06, 'epoch': 0.82}
{'loss': 0.3695, 'learning_rate': 8.141676086873572e-06, 'epoch': 0.82}
{'loss': 0.3643, 'learning_rate': 8.128459308820118e-06, 'epoch': 0.82}
{'loss': 0.3665, 'learning_rate': 8.115252317984707e-06, 'epoch': 0.82}
 82%| | 5301/6500 [10:01:20<2:33:53,  7.70s/it]                                                         82%| | 5301/6500 [10:01:20<2:33:53,  7.70s/it] 82%| | 5302/6500 [10:01:26<2:26:49,  7.35s/it]                                                         82%| | 5302/6500 [10:01:26<2:26:49,  7.35s/it] 82%| | 5303/6500 [10:01:33<2:21:50,  7.11s/it]                                                         82%| | 5303/6500 [10:01:33<2:21:50,  7.11s/it] 82%| | 5304/6500 [10:01:39<2:18:24,  6.94s/it]                                                         82%| | 5304/6500 [10:01:39<2:18:24,  6.94s/it] 82%| | 5305/6500 [10:01:46<2:15:59,  6.83s/it]                                                         82%| | 5305/6500 [10:01:46<2:1{'loss': 0.3575, 'learning_rate': 8.1020551174544e-06, 'epoch': 0.82}
{'loss': 0.3606, 'learning_rate': 8.088867710313969e-06, 'epoch': 0.82}
{'loss': 0.3921, 'learning_rate': 8.075690099645883e-06, 'epoch': 0.82}
{'loss': 0.3689, 'learning_rate': 8.062522288530333e-06, 'epoch': 0.82}
{'loss': 0.6355, 'learning_rate': 8.04936428004522e-06, 'epoch': 0.82}
5:59,  6.83s/it] 82%| | 5306/6500 [10:01:52<2:14:09,  6.74s/it]                                                         82%| | 5306/6500 [10:01:52<2:14:09,  6.74s/it] 82%| | 5307/6500 [10:01:59<2:12:46,  6.68s/it]                                                         82%| | 5307/6500 [10:01:59<2:12:46,  6.68s/it] 82%| | 5308/6500 [10:02:05<2:11:56,  6.64s/it]                                                         82%| | 5308/6500 [10:02:05<2:11:56,  6.64s/it] 82%| | 5309/6500 [10:02:12<2:11:17,  6.61s/it]                                                         82%| | 5309/6500 [10:02:12<2:11:17,  6.61s/it] 82%| | 5310/6500 [10:02:19<2:10:48,  6.60s/it]                                                         82%| | 5310/6500 [10:02:19<2:10:48,  6.60s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.878451406955719, 'eval_runtime': 1.4768, 'eval_samples_per_second': 8.126, 'eval_steps_per_second': 2.031, 'epoch': 0.82}
                                                         82%| | 5310/6500 [10:02:20<2:10:48,  6.60s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-5310
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5310
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5310
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5310
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5310
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5310/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5310/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5310/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5310/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3637, 'learning_rate': 8.036216077266134e-06, 'epoch': 0.82}
{'loss': 0.3711, 'learning_rate': 8.0230776832664e-06, 'epoch': 0.82}
{'loss': 0.3583, 'learning_rate': 8.009949101117037e-06, 'epoch': 0.82}
{'loss': 0.3518, 'learning_rate': 7.996830333886762e-06, 'epoch': 0.82}
{'loss': 0.361, 'learning_rate': 7.983721384642029e-06, 'epoch': 0.82}
 82%| | 5311/6500 [10:02:27<2:22:06,  7.17s/it]                                                         82%| | 5311/6500 [10:02:27<2:22:06,  7.17s/it] 82%| | 5312/6500 [10:02:34<2:18:17,  6.98s/it]                                                         82%| | 5312/6500 [10:02:34<2:18:17,  6.98s/it] 82%| | 5313/6500 [10:02:40<2:15:33,  6.85s/it]                                                         82%| | 5313/6500 [10:02:40<2:15:33,  6.85s/it] 82%| | 5314/6500 [10:02:47<2:13:36,  6.76s/it]                                                         82%| | 5314/6500 [10:02:47<2:13:36,  6.76s/it] 82%| | 5315/6500 [10:02:53<2:12:14,  6.70s/it]                                                         82%| | 5315/6500 [10:02:53<2:1{'loss': 0.3643, 'learning_rate': 7.970622256446946e-06, 'epoch': 0.82}
{'loss': 0.3706, 'learning_rate': 7.957532952363367e-06, 'epoch': 0.82}
{'loss': 0.3683, 'learning_rate': 7.944453475450842e-06, 'epoch': 0.82}
{'loss': 0.3669, 'learning_rate': 7.931383828766609e-06, 'epoch': 0.82}
{'loss': 0.3767, 'learning_rate': 7.918324015365624e-06, 'epoch': 0.82}
2:14,  6.70s/it] 82%| | 5316/6500 [10:03:00<2:11:16,  6.65s/it]                                                         82%| | 5316/6500 [10:03:00<2:11:16,  6.65s/it] 82%| | 5317/6500 [10:03:07<2:16:26,  6.92s/it]                                                         82%| | 5317/6500 [10:03:07<2:16:26,  6.92s/it] 82%| | 5318/6500 [10:03:14<2:14:08,  6.81s/it]                                                         82%| | 5318/6500 [10:03:14<2:14:08,  6.81s/it] 82%| | 5319/6500 [10:03:20<2:12:29,  6.73s/it]                                                         82%| | 5319/6500 [10:03:20<2:12:29,  6.73s/it] 82%| | 5320/6500 [10:03:27<2:11:15,  6.67s/it]                                                         82%| | 5320/6500 [10:03:27<2:11:15,  6.67s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.881692111492157, 'eval_runtime': 1.477, 'eval_samples_per_second': 8.124, 'eval_steps_per_second': 2.031, 'epoch': 0.82}
                                                         82%| | 5320/6500 [10:03:28<2:11:15,  6.67s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-5320
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5320
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5320
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5320
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5320
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5320/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5320/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5320/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5320/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3543, 'learning_rate': 7.905274038300547e-06, 'epoch': 0.82}
{'loss': 0.3705, 'learning_rate': 7.89223390062172e-06, 'epoch': 0.82}
{'loss': 0.3885, 'learning_rate': 7.879203605377201e-06, 'epoch': 0.82}
{'loss': 0.3696, 'learning_rate': 7.866183155612734e-06, 'epoch': 0.82}
{'loss': 0.6481, 'learning_rate': 7.85317255437178e-06, 'epoch': 0.82}
 82%| | 5321/6500 [10:03:35<2:21:38,  7.21s/it]                                                         82%| | 5321/6500 [10:03:35<2:21:38,  7.21s/it] 82%| | 5322/6500 [10:03:42<2:17:34,  7.01s/it]                                                         82%| | 5322/6500 [10:03:42<2:17:34,  7.01s/it] 82%| | 5323/6500 [10:03:49<2:14:46,  6.87s/it]                                                         82%| | 5323/6500 [10:03:49<2:14:46,  6.87s/it] 82%| | 5324/6500 [10:03:55<2:12:43,  6.77s/it]                                                         82%| | 5324/6500 [10:03:55<2:12:43,  6.77s/it] 82%| | 5325/6500 [10:04:02<2:11:17,  6.70s/it]                                                         82%| | 5325/6500 [10:04:02<2:1{'loss': 0.3648, 'learning_rate': 7.84017180469549e-06, 'epoch': 0.82}
{'loss': 0.3838, 'learning_rate': 7.827180909622711e-06, 'epoch': 0.82}
{'loss': 0.3481, 'learning_rate': 7.814199872189964e-06, 'epoch': 0.82}
{'loss': 0.3638, 'learning_rate': 7.801228695431501e-06, 'epoch': 0.82}
{'loss': 0.3657, 'learning_rate': 7.78826738237926e-06, 'epoch': 0.82}
1:17,  6.70s/it] 82%| | 5326/6500 [10:04:08<2:10:16,  6.66s/it]                                                         82%| | 5326/6500 [10:04:08<2:10:16,  6.66s/it] 82%| | 5327/6500 [10:04:15<2:09:24,  6.62s/it]                                                         82%| | 5327/6500 [10:04:15<2:09:24,  6.62s/it] 82%| | 5328/6500 [10:04:21<2:08:48,  6.59s/it]                                                         82%| | 5328/6500 [10:04:21<2:08:48,  6.59s/it] 82%| | 5329/6500 [10:04:28<2:08:25,  6.58s/it]                                                         82%| | 5329/6500 [10:04:28<2:08:25,  6.58s/it] 82%| | 5330/6500 [10:04:34<2:08:07,  6.57s/it]                                                         82%| | 5330/6500 [10:04:34<2:08:07,  6.57s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8786907196044922, 'eval_runtime': 1.4774, 'eval_samples_per_second': 8.123, 'eval_steps_per_second': 2.031, 'epoch': 0.82}
                                                         82%| | 5330/6500 [10:04:36<2:08:07,  6.57s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-5330
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5330
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5330
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5330
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5330
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5330/pytorch_model.bin
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5330/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5330/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5330/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3655, 'learning_rate': 7.775315936062872e-06, 'epoch': 0.82}
{'loss': 0.3732, 'learning_rate': 7.762374359509656e-06, 'epoch': 0.82}
{'loss': 0.3798, 'learning_rate': 7.749442655744621e-06, 'epoch': 0.82}
{'loss': 0.3571, 'learning_rate': 7.736520827790477e-06, 'epoch': 0.82}
{'loss': 0.3697, 'learning_rate': 7.723608878667632e-06, 'epoch': 0.82}
 82%| | 5331/6500 [10:04:43<2:18:35,  7.11s/it]                                                         82%| | 5331/6500 [10:04:43<2:18:35,  7.11s/it] 82%| | 5332/6500 [10:04:49<2:15:10,  6.94s/it]                                                         82%| | 5332/6500 [10:04:49<2:15:10,  6.94s/it] 82%| | 5333/6500 [10:04:56<2:16:18,  7.01s/it]                                                         82%| | 5333/6500 [10:04:56<2:16:18,  7.01s/it] 82%| | 5334/6500 [10:05:03<2:13:29,  6.87s/it]                                                         82%| | 5334/6500 [10:05:03<2:13:29,  6.87s/it] 82%| | 5335/6500 [10:05:09<2:11:23,  6.77s/it]                                                         82%| | 5335/6500 [10:05:09<2:1{'loss': 0.3526, 'learning_rate': 7.710706811394163e-06, 'epoch': 0.82}
{'loss': 0.398, 'learning_rate': 7.69781462898585e-06, 'epoch': 0.82}
{'loss': 0.3508, 'learning_rate': 7.684932334456162e-06, 'epoch': 0.82}
{'loss': 0.6341, 'learning_rate': 7.672059930816266e-06, 'epoch': 0.82}
{'loss': 0.3801, 'learning_rate': 7.659197421075004e-06, 'epoch': 0.82}
1:23,  6.77s/it] 82%| | 5336/6500 [10:05:16<2:09:57,  6.70s/it]                                                         82%| | 5336/6500 [10:05:16<2:09:57,  6.70s/it] 82%| | 5337/6500 [10:05:23<2:08:56,  6.65s/it]                                                         82%| | 5337/6500 [10:05:23<2:08:56,  6.65s/it] 82%| | 5338/6500 [10:05:29<2:08:11,  6.62s/it]                                                         82%| | 5338/6500 [10:05:29<2:08:11,  6.62s/it] 82%| | 5339/6500 [10:05:36<2:07:37,  6.60s/it]                                                         82%| | 5339/6500 [10:05:36<2:07:37,  6.60s/it] 82%| | 5340/6500 [10:05:42<2:07:14,  6.58s/it]                                                         82%| | 5340/6500 [10:05:42<2:07:14,  6.58s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8789248466491699, 'eval_runtime': 1.732, 'eval_samples_per_second': 6.928, 'eval_steps_per_second': 1.732, 'epoch': 0.82}
                                                         82%| | 5340/6500 [10:05:44<2:07:14,  6.58s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-5340
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5340I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5340

I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5340
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5340
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5340/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5340/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5340/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5340/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3675, 'learning_rate': 7.646344808238903e-06, 'epoch': 0.82}
{'loss': 0.3727, 'learning_rate': 7.633502095312196e-06, 'epoch': 0.82}
{'loss': 0.359, 'learning_rate': 7.6206692852967775e-06, 'epoch': 0.82}
{'loss': 0.3654, 'learning_rate': 7.607846381192241e-06, 'epoch': 0.82}
{'loss': 0.3681, 'learning_rate': 7.595033385995865e-06, 'epoch': 0.82}
 82%| | 5341/6500 [10:05:51<2:19:15,  7.21s/it]                                                         82%| | 5341/6500 [10:05:51<2:19:15,  7.21s/it] 82%| | 5342/6500 [10:05:57<2:15:11,  7.00s/it]                                                         82%| | 5342/6500 [10:05:57<2:15:11,  7.00s/it] 82%| | 5343/6500 [10:06:04<2:12:26,  6.87s/it]                                                         82%| | 5343/6500 [10:06:04<2:12:26,  6.87s/it] 82%| | 5344/6500 [10:06:13<2:22:42,  7.41s/it]                                                         82%| | 5344/6500 [10:06:13<2:22:42,  7.41s/it] 82%| | 5345/6500 [10:06:19<2:17:43,  7.15s/it]                                                         82%| | 5345/6500 [10:06:19<2:1{'loss': 0.3686, 'learning_rate': 7.582230302702626e-06, 'epoch': 0.82}
{'loss': 0.3698, 'learning_rate': 7.569437134305129e-06, 'epoch': 0.82}
{'loss': 0.3744, 'learning_rate': 7.556653883793724e-06, 'epoch': 0.82}
{'loss': 0.3615, 'learning_rate': 7.5438805541564185e-06, 'epoch': 0.82}
{'loss': 0.3672, 'learning_rate': 7.531117148378891e-06, 'epoch': 0.82}
7:43,  7.15s/it] 82%| | 5346/6500 [10:06:26<2:13:59,  6.97s/it]                                                         82%| | 5346/6500 [10:06:26<2:13:59,  6.97s/it] 82%| | 5347/6500 [10:06:32<2:11:30,  6.84s/it]                                                         82%| | 5347/6500 [10:06:32<2:11:30,  6.84s/it] 82%| | 5348/6500 [10:06:39<2:09:36,  6.75s/it]                                                         82%| | 5348/6500 [10:06:39<2:09:36,  6.75s/it] 82%| | 5349/6500 [10:06:45<2:08:15,  6.69s/it]                                                         82%| | 5349/6500 [10:06:45<2:08:15,  6.69s/it] 82%| | 5350/6500 [10:06:53<2:11:13,  6.85s/it]                                                         82%| | 5350/6500 [10:06:53<2:11:13,  6.85s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8798438906669617, 'eval_runtime': 1.5044, 'eval_samples_per_second': 7.977, 'eval_steps_per_second': 1.994, 'epoch': 0.82}
                                                         82%| | 5350/6500 [10:06:54<2:11:13,  6.85s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-5350
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5350
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5350
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5350
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5350
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5350/pytorch_model.bin
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5350/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5350/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5350/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3503, 'learning_rate': 7.518363669444517e-06, 'epoch': 0.82}
{'loss': 0.3953, 'learning_rate': 7.505620120334339e-06, 'epoch': 0.82}
{'loss': 0.3599, 'learning_rate': 7.4928865040270915e-06, 'epoch': 0.82}
{'loss': 0.6428, 'learning_rate': 7.480162823499176e-06, 'epoch': 0.82}
{'loss': 0.3782, 'learning_rate': 7.46744908172467e-06, 'epoch': 0.82}
 82%| | 5351/6500 [10:07:01<2:21:05,  7.37s/it]                                                         82%| | 5351/6500 [10:07:01<2:21:05,  7.37s/it] 82%| | 5352/6500 [10:07:08<2:16:08,  7.12s/it]                                                         82%| | 5352/6500 [10:07:08<2:16:08,  7.12s/it] 82%| | 5353/6500 [10:07:14<2:12:45,  6.95s/it]                                                         82%| | 5353/6500 [10:07:14<2:12:45,  6.95s/it] 82%| | 5354/6500 [10:07:21<2:10:19,  6.82s/it]                                                         82%| | 5354/6500 [10:07:21<2:10:19,  6.82s/it] 82%| | 5355/6500 [10:07:27<2:08:38,  6.74s/it]                                                         82%| | 5355/6500 [10:07:27<2:0{'loss': 0.3534, 'learning_rate': 7.454745281675346e-06, 'epoch': 0.82}
{'loss': 0.3746, 'learning_rate': 7.442051426320628e-06, 'epoch': 0.82}
{'loss': 0.3561, 'learning_rate': 7.42936751862765e-06, 'epoch': 0.82}
{'loss': 0.3624, 'learning_rate': 7.4166935615611664e-06, 'epoch': 0.82}
{'loss': 0.3651, 'learning_rate': 7.404029558083653e-06, 'epoch': 0.82}
8:38,  6.74s/it] 82%| | 5356/6500 [10:07:34<2:07:20,  6.68s/it]                                                         82%| | 5356/6500 [10:07:34<2:07:20,  6.68s/it] 82%| | 5357/6500 [10:07:40<2:06:29,  6.64s/it]                                                         82%| | 5357/6500 [10:07:40<2:06:29,  6.64s/it] 82%| | 5358/6500 [10:07:47<2:05:49,  6.61s/it]                                                         82%| | 5358/6500 [10:07:47<2:05:49,  6.61s/it] 82%| | 5359/6500 [10:07:53<2:05:16,  6.59s/it]                                                         82%| | 5359/6500 [10:07:53<2:05:16,  6.59s/it] 82%| | 5360/6500 [10:08:00<2:04:49,  6.57s/it]                                                         82%| | 5360/6500 [10:08:00<2:04:49,  6.57s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8798526525497437, 'eval_runtime': 1.4715, 'eval_samples_per_second': 8.155, 'eval_steps_per_second': 2.039, 'epoch': 0.82}
                                                         82%| | 5360/6500 [10:08:01<2:04:49,  6.57s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-5360
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5360
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5360
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5360
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5360
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5360/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5360/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5360/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5360/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3639, 'learning_rate': 7.391375511155241e-06, 'epoch': 0.82}
{'loss': 0.364, 'learning_rate': 7.378731423733737e-06, 'epoch': 0.82}
{'loss': 0.3668, 'learning_rate': 7.366097298774622e-06, 'epoch': 0.83}
{'loss': 0.3663, 'learning_rate': 7.353473139231049e-06, 'epoch': 0.83}
{'loss': 0.3676, 'learning_rate': 7.340858948053831e-06, 'epoch': 0.83}
 82%| | 5361/6500 [10:08:08<2:15:13,  7.12s/it]                                                         82%| | 5361/6500 [10:08:08<2:15:13,  7.12s/it] 82%| | 5362/6500 [10:08:15<2:11:45,  6.95s/it]                                                         82%| | 5362/6500 [10:08:15<2:11:45,  6.95s/it] 83%| | 5363/6500 [10:08:21<2:09:18,  6.82s/it]                                                         83%| | 5363/6500 [10:08:21<2:09:18,  6.82s/it] 83%| | 5364/6500 [10:08:28<2:07:31,  6.74s/it]                                                         83%| | 5364/6500 [10:08:28<2:07:31,  6.74s/it] 83%| | 5365/6500 [10:08:35<2:06:19,  6.68s/it]                                                         83%| | 5365/6500 [10:08:35<2:0{'loss': 0.3573, 'learning_rate': 7.328254728191464e-06, 'epoch': 0.83}
{'loss': 0.4007, 'learning_rate': 7.315660482590103e-06, 'epoch': 0.83}
{'loss': 0.3648, 'learning_rate': 7.3030762141935825e-06, 'epoch': 0.83}
{'loss': 0.6391, 'learning_rate': 7.290501925943405e-06, 'epoch': 0.83}
{'loss': 0.3767, 'learning_rate': 7.277937620778713e-06, 'epoch': 0.83}
6:19,  6.68s/it] 83%| | 5366/6500 [10:08:42<2:11:10,  6.94s/it]                                                         83%| | 5366/6500 [10:08:42<2:11:10,  6.94s/it] 83%| | 5367/6500 [10:08:49<2:08:51,  6.82s/it]                                                         83%| | 5367/6500 [10:08:49<2:08:51,  6.82s/it] 83%| | 5368/6500 [10:08:55<2:07:13,  6.74s/it]                                                         83%| | 5368/6500 [10:08:55<2:07:13,  6.74s/it] 83%| | 5369/6500 [10:09:02<2:06:04,  6.69s/it]                                                         83%| | 5369/6500 [10:09:02<2:06:04,  6.69s/it] 83%| | 5370/6500 [10:09:08<2:05:14,  6.65s/it]                                                         83%| | 5370/6500 [10:09:08<2:05:14,  6.65s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8798548579216003, 'eval_runtime': 1.4751, 'eval_samples_per_second': 8.135, 'eval_steps_per_second': 2.034, 'epoch': 0.83}
                                                         83%| | 5370/6500 [10:09:10<2:05:14,  6.65s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-5370
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5370
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5370
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5370
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5370
the pytorch model path isthe pytorch model path isthe pytorch model path is  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5370/pytorch_model.bin/projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5370/pytorch_model.bin

 /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5370/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5370/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3576, 'learning_rate': 7.265383301636347e-06, 'epoch': 0.83}
{'loss': 0.3592, 'learning_rate': 7.252838971450804e-06, 'epoch': 0.83}
{'loss': 0.3617, 'learning_rate': 7.240304633154243e-06, 'epoch': 0.83}
{'loss': 0.3698, 'learning_rate': 7.227780289676494e-06, 'epoch': 0.83}
{'loss': 0.3736, 'learning_rate': 7.215265943945038e-06, 'epoch': 0.83}
 83%| | 5371/6500 [10:09:17<2:14:50,  7.17s/it]                                                         83%| | 5371/6500 [10:09:17<2:14:50,  7.17s/it] 83%| | 5372/6500 [10:09:23<2:11:10,  6.98s/it]                                                         83%| | 5372/6500 [10:09:23<2:11:10,  6.98s/it] 83%| | 5373/6500 [10:09:30<2:08:39,  6.85s/it]                                                         83%| | 5373/6500 [10:09:30<2:08:39,  6.85s/it] 83%| | 5374/6500 [10:09:36<2:06:51,  6.76s/it]                                                         83%| | 5374/6500 [10:09:36<2:06:51,  6.76s/it] 83%| | 5375/6500 [10:09:43<2:05:30,  6.69s/it]                                                         83%| | 5375/6500 [10:09:43<2:0{'loss': 0.3779, 'learning_rate': 7.202761598885038e-06, 'epoch': 0.83}
{'loss': 0.3636, 'learning_rate': 7.190267257419297e-06, 'epoch': 0.83}
{'loss': 0.3725, 'learning_rate': 7.1777829224683014e-06, 'epoch': 0.83}
{'loss': 0.372, 'learning_rate': 7.165308596950182e-06, 'epoch': 0.83}
{'loss': 0.3585, 'learning_rate': 7.152844283780752e-06, 'epoch': 0.83}
5:30,  6.69s/it] 83%| | 5376/6500 [10:09:49<2:04:33,  6.65s/it]                                                         83%| | 5376/6500 [10:09:49<2:04:33,  6.65s/it] 83%| | 5377/6500 [10:09:56<2:03:48,  6.61s/it]                                                         83%| | 5377/6500 [10:09:56<2:03:48,  6.61s/it] 83%| | 5378/6500 [10:10:02<2:03:13,  6.59s/it]                                                         83%| | 5378/6500 [10:10:02<2:03:13,  6.59s/it] 83%| | 5379/6500 [10:10:09<2:02:47,  6.57s/it]                                                         83%| | 5379/6500 [10:10:09<2:02:47,  6.57s/it] 83%| | 5380/6500 [10:10:16<2:02:27,  6.56s/it]                                                         83%| | 5380/6500 [10:10:16<2:02:27,  6.56s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8795738220214844, 'eval_runtime': 1.4728, 'eval_samples_per_second': 8.148, 'eval_steps_per_second': 2.037, 'epoch': 0.83}
                                                         83%| | 5380/6500 [10:10:17<2:02:27,  6.56s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-5380
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5380
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5380
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5380
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5380
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5380/pytorch_model.bin
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5380/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5380/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5380/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3615, 'learning_rate': 7.140389985873447e-06, 'epoch': 0.83}
{'loss': 0.3845, 'learning_rate': 7.127945706139388e-06, 'epoch': 0.83}
{'loss': 0.3723, 'learning_rate': 7.115511447487355e-06, 'epoch': 0.83}
{'loss': 0.6376, 'learning_rate': 7.103087212823778e-06, 'epoch': 0.83}
{'loss': 0.3746, 'learning_rate': 7.090673005052751e-06, 'epoch': 0.83}
 83%| | 5381/6500 [10:10:24<2:12:43,  7.12s/it]                                                         83%| | 5381/6500 [10:10:24<2:12:43,  7.12s/it] 83%| | 5382/6500 [10:10:31<2:14:12,  7.20s/it]                                                         83%| | 5382/6500 [10:10:31<2:14:12,  7.20s/it] 83%| | 5383/6500 [10:10:38<2:10:23,  7.00s/it]                                                         83%| | 5383/6500 [10:10:38<2:10:23,  7.00s/it] 83%| | 5384/6500 [10:10:44<2:07:39,  6.86s/it]                                                         83%| | 5384/6500 [10:10:44<2:07:39,  6.86s/it] 83%| | 5385/6500 [10:10:51<2:05:45,  6.77s/it]                                                         83%| | 5385/6500 [10:10:51<2:0{'loss': 0.3627, 'learning_rate': 7.078268827076012e-06, 'epoch': 0.83}
{'loss': 0.3573, 'learning_rate': 7.065874681792966e-06, 'epoch': 0.83}
{'loss': 0.3535, 'learning_rate': 7.053490572100669e-06, 'epoch': 0.83}
{'loss': 0.3621, 'learning_rate': 7.041116500893835e-06, 'epoch': 0.83}
{'loss': 0.3644, 'learning_rate': 7.0287524710648256e-06, 'epoch': 0.83}
5:45,  6.77s/it] 83%| | 5386/6500 [10:10:58<2:04:23,  6.70s/it]                                                         83%| | 5386/6500 [10:10:58<2:04:23,  6.70s/it] 83%| | 5387/6500 [10:11:04<2:03:22,  6.65s/it]                                                         83%| | 5387/6500 [10:11:04<2:03:22,  6.65s/it] 83%| | 5388/6500 [10:11:11<2:02:42,  6.62s/it]                                                         83%| | 5388/6500 [10:11:11<2:02:42,  6.62s/it] 83%| | 5389/6500 [10:11:17<2:02:12,  6.60s/it]                                                         83%| | 5389/6500 [10:11:17<2:02:12,  6.60s/it] 83%| | 5390/6500 [10:11:24<2:01:44,  6.58s/it]                                                         83%| | 5390/6500 [10:11:24<2:01:44,  6.58s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8813006281852722, 'eval_runtime': 1.4706, 'eval_samples_per_second': 8.16, 'eval_steps_per_second': 2.04, 'epoch': 0.83}
                                                         83%| | 5390/6500 [10:11:25<2:01:44,  6.58s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-5390
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5390I AM HERE AND I AM SAVING THE MODEL

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5390
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5390
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5390
the pytorch model path isthe pytorch model path isthe pytorch model path is  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5390/pytorch_model.bin/projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5390/pytorch_model.bin

 /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5390/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5390/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3716, 'learning_rate': 7.016398485503662e-06, 'epoch': 0.83}
{'loss': 0.3758, 'learning_rate': 7.004054547098004e-06, 'epoch': 0.83}
{'loss': 0.3584, 'learning_rate': 6.991720658733169e-06, 'epoch': 0.83}
{'loss': 0.3737, 'learning_rate': 6.979396823292139e-06, 'epoch': 0.83}
{'loss': 0.3503, 'learning_rate': 6.967083043655531e-06, 'epoch': 0.83}
 83%| | 5391/6500 [10:11:32<2:11:37,  7.12s/it]                                                         83%| | 5391/6500 [10:11:32<2:11:37,  7.12s/it] 83%| | 5392/6500 [10:11:39<2:08:16,  6.95s/it]                                                         83%| | 5392/6500 [10:11:39<2:08:16,  6.95s/it] 83%| | 5393/6500 [10:11:45<2:05:58,  6.83s/it]                                                         83%| | 5393/6500 [10:11:45<2:05:58,  6.83s/it] 83%| | 5394/6500 [10:11:52<2:04:17,  6.74s/it]                                                         83%| | 5394/6500 [10:11:52<2:04:17,  6.74s/it] 83%| | 5395/6500 [10:11:58<2:02:59,  6.68s/it]                                                         83%| | 5395/6500 [10:11:58<2:0{'loss': 0.3585, 'learning_rate': 6.954779322701621e-06, 'epoch': 0.83}
{'loss': 0.3879, 'learning_rate': 6.9424856633063195e-06, 'epoch': 0.83}
{'loss': 0.3709, 'learning_rate': 6.9302020683432055e-06, 'epoch': 0.83}
{'loss': 0.6393, 'learning_rate': 6.917928540683483e-06, 'epoch': 0.83}
{'loss': 0.363, 'learning_rate': 6.905665083196028e-06, 'epoch': 0.83}
2:59,  6.68s/it] 83%| | 5396/6500 [10:12:05<2:02:03,  6.63s/it]                                                         83%| | 5396/6500 [10:12:05<2:02:03,  6.63s/it] 83%| | 5397/6500 [10:12:11<2:01:23,  6.60s/it]                                                         83%| | 5397/6500 [10:12:11<2:01:23,  6.60s/it] 83%| | 5398/6500 [10:12:19<2:05:57,  6.86s/it]                                                         83%| | 5398/6500 [10:12:19<2:05:57,  6.86s/it] 83%| | 5399/6500 [10:12:25<2:04:11,  6.77s/it]                                                         83%| | 5399/6500 [10:12:25<2:04:11,  6.77s/it] 83%| | 5400/6500 [10:12:32<2:02:50,  6.70s/it]                                                         83%| | 5400/6500 [10:12:32<2:02:50,  6.70s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8794369697570801, 'eval_runtime': 1.4765, 'eval_samples_per_second': 8.128, 'eval_steps_per_second': 2.032, 'epoch': 0.83}
                                                         83%| | 5400/6500 [10:12:33<2:02:50,  6.70s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-5400
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5400
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5400
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5400
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5400
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5400/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5400/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5400/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5400/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3741, 'learning_rate': 6.893411698747337e-06, 'epoch': 0.83}
{'loss': 0.3491, 'learning_rate': 6.881168390201581e-06, 'epoch': 0.83}
{'loss': 0.3561, 'learning_rate': 6.868935160420537e-06, 'epoch': 0.83}
{'loss': 0.3621, 'learning_rate': 6.856712012263655e-06, 'epoch': 0.83}
{'loss': 0.3632, 'learning_rate': 6.844498948588018e-06, 'epoch': 0.83}
 83%| | 5401/6500 [10:12:40<2:12:07,  7.21s/it]                                                         83%| | 5401/6500 [10:12:40<2:12:07,  7.21s/it] 83%| | 5402/6500 [10:12:47<2:08:15,  7.01s/it]                                                         83%| | 5402/6500 [10:12:47<2:08:15,  7.01s/it] 83%| | 5403/6500 [10:12:53<2:05:36,  6.87s/it]                                                         83%| | 5403/6500 [10:12:53<2:05:36,  6.87s/it] 83%| | 5404/6500 [10:13:00<2:03:39,  6.77s/it]                                                         83%| | 5404/6500 [10:13:00<2:03:39,  6.77s/it] 83%| | 5405/6500 [10:13:06<2:02:16,  6.70s/it]                                                         83%| | 5405/6500 [10:13:06<2:0{'loss': 0.3759, 'learning_rate': 6.83229597224837e-06, 'epoch': 0.83}
{'loss': 0.3755, 'learning_rate': 6.820103086097074e-06, 'epoch': 0.83}
{'loss': 0.3637, 'learning_rate': 6.807920292984144e-06, 'epoch': 0.83}
{'loss': 0.3688, 'learning_rate': 6.795747595757235e-06, 'epoch': 0.83}
{'loss': 0.3506, 'learning_rate': 6.7835849972616386e-06, 'epoch': 0.83}
2:16,  6.70s/it] 83%| | 5406/6500 [10:13:13<2:01:14,  6.65s/it]                                                         83%| | 5406/6500 [10:13:13<2:01:14,  6.65s/it] 83%| | 5407/6500 [10:13:19<2:00:28,  6.61s/it]                                                         83%| | 5407/6500 [10:13:19<2:00:28,  6.61s/it] 83%| | 5408/6500 [10:13:26<2:00:00,  6.59s/it]                                                         83%| | 5408/6500 [10:13:26<2:00:00,  6.59s/it] 83%| | 5409/6500 [10:13:33<1:59:31,  6.57s/it]                                                         83%| | 5409/6500 [10:13:33<1:59:31,  6.57s/it] 83%| | 5410/6500 [10:13:39<1:59:14,  6.56s/it]                                                         83%| | 5410/6500 [10:13:39<1:59:14,  6.56s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8801583051681519, 'eval_runtime': 1.477, 'eval_samples_per_second': 8.124, 'eval_steps_per_second': 2.031, 'epoch': 0.83}
                                                         83%| | 5410/6500 [10:13:41<1:59:14,  6.56s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-5410
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5410
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5410
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5410
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5410
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5410/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5410/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5410/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5410/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3894, 'learning_rate': 6.771432500340302e-06, 'epoch': 0.83}
{'loss': 0.3562, 'learning_rate': 6.759290107833771e-06, 'epoch': 0.83}
{'loss': 0.5334, 'learning_rate': 6.747157822580269e-06, 'epoch': 0.83}
{'loss': 0.4788, 'learning_rate': 6.735035647415644e-06, 'epoch': 0.83}
{'loss': 0.3596, 'learning_rate': 6.722923585173385e-06, 'epoch': 0.83}
 83%| | 5411/6500 [10:13:48<2:09:19,  7.12s/it]                                                         83%| | 5411/6500 [10:13:48<2:09:19,  7.12s/it] 83%| | 5412/6500 [10:13:54<2:06:04,  6.95s/it]                                                         83%| | 5412/6500 [10:13:54<2:06:04,  6.95s/it] 83%| | 5413/6500 [10:14:01<2:03:46,  6.83s/it]                                                         83%| | 5413/6500 [10:14:01<2:03:46,  6.83s/it] 83%| | 5414/6500 [10:14:08<2:05:27,  6.93s/it]                                                         83%| | 5414/6500 [10:14:08<2:05:27,  6.93s/it] 83%| | 5415/6500 [10:14:14<2:03:14,  6.81s/it]                                                         83%| | 5415/6500 [10:14:14<2:0{'loss': 0.3795, 'learning_rate': 6.710821638684606e-06, 'epoch': 0.83}
{'loss': 0.3549, 'learning_rate': 6.698729810778065e-06, 'epoch': 0.83}
{'loss': 0.3635, 'learning_rate': 6.6866481042801575e-06, 'epoch': 0.83}
{'loss': 0.365, 'learning_rate': 6.674576522014908e-06, 'epoch': 0.83}
{'loss': 0.3644, 'learning_rate': 6.66251506680397e-06, 'epoch': 0.83}
3:14,  6.81s/it] 83%| | 5416/6500 [10:14:21<2:01:43,  6.74s/it]                                                         83%| | 5416/6500 [10:14:21<2:01:43,  6.74s/it] 83%| | 5417/6500 [10:14:27<2:00:34,  6.68s/it]                                                         83%| | 5417/6500 [10:14:27<2:00:34,  6.68s/it] 83%| | 5418/6500 [10:14:34<1:59:51,  6.65s/it]                                                         83%| | 5418/6500 [10:14:34<1:59:51,  6.65s/it] 83%| | 5419/6500 [10:14:41<1:59:13,  6.62s/it]                                                         83%| | 5419/6500 [10:14:41<1:59:13,  6.62s/it] 83%| | 5420/6500 [10:14:47<1:58:47,  6.60s/it]                                                         83%| | 5420/6500 [10:14:47<1:58:47,  6.60s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.880634069442749, 'eval_runtime': 1.7378, 'eval_samples_per_second': 6.905, 'eval_steps_per_second': 1.726, 'epoch': 0.83}
                                                         83%| | 5420/6500 [10:14:49<1:58:47,  6.60s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-5420
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5420/projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5420

I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5420
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5420
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5420/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5420/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5420/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5420/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.377, 'learning_rate': 6.6504637414666395e-06, 'epoch': 0.83}
{'loss': 0.373, 'learning_rate': 6.638422548819851e-06, 'epoch': 0.83}
{'loss': 0.3605, 'learning_rate': 6.626391491678136e-06, 'epoch': 0.83}
{'loss': 0.3663, 'learning_rate': 6.614370572853695e-06, 'epoch': 0.83}
{'loss': 0.3526, 'learning_rate': 6.602359795156348e-06, 'epoch': 0.83}
 83%| | 5421/6500 [10:14:56<2:10:04,  7.23s/it]                                                         83%| | 5421/6500 [10:14:56<2:10:04,  7.23s/it] 83%| | 5422/6500 [10:15:02<2:06:16,  7.03s/it]                                                         83%| | 5422/6500 [10:15:02<2:06:16,  7.03s/it] 83%| | 5423/6500 [10:15:09<2:03:36,  6.89s/it]                                                         83%| | 5423/6500 [10:15:09<2:03:36,  6.89s/it] 83%| | 5424/6500 [10:15:15<2:01:37,  6.78s/it]                                                         83%| | 5424/6500 [10:15:15<2:01:37,  6.78s/it] 83%| | 5425/6500 [10:15:22<2:00:14,  6.71s/it]                                                         83%| | 5425/6500 [10:15:22<2:0{'loss': 0.3954, 'learning_rate': 6.590359161393533e-06, 'epoch': 0.83}
{'loss': 0.3576, 'learning_rate': 6.578368674370328e-06, 'epoch': 0.83}
{'loss': 0.6379, 'learning_rate': 6.56638833688944e-06, 'epoch': 0.84}
{'loss': 0.3731, 'learning_rate': 6.554418151751196e-06, 'epoch': 0.84}
{'loss': 0.3579, 'learning_rate': 6.542458121753558e-06, 'epoch': 0.84}
0:14,  6.71s/it] 83%| | 5426/6500 [10:15:29<1:59:16,  6.66s/it]                                                         83%| | 5426/6500 [10:15:29<1:59:16,  6.66s/it] 83%| | 5427/6500 [10:15:35<1:58:35,  6.63s/it]                                                         83%| | 5427/6500 [10:15:35<1:58:35,  6.63s/it] 84%| | 5428/6500 [10:15:42<1:58:01,  6.61s/it]                                                         84%| | 5428/6500 [10:15:42<1:58:01,  6.61s/it] 84%| | 5429/6500 [10:15:48<1:57:40,  6.59s/it]                                                         84%| | 5429/6500 [10:15:48<1:57:40,  6.59s/it] 84%| | 5430/6500 [10:15:55<2:00:29,  6.76s/it]                                                         84%| | 5430/6500 [10:15:55<2:00:29,  6.76s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8788519501686096, 'eval_runtime': 1.4922, 'eval_samples_per_second': 8.042, 'eval_steps_per_second': 2.01, 'epoch': 0.84}
                                                         84%| | 5430/6500 [10:15:57<2:00:29,  6.76s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-5430
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5430
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5430
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5430
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5430
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5430/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5430/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5430/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5430/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3702, 'learning_rate': 6.530508249692107e-06, 'epoch': 0.84}
{'loss': 0.3507, 'learning_rate': 6.518568538360054e-06, 'epoch': 0.84}
{'loss': 0.3587, 'learning_rate': 6.506638990548242e-06, 'epoch': 0.84}
{'loss': 0.3644, 'learning_rate': 6.494719609045113e-06, 'epoch': 0.84}
{'loss': 0.3651, 'learning_rate': 6.482810396636757e-06, 'epoch': 0.84}
 84%| | 5431/6500 [10:16:04<2:09:39,  7.28s/it]                                                         84%| | 5431/6500 [10:16:04<2:09:39,  7.28s/it] 84%| | 5432/6500 [10:16:10<2:05:40,  7.06s/it]                                                         84%| | 5432/6500 [10:16:10<2:05:40,  7.06s/it] 84%| | 5433/6500 [10:16:17<2:02:52,  6.91s/it]                                                         84%| | 5433/6500 [10:16:17<2:02:52,  6.91s/it] 84%| | 5434/6500 [10:16:24<2:00:50,  6.80s/it]                                                         84%| | 5434/6500 [10:16:24<2:00:50,  6.80s/it] 84%| | 5435/6500 [10:16:30<1:59:25,  6.73s/it]                                                         84%| | 5435/6500 [10:16:30<1:5{'loss': 0.3657, 'learning_rate': 6.470911356106885e-06, 'epoch': 0.84}
{'loss': 0.3686, 'learning_rate': 6.4590224902368215e-06, 'epoch': 0.84}
{'loss': 0.3619, 'learning_rate': 6.447143801805516e-06, 'epoch': 0.84}
{'loss': 0.3661, 'learning_rate': 6.4352752935895435e-06, 'epoch': 0.84}
{'loss': 0.3502, 'learning_rate': 6.423416968363088e-06, 'epoch': 0.84}
9:25,  6.73s/it] 84%| | 5436/6500 [10:16:37<1:58:24,  6.68s/it]                                                         84%| | 5436/6500 [10:16:37<1:58:24,  6.68s/it] 84%| | 5437/6500 [10:16:43<1:57:43,  6.65s/it]                                                         84%| | 5437/6500 [10:16:43<1:57:43,  6.65s/it] 84%| | 5438/6500 [10:16:50<1:57:12,  6.62s/it]                                                         84%| | 5438/6500 [10:16:50<1:57:12,  6.62s/it] 84%| | 5439/6500 [10:16:56<1:56:44,  6.60s/it]                                                         84%| | 5439/6500 [10:16:56<1:56:44,  6.60s/it] 84%| | 5440/6500 [10:17:03<1:56:23,  6.59s/it]                                                         84%| | 5440/6500 [10:17:03<1:56:23,  6.59s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8800920248031616, 'eval_runtime': 1.4771, 'eval_samples_per_second': 8.124, 'eval_steps_per_second': 2.031, 'epoch': 0.84}
                                                         84%| | 5440/6500 [10:17:04<1:56:23,  6.59s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-5440
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5440
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5440
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5440
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5440
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5440/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5440/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5440/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5440/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.4011, 'learning_rate': 6.411568828897973e-06, 'epoch': 0.84}
{'loss': 0.3704, 'learning_rate': 6.399730877963617e-06, 'epoch': 0.84}
{'loss': 0.6307, 'learning_rate': 6.387903118327077e-06, 'epoch': 0.84}
{'loss': 0.3792, 'learning_rate': 6.37608555275302e-06, 'epoch': 0.84}
{'loss': 0.361, 'learning_rate': 6.36427818400373e-06, 'epoch': 0.84}
 84%| | 5441/6500 [10:17:11<2:06:22,  7.16s/it]                                                         84%| | 5441/6500 [10:17:11<2:06:22,  7.16s/it] 84%| | 5442/6500 [10:17:18<2:03:03,  6.98s/it]                                                         84%| | 5442/6500 [10:17:18<2:03:03,  6.98s/it] 84%| | 5443/6500 [10:17:24<2:00:39,  6.85s/it]                                                         84%| | 5443/6500 [10:17:24<2:00:39,  6.85s/it] 84%| | 5444/6500 [10:17:31<1:59:02,  6.76s/it]                                                         84%| | 5444/6500 [10:17:31<1:59:02,  6.76s/it] 84%| | 5445/6500 [10:17:38<1:57:48,  6.70s/it]                                                         84%| | 5445/6500 [10:17:38<1:5{'loss': 0.358, 'learning_rate': 6.352481014839101e-06, 'epoch': 0.84}
{'loss': 0.3603, 'learning_rate': 6.340694048016649e-06, 'epoch': 0.84}
{'loss': 0.3662, 'learning_rate': 6.328917286291514e-06, 'epoch': 0.84}
{'loss': 0.3609, 'learning_rate': 6.317150732416438e-06, 'epoch': 0.84}
{'loss': 0.3767, 'learning_rate': 6.305394389141784e-06, 'epoch': 0.84}
7:48,  6.70s/it] 84%| | 5446/6500 [10:17:44<1:56:53,  6.65s/it]                                                         84%| | 5446/6500 [10:17:44<1:56:53,  6.65s/it] 84%| | 5447/6500 [10:17:52<2:00:55,  6.89s/it]                                                         84%| | 5447/6500 [10:17:52<2:00:55,  6.89s/it] 84%| | 5448/6500 [10:17:58<1:59:02,  6.79s/it]                                                         84%| | 5448/6500 [10:17:58<1:59:02,  6.79s/it] 84%| | 5449/6500 [10:18:05<1:57:37,  6.71s/it]                                                         84%| | 5449/6500 [10:18:05<1:57:37,  6.71s/it] 84%| | 5450/6500 [10:18:11<1:56:41,  6.67s/it]                                                         84%| | 5450/6500 [10:18:11<1:56:41,  6.67s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8795478940010071, 'eval_runtime': 1.4819, 'eval_samples_per_second': 8.098, 'eval_steps_per_second': 2.024, 'epoch': 0.84}
                                                         84%| | 5450/6500 [10:18:13<1:56:41,  6.67s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-5450
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5450I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5450

I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5450
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5450
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5450/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5450/pytorch_model.binthe pytorch model path is
 /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5450/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5450/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3709, 'learning_rate': 6.293648259215517e-06, 'epoch': 0.84}
{'loss': 0.3646, 'learning_rate': 6.281912345383239e-06, 'epoch': 0.84}
{'loss': 0.3775, 'learning_rate': 6.270186650388133e-06, 'epoch': 0.84}
{'loss': 0.3604, 'learning_rate': 6.25847117697102e-06, 'epoch': 0.84}
{'loss': 0.3531, 'learning_rate': 6.246765927870313e-06, 'epoch': 0.84}
 84%| | 5451/6500 [10:18:20<2:05:43,  7.19s/it]                                                         84%| | 5451/6500 [10:18:20<2:05:43,  7.19s/it] 84%| | 5452/6500 [10:18:26<2:02:14,  7.00s/it]                                                         84%| | 5452/6500 [10:18:26<2:02:14,  7.00s/it] 84%| | 5453/6500 [10:18:33<1:59:47,  6.86s/it]                                                         84%| | 5453/6500 [10:18:33<1:59:47,  6.86s/it] 84%| | 5454/6500 [10:18:39<1:58:00,  6.77s/it]                                                         84%| | 5454/6500 [10:18:39<1:58:00,  6.77s/it] 84%| | 5455/6500 [10:18:46<1:56:43,  6.70s/it]                                                         84%| | 5455/6500 [10:18:46<1:5{'loss': 0.3992, 'learning_rate': 6.23507090582206e-06, 'epoch': 0.84}
{'loss': 0.3663, 'learning_rate': 6.2233861135598756e-06, 'epoch': 0.84}
{'loss': 0.642, 'learning_rate': 6.211711553815025e-06, 'epoch': 0.84}
{'loss': 0.3769, 'learning_rate': 6.200047229316358e-06, 'epoch': 0.84}
{'loss': 0.3647, 'learning_rate': 6.188393142790344e-06, 'epoch': 0.84}
6:43,  6.70s/it] 84%| | 5456/6500 [10:18:52<1:55:46,  6.65s/it]                                                         84%| | 5456/6500 [10:18:52<1:55:46,  6.65s/it] 84%| | 5457/6500 [10:18:59<1:55:08,  6.62s/it]                                                         84%| | 5457/6500 [10:18:59<1:55:08,  6.62s/it] 84%| | 5458/6500 [10:19:05<1:54:36,  6.60s/it]                                                         84%| | 5458/6500 [10:19:05<1:54:36,  6.60s/it] 84%| | 5459/6500 [10:19:12<1:54:15,  6.59s/it]                                                         84%| | 5459/6500 [10:19:12<1:54:15,  6.59s/it] 84%| | 5460/6500 [10:19:19<1:53:55,  6.57s/it]                                                         84%| | 5460/6500 [10:19:19<1:53:55,  6.57s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8793253898620605, 'eval_runtime': 1.4742, 'eval_samples_per_second': 8.14, 'eval_steps_per_second': 2.035, 'epoch': 0.84}
                                                         84%| | 5460/6500 [10:19:20<1:53:55,  6.57s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-5460
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5460
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5460
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5460
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5460
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5460/pytorch_model.bin
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5460/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5460/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5460/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3586, 'learning_rate': 6.176749296961054e-06, 'epoch': 0.84}
{'loss': 0.363, 'learning_rate': 6.165115694550161e-06, 'epoch': 0.84}
{'loss': 0.3616, 'learning_rate': 6.1534923382769615e-06, 'epoch': 0.84}
{'loss': 0.3654, 'learning_rate': 6.141879230858333e-06, 'epoch': 0.84}
{'loss': 0.3819, 'learning_rate': 6.130276375008775e-06, 'epoch': 0.84}
 84%| | 5461/6500 [10:19:27<2:03:16,  7.12s/it]                                                         84%| | 5461/6500 [10:19:27<2:03:16,  7.12s/it] 84%| | 5462/6500 [10:19:33<2:00:06,  6.94s/it]                                                         84%| | 5462/6500 [10:19:34<2:00:06,  6.94s/it] 84%| | 5463/6500 [10:19:41<2:02:21,  7.08s/it]                                                         84%| | 5463/6500 [10:19:41<2:02:21,  7.08s/it] 84%| | 5464/6500 [10:19:47<1:59:31,  6.92s/it]                                                         84%| | 5464/6500 [10:19:47<1:59:31,  6.92s/it] 84%| | 5465/6500 [10:19:54<1:57:24,  6.81s/it]                                                         84%| | 5465/6500 [10:19:54<1:5{'loss': 0.3775, 'learning_rate': 6.118683773440376e-06, 'epoch': 0.84}
{'loss': 0.3685, 'learning_rate': 6.107101428862861e-06, 'epoch': 0.84}
{'loss': 0.3717, 'learning_rate': 6.095529343983497e-06, 'epoch': 0.84}
{'loss': 0.3577, 'learning_rate': 6.083967521507206e-06, 'epoch': 0.84}
{'loss': 0.3643, 'learning_rate': 6.072415964136496e-06, 'epoch': 0.84}
7:24,  6.81s/it] 84%| | 5466/6500 [10:20:01<1:55:55,  6.73s/it]                                                         84%| | 5466/6500 [10:20:01<1:55:55,  6.73s/it] 84%| | 5467/6500 [10:20:07<1:55:06,  6.69s/it]                                                         84%| | 5467/6500 [10:20:07<1:55:06,  6.69s/it] 84%| | 5468/6500 [10:20:14<1:54:12,  6.64s/it]                                                         84%| | 5468/6500 [10:20:14<1:54:12,  6.64s/it] 84%| | 5469/6500 [10:20:20<1:53:27,  6.60s/it]                                                         84%| | 5469/6500 [10:20:20<1:53:27,  6.60s/it] 84%| | 5470/6500 [10:20:27<1:53:01,  6.58s/it]                                                         84%| | 5470/6500 [10:20:27<1:53:01,  6.58s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8798912763595581, 'eval_runtime': 1.4765, 'eval_samples_per_second': 8.127, 'eval_steps_per_second': 2.032, 'epoch': 0.84}
                                                         84%| | 5470/6500 [10:20:28<1:53:01,  6.58s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-5470
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5470I AM HERE AND I AM SAVING THE MODEL

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5470
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5470
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5470
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5470/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5470/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5470/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5470/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3932, 'learning_rate': 6.06087467457147e-06, 'epoch': 0.84}
{'loss': 0.3699, 'learning_rate': 6.049343655509831e-06, 'epoch': 0.84}
{'loss': 0.64, 'learning_rate': 6.03782290964689e-06, 'epoch': 0.84}
{'loss': 0.3611, 'learning_rate': 6.026312439675552e-06, 'epoch': 0.84}
{'loss': 0.3728, 'learning_rate': 6.0148122482863115e-06, 'epoch': 0.84}
 84%| | 5471/6500 [10:20:35<2:02:16,  7.13s/it]                                                         84%| | 5471/6500 [10:20:35<2:02:16,  7.13s/it] 84%| | 5472/6500 [10:20:42<1:59:07,  6.95s/it]                                                         84%| | 5472/6500 [10:20:42<1:59:07,  6.95s/it] 84%| | 5473/6500 [10:20:48<1:56:51,  6.83s/it]                                                         84%| | 5473/6500 [10:20:48<1:56:51,  6.83s/it] 84%| | 5474/6500 [10:20:55<1:55:15,  6.74s/it]                                                         84%| | 5474/6500 [10:20:55<1:55:15,  6.74s/it] 84%| | 5475/6500 [10:21:01<1:54:05,  6.68s/it]                                                         84%| | 5475/6500 [10:21:01<1:5{'loss': 0.3593, 'learning_rate': 6.003322338167277e-06, 'epoch': 0.84}
{'loss': 0.3559, 'learning_rate': 5.991842712004142e-06, 'epoch': 0.84}
{'loss': 0.3574, 'learning_rate': 5.980373372480208e-06, 'epoch': 0.84}
{'loss': 0.3749, 'learning_rate': 5.968914322276348e-06, 'epoch': 0.84}
{'loss': 0.3745, 'learning_rate': 5.957465564071035e-06, 'epoch': 0.84}
4:05,  6.68s/it] 84%| | 5476/6500 [10:21:08<1:53:31,  6.65s/it]                                                         84%| | 5476/6500 [10:21:08<1:53:31,  6.65s/it] 84%| | 5477/6500 [10:21:14<1:53:05,  6.63s/it]                                                         84%| | 5477/6500 [10:21:14<1:53:05,  6.63s/it] 84%| | 5478/6500 [10:21:21<1:52:28,  6.60s/it]                                                         84%| | 5478/6500 [10:21:21<1:52:28,  6.60s/it] 84%| | 5479/6500 [10:21:28<1:56:27,  6.84s/it]                                                         84%| | 5479/6500 [10:21:28<1:56:27,  6.84s/it] 84%| | 5480/6500 [10:21:35<1:54:53,  6.76s/it]                                                         84%| | 5480/6500 [10:21:35<1:54:53,  6.76s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8800131678581238, 'eval_runtime': 1.4756, 'eval_samples_per_second': 8.133, 'eval_steps_per_second': 2.033, 'epoch': 0.84}
                                                         84%| | 5480/6500 [10:21:36<1:54:53,  6.76s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-5480
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5480I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5480

I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5480
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5480
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5480/pytorch_model.bin
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5480/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5480/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5480/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3653, 'learning_rate': 5.94602710054038e-06, 'epoch': 0.84}
{'loss': 0.3632, 'learning_rate': 5.934598934358038e-06, 'epoch': 0.84}
{'loss': 0.3687, 'learning_rate': 5.923181068195266e-06, 'epoch': 0.84}
{'loss': 0.351, 'learning_rate': 5.9117735047209355e-06, 'epoch': 0.84}
{'loss': 0.3632, 'learning_rate': 5.90037624660148e-06, 'epoch': 0.84}
 84%| | 5481/6500 [10:21:43<2:03:28,  7.27s/it]                                                         84%| | 5481/6500 [10:21:43<2:03:28,  7.27s/it] 84%| | 5482/6500 [10:21:50<1:59:38,  7.05s/it]                                                         84%| | 5482/6500 [10:21:50<1:59:38,  7.05s/it] 84%| | 5483/6500 [10:21:56<1:56:52,  6.90s/it]                                                         84%| | 5483/6500 [10:21:56<1:56:52,  6.90s/it] 84%| | 5484/6500 [10:22:03<1:54:50,  6.78s/it]                                                         84%| | 5484/6500 [10:22:03<1:54:50,  6.78s/it] 84%| | 5485/6500 [10:22:10<1:53:47,  6.73s/it]                                                         84%| | 5485/6500 [10:22:10<1:5{'loss': 0.3812, 'learning_rate': 5.888989296500952e-06, 'epoch': 0.84}
{'loss': 0.3697, 'learning_rate': 5.877612657080983e-06, 'epoch': 0.84}
{'loss': 0.6329, 'learning_rate': 5.8662463310007796e-06, 'epoch': 0.84}
{'loss': 0.3614, 'learning_rate': 5.8548903209171614e-06, 'epoch': 0.84}
{'loss': 0.3766, 'learning_rate': 5.843544629484521e-06, 'epoch': 0.84}
3:47,  6.73s/it] 84%| | 5486/6500 [10:22:16<1:52:44,  6.67s/it]                                                         84%| | 5486/6500 [10:22:16<1:52:44,  6.67s/it] 84%| | 5487/6500 [10:22:23<1:51:57,  6.63s/it]                                                         84%| | 5487/6500 [10:22:23<1:51:57,  6.63s/it] 84%| | 5488/6500 [10:22:29<1:51:20,  6.60s/it]                                                         84%| | 5488/6500 [10:22:29<1:51:20,  6.60s/it] 84%| | 5489/6500 [10:22:36<1:50:52,  6.58s/it]                                                         84%| | 5489/6500 [10:22:36<1:50:52,  6.58s/it] 84%| | 5490/6500 [10:22:42<1:50:34,  6.57s/it]                                                         84%| | 5490/6500 [10:22:42<1:50:34,  6.57s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8806756138801575, 'eval_runtime': 1.4775, 'eval_samples_per_second': 8.122, 'eval_steps_per_second': 2.03, 'epoch': 0.84}
                                                         84%| | 5490/6500 [10:22:44<1:50:34,  6.57s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-5490
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5490
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5490
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5490
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5490
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5490/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5490/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5490/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5490/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3468, 'learning_rate': 5.832209259354848e-06, 'epoch': 0.84}
{'loss': 0.354, 'learning_rate': 5.820884213177713e-06, 'epoch': 0.84}
{'loss': 0.3687, 'learning_rate': 5.809569493600281e-06, 'epoch': 0.85}
{'loss': 0.3501, 'learning_rate': 5.798265103267303e-06, 'epoch': 0.85}
{'loss': 0.3776, 'learning_rate': 5.786971044821099e-06, 'epoch': 0.85}
 84%| | 5491/6500 [10:22:51<1:59:57,  7.13s/it]                                                         84%| | 5491/6500 [10:22:51<1:59:57,  7.13s/it] 84%| | 5492/6500 [10:22:57<1:56:53,  6.96s/it]                                                         84%| | 5492/6500 [10:22:57<1:56:53,  6.96s/it] 85%| | 5493/6500 [10:23:04<1:54:43,  6.84s/it]                                                         85%| | 5493/6500 [10:23:04<1:54:43,  6.84s/it] 85%| | 5494/6500 [10:23:10<1:53:09,  6.75s/it]                                                         85%| | 5494/6500 [10:23:10<1:53:09,  6.75s/it] 85%| | 5495/6500 [10:23:18<1:57:22,  7.01s/it]                                                         85%| | 5495/6500 [10:23:18<1:5{'loss': 0.3817, 'learning_rate': 5.775687320901596e-06, 'epoch': 0.85}
{'loss': 0.3571, 'learning_rate': 5.7644139341462955e-06, 'epoch': 0.85}
{'loss': 0.3679, 'learning_rate': 5.75315088719029e-06, 'epoch': 0.85}
{'loss': 0.3558, 'learning_rate': 5.741898182666227e-06, 'epoch': 0.85}
{'loss': 0.3984, 'learning_rate': 5.7306558232043784e-06, 'epoch': 0.85}
7:22,  7.01s/it] 85%| | 5496/6500 [10:23:25<1:54:54,  6.87s/it]                                                         85%| | 5496/6500 [10:23:25<1:54:54,  6.87s/it] 85%| | 5497/6500 [10:23:31<1:53:09,  6.77s/it]                                                         85%| | 5497/6500 [10:23:31<1:53:09,  6.77s/it] 85%| | 5498/6500 [10:23:38<1:51:59,  6.71s/it]                                                         85%| | 5498/6500 [10:23:38<1:51:59,  6.71s/it] 85%| | 5499/6500 [10:23:44<1:51:08,  6.66s/it]                                                         85%| | 5499/6500 [10:23:44<1:51:08,  6.66s/it] 85%| | 5500/6500 [10:23:51<1:50:32,  6.63s/it]                                                         85%| | 5500/6500 [10:23:51<1:50:32,  6.63s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8786321878433228, 'eval_runtime': 1.4759, 'eval_samples_per_second': 8.131, 'eval_steps_per_second': 2.033, 'epoch': 0.85}
                                                         85%| | 5500/6500 [10:23:52<1:50:32,  6.63s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-5500
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5500 /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5500

I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5500
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5500
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5500/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5500/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5500/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5500/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3579, 'learning_rate': 5.719423811432562e-06, 'epoch': 0.85}
{'loss': 0.6399, 'learning_rate': 5.7082021499761994e-06, 'epoch': 0.85}
{'loss': 0.3788, 'learning_rate': 5.696990841458289e-06, 'epoch': 0.85}
{'loss': 0.3657, 'learning_rate': 5.685789888499398e-06, 'epoch': 0.85}
{'loss': 0.3739, 'learning_rate': 5.6745992937176865e-06, 'epoch': 0.85}
 85%| | 5501/6500 [10:23:59<1:59:19,  7.17s/it]                                                         85%| | 5501/6500 [10:23:59<1:59:19,  7.17s/it] 85%| | 5502/6500 [10:24:06<1:56:04,  6.98s/it]                                                         85%| | 5502/6500 [10:24:06<1:56:04,  6.98s/it] 85%| | 5503/6500 [10:24:12<1:53:48,  6.85s/it]                                                         85%| | 5503/6500 [10:24:12<1:53:48,  6.85s/it] 85%| | 5504/6500 [10:24:19<1:52:10,  6.76s/it]                                                         85%| | 5504/6500 [10:24:19<1:52:10,  6.76s/it] 85%| | 5505/6500 [10:24:25<1:51:02,  6.70s/it]                                                         85%| | 5505/6500 [10:24:25<1:5{'loss': 0.3502, 'learning_rate': 5.663419059728892e-06, 'epoch': 0.85}
{'loss': 0.3644, 'learning_rate': 5.652249189146319e-06, 'epoch': 0.85}
{'loss': 0.3681, 'learning_rate': 5.641089684580858e-06, 'epoch': 0.85}
{'loss': 0.3679, 'learning_rate': 5.629940548640988e-06, 'epoch': 0.85}
{'loss': 0.3657, 'learning_rate': 5.618801783932725e-06, 'epoch': 0.85}
1:02,  6.70s/it] 85%| | 5506/6500 [10:24:32<1:50:10,  6.65s/it]                                                         85%| | 5506/6500 [10:24:32<1:50:10,  6.65s/it] 85%| | 5507/6500 [10:24:38<1:49:33,  6.62s/it]                                                         85%| | 5507/6500 [10:24:38<1:49:33,  6.62s/it] 85%| | 5508/6500 [10:24:45<1:49:06,  6.60s/it]                                                         85%| | 5508/6500 [10:24:45<1:49:06,  6.60s/it] 85%| | 5509/6500 [10:24:52<1:48:47,  6.59s/it]                                                         85%| | 5509/6500 [10:24:52<1:48:47,  6.59s/it] 85%| | 5510/6500 [10:24:58<1:48:29,  6.58s/it]                                                         85%| | 5510/6500 [10:24:58<1:48:29,  6.58s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8790668249130249, 'eval_runtime': 1.477, 'eval_samples_per_second': 8.124, 'eval_steps_per_second': 2.031, 'epoch': 0.85}
                                                         85%| | 5510/6500 [10:25:00<1:48:29,  6.58s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-5510
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5510
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5510
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5510
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5510
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5510/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5510/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5510/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5510/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3712, 'learning_rate': 5.607673393059709e-06, 'epoch': 0.85}
{'loss': 0.3595, 'learning_rate': 5.596555378623125e-06, 'epoch': 0.85}
{'loss': 0.3666, 'learning_rate': 5.58544774322175e-06, 'epoch': 0.85}
{'loss': 0.348, 'learning_rate': 5.574350489451913e-06, 'epoch': 0.85}
{'loss': 0.399, 'learning_rate': 5.563263619907538e-06, 'epoch': 0.85}
 85%| | 5511/6500 [10:25:07<2:00:38,  7.32s/it]                                                         85%| | 5511/6500 [10:25:07<2:00:38,  7.32s/it] 85%| | 5512/6500 [10:25:14<1:56:40,  7.09s/it]                                                         85%| | 5512/6500 [10:25:14<1:56:40,  7.09s/it] 85%| | 5513/6500 [10:25:20<1:53:57,  6.93s/it]                                                         85%| | 5513/6500 [10:25:20<1:53:57,  6.93s/it] 85%| | 5514/6500 [10:25:27<1:51:55,  6.81s/it]                                                         85%| | 5514/6500 [10:25:27<1:51:55,  6.81s/it] 85%| | 5515/6500 [10:25:33<1:50:35,  6.74s/it]                                                         85%| | 5515/6500 [10:25:33<1:5{'loss': 0.3561, 'learning_rate': 5.552187137180115e-06, 'epoch': 0.85}
{'loss': 0.6374, 'learning_rate': 5.5411210438586995e-06, 'epoch': 0.85}
{'loss': 0.3795, 'learning_rate': 5.530065342529922e-06, 'epoch': 0.85}
{'loss': 0.3612, 'learning_rate': 5.519020035777994e-06, 'epoch': 0.85}
{'loss': 0.3726, 'learning_rate': 5.507985126184695e-06, 'epoch': 0.85}
0:35,  6.74s/it] 85%| | 5516/6500 [10:25:40<1:49:29,  6.68s/it]                                                         85%| | 5516/6500 [10:25:40<1:49:29,  6.68s/it] 85%| | 5517/6500 [10:25:46<1:48:41,  6.63s/it]                                                         85%| | 5517/6500 [10:25:46<1:48:41,  6.63s/it] 85%| | 5518/6500 [10:25:53<1:48:08,  6.61s/it]                                                         85%| | 5518/6500 [10:25:53<1:48:08,  6.61s/it] 85%| | 5519/6500 [10:25:59<1:47:41,  6.59s/it]                                                         85%| | 5519/6500 [10:25:59<1:47:41,  6.59s/it] 85%| | 5520/6500 [10:26:06<1:47:25,  6.58s/it]                                                         85%| | 5520/6500 [10:26:06<1:47:25,  6.58s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8794463872909546, 'eval_runtime': 1.4888, 'eval_samples_per_second': 8.06, 'eval_steps_per_second': 2.015, 'epoch': 0.85}
                                                         85%| | 5520/6500 [10:26:08<1:47:25,  6.58s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-5520
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5520
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5520
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5520
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5520
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5520/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5520/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5520/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5520/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3613, 'learning_rate': 5.4969606163293445e-06, 'epoch': 0.85}
{'loss': 0.3645, 'learning_rate': 5.485946508788864e-06, 'epoch': 0.85}
{'loss': 0.3702, 'learning_rate': 5.47494280613774e-06, 'epoch': 0.85}
{'loss': 0.3704, 'learning_rate': 5.4639495109480185e-06, 'epoch': 0.85}
{'loss': 0.37, 'learning_rate': 5.452966625789313e-06, 'epoch': 0.85}
 85%| | 5521/6500 [10:26:15<1:56:30,  7.14s/it]                                                         85%| | 5521/6500 [10:26:15<1:56:30,  7.14s/it] 85%| | 5522/6500 [10:26:21<1:53:27,  6.96s/it]                                                         85%| | 5522/6500 [10:26:21<1:53:27,  6.96s/it] 85%| | 5523/6500 [10:26:28<1:51:19,  6.84s/it]                                                         85%| | 5523/6500 [10:26:28<1:51:19,  6.84s/it] 85%| | 5524/6500 [10:26:34<1:49:48,  6.75s/it]                                                         85%| | 5524/6500 [10:26:34<1:49:48,  6.75s/it] 85%| | 5525/6500 [10:26:41<1:48:40,  6.69s/it]                                                         85%| | 5525/6500 [10:26:41<1:4{'loss': 0.3698, 'learning_rate': 5.441994153228813e-06, 'epoch': 0.85}
{'loss': 0.3667, 'learning_rate': 5.431032095831262e-06, 'epoch': 0.85}
{'loss': 0.3556, 'learning_rate': 5.420080456158971e-06, 'epoch': 0.85}
{'loss': 0.3499, 'learning_rate': 5.409139236771827e-06, 'epoch': 0.85}
{'loss': 0.3976, 'learning_rate': 5.398208440227264e-06, 'epoch': 0.85}
8:40,  6.69s/it] 85%| | 5526/6500 [10:26:47<1:47:51,  6.64s/it]                                                         85%| | 5526/6500 [10:26:47<1:47:51,  6.64s/it] 85%| | 5527/6500 [10:26:55<1:51:30,  6.88s/it]                                                         85%| | 5527/6500 [10:26:55<1:51:30,  6.88s/it] 85%| | 5528/6500 [10:27:01<1:49:52,  6.78s/it]                                                         85%| | 5528/6500 [10:27:01<1:49:52,  6.78s/it] 85%| | 5529/6500 [10:27:08<1:48:39,  6.71s/it]                                                         85%| | 5529/6500 [10:27:08<1:48:39,  6.71s/it] 85%| | 5530/6500 [10:27:14<1:47:45,  6.67s/it]                                                         85%| | 5530/6500 [10:27:14<1:47:45,  6.67s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8804427981376648, 'eval_runtime': 1.4783, 'eval_samples_per_second': 8.117, 'eval_steps_per_second': 2.029, 'epoch': 0.85}
                                                         85%| | 5530/6500 [10:27:16<1:47:45,  6.67s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-5530
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5530
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5530
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5530
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5530
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5530/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5530/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5530/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5530/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3691, 'learning_rate': 5.387288069080299e-06, 'epoch': 0.85}
{'loss': 0.6363, 'learning_rate': 5.376378125883508e-06, 'epoch': 0.85}
{'loss': 0.3756, 'learning_rate': 5.365478613187003e-06, 'epoch': 0.85}
{'loss': 0.3607, 'learning_rate': 5.35458953353849e-06, 'epoch': 0.85}
{'loss': 0.3609, 'learning_rate': 5.343710889483222e-06, 'epoch': 0.85}
 85%| | 5531/6500 [10:27:23<1:56:06,  7.19s/it]                                                         85%| | 5531/6500 [10:27:23<1:56:06,  7.19s/it] 85%| | 5532/6500 [10:27:29<1:52:49,  6.99s/it]                                                         85%| | 5532/6500 [10:27:29<1:52:49,  6.99s/it] 85%| | 5533/6500 [10:27:36<1:50:32,  6.86s/it]                                                         85%| | 5533/6500 [10:27:36<1:50:32,  6.86s/it] 85%| | 5534/6500 [10:27:42<1:48:56,  6.77s/it]                                                         85%| | 5534/6500 [10:27:42<1:48:56,  6.77s/it] 85%| | 5535/6500 [10:27:49<1:47:47,  6.70s/it]                                                         85%| | 5535/6500 [10:27:49<1:4{'loss': 0.3549, 'learning_rate': 5.332842683564021e-06, 'epoch': 0.85}
{'loss': 0.3645, 'learning_rate': 5.321984918321266e-06, 'epoch': 0.85}
{'loss': 0.3597, 'learning_rate': 5.3111375962928865e-06, 'epoch': 0.85}
{'loss': 0.3689, 'learning_rate': 5.300300720014378e-06, 'epoch': 0.85}
{'loss': 0.3699, 'learning_rate': 5.2894742920188036e-06, 'epoch': 0.85}
7:47,  6.70s/it] 85%| | 5536/6500 [10:27:55<1:46:59,  6.66s/it]                                                         85%| | 5536/6500 [10:27:55<1:46:59,  6.66s/it] 85%| | 5537/6500 [10:28:02<1:46:22,  6.63s/it]                                                         85%| | 5537/6500 [10:28:02<1:46:22,  6.63s/it] 85%| | 5538/6500 [10:28:09<1:45:53,  6.60s/it]                                                         85%| | 5538/6500 [10:28:09<1:45:53,  6.60s/it] 85%| | 5539/6500 [10:28:15<1:45:29,  6.59s/it]                                                         85%| | 5539/6500 [10:28:15<1:45:29,  6.59s/it] 85%| | 5540/6500 [10:28:22<1:45:11,  6.57s/it]                                                         85%| | 5540/6500 [10:28:22<1:45:11,  6.57s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8797410726547241, 'eval_runtime': 1.4786, 'eval_samples_per_second': 8.116, 'eval_steps_per_second': 2.029, 'epoch': 0.85}
                                                         85%| | 5540/6500 [10:28:23<1:45:11,  6.57s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-5540
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5540
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5540
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5540
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5540
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5540/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5540/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5540/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5540/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3569, 'learning_rate': 5.278658314836765e-06, 'epoch': 0.85}
{'loss': 0.3735, 'learning_rate': 5.267852790996436e-06, 'epoch': 0.85}
{'loss': 0.3647, 'learning_rate': 5.257057723023551e-06, 'epoch': 0.85}
{'loss': 0.3625, 'learning_rate': 5.246273113441369e-06, 'epoch': 0.85}
{'loss': 0.3937, 'learning_rate': 5.235498964770747e-06, 'epoch': 0.85}
 85%| | 5541/6500 [10:28:30<1:54:03,  7.14s/it]                                                         85%| | 5541/6500 [10:28:30<1:54:03,  7.14s/it] 85%| | 5542/6500 [10:28:37<1:51:08,  6.96s/it]                                                         85%| | 5542/6500 [10:28:37<1:51:08,  6.96s/it] 85%| | 5543/6500 [10:28:44<1:53:17,  7.10s/it]                                                         85%| | 5543/6500 [10:28:44<1:53:17,  7.10s/it] 85%| | 5544/6500 [10:28:51<1:50:33,  6.94s/it]                                                         85%| | 5544/6500 [10:28:51<1:50:33,  6.94s/it] 85%| | 5545/6500 [10:28:57<1:48:34,  6.82s/it]                                                         85%| | 5545/6500 [10:28:57<1:4{'loss': 0.3659, 'learning_rate': 5.224735279530063e-06, 'epoch': 0.85}
{'loss': 0.6385, 'learning_rate': 5.213982060235268e-06, 'epoch': 0.85}
{'loss': 0.369, 'learning_rate': 5.203239309399865e-06, 'epoch': 0.85}
{'loss': 0.3656, 'learning_rate': 5.19250702953491e-06, 'epoch': 0.85}
{'loss': 0.3554, 'learning_rate': 5.181785223148999e-06, 'epoch': 0.85}
8:34,  6.82s/it] 85%| | 5546/6500 [10:29:04<1:47:10,  6.74s/it]                                                         85%| | 5546/6500 [10:29:04<1:47:10,  6.74s/it] 85%| | 5547/6500 [10:29:10<1:46:07,  6.68s/it]                                                         85%| | 5547/6500 [10:29:10<1:46:07,  6.68s/it] 85%| | 5548/6500 [10:29:17<1:45:24,  6.64s/it]                                                         85%| | 5548/6500 [10:29:17<1:45:24,  6.64s/it] 85%| | 5549/6500 [10:29:23<1:44:49,  6.61s/it]                                                         85%| | 5549/6500 [10:29:23<1:44:49,  6.61s/it] 85%| | 5550/6500 [10:29:30<1:44:26,  6.60s/it]                                                         85%| | 5550/6500 [10:29:30<1:44:26,  6.60s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.879915714263916, 'eval_runtime': 1.4824, 'eval_samples_per_second': 8.095, 'eval_steps_per_second': 2.024, 'epoch': 0.85}
                                                         85%| | 5550/6500 [10:29:31<1:44:26,  6.60s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-5550
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5550
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5550
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5550
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5550
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5550/pytorch_model.bin
the pytorch model path is the pytorch model path is/projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5550/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5550/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5550/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3599, 'learning_rate': 5.17107389274829e-06, 'epoch': 0.85}
{'loss': 0.3631, 'learning_rate': 5.160373040836497e-06, 'epoch': 0.85}
{'loss': 0.3702, 'learning_rate': 5.149682669914874e-06, 'epoch': 0.85}
{'loss': 0.3761, 'learning_rate': 5.139002782482244e-06, 'epoch': 0.85}
{'loss': 0.3648, 'learning_rate': 5.12833338103495e-06, 'epoch': 0.85}
 85%| | 5551/6500 [10:29:38<1:53:23,  7.17s/it]                                                         85%| | 5551/6500 [10:29:38<1:53:23,  7.17s/it] 85%| | 5552/6500 [10:29:45<1:50:18,  6.98s/it]                                                         85%| | 5552/6500 [10:29:45<1:50:18,  6.98s/it] 85%| | 5553/6500 [10:29:52<1:48:08,  6.85s/it]                                                         85%| | 5553/6500 [10:29:52<1:48:08,  6.85s/it] 85%| | 5554/6500 [10:29:58<1:46:36,  6.76s/it]                                                         85%| | 5554/6500 [10:29:58<1:46:36,  6.76s/it] 85%| | 5555/6500 [10:30:05<1:45:27,  6.70s/it]                                                         85%| | 5555/6500 [10:30:05<1:4{'loss': 0.3648, 'learning_rate': 5.117674468066885e-06, 'epoch': 0.85}
{'loss': 0.3755, 'learning_rate': 5.107026046069541e-06, 'epoch': 0.85}
{'loss': 0.3514, 'learning_rate': 5.096388117531897e-06, 'epoch': 0.86}
{'loss': 0.364, 'learning_rate': 5.0857606849405214e-06, 'epoch': 0.86}
{'loss': 0.382, 'learning_rate': 5.075143750779499e-06, 'epoch': 0.86}
5:27,  6.70s/it] 85%| | 5556/6500 [10:30:11<1:44:38,  6.65s/it]                                                         85%| | 5556/6500 [10:30:11<1:44:38,  6.65s/it] 85%| | 5557/6500 [10:30:18<1:44:02,  6.62s/it]                                                         85%| | 5557/6500 [10:30:18<1:44:02,  6.62s/it] 86%| | 5558/6500 [10:30:24<1:43:35,  6.60s/it]                                                         86%| | 5558/6500 [10:30:24<1:43:35,  6.60s/it] 86%| | 5559/6500 [10:30:31<1:43:14,  6.58s/it]                                                         86%| | 5559/6500 [10:30:31<1:43:14,  6.58s/it] 86%| | 5560/6500 [10:30:38<1:47:21,  6.85s/it]                                                         86%| | 5560/6500 [10:30:38<1:47:21,  6.85s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8806684017181396, 'eval_runtime': 1.4801, 'eval_samples_per_second': 8.108, 'eval_steps_per_second': 2.027, 'epoch': 0.86}
                                                         86%| | 5560/6500 [10:30:40<1:47:21,  6.85s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-5560
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5560
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5560
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5560
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5560
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5560/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5560/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5560/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5560/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3721, 'learning_rate': 5.0645373175304714e-06, 'epoch': 0.86}
{'loss': 0.6421, 'learning_rate': 5.053941387672639e-06, 'epoch': 0.86}
{'loss': 0.3627, 'learning_rate': 5.0433559636827444e-06, 'epoch': 0.86}
{'loss': 0.3722, 'learning_rate': 5.032781048035034e-06, 'epoch': 0.86}
{'loss': 0.3493, 'learning_rate': 5.022216643201355e-06, 'epoch': 0.86}
 86%| | 5561/6500 [10:30:47<1:54:33,  7.32s/it]                                                         86%| | 5561/6500 [10:30:47<1:54:33,  7.32s/it] 86%| | 5562/6500 [10:30:53<1:50:44,  7.08s/it]                                                         86%| | 5562/6500 [10:30:53<1:50:44,  7.08s/it] 86%| | 5563/6500 [10:31:00<1:48:06,  6.92s/it]                                                         86%| | 5563/6500 [10:31:00<1:48:06,  6.92s/it] 86%| | 5564/6500 [10:31:06<1:46:15,  6.81s/it]                                                         86%| | 5564/6500 [10:31:06<1:46:15,  6.81s/it] 86%| | 5565/6500 [10:31:13<1:44:53,  6.73s/it]                                                         86%| | 5565/6500 [10:31:13<1:4{'loss': 0.3574, 'learning_rate': 5.011662751651064e-06, 'epoch': 0.86}
{'loss': 0.3628, 'learning_rate': 5.001119375851071e-06, 'epoch': 0.86}
{'loss': 0.3593, 'learning_rate': 4.9905865182658275e-06, 'epoch': 0.86}
{'loss': 0.3727, 'learning_rate': 4.980064181357319e-06, 'epoch': 0.86}
{'loss': 0.3797, 'learning_rate': 4.96955236758509e-06, 'epoch': 0.86}
4:53,  6.73s/it] 86%| | 5566/6500 [10:31:19<1:43:50,  6.67s/it]                                                         86%| | 5566/6500 [10:31:19<1:43:50,  6.67s/it] 86%| | 5567/6500 [10:31:26<1:43:09,  6.63s/it]                                                         86%| | 5567/6500 [10:31:26<1:43:09,  6.63s/it] 86%| | 5568/6500 [10:31:33<1:42:37,  6.61s/it]                                                         86%| | 5568/6500 [10:31:33<1:42:37,  6.61s/it] 86%| | 5569/6500 [10:31:39<1:42:14,  6.59s/it]                                                         86%| | 5569/6500 [10:31:39<1:42:14,  6.59s/it] 86%| | 5570/6500 [10:31:46<1:41:53,  6.57s/it]                                                         86%| | 5570/6500 [10:31:46<1:41:53,  6.57s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8817731738090515, 'eval_runtime': 1.4772, 'eval_samples_per_second': 8.124, 'eval_steps_per_second': 2.031, 'epoch': 0.86}
                                                         86%| | 5570/6500 [10:31:47<1:41:53,  6.57s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-5570
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5570
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5570
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5570
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5570
the pytorch model path isthe pytorch model path isthe pytorch model path is  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5570/pytorch_model.bin/projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5570/pytorch_model.bin

 /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5570/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5570/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3557, 'learning_rate': 4.959051079406202e-06, 'epoch': 0.86}
{'loss': 0.3646, 'learning_rate': 4.94856031927527e-06, 'epoch': 0.86}
{'loss': 0.3514, 'learning_rate': 4.9380800896444424e-06, 'epoch': 0.86}
{'loss': 0.3887, 'learning_rate': 4.927610392963428e-06, 'epoch': 0.86}
{'loss': 0.3532, 'learning_rate': 4.917151231679429e-06, 'epoch': 0.86}
 86%| | 5571/6500 [10:31:54<1:50:25,  7.13s/it]                                                         86%| | 5571/6500 [10:31:54<1:50:25,  7.13s/it] 86%| | 5572/6500 [10:32:01<1:47:34,  6.96s/it]                                                         86%| | 5572/6500 [10:32:01<1:47:34,  6.96s/it] 86%| | 5573/6500 [10:32:07<1:45:32,  6.83s/it]                                                         86%| | 5573/6500 [10:32:07<1:45:32,  6.83s/it] 86%| | 5574/6500 [10:32:14<1:44:06,  6.75s/it]                                                         86%| | 5574/6500 [10:32:14<1:44:06,  6.75s/it] 86%| | 5575/6500 [10:32:20<1:43:02,  6.68s/it]                                                         86%| | 5575/6500 [10:32:20<1:4{'loss': 0.6363, 'learning_rate': 4.9067026082372185e-06, 'epoch': 0.86}
{'loss': 0.3832, 'learning_rate': 4.896264525079109e-06, 'epoch': 0.86}
{'loss': 0.3589, 'learning_rate': 4.885836984644926e-06, 'epoch': 0.86}
{'loss': 0.3715, 'learning_rate': 4.8754199893720486e-06, 'epoch': 0.86}
{'loss': 0.3508, 'learning_rate': 4.865013541695384e-06, 'epoch': 0.86}
3:02,  6.68s/it] 86%| | 5576/6500 [10:32:27<1:45:12,  6.83s/it]                                                         86%| | 5576/6500 [10:32:27<1:45:12,  6.83s/it] 86%| | 5577/6500 [10:32:34<1:43:46,  6.75s/it]                                                         86%| | 5577/6500 [10:32:34<1:43:46,  6.75s/it] 86%| | 5578/6500 [10:32:40<1:42:43,  6.69s/it]                                                         86%| | 5578/6500 [10:32:40<1:42:43,  6.69s/it] 86%| | 5579/6500 [10:32:47<1:41:58,  6.64s/it]                                                         86%| | 5579/6500 [10:32:47<1:41:58,  6.64s/it] 86%| | 5580/6500 [10:32:54<1:41:29,  6.62s/it]                                                         86%| | 5580/6500 [10:32:54<1:41:29,  6.62s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8808470368385315, 'eval_runtime': 1.4888, 'eval_samples_per_second': 8.06, 'eval_steps_per_second': 2.015, 'epoch': 0.86}
                                                         86%| | 5580/6500 [10:32:55<1:41:29,  6.62s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-5580
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5580
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5580
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5580
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5580
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5580/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5580/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5580/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5580/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.357, 'learning_rate': 4.854617644047382e-06, 'epoch': 0.86}
{'loss': 0.3636, 'learning_rate': 4.84423229885802e-06, 'epoch': 0.86}
{'loss': 0.3602, 'learning_rate': 4.833857508554807e-06, 'epoch': 0.86}
{'loss': 0.3689, 'learning_rate': 4.823493275562785e-06, 'epoch': 0.86}
{'loss': 0.3708, 'learning_rate': 4.81313960230454e-06, 'epoch': 0.86}
 86%| | 5581/6500 [10:33:02<1:49:36,  7.16s/it]                                                         86%| | 5581/6500 [10:33:02<1:49:36,  7.16s/it] 86%| | 5582/6500 [10:33:09<1:46:37,  6.97s/it]                                                         86%| | 5582/6500 [10:33:09<1:46:37,  6.97s/it] 86%| | 5583/6500 [10:33:15<1:44:38,  6.85s/it]                                                         86%| | 5583/6500 [10:33:15<1:44:38,  6.85s/it] 86%| | 5584/6500 [10:33:22<1:43:12,  6.76s/it]                                                         86%| | 5584/6500 [10:33:22<1:43:12,  6.76s/it] 86%| | 5585/6500 [10:33:28<1:42:09,  6.70s/it]                                                         86%| | 5585/6500 [10:33:28<1:4{'loss': 0.3531, 'learning_rate': 4.8027964912001624e-06, 'epoch': 0.86}
{'loss': 0.3721, 'learning_rate': 4.792463944667303e-06, 'epoch': 0.86}
{'loss': 0.3518, 'learning_rate': 4.782141965121128e-06, 'epoch': 0.86}
{'loss': 0.3917, 'learning_rate': 4.771830554974344e-06, 'epoch': 0.86}
{'loss': 0.3589, 'learning_rate': 4.761529716637169e-06, 'epoch': 0.86}
2:09,  6.70s/it] 86%| | 5586/6500 [10:33:35<1:41:18,  6.65s/it]                                                         86%| | 5586/6500 [10:33:35<1:41:18,  6.65s/it] 86%| | 5587/6500 [10:33:41<1:40:44,  6.62s/it]                                                         86%| | 5587/6500 [10:33:41<1:40:44,  6.62s/it] 86%| | 5588/6500 [10:33:48<1:40:14,  6.59s/it]                                                         86%| | 5588/6500 [10:33:48<1:40:14,  6.59s/it] 86%| | 5589/6500 [10:33:54<1:39:56,  6.58s/it]                                                         86%| | 5589/6500 [10:33:54<1:39:56,  6.58s/it] 86%| | 5590/6500 [10:34:01<1:39:40,  6.57s/it]                                                         86%| | 5590/6500 [10:34:01<1:39:40,  6.57s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8792033195495605, 'eval_runtime': 1.7243, 'eval_samples_per_second': 6.959, 'eval_steps_per_second': 1.74, 'epoch': 0.86}
                                                         86%| | 5590/6500 [10:34:03<1:39:40,  6.57s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-5590
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5590
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5590
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5590
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5590
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5590/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5590/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5590/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5590/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.6319, 'learning_rate': 4.751239452517375e-06, 'epoch': 0.86}
{'loss': 0.3808, 'learning_rate': 4.740959765020242e-06, 'epoch': 0.86}
{'loss': 0.3557, 'learning_rate': 4.730690656548581e-06, 'epoch': 0.86}
{'loss': 0.3745, 'learning_rate': 4.720432129502738e-06, 'epoch': 0.86}
{'loss': 0.3582, 'learning_rate': 4.710184186280581e-06, 'epoch': 0.86}
 86%| | 5591/6500 [10:34:10<1:49:28,  7.23s/it]                                                         86%| | 5591/6500 [10:34:10<1:49:28,  7.23s/it] 86%| | 5592/6500 [10:34:17<1:49:08,  7.21s/it]                                                         86%| | 5592/6500 [10:34:17<1:49:08,  7.21s/it] 86%| | 5593/6500 [10:34:23<1:45:58,  7.01s/it]                                                         86%| | 5593/6500 [10:34:23<1:45:58,  7.01s/it] 86%| | 5594/6500 [10:34:30<1:43:48,  6.87s/it]                                                         86%| | 5594/6500 [10:34:30<1:43:48,  6.87s/it] 86%| | 5595/6500 [10:34:37<1:42:12,  6.78s/it]                                                         86%| | 5595/6500 [10:34:37<1:4{'loss': 0.3675, 'learning_rate': 4.699946829277513e-06, 'epoch': 0.86}
{'loss': 0.3688, 'learning_rate': 4.6897200608864374e-06, 'epoch': 0.86}
{'loss': 0.3731, 'learning_rate': 4.679503883497804e-06, 'epoch': 0.86}
{'loss': 0.3611, 'learning_rate': 4.669298299499586e-06, 'epoch': 0.86}
{'loss': 0.3631, 'learning_rate': 4.659103311277274e-06, 'epoch': 0.86}
2:12,  6.78s/it] 86%| | 5596/6500 [10:34:43<1:41:02,  6.71s/it]                                                         86%| | 5596/6500 [10:34:43<1:41:02,  6.71s/it] 86%| | 5597/6500 [10:34:50<1:40:13,  6.66s/it]                                                         86%| | 5597/6500 [10:34:50<1:40:13,  6.66s/it] 86%| | 5598/6500 [10:34:56<1:39:38,  6.63s/it]                                                         86%| | 5598/6500 [10:34:56<1:39:38,  6.63s/it] 86%| | 5599/6500 [10:35:03<1:39:08,  6.60s/it]                                                         86%| | 5599/6500 [10:35:03<1:39:08,  6.60s/it] 86%| | 5600/6500 [10:35:09<1:38:52,  6.59s/it]                                                         86%| | 5600/6500 [10:35:09<1:38:52,  6.59s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8792427778244019, 'eval_runtime': 1.4776, 'eval_samples_per_second': 8.121, 'eval_steps_per_second': 2.03, 'epoch': 0.86}
                                                         86%| | 5600/6500 [10:35:11<1:38:52,  6.59s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-5600
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5600
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5600
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5600
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5600
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5600/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5600/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5600/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5600/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3631, 'learning_rate': 4.648918921213885e-06, 'epoch': 0.86}
{'loss': 0.3613, 'learning_rate': 4.638745131689959e-06, 'epoch': 0.86}
{'loss': 0.3489, 'learning_rate': 4.62858194508356e-06, 'epoch': 0.86}
{'loss': 0.3947, 'learning_rate': 4.618429363770271e-06, 'epoch': 0.86}
{'loss': 0.3677, 'learning_rate': 4.60828739012319e-06, 'epoch': 0.86}
 86%| | 5601/6500 [10:35:18<1:47:01,  7.14s/it]                                                         86%| | 5601/6500 [10:35:18<1:47:01,  7.14s/it] 86%| | 5602/6500 [10:35:24<1:44:18,  6.97s/it]                                                         86%| | 5602/6500 [10:35:24<1:44:18,  6.97s/it] 86%| | 5603/6500 [10:35:31<1:42:16,  6.84s/it]                                                         86%| | 5603/6500 [10:35:31<1:42:16,  6.84s/it] 86%| | 5604/6500 [10:35:37<1:40:51,  6.75s/it]                                                         86%| | 5604/6500 [10:35:37<1:40:51,  6.75s/it] 86%| | 5605/6500 [10:35:44<1:39:46,  6.69s/it]                                                         86%| | 5605/6500 [10:35:44<1:3{'loss': 0.6323, 'learning_rate': 4.598156026512945e-06, 'epoch': 0.86}
{'loss': 0.3743, 'learning_rate': 4.588035275307689e-06, 'epoch': 0.86}
{'loss': 0.3606, 'learning_rate': 4.5779251388730735e-06, 'epoch': 0.86}
{'loss': 0.3537, 'learning_rate': 4.5678256195722804e-06, 'epoch': 0.86}
{'loss': 0.3528, 'learning_rate': 4.5577367197660205e-06, 'epoch': 0.86}
9:46,  6.69s/it] 86%| | 5606/6500 [10:35:50<1:38:58,  6.64s/it]                                                         86%| | 5606/6500 [10:35:50<1:38:58,  6.64s/it] 86%| | 5607/6500 [10:35:57<1:38:26,  6.61s/it]                                                         86%| | 5607/6500 [10:35:57<1:38:26,  6.61s/it] 86%| | 5608/6500 [10:36:04<1:41:54,  6.86s/it]                                                         86%| | 5608/6500 [10:36:04<1:41:54,  6.86s/it] 86%| | 5609/6500 [10:36:11<1:40:29,  6.77s/it]                                                         86%| | 5609/6500 [10:36:11<1:40:29,  6.77s/it] 86%| | 5610/6500 [10:36:17<1:39:22,  6.70s/it]                                                         86%| | 5610/6500 [10:36:17<1:39:22,  6.70s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8799337148666382, 'eval_runtime': 1.4741, 'eval_samples_per_second': 8.14, 'eval_steps_per_second': 2.035, 'epoch': 0.86}
                                                         86%| | 5610/6500 [10:36:19<1:39:22,  6.70s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-5610
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5610 /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5610

I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5610
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5610
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5610/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5610/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5610/pytorch_model.binthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5610/pytorch_model.bin

/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3642, 'learning_rate': 4.547658441812508e-06, 'epoch': 0.86}
{'loss': 0.3605, 'learning_rate': 4.537590788067481e-06, 'epoch': 0.86}
{'loss': 0.3699, 'learning_rate': 4.52753376088419e-06, 'epoch': 0.86}
{'loss': 0.3638, 'learning_rate': 4.517487362613404e-06, 'epoch': 0.86}
{'loss': 0.3709, 'learning_rate': 4.507451595603412e-06, 'epoch': 0.86}
 86%| | 5611/6500 [10:36:26<1:46:41,  7.20s/it]                                                         86%| | 5611/6500 [10:36:26<1:46:41,  7.20s/it] 86%| | 5612/6500 [10:36:32<1:43:42,  7.01s/it]                                                         86%| | 5612/6500 [10:36:32<1:43:42,  7.01s/it] 86%| | 5613/6500 [10:36:39<1:41:30,  6.87s/it]                                                         86%| | 5613/6500 [10:36:39<1:41:30,  6.87s/it] 86%| | 5614/6500 [10:36:45<1:39:55,  6.77s/it]                                                         86%| | 5614/6500 [10:36:45<1:39:55,  6.77s/it] 86%| | 5615/6500 [10:36:52<1:38:50,  6.70s/it]                                                         86%| | 5615/6500 [10:36:52<1:3{'loss': 0.3689, 'learning_rate': 4.497426462200011e-06, 'epoch': 0.86}
{'loss': 0.3568, 'learning_rate': 4.487411964746507e-06, 'epoch': 0.86}
{'loss': 0.3491, 'learning_rate': 4.477408105583741e-06, 'epoch': 0.86}
{'loss': 0.3986, 'learning_rate': 4.467414887050059e-06, 'epoch': 0.86}
{'loss': 0.3703, 'learning_rate': 4.457432311481291e-06, 'epoch': 0.86}
8:50,  6.70s/it] 86%| | 5616/6500 [10:36:59<1:38:02,  6.65s/it]                                                         86%| | 5616/6500 [10:36:59<1:38:02,  6.65s/it] 86%| | 5617/6500 [10:37:05<1:37:27,  6.62s/it]                                                         86%| | 5617/6500 [10:37:05<1:37:27,  6.62s/it] 86%| | 5618/6500 [10:37:12<1:37:03,  6.60s/it]                                                         86%| | 5618/6500 [10:37:12<1:37:03,  6.60s/it] 86%| | 5619/6500 [10:37:18<1:36:43,  6.59s/it]                                                         86%| | 5619/6500 [10:37:18<1:36:43,  6.59s/it] 86%| | 5620/6500 [10:37:25<1:36:26,  6.58s/it]                                                         86%| | 5620/6500 [10:37:25<1:36:26,  6.58s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8795261979103088, 'eval_runtime': 1.4741, 'eval_samples_per_second': 8.141, 'eval_steps_per_second': 2.035, 'epoch': 0.86}
                                                         86%| | 5620/6500 [10:37:26<1:36:26,  6.58s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-5620
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5620
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5620
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5620
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5620
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5620/pytorch_model.bin
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5620/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5620/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5620/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.6392, 'learning_rate': 4.447460381210822e-06, 'epoch': 0.86}
{'loss': 0.3729, 'learning_rate': 4.43749909856952e-06, 'epoch': 0.86}
{'loss': 0.3651, 'learning_rate': 4.427548465885783e-06, 'epoch': 0.87}
{'loss': 0.3517, 'learning_rate': 4.417608485485502e-06, 'epoch': 0.87}
{'loss': 0.3554, 'learning_rate': 4.407679159692097e-06, 'epoch': 0.87}
 86%| | 5621/6500 [10:37:33<1:44:22,  7.12s/it]                                                         86%| | 5621/6500 [10:37:33<1:44:22,  7.12s/it] 86%| | 5622/6500 [10:37:40<1:41:43,  6.95s/it]                                                         86%| | 5622/6500 [10:37:40<1:41:43,  6.95s/it] 87%| | 5623/6500 [10:37:46<1:39:46,  6.83s/it]                                                         87%| | 5623/6500 [10:37:46<1:39:46,  6.83s/it] 87%| | 5624/6500 [10:37:54<1:42:15,  7.00s/it]                                                         87%| | 5624/6500 [10:37:54<1:42:15,  7.00s/it] 87%| | 5625/6500 [10:38:00<1:40:02,  6.86s/it]                                                         87%| | 5625/6500 [10:38:00<1:4{'loss': 0.3624, 'learning_rate': 4.397760490826481e-06, 'epoch': 0.87}
{'loss': 0.3691, 'learning_rate': 4.387852481207083e-06, 'epoch': 0.87}
{'loss': 0.3688, 'learning_rate': 4.377955133149841e-06, 'epoch': 0.87}
{'loss': 0.3718, 'learning_rate': 4.368068448968199e-06, 'epoch': 0.87}
{'loss': 0.3663, 'learning_rate': 4.358192430973124e-06, 'epoch': 0.87}
0:02,  6.86s/it] 87%| | 5626/6500 [10:38:07<1:38:33,  6.77s/it]                                                         87%| | 5626/6500 [10:38:07<1:38:33,  6.77s/it] 87%| | 5627/6500 [10:38:13<1:37:36,  6.71s/it]                                                         87%| | 5627/6500 [10:38:13<1:37:36,  6.71s/it] 87%| | 5628/6500 [10:38:20<1:36:48,  6.66s/it]                                                         87%| | 5628/6500 [10:38:20<1:36:48,  6.66s/it] 87%| | 5629/6500 [10:38:26<1:36:14,  6.63s/it]                                                         87%| | 5629/6500 [10:38:26<1:36:14,  6.63s/it] 87%| | 5630/6500 [10:38:33<1:35:43,  6.60s/it]                                                         87%| | 5630/6500 [10:38:33<1:35:43,  6.60s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8801196813583374, 'eval_runtime': 1.4792, 'eval_samples_per_second': 8.113, 'eval_steps_per_second': 2.028, 'epoch': 0.87}
                                                         87%| | 5630/6500 [10:38:34<1:35:43,  6.60s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-5630
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5630
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5630
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5630
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5630
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5630/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5630/pytorch_model.binthe pytorch model path is
 /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5630/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5630/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3759, 'learning_rate': 4.348327081473047e-06, 'epoch': 0.87}
{'loss': 0.3561, 'learning_rate': 4.338472402773941e-06, 'epoch': 0.87}
{'loss': 0.3626, 'learning_rate': 4.3286283971792965e-06, 'epoch': 0.87}
{'loss': 0.3914, 'learning_rate': 4.318795066990072e-06, 'epoch': 0.87}
{'loss': 0.3705, 'learning_rate': 4.308972414504759e-06, 'epoch': 0.87}
 87%| | 5631/6500 [10:38:41<1:43:31,  7.15s/it]                                                         87%| | 5631/6500 [10:38:41<1:43:31,  7.15s/it] 87%| | 5632/6500 [10:38:48<1:40:45,  6.96s/it]                                                         87%| | 5632/6500 [10:38:48<1:40:45,  6.96s/it] 87%| | 5633/6500 [10:38:54<1:38:48,  6.84s/it]                                                         87%| | 5633/6500 [10:38:54<1:38:48,  6.84s/it] 87%| | 5634/6500 [10:39:01<1:37:25,  6.75s/it]                                                         87%| | 5634/6500 [10:39:01<1:37:25,  6.75s/it] 87%| | 5635/6500 [10:39:08<1:36:23,  6.69s/it]                                                         87%| | 5635/6500 [10:39:08<1:3{'loss': 0.6452, 'learning_rate': 4.29916044201934e-06, 'epoch': 0.87}
{'loss': 0.3613, 'learning_rate': 4.289359151827293e-06, 'epoch': 0.87}
{'loss': 0.3789, 'learning_rate': 4.279568546219625e-06, 'epoch': 0.87}
{'loss': 0.3578, 'learning_rate': 4.269788627484833e-06, 'epoch': 0.87}
{'loss': 0.3587, 'learning_rate': 4.260019397908898e-06, 'epoch': 0.87}
6:23,  6.69s/it] 87%| | 5636/6500 [10:39:14<1:35:38,  6.64s/it]                                                         87%| | 5636/6500 [10:39:14<1:35:38,  6.64s/it] 87%| | 5637/6500 [10:39:21<1:35:05,  6.61s/it]                                                         87%| | 5637/6500 [10:39:21<1:35:05,  6.61s/it] 87%| | 5638/6500 [10:39:27<1:34:40,  6.59s/it]                                                         87%| | 5638/6500 [10:39:27<1:34:40,  6.59s/it] 87%| | 5639/6500 [10:39:34<1:34:23,  6.58s/it]                                                         87%| | 5639/6500 [10:39:34<1:34:23,  6.58s/it] 87%| | 5640/6500 [10:39:41<1:37:58,  6.84s/it]                                                         87%| | 5640/6500 [10:39:41<1:37:58,  6.84s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8790571689605713, 'eval_runtime': 1.4879, 'eval_samples_per_second': 8.065, 'eval_steps_per_second': 2.016, 'epoch': 0.87}
                                                         87%| | 5640/6500 [10:39:43<1:37:58,  6.84s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-5640
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5640
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5640
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5640
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5640
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5640/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5640/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5640/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5640/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3595, 'learning_rate': 4.250260859775323e-06, 'epoch': 0.87}
{'loss': 0.3716, 'learning_rate': 4.240513015365111e-06, 'epoch': 0.87}
{'loss': 0.3832, 'learning_rate': 4.230775866956754e-06, 'epoch': 0.87}
{'loss': 0.3727, 'learning_rate': 4.221049416826262e-06, 'epoch': 0.87}
{'loss': 0.3622, 'learning_rate': 4.2113336672471245e-06, 'epoch': 0.87}
 87%| | 5641/6500 [10:39:50<1:44:35,  7.31s/it]                                                         87%| | 5641/6500 [10:39:50<1:44:35,  7.31s/it] 87%| | 5642/6500 [10:39:56<1:41:11,  7.08s/it]                                                         87%| | 5642/6500 [10:39:56<1:41:11,  7.08s/it] 87%| | 5643/6500 [10:40:03<1:38:50,  6.92s/it]                                                         87%| | 5643/6500 [10:40:03<1:38:50,  6.92s/it] 87%| | 5644/6500 [10:40:09<1:37:04,  6.80s/it]                                                         87%| | 5644/6500 [10:40:09<1:37:04,  6.80s/it] 87%| | 5645/6500 [10:40:16<1:35:52,  6.73s/it]                                                         87%| | 5645/6500 [10:40:16<1:3{'loss': 0.3727, 'learning_rate': 4.20162862049035e-06, 'epoch': 0.87}
{'loss': 0.3515, 'learning_rate': 4.191934278824417e-06, 'epoch': 0.87}
{'loss': 0.3719, 'learning_rate': 4.182250644515334e-06, 'epoch': 0.87}
{'loss': 0.3795, 'learning_rate': 4.172577719826587e-06, 'epoch': 0.87}
{'loss': 0.4156, 'learning_rate': 4.162915507019172e-06, 'epoch': 0.87}
5:52,  6.73s/it] 87%| | 5646/6500 [10:40:22<1:34:59,  6.67s/it]                                                         87%| | 5646/6500 [10:40:22<1:34:59,  6.67s/it] 87%| | 5647/6500 [10:40:29<1:34:20,  6.64s/it]                                                         87%| | 5647/6500 [10:40:29<1:34:20,  6.64s/it] 87%| | 5648/6500 [10:40:35<1:33:52,  6.61s/it]                                                         87%| | 5648/6500 [10:40:35<1:33:52,  6.61s/it] 87%| | 5649/6500 [10:40:42<1:33:29,  6.59s/it]                                                         87%| | 5649/6500 [10:40:42<1:33:29,  6.59s/it] 87%| | 5650/6500 [10:40:48<1:33:06,  6.57s/it]                                                         87%| | 5650/6500 [10:40:48<1:33:06,  6.57s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8784543871879578, 'eval_runtime': 1.4822, 'eval_samples_per_second': 8.096, 'eval_steps_per_second': 2.024, 'epoch': 0.87}
                                                         87%| | 5650/6500 [10:40:50<1:33:06,  6.57s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-5650
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5650I AM HERE AND I AM SAVING THE MODEL

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5650
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5650
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5650
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5650/pytorch_model.bin
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5650/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5650/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5650/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.6032, 'learning_rate': 4.153264008351549e-06, 'epoch': 0.87}
{'loss': 0.3618, 'learning_rate': 4.143623226079718e-06, 'epoch': 0.87}
{'loss': 0.3771, 'learning_rate': 4.1339931624571396e-06, 'epoch': 0.87}
{'loss': 0.3501, 'learning_rate': 4.124373819734795e-06, 'epoch': 0.87}
{'loss': 0.3531, 'learning_rate': 4.114765200161141e-06, 'epoch': 0.87}
 87%| | 5651/6500 [10:40:57<1:40:44,  7.12s/it]                                                         87%| | 5651/6500 [10:40:57<1:40:44,  7.12s/it] 87%| | 5652/6500 [10:41:03<1:38:08,  6.94s/it]                                                         87%| | 5652/6500 [10:41:03<1:38:08,  6.94s/it] 87%| | 5653/6500 [10:41:10<1:36:18,  6.82s/it]                                                         87%| | 5653/6500 [10:41:10<1:36:18,  6.82s/it] 87%| | 5654/6500 [10:41:16<1:35:02,  6.74s/it]                                                         87%| | 5654/6500 [10:41:16<1:35:02,  6.74s/it] 87%| | 5655/6500 [10:41:23<1:34:08,  6.69s/it]                                                         87%| | 5655/6500 [10:41:23<1:3{'loss': 0.3656, 'learning_rate': 4.1051673059821326e-06, 'epoch': 0.87}
{'loss': 0.3626, 'learning_rate': 4.095580139441219e-06, 'epoch': 0.87}
{'loss': 0.3714, 'learning_rate': 4.08600370277935e-06, 'epoch': 0.87}
{'loss': 0.3697, 'learning_rate': 4.07643799823495e-06, 'epoch': 0.87}
{'loss': 0.3568, 'learning_rate': 4.06688302804395e-06, 'epoch': 0.87}
4:08,  6.69s/it] 87%| | 5656/6500 [10:41:30<1:33:25,  6.64s/it]                                                         87%| | 5656/6500 [10:41:30<1:33:25,  6.64s/it] 87%| | 5657/6500 [10:41:37<1:35:30,  6.80s/it]                                                         87%| | 5657/6500 [10:41:37<1:35:30,  6.80s/it] 87%| | 5658/6500 [10:41:43<1:34:20,  6.72s/it]                                                         87%| | 5658/6500 [10:41:43<1:34:20,  6.72s/it] 87%| | 5659/6500 [10:41:50<1:33:28,  6.67s/it]                                                         87%| | 5659/6500 [10:41:50<1:33:28,  6.67s/it] 87%| | 5660/6500 [10:41:56<1:32:51,  6.63s/it]                                                         87%| | 5660/6500 [10:41:56<1:32:51,  6.63s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8811520934104919, 'eval_runtime': 1.4974, 'eval_samples_per_second': 8.014, 'eval_steps_per_second': 2.003, 'epoch': 0.87}
                                                         87%| | 5660/6500 [10:41:58<1:32:51,  6.63s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-5660
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5660
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5660
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5660
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5660
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5660/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5660/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5660/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5660/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3672, 'learning_rate': 4.0573387944397744e-06, 'epoch': 0.87}
{'loss': 0.3518, 'learning_rate': 4.047805299653307e-06, 'epoch': 0.87}
{'loss': 0.3948, 'learning_rate': 4.038282545912958e-06, 'epoch': 0.87}
{'loss': 0.3524, 'learning_rate': 4.028770535444615e-06, 'epoch': 0.87}
{'loss': 0.6364, 'learning_rate': 4.019269270471649e-06, 'epoch': 0.87}
 87%| | 5661/6500 [10:42:05<1:40:25,  7.18s/it]                                                         87%| | 5661/6500 [10:42:05<1:40:25,  7.18s/it] 87%| | 5662/6500 [10:42:11<1:37:39,  6.99s/it]                                                         87%| | 5662/6500 [10:42:11<1:37:39,  6.99s/it] 87%| | 5663/6500 [10:42:18<1:35:43,  6.86s/it]                                                         87%| | 5663/6500 [10:42:18<1:35:43,  6.86s/it] 87%| | 5664/6500 [10:42:24<1:34:13,  6.76s/it]                                                         87%| | 5664/6500 [10:42:24<1:34:13,  6.76s/it] 87%| | 5665/6500 [10:42:31<1:33:07,  6.69s/it]                                                         87%| | 5665/6500 [10:42:31<1:3{'loss': 0.3702, 'learning_rate': 4.0097787532149215e-06, 'epoch': 0.87}
{'loss': 0.3579, 'learning_rate': 4.000298985892787e-06, 'epoch': 0.87}
{'loss': 0.3665, 'learning_rate': 3.9908299707210775e-06, 'epoch': 0.87}
{'loss': 0.3507, 'learning_rate': 3.981371709913123e-06, 'epoch': 0.87}
{'loss': 0.3575, 'learning_rate': 3.971924205679739e-06, 'epoch': 0.87}
3:07,  6.69s/it] 87%| | 5666/6500 [10:42:38<1:32:25,  6.65s/it]                                                         87%| | 5666/6500 [10:42:38<1:32:25,  6.65s/it] 87%| | 5667/6500 [10:42:44<1:31:52,  6.62s/it]                                                         87%| | 5667/6500 [10:42:44<1:31:52,  6.62s/it] 87%| | 5668/6500 [10:42:51<1:31:25,  6.59s/it]                                                         87%| | 5668/6500 [10:42:51<1:31:25,  6.59s/it] 87%| | 5669/6500 [10:42:57<1:31:10,  6.58s/it]                                                         87%| | 5669/6500 [10:42:57<1:31:10,  6.58s/it] 87%| | 5670/6500 [10:43:04<1:30:54,  6.57s/it]                                                         87%| | 5670/6500 [10:43:04<1:30:54,  6.57s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8799999356269836, 'eval_runtime': 1.4813, 'eval_samples_per_second': 8.101, 'eval_steps_per_second': 2.025, 'epoch': 0.87}
                                                         87%| | 5670/6500 [10:43:05<1:30:54,  6.57s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-5670
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5670
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5670
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5670
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5670
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5670/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5670/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5670/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5670/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3592, 'learning_rate': 3.962487460229214e-06, 'epoch': 0.87}
{'loss': 0.3662, 'learning_rate': 3.953061475767339e-06, 'epoch': 0.87}
{'loss': 0.3657, 'learning_rate': 3.9436462544973685e-06, 'epoch': 0.87}
{'loss': 0.37, 'learning_rate': 3.934241798620058e-06, 'epoch': 0.87}
{'loss': 0.3641, 'learning_rate': 3.92484811033364e-06, 'epoch': 0.87}
 87%| | 5671/6500 [10:43:12<1:38:25,  7.12s/it]                                                         87%| | 5671/6500 [10:43:12<1:38:25,  7.12s/it] 87%| | 5672/6500 [10:43:19<1:35:53,  6.95s/it]                                                         87%| | 5672/6500 [10:43:19<1:35:53,  6.95s/it] 87%| | 5673/6500 [10:43:26<1:38:15,  7.13s/it]                                                         87%| | 5673/6500 [10:43:26<1:38:15,  7.13s/it] 87%| | 5674/6500 [10:43:33<1:35:46,  6.96s/it]                                                         87%| | 5674/6500 [10:43:33<1:35:46,  6.96s/it] 87%| | 5675/6500 [10:43:39<1:33:58,  6.83s/it]                                                         87%| | 5675/6500 [10:43:39<1:3{'loss': 0.3623, 'learning_rate': 3.915465191833833e-06, 'epoch': 0.87}
{'loss': 0.348, 'learning_rate': 3.906093045313847e-06, 'epoch': 0.87}
{'loss': 0.399, 'learning_rate': 3.896731672964349e-06, 'epoch': 0.87}
{'loss': 0.3695, 'learning_rate': 3.887381076973512e-06, 'epoch': 0.87}
{'loss': 0.6317, 'learning_rate': 3.878041259526982e-06, 'epoch': 0.87}
3:58,  6.83s/it] 87%| | 5676/6500 [10:43:46<1:32:41,  6.75s/it]                                                         87%| | 5676/6500 [10:43:46<1:32:41,  6.75s/it] 87%| | 5677/6500 [10:43:52<1:31:48,  6.69s/it]                                                         87%| | 5677/6500 [10:43:52<1:31:48,  6.69s/it] 87%| | 5678/6500 [10:43:59<1:31:06,  6.65s/it]                                                         87%| | 5678/6500 [10:43:59<1:31:06,  6.65s/it] 87%| | 5679/6500 [10:44:06<1:30:32,  6.62s/it]                                                         87%| | 5679/6500 [10:44:06<1:30:32,  6.62s/it] 87%| | 5680/6500 [10:44:12<1:30:07,  6.59s/it]                                                         87%| | 5680/6500 [10:44:12<1:30:07,  6.59s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8803325891494751, 'eval_runtime': 1.4698, 'eval_samples_per_second': 8.164, 'eval_steps_per_second': 2.041, 'epoch': 0.87}
                                                         87%| | 5680/6500 [10:44:14<1:30:07,  6.59s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-5680
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5680
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5680
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5680
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5680
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5680/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5680/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5680/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5680/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3809, 'learning_rate': 3.86871222280788e-06, 'epoch': 0.87}
{'loss': 0.3571, 'learning_rate': 3.85939396899681e-06, 'epoch': 0.87}
{'loss': 0.3672, 'learning_rate': 3.850086500271871e-06, 'epoch': 0.87}
{'loss': 0.3602, 'learning_rate': 3.840789818808605e-06, 'epoch': 0.87}
{'loss': 0.364, 'learning_rate': 3.831503926780072e-06, 'epoch': 0.87}
 87%| | 5681/6500 [10:44:20<1:37:25,  7.14s/it]                                                         87%| | 5681/6500 [10:44:20<1:37:25,  7.14s/it] 87%| | 5682/6500 [10:44:27<1:34:50,  6.96s/it]                                                         87%| | 5682/6500 [10:44:27<1:34:50,  6.96s/it] 87%| | 5683/6500 [10:44:34<1:33:03,  6.83s/it]                                                         87%| | 5683/6500 [10:44:34<1:33:03,  6.83s/it] 87%| | 5684/6500 [10:44:40<1:31:45,  6.75s/it]                                                         87%| | 5684/6500 [10:44:40<1:31:45,  6.75s/it] 87%| | 5685/6500 [10:44:47<1:30:48,  6.69s/it]                                                         87%| | 5685/6500 [10:44:47<1:3{'loss': 0.3636, 'learning_rate': 3.822228826356783e-06, 'epoch': 0.87}
{'loss': 0.3766, 'learning_rate': 3.812964519706741e-06, 'epoch': 0.87}
{'loss': 0.36, 'learning_rate': 3.80371100899542e-06, 'epoch': 0.88}
{'loss': 0.3659, 'learning_rate': 3.7944682963857727e-06, 'epoch': 0.88}
{'loss': 0.3614, 'learning_rate': 3.785236384038232e-06, 'epoch': 0.88}
0:48,  6.69s/it] 87%| | 5686/6500 [10:44:53<1:30:03,  6.64s/it]                                                         87%| | 5686/6500 [10:44:53<1:30:03,  6.64s/it] 87%| | 5687/6500 [10:45:00<1:29:32,  6.61s/it]                                                         87%| | 5687/6500 [10:45:00<1:29:32,  6.61s/it] 88%| | 5688/6500 [10:45:06<1:29:08,  6.59s/it]                                                         88%| | 5688/6500 [10:45:06<1:29:08,  6.59s/it] 88%| | 5689/6500 [10:45:14<1:32:30,  6.84s/it]                                                         88%| | 5689/6500 [10:45:14<1:32:30,  6.84s/it] 88%| | 5690/6500 [10:45:20<1:31:12,  6.76s/it]                                                         88%| | 5690/6500 [10:45:20<1:31:12,  6.76s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8807129859924316, 'eval_runtime': 1.4672, 'eval_samples_per_second': 8.179, 'eval_steps_per_second': 2.045, 'epoch': 0.88}
                                                         88%| | 5690/6500 [10:45:22<1:31:12,  6.76s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-5690
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5690
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5690
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5690
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5690
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5690/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5690/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5690/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5690/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3575, 'learning_rate': 3.776015274110689e-06, 'epoch': 0.88}
{'loss': 0.3521, 'learning_rate': 3.766804968758536e-06, 'epoch': 0.88}
{'loss': 0.4, 'learning_rate': 3.757605470134612e-06, 'epoch': 0.88}
{'loss': 0.3585, 'learning_rate': 3.748416780389263e-06, 'epoch': 0.88}
{'loss': 0.6363, 'learning_rate': 3.7392389016702666e-06, 'epoch': 0.88}
 88%| | 5691/6500 [10:45:29<1:37:38,  7.24s/it]                                                         88%| | 5691/6500 [10:45:29<1:37:38,  7.24s/it] 88%| | 5692/6500 [10:45:35<1:34:40,  7.03s/it]                                                         88%| | 5692/6500 [10:45:35<1:34:40,  7.03s/it] 88%| | 5693/6500 [10:45:42<1:32:31,  6.88s/it]                                                         88%| | 5693/6500 [10:45:42<1:32:31,  6.88s/it] 88%| | 5694/6500 [10:45:48<1:31:02,  6.78s/it]                                                         88%| | 5694/6500 [10:45:48<1:31:02,  6.78s/it] 88%| | 5695/6500 [10:45:55<1:29:55,  6.70s/it]                                                         88%| | 5695/6500 [10:45:55<1:2{'loss': 0.3784, 'learning_rate': 3.7300718361229112e-06, 'epoch': 0.88}
{'loss': 0.3648, 'learning_rate': 3.7209155858899393e-06, 'epoch': 0.88}
{'loss': 0.3619, 'learning_rate': 3.71177015311156e-06, 'epoch': 0.88}
{'loss': 0.358, 'learning_rate': 3.702635539925475e-06, 'epoch': 0.88}
{'loss': 0.3652, 'learning_rate': 3.6935117484668436e-06, 'epoch': 0.88}
9:55,  6.70s/it] 88%| | 5696/6500 [10:46:01<1:29:09,  6.65s/it]                                                         88%| | 5696/6500 [10:46:01<1:29:09,  6.65s/it] 88%| | 5697/6500 [10:46:08<1:28:31,  6.61s/it]                                                         88%| | 5697/6500 [10:46:08<1:28:31,  6.61s/it] 88%| | 5698/6500 [10:46:14<1:28:09,  6.59s/it]                                                         88%| | 5698/6500 [10:46:14<1:28:09,  6.59s/it] 88%| | 5699/6500 [10:46:21<1:27:46,  6.58s/it]                                                         88%| | 5699/6500 [10:46:21<1:27:46,  6.58s/it] 88%| | 5700/6500 [10:46:27<1:27:31,  6.56s/it]                                                         88%| | 5700/6500 [10:46:27<1:27:31,  6.56s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8809893131256104, 'eval_runtime': 1.4705, 'eval_samples_per_second': 8.16, 'eval_steps_per_second': 2.04, 'epoch': 0.88}
                                                         88%| | 5700/6500 [10:46:29<1:27:31,  6.56s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-5700
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5700
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5700
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5700
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5700
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5700/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5700/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5700/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5700/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3621, 'learning_rate': 3.6843987808682868e-06, 'epoch': 0.88}
{'loss': 0.3755, 'learning_rate': 3.675296639259912e-06, 'epoch': 0.88}
{'loss': 0.3739, 'learning_rate': 3.666205325769295e-06, 'epoch': 0.88}
{'loss': 0.3636, 'learning_rate': 3.657124842521464e-06, 'epoch': 0.88}
{'loss': 0.37, 'learning_rate': 3.6480551916389327e-06, 'epoch': 0.88}
 88%| | 5701/6500 [10:46:36<1:34:38,  7.11s/it]                                                         88%| | 5701/6500 [10:46:36<1:34:38,  7.11s/it] 88%| | 5702/6500 [10:46:42<1:32:15,  6.94s/it]                                                         88%| | 5702/6500 [10:46:42<1:32:15,  6.94s/it] 88%| | 5703/6500 [10:46:49<1:30:34,  6.82s/it]                                                         88%| | 5703/6500 [10:46:49<1:30:34,  6.82s/it] 88%| | 5704/6500 [10:46:55<1:29:18,  6.73s/it]                                                         88%| | 5704/6500 [10:46:55<1:29:18,  6.73s/it] 88%| | 5705/6500 [10:47:03<1:31:57,  6.94s/it]                                                         88%| | 5705/6500 [10:47:03<1:3{'loss': 0.3529, 'learning_rate': 3.638996375241682e-06, 'epoch': 0.88}
{'loss': 0.3561, 'learning_rate': 3.6299483954471356e-06, 'epoch': 0.88}
{'loss': 0.3912, 'learning_rate': 3.620911254370224e-06, 'epoch': 0.88}
{'loss': 0.3674, 'learning_rate': 3.6118849541233178e-06, 'epoch': 0.88}
{'loss': 0.6438, 'learning_rate': 3.602869496816258e-06, 'epoch': 0.88}
1:57,  6.94s/it] 88%| | 5706/6500 [10:47:09<1:30:15,  6.82s/it]                                                         88%| | 5706/6500 [10:47:09<1:30:15,  6.82s/it] 88%| | 5707/6500 [10:47:16<1:29:01,  6.74s/it]                                                         88%| | 5707/6500 [10:47:16<1:29:01,  6.74s/it] 88%| | 5708/6500 [10:47:22<1:28:10,  6.68s/it]                                                         88%| | 5708/6500 [10:47:22<1:28:10,  6.68s/it] 88%| | 5709/6500 [10:47:29<1:27:31,  6.64s/it]                                                         88%| | 5709/6500 [10:47:29<1:27:31,  6.64s/it] 88%| | 5710/6500 [10:47:36<1:27:00,  6.61s/it]                                                         88%| | 5710/6500 [10:47:36<1:27:00,  6.61s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8808091282844543, 'eval_runtime': 1.4736, 'eval_samples_per_second': 8.143, 'eval_steps_per_second': 2.036, 'epoch': 0.88}
                                                         88%| | 5710/6500 [10:47:37<1:27:00,  6.61s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-5710
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5710
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5710
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5710
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5710
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5710/pytorch_model.bin
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5710/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5710/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5710/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3616, 'learning_rate': 3.5938648845563585e-06, 'epoch': 0.88}
{'loss': 0.3765, 'learning_rate': 3.584871119448385e-06, 'epoch': 0.88}
{'loss': 0.357, 'learning_rate': 3.5758882035945795e-06, 'epoch': 0.88}
{'loss': 0.3547, 'learning_rate': 3.5669161390946503e-06, 'epoch': 0.88}
{'loss': 0.357, 'learning_rate': 3.557954928045748e-06, 'epoch': 0.88}
 88%| | 5711/6500 [10:47:44<1:34:02,  7.15s/it]                                                         88%| | 5711/6500 [10:47:44<1:34:02,  7.15s/it] 88%| | 5712/6500 [10:47:51<1:31:32,  6.97s/it]                                                         88%| | 5712/6500 [10:47:51<1:31:32,  6.97s/it] 88%| | 5713/6500 [10:47:57<1:29:41,  6.84s/it]                                                         88%| | 5713/6500 [10:47:57<1:29:41,  6.84s/it] 88%| | 5714/6500 [10:48:04<1:28:21,  6.74s/it]                                                         88%| | 5714/6500 [10:48:04<1:28:21,  6.74s/it] 88%| | 5715/6500 [10:48:10<1:27:25,  6.68s/it]                                                         88%| | 5715/6500 [10:48:10<1:2{'loss': 0.3664, 'learning_rate': 3.549004572542508e-06, 'epoch': 0.88}
{'loss': 0.3667, 'learning_rate': 3.5400650746770236e-06, 'epoch': 0.88}
{'loss': 0.3643, 'learning_rate': 3.5311364365388455e-06, 'epoch': 0.88}
{'loss': 0.3567, 'learning_rate': 3.5222186602149933e-06, 'epoch': 0.88}
{'loss': 0.3803, 'learning_rate': 3.513311747789938e-06, 'epoch': 0.88}
7:25,  6.68s/it] 88%| | 5716/6500 [10:48:17<1:26:43,  6.64s/it]                                                         88%| | 5716/6500 [10:48:17<1:26:43,  6.64s/it] 88%| | 5717/6500 [10:48:23<1:26:11,  6.61s/it]                                                         88%| | 5717/6500 [10:48:23<1:26:11,  6.61s/it] 88%| | 5718/6500 [10:48:30<1:25:48,  6.58s/it]                                                         88%| | 5718/6500 [10:48:30<1:25:48,  6.58s/it] 88%| | 5719/6500 [10:48:36<1:25:32,  6.57s/it]                                                         88%| | 5719/6500 [10:48:36<1:25:32,  6.57s/it] 88%| | 5720/6500 [10:48:43<1:25:19,  6.56s/it]                                                         88%| | 5720/6500 [10:48:43<1:25:19,  6.56s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.880023717880249, 'eval_runtime': 1.4694, 'eval_samples_per_second': 8.167, 'eval_steps_per_second': 2.042, 'epoch': 0.88}
                                                         88%| | 5720/6500 [10:48:44<1:25:19,  6.56s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-5720
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5720
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5720
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5720
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5720
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5720/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5720/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5720/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5720/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3576, 'learning_rate': 3.504415701345615e-06, 'epoch': 0.88}
{'loss': 0.3649, 'learning_rate': 3.4955305229614267e-06, 'epoch': 0.88}
{'loss': 0.3821, 'learning_rate': 3.486656214714229e-06, 'epoch': 0.88}
{'loss': 0.3718, 'learning_rate': 3.4777927786783347e-06, 'epoch': 0.88}
{'loss': 0.6391, 'learning_rate': 3.468940216925515e-06, 'epoch': 0.88}
 88%| | 5721/6500 [10:48:52<1:35:43,  7.37s/it]                                                         88%| | 5721/6500 [10:48:52<1:35:43,  7.37s/it] 88%| | 5722/6500 [10:48:59<1:32:23,  7.13s/it]                                                         88%| | 5722/6500 [10:48:59<1:32:23,  7.13s/it] 88%| | 5723/6500 [10:49:05<1:30:02,  6.95s/it]                                                         88%| | 5723/6500 [10:49:05<1:30:02,  6.95s/it] 88%| | 5724/6500 [10:49:12<1:28:17,  6.83s/it]                                                         88%| | 5724/6500 [10:49:12<1:28:17,  6.83s/it] 88%| | 5725/6500 [10:49:18<1:27:00,  6.74s/it]                                                         88%| | 5725/6500 [10:49:18<1:2{'loss': 0.3589, 'learning_rate': 3.460098531525019e-06, 'epoch': 0.88}
{'loss': 0.3758, 'learning_rate': 3.451267724543511e-06, 'epoch': 0.88}
{'loss': 0.3533, 'learning_rate': 3.442447798045151e-06, 'epoch': 0.88}
{'loss': 0.3614, 'learning_rate': 3.4336387540915505e-06, 'epoch': 0.88}
{'loss': 0.3631, 'learning_rate': 3.4248405947417572e-06, 'epoch': 0.88}
7:00,  6.74s/it] 88%| | 5726/6500 [10:49:25<1:26:09,  6.68s/it]                                                         88%| | 5726/6500 [10:49:25<1:26:09,  6.68s/it] 88%| | 5727/6500 [10:49:31<1:25:30,  6.64s/it]                                                         88%| | 5727/6500 [10:49:31<1:25:30,  6.64s/it] 88%| | 5728/6500 [10:49:38<1:25:01,  6.61s/it]                                                         88%| | 5728/6500 [10:49:38<1:25:01,  6.61s/it] 88%| | 5729/6500 [10:49:44<1:24:41,  6.59s/it]                                                         88%| | 5729/6500 [10:49:44<1:24:41,  6.59s/it] 88%| | 5730/6500 [10:49:51<1:24:24,  6.58s/it]                                                         88%| | 5730/6500 [10:49:51<1:24:24,  6.58s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8798978328704834, 'eval_runtime': 1.4739, 'eval_samples_per_second': 8.142, 'eval_steps_per_second': 2.035, 'epoch': 0.88}
                                                         88%| | 5730/6500 [10:49:52<1:24:24,  6.58s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-5730
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5730
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5730
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5730
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5730
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5730/pytorch_model.bin
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5730/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5730/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5730/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3653, 'learning_rate': 3.416053322052293e-06, 'epoch': 0.88}
{'loss': 0.3739, 'learning_rate': 3.4072769380771363e-06, 'epoch': 0.88}
{'loss': 0.3807, 'learning_rate': 3.3985114448677024e-06, 'epoch': 0.88}
{'loss': 0.3569, 'learning_rate': 3.3897568444728746e-06, 'epoch': 0.88}
{'loss': 0.3734, 'learning_rate': 3.381013138938993e-06, 'epoch': 0.88}
 88%| | 5731/6500 [10:49:59<1:31:19,  7.13s/it]                                                         88%| | 5731/6500 [10:49:59<1:31:19,  7.13s/it] 88%| | 5732/6500 [10:50:06<1:28:56,  6.95s/it]                                                         88%| | 5732/6500 [10:50:06<1:28:56,  6.95s/it] 88%| | 5733/6500 [10:50:12<1:27:14,  6.82s/it]                                                         88%| | 5733/6500 [10:50:12<1:27:14,  6.82s/it] 88%| | 5734/6500 [10:50:19<1:25:58,  6.73s/it]                                                         88%| | 5734/6500 [10:50:19<1:25:58,  6.73s/it] 88%| | 5735/6500 [10:50:25<1:25:05,  6.67s/it]                                                         88%| | 5735/6500 [10:50:25<1:2{'loss': 0.3526, 'learning_rate': 3.3722803303098403e-06, 'epoch': 0.88}
{'loss': 0.3957, 'learning_rate': 3.363558420626667e-06, 'epoch': 0.88}
{'loss': 0.3526, 'learning_rate': 3.3548474119281526e-06, 'epoch': 0.88}
{'loss': 0.6387, 'learning_rate': 3.346147306250447e-06, 'epoch': 0.88}
{'loss': 0.3787, 'learning_rate': 3.3374581056271447e-06, 'epoch': 0.88}
5:05,  6.67s/it] 88%| | 5736/6500 [10:50:32<1:24:22,  6.63s/it]                                                         88%| | 5736/6500 [10:50:32<1:24:22,  6.63s/it] 88%| | 5737/6500 [10:50:39<1:26:17,  6.79s/it]                                                         88%| | 5737/6500 [10:50:39<1:26:17,  6.79s/it] 88%| | 5738/6500 [10:50:46<1:25:50,  6.76s/it]                                                         88%| | 5738/6500 [10:50:46<1:25:50,  6.76s/it] 88%| | 5739/6500 [10:50:52<1:24:51,  6.69s/it]                                                         88%| | 5739/6500 [10:50:52<1:24:51,  6.69s/it] 88%| | 5740/6500 [10:50:59<1:24:10,  6.65s/it]                                                         88%| | 5740/6500 [10:50:59<1:24:10,  6.65s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8803893327713013, 'eval_runtime': 1.5166, 'eval_samples_per_second': 7.912, 'eval_steps_per_second': 1.978, 'epoch': 0.88}
                                                         88%| | 5740/6500 [10:51:00<1:24:10,  6.65s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-5740
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5740
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5740
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5740
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5740
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5740/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5740/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5740/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5740/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3596, 'learning_rate': 3.3287798120893055e-06, 'epoch': 0.88}
{'loss': 0.3671, 'learning_rate': 3.3201124276654118e-06, 'epoch': 0.88}
{'loss': 0.3505, 'learning_rate': 3.311455954381426e-06, 'epoch': 0.88}
{'loss': 0.3609, 'learning_rate': 3.302810394260736e-06, 'epoch': 0.88}
{'loss': 0.3666, 'learning_rate': 3.294175749324191e-06, 'epoch': 0.88}
 88%| | 5741/6500 [10:51:07<1:30:48,  7.18s/it]                                                         88%| | 5741/6500 [10:51:07<1:30:48,  7.18s/it] 88%| | 5742/6500 [10:51:14<1:28:14,  6.99s/it]                                                         88%| | 5742/6500 [10:51:14<1:28:14,  6.99s/it] 88%| | 5743/6500 [10:51:20<1:26:28,  6.85s/it]                                                         88%| | 5743/6500 [10:51:20<1:26:28,  6.85s/it] 88%| | 5744/6500 [10:51:27<1:25:11,  6.76s/it]                                                         88%| | 5744/6500 [10:51:27<1:25:11,  6.76s/it] 88%| | 5745/6500 [10:51:34<1:24:13,  6.69s/it]                                                         88%| | 5745/6500 [10:51:34<1:2{'loss': 0.3587, 'learning_rate': 3.285552021590094e-06, 'epoch': 0.88}
{'loss': 0.3679, 'learning_rate': 3.2769392130741816e-06, 'epoch': 0.88}
{'loss': 0.372, 'learning_rate': 3.2683373257896497e-06, 'epoch': 0.88}
{'loss': 0.3522, 'learning_rate': 3.2597463617471346e-06, 'epoch': 0.88}
{'loss': 0.3669, 'learning_rate': 3.2511663229547183e-06, 'epoch': 0.88}
4:13,  6.69s/it] 88%| | 5746/6500 [10:51:40<1:23:29,  6.64s/it]                                                         88%| | 5746/6500 [10:51:40<1:23:29,  6.64s/it] 88%| | 5747/6500 [10:51:47<1:22:54,  6.61s/it]                                                         88%| | 5747/6500 [10:51:47<1:22:54,  6.61s/it] 88%| | 5748/6500 [10:51:53<1:22:33,  6.59s/it]                                                         88%| | 5748/6500 [10:51:53<1:22:33,  6.59s/it] 88%| | 5749/6500 [10:52:00<1:22:14,  6.57s/it]                                                         88%| | 5749/6500 [10:52:00<1:22:14,  6.57s/it] 88%| | 5750/6500 [10:52:06<1:22:00,  6.56s/it]                                                         88%| | 5750/6500 [10:52:06<1:22:00,  6.56s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8821675777435303, 'eval_runtime': 1.833, 'eval_samples_per_second': 6.547, 'eval_steps_per_second': 1.637, 'epoch': 0.88}
                                                         88%| | 5750/6500 [10:52:08<1:22:00,  6.56s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-5750
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5750I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5750

I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5750
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5750
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5750/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5750/pytorch_model.binthe pytorch model path is
 /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5750/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5750/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3434, 'learning_rate': 3.242597211417936e-06, 'epoch': 0.88}
{'loss': 0.3986, 'learning_rate': 3.2340390291397684e-06, 'epoch': 0.88}
{'loss': 0.3527, 'learning_rate': 3.225491778120632e-06, 'epoch': 0.89}
{'loss': 0.6415, 'learning_rate': 3.2169554603584064e-06, 'epoch': 0.89}
{'loss': 0.3789, 'learning_rate': 3.20843007784839e-06, 'epoch': 0.89}
 88%| | 5751/6500 [10:52:15<1:30:08,  7.22s/it]                                                         88%| | 5751/6500 [10:52:15<1:30:08,  7.22s/it] 88%| | 5752/6500 [10:52:21<1:27:29,  7.02s/it]                                                         88%| | 5752/6500 [10:52:21<1:27:29,  7.02s/it] 89%| | 5753/6500 [10:52:28<1:25:32,  6.87s/it]                                                         89%| | 5753/6500 [10:52:28<1:25:32,  6.87s/it] 89%| | 5754/6500 [10:52:35<1:26:39,  6.97s/it]                                                         89%| | 5754/6500 [10:52:35<1:26:39,  6.97s/it] 89%| | 5755/6500 [10:52:42<1:24:56,  6.84s/it]                                                         89%| | 5755/6500 [10:52:42<1:2{'loss': 0.3506, 'learning_rate': 3.1999156325833444e-06, 'epoch': 0.89}
{'loss': 0.3702, 'learning_rate': 3.1914121265534723e-06, 'epoch': 0.89}
{'loss': 0.3506, 'learning_rate': 3.182919561746417e-06, 'epoch': 0.89}
{'loss': 0.3571, 'learning_rate': 3.1744379401472677e-06, 'epoch': 0.89}
{'loss': 0.3636, 'learning_rate': 3.1659672637385397e-06, 'epoch': 0.89}
4:56,  6.84s/it] 89%| | 5756/6500 [10:52:48<1:23:49,  6.76s/it]                                                         89%| | 5756/6500 [10:52:48<1:23:49,  6.76s/it] 89%| | 5757/6500 [10:52:55<1:22:55,  6.70s/it]                                                         89%| | 5757/6500 [10:52:55<1:22:55,  6.70s/it] 89%| | 5758/6500 [10:53:01<1:22:17,  6.65s/it]                                                         89%| | 5758/6500 [10:53:01<1:22:17,  6.65s/it] 89%| | 5759/6500 [10:53:08<1:21:48,  6.62s/it]                                                         89%| | 5759/6500 [10:53:08<1:21:48,  6.62s/it] 89%| | 5760/6500 [10:53:15<1:21:22,  6.60s/it]                                                         89%| | 5760/6500 [10:53:15<1:21:22,  6.60s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8802835941314697, 'eval_runtime': 1.4749, 'eval_samples_per_second': 8.136, 'eval_steps_per_second': 2.034, 'epoch': 0.89}
                                                         89%| | 5760/6500 [10:53:16<1:21:22,  6.60s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-5760
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5760I AM HERE AND I AM SAVING THE MODEL

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5760
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5760
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5760
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5760/pytorch_model.bin
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5760/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5760/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5760/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3646, 'learning_rate': 3.1575075345002037e-06, 'epoch': 0.89}
{'loss': 0.3691, 'learning_rate': 3.1490587544096782e-06, 'epoch': 0.89}
{'loss': 0.367, 'learning_rate': 3.1406209254418116e-06, 'epoch': 0.89}
{'loss': 0.3595, 'learning_rate': 3.1321940495688874e-06, 'epoch': 0.89}
{'loss': 0.3588, 'learning_rate': 3.1237781287606516e-06, 'epoch': 0.89}
 89%| | 5761/6500 [10:53:23<1:28:02,  7.15s/it]                                                         89%| | 5761/6500 [10:53:23<1:28:02,  7.15s/it] 89%| | 5762/6500 [10:53:29<1:25:42,  6.97s/it]                                                         89%| | 5762/6500 [10:53:29<1:25:42,  6.97s/it] 89%| | 5763/6500 [10:53:36<1:24:05,  6.85s/it]                                                         89%| | 5763/6500 [10:53:36<1:24:05,  6.85s/it] 89%| | 5764/6500 [10:53:43<1:22:54,  6.76s/it]                                                         89%| | 5764/6500 [10:53:43<1:22:54,  6.76s/it] 89%| | 5765/6500 [10:53:49<1:21:59,  6.69s/it]                                                         89%| | 5765/6500 [10:53:49<1:2{'loss': 0.3528, 'learning_rate': 3.115373164984259e-06, 'epoch': 0.89}
{'loss': 0.3949, 'learning_rate': 3.1069791602043317e-06, 'epoch': 0.89}
{'loss': 0.3654, 'learning_rate': 3.098596116382907e-06, 'epoch': 0.89}
{'loss': 0.6325, 'learning_rate': 3.0902240354794775e-06, 'epoch': 0.89}
{'loss': 0.3736, 'learning_rate': 3.081862919450973e-06, 'epoch': 0.89}
1:59,  6.69s/it] 89%| | 5766/6500 [10:53:56<1:21:23,  6.65s/it]                                                         89%| | 5766/6500 [10:53:56<1:21:23,  6.65s/it] 89%| | 5767/6500 [10:54:02<1:20:57,  6.63s/it]                                                         89%| | 5767/6500 [10:54:02<1:20:57,  6.63s/it] 89%| | 5768/6500 [10:54:09<1:20:34,  6.60s/it]                                                         89%| | 5768/6500 [10:54:09<1:20:34,  6.60s/it] 89%| | 5769/6500 [10:54:15<1:20:14,  6.59s/it]                                                         89%| | 5769/6500 [10:54:15<1:20:14,  6.59s/it] 89%| | 5770/6500 [10:54:23<1:23:19,  6.85s/it]                                                         89%| | 5770/6500 [10:54:23<1:23:19,  6.85s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8817852735519409, 'eval_runtime': 1.4695, 'eval_samples_per_second': 8.166, 'eval_steps_per_second': 2.042, 'epoch': 0.89}
                                                         89%| | 5770/6500 [10:54:24<1:23:19,  6.85s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-5770
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5770
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5770
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5770
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5770
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5770/pytorch_model.bin
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5770/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5770/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5770/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3584, 'learning_rate': 3.0735127702517296e-06, 'epoch': 0.89}
{'loss': 0.3611, 'learning_rate': 3.0651735898335644e-06, 'epoch': 0.89}
{'loss': 0.3621, 'learning_rate': 3.056845380145701e-06, 'epoch': 0.89}
{'loss': 0.3632, 'learning_rate': 3.0485281431348157e-06, 'epoch': 0.89}
{'loss': 0.3671, 'learning_rate': 3.0402218807450035e-06, 'epoch': 0.89}
 89%| | 5771/6500 [10:54:31<1:28:51,  7.31s/it]                                                         89%| | 5771/6500 [10:54:31<1:28:51,  7.31s/it] 89%| | 5772/6500 [10:54:38<1:25:54,  7.08s/it]                                                         89%| | 5772/6500 [10:54:38<1:25:54,  7.08s/it] 89%| | 5773/6500 [10:54:44<1:23:46,  6.91s/it]                                                         89%| | 5773/6500 [10:54:44<1:23:46,  6.91s/it] 89%| | 5774/6500 [10:54:51<1:22:17,  6.80s/it]                                                         89%| | 5774/6500 [10:54:51<1:22:17,  6.80s/it] 89%| | 5775/6500 [10:54:57<1:21:10,  6.72s/it]                                                         89%| | 5775/6500 [10:54:57<1:2{'loss': 0.3744, 'learning_rate': 3.0319265949178054e-06, 'epoch': 0.89}
{'loss': 0.3688, 'learning_rate': 3.0236422875921988e-06, 'epoch': 0.89}
{'loss': 0.364, 'learning_rate': 3.0153689607045845e-06, 'epoch': 0.89}
{'loss': 0.3652, 'learning_rate': 3.007106616188804e-06, 'epoch': 0.89}
{'loss': 0.3562, 'learning_rate': 2.9988552559761294e-06, 'epoch': 0.89}
1:10,  6.72s/it] 89%| | 5776/6500 [10:55:04<1:20:24,  6.66s/it]                                                         89%| | 5776/6500 [10:55:04<1:20:24,  6.66s/it] 89%| | 5777/6500 [10:55:10<1:19:52,  6.63s/it]                                                         89%| | 5777/6500 [10:55:10<1:19:52,  6.63s/it] 89%| | 5778/6500 [10:55:17<1:19:28,  6.60s/it]                                                         89%| | 5778/6500 [10:55:17<1:19:28,  6.60s/it] 89%| | 5779/6500 [10:55:24<1:19:07,  6.58s/it]                                                         89%| | 5779/6500 [10:55:24<1:19:07,  6.58s/it] 89%| | 5780/6500 [10:55:30<1:18:51,  6.57s/it]                                                         89%| | 5780/6500 [10:55:30<1:18:51,  6.57s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8814797401428223, 'eval_runtime': 1.4723, 'eval_samples_per_second': 8.15, 'eval_steps_per_second': 2.038, 'epoch': 0.89}
                                                         89%| | 5780/6500 [10:55:32<1:18:51,  6.57s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-5780
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5780
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5780
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5780
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5780
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5780/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5780/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5780/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5780/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3551, 'learning_rate': 2.9906148819952797e-06, 'epoch': 0.89}
{'loss': 0.3935, 'learning_rate': 2.9823854961723686e-06, 'epoch': 0.89}
{'loss': 0.3633, 'learning_rate': 2.9741671004309635e-06, 'epoch': 0.89}
{'loss': 0.6341, 'learning_rate': 2.9659596966920945e-06, 'epoch': 0.89}
{'loss': 0.3692, 'learning_rate': 2.9577632868741654e-06, 'epoch': 0.89}
 89%| | 5781/6500 [10:55:38<1:25:10,  7.11s/it]                                                         89%| | 5781/6500 [10:55:38<1:25:10,  7.11s/it] 89%| | 5782/6500 [10:55:45<1:23:02,  6.94s/it]                                                         89%| | 5782/6500 [10:55:45<1:23:02,  6.94s/it] 89%| | 5783/6500 [10:55:52<1:21:30,  6.82s/it]                                                         89%| | 5783/6500 [10:55:52<1:21:30,  6.82s/it] 89%| | 5784/6500 [10:55:58<1:20:22,  6.74s/it]                                                         89%| | 5784/6500 [10:55:58<1:20:22,  6.74s/it] 89%| | 5785/6500 [10:56:05<1:19:34,  6.68s/it]                                                         89%| | 5785/6500 [10:56:05<1:1{'loss': 0.364, 'learning_rate': 2.94957787289305e-06, 'epoch': 0.89}
{'loss': 0.3466, 'learning_rate': 2.94140345666204e-06, 'epoch': 0.89}
{'loss': 0.353, 'learning_rate': 2.9332400400918447e-06, 'epoch': 0.89}
{'loss': 0.3612, 'learning_rate': 2.9250876250906224e-06, 'epoch': 0.89}
{'loss': 0.3599, 'learning_rate': 2.9169462135639535e-06, 'epoch': 0.89}
9:34,  6.68s/it] 89%| | 5786/6500 [10:56:12<1:22:06,  6.90s/it]                                                         89%| | 5786/6500 [10:56:12<1:22:06,  6.90s/it] 89%| | 5787/6500 [10:56:19<1:20:44,  6.79s/it]                                                         89%| | 5787/6500 [10:56:19<1:20:44,  6.79s/it] 89%| | 5788/6500 [10:56:25<1:19:44,  6.72s/it]                                                         89%| | 5788/6500 [10:56:25<1:19:44,  6.72s/it] 89%| | 5789/6500 [10:56:32<1:19:00,  6.67s/it]                                                         89%| | 5789/6500 [10:56:32<1:19:00,  6.67s/it] 89%| | 5790/6500 [10:56:38<1:18:28,  6.63s/it]                                                         89%| | 5790/6500 [10:56:38<1:18:28,  6.63s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8804136514663696, 'eval_runtime': 1.4716, 'eval_samples_per_second': 8.155, 'eval_steps_per_second': 2.039, 'epoch': 0.89}
                                                         89%| | 5790/6500 [10:56:40<1:18:28,  6.63s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-5790
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in I AM HERE AND I AM SAVING THE MODEL
 the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5790
/projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5790
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5790
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5790
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5790/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5790/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5790/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5790/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3716, 'learning_rate': 2.908815807414833e-06, 'epoch': 0.89}
{'loss': 0.3648, 'learning_rate': 2.900696408543696e-06, 'epoch': 0.89}
{'loss': 0.3644, 'learning_rate': 2.8925880188484077e-06, 'epoch': 0.89}
{'loss': 0.3726, 'learning_rate': 2.8844906402242465e-06, 'epoch': 0.89}
{'loss': 0.3539, 'learning_rate': 2.8764042745639373e-06, 'epoch': 0.89}
 89%| | 5791/6500 [10:56:47<1:24:28,  7.15s/it]                                                         89%| | 5791/6500 [10:56:47<1:24:28,  7.15s/it] 89%| | 5792/6500 [10:56:53<1:22:14,  6.97s/it]                                                         89%| | 5792/6500 [10:56:53<1:22:14,  6.97s/it] 89%| | 5793/6500 [10:57:00<1:20:34,  6.84s/it]                                                         89%| | 5793/6500 [10:57:00<1:20:34,  6.84s/it] 89%| | 5794/6500 [10:57:06<1:19:25,  6.75s/it]                                                         89%| | 5794/6500 [10:57:06<1:19:25,  6.75s/it] 89%| | 5795/6500 [10:57:13<1:20:18,  6.83s/it]                                                         89%| | 5795/6500 [10:57:13<1:2{'loss': 0.362, 'learning_rate': 2.868328923757607e-06, 'epoch': 0.89}
{'loss': 0.3884, 'learning_rate': 2.8602645896928295e-06, 'epoch': 0.89}
{'loss': 0.3644, 'learning_rate': 2.85221127425459e-06, 'epoch': 0.89}
{'loss': 0.6429, 'learning_rate': 2.8441689793253013e-06, 'epoch': 0.89}
{'loss': 0.3595, 'learning_rate': 2.8361377067848027e-06, 'epoch': 0.89}
0:18,  6.83s/it] 89%| | 5796/6500 [10:57:20<1:19:18,  6.76s/it]                                                         89%| | 5796/6500 [10:57:20<1:19:18,  6.76s/it] 89%| | 5797/6500 [10:57:26<1:18:22,  6.69s/it]                                                         89%| | 5797/6500 [10:57:26<1:18:22,  6.69s/it] 89%| | 5798/6500 [10:57:33<1:17:42,  6.64s/it]                                                         89%| | 5798/6500 [10:57:33<1:17:42,  6.64s/it] 89%| | 5799/6500 [10:57:39<1:17:11,  6.61s/it]                                                         89%| | 5799/6500 [10:57:39<1:17:11,  6.61s/it] 89%| | 5800/6500 [10:57:46<1:16:55,  6.59s/it]                                                         89%| | 5800/6500 [10:57:46<1:16:55,  6.59s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8807246088981628, 'eval_runtime': 1.4744, 'eval_samples_per_second': 8.139, 'eval_steps_per_second': 2.035, 'epoch': 0.89}
                                                         89%| | 5800/6500 [10:57:47<1:16:55,  6.59s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-5800
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5800
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5800
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5800
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5800
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5800/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5800/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5800/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5800/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3733, 'learning_rate': 2.82811745851036e-06, 'epoch': 0.89}
{'loss': 0.3499, 'learning_rate': 2.820108236376645e-06, 'epoch': 0.89}
{'loss': 0.3616, 'learning_rate': 2.812110042255772e-06, 'epoch': 0.89}
{'loss': 0.3563, 'learning_rate': 2.8041228780172678e-06, 'epoch': 0.89}
{'loss': 0.3665, 'learning_rate': 2.7961467455280834e-06, 'epoch': 0.89}
 89%| | 5801/6500 [10:57:54<1:23:20,  7.15s/it]                                                         89%| | 5801/6500 [10:57:54<1:23:20,  7.15s/it] 89%| | 5802/6500 [10:58:02<1:24:45,  7.29s/it]                                                         89%| | 5802/6500 [10:58:02<1:24:45,  7.29s/it] 89%| | 5803/6500 [10:58:09<1:22:08,  7.07s/it]                                                         89%| | 5803/6500 [10:58:09<1:22:08,  7.07s/it] 89%| | 5804/6500 [10:58:15<1:20:18,  6.92s/it]                                                         89%| | 5804/6500 [10:58:15<1:20:18,  6.92s/it] 89%| | 5805/6500 [10:58:22<1:18:58,  6.82s/it]                                                         89%| | 5805/6500 [10:58:22<1:1{'loss': 0.3725, 'learning_rate': 2.7881816466525935e-06, 'epoch': 0.89}
{'loss': 0.3734, 'learning_rate': 2.7802275832525927e-06, 'epoch': 0.89}
{'loss': 0.3659, 'learning_rate': 2.7722845571872937e-06, 'epoch': 0.89}
{'loss': 0.3668, 'learning_rate': 2.7643525703133334e-06, 'epoch': 0.89}
{'loss': 0.3535, 'learning_rate': 2.7564316244847565e-06, 'epoch': 0.89}
8:58,  6.82s/it] 89%| | 5806/6500 [10:58:28<1:18:02,  6.75s/it]                                                         89%| | 5806/6500 [10:58:28<1:18:02,  6.75s/it] 89%| | 5807/6500 [10:58:35<1:17:22,  6.70s/it]                                                         89%| | 5807/6500 [10:58:35<1:17:22,  6.70s/it] 89%| | 5808/6500 [10:58:41<1:16:53,  6.67s/it]                                                         89%| | 5808/6500 [10:58:41<1:16:53,  6.67s/it] 89%| | 5809/6500 [10:58:48<1:16:29,  6.64s/it]                                                         89%| | 5809/6500 [10:58:48<1:16:29,  6.64s/it] 89%| | 5810/6500 [10:58:55<1:16:10,  6.62s/it]                                                         89%| | 5810/6500 [10:58:55<1:16:10,  6.62s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8799440264701843, 'eval_runtime': 1.4795, 'eval_samples_per_second': 8.111, 'eval_steps_per_second': 2.028, 'epoch': 0.89}
                                                         89%| | 5810/6500 [10:58:56<1:16:10,  6.62s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-5810
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5810
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5810
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5810
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5810
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5810/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5810/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5810/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5810/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3894, 'learning_rate': 2.7485217215530434e-06, 'epoch': 0.89}
{'loss': 0.3608, 'learning_rate': 2.7406228633670872e-06, 'epoch': 0.89}
{'loss': 0.6265, 'learning_rate': 2.7327350517732e-06, 'epoch': 0.89}
{'loss': 0.391, 'learning_rate': 2.7248582886151008e-06, 'epoch': 0.89}
{'loss': 0.3633, 'learning_rate': 2.7169925757339344e-06, 'epoch': 0.89}
 89%| | 5811/6500 [10:59:03<1:23:11,  7.24s/it]                                                         89%| | 5811/6500 [10:59:03<1:23:11,  7.24s/it] 89%| | 5812/6500 [10:59:10<1:20:41,  7.04s/it]                                                         89%| | 5812/6500 [10:59:10<1:20:41,  7.04s/it] 89%| | 5813/6500 [10:59:16<1:18:57,  6.90s/it]                                                         89%| | 5813/6500 [10:59:16<1:18:57,  6.90s/it] 89%| | 5814/6500 [10:59:23<1:17:44,  6.80s/it]                                                         89%| | 5814/6500 [10:59:23<1:17:44,  6.80s/it] 89%| | 5815/6500 [10:59:30<1:16:49,  6.73s/it]                                                         89%| | 5815/6500 [10:59:30<1:1{'loss': 0.3755, 'learning_rate': 2.7091379149682685e-06, 'epoch': 0.89}
{'loss': 0.3558, 'learning_rate': 2.701294308154084e-06, 'epoch': 0.89}
{'loss': 0.3614, 'learning_rate': 2.693461757124771e-06, 'epoch': 0.9}
{'loss': 0.3597, 'learning_rate': 2.685640263711148e-06, 'epoch': 0.9}
{'loss': 0.3713, 'learning_rate': 2.677829829741435e-06, 'epoch': 0.9}
6:49,  6.73s/it] 89%| | 5816/6500 [10:59:36<1:16:09,  6.68s/it]                                                         89%| | 5816/6500 [10:59:36<1:16:09,  6.68s/it] 89%| | 5817/6500 [10:59:43<1:15:40,  6.65s/it]                                                         89%| | 5817/6500 [10:59:43<1:15:40,  6.65s/it] 90%| | 5818/6500 [10:59:50<1:17:41,  6.83s/it]                                                         90%| | 5818/6500 [10:59:50<1:17:41,  6.83s/it] 90%| | 5819/6500 [10:59:57<1:16:38,  6.75s/it]                                                         90%| | 5819/6500 [10:59:57<1:16:38,  6.75s/it] 90%| | 5820/6500 [11:00:03<1:15:59,  6.70s/it]                                                         90%| | 5820/6500 [11:00:03<1:15:59,  6.70s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8801040649414062, 'eval_runtime': 1.5031, 'eval_samples_per_second': 7.984, 'eval_steps_per_second': 1.996, 'epoch': 0.9}
                                                         90%| | 5820/6500 [11:00:05<1:15:59,  6.70s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-5820
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5820
/projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5820
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5820
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5820
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5820/pytorch_model.bin
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5820/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5820/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5820/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3751, 'learning_rate': 2.6700304570412726e-06, 'epoch': 0.9}
{'loss': 0.3723, 'learning_rate': 2.6622421474337243e-06, 'epoch': 0.9}
{'loss': 0.3553, 'learning_rate': 2.6544649027392566e-06, 'epoch': 0.9}
{'loss': 0.371, 'learning_rate': 2.646698724775759e-06, 'epoch': 0.9}
{'loss': 0.3544, 'learning_rate': 2.6389436153585132e-06, 'epoch': 0.9}
 90%| | 5821/6500 [11:00:12<1:21:51,  7.23s/it]                                                         90%| | 5821/6500 [11:00:12<1:21:51,  7.23s/it] 90%| | 5822/6500 [11:00:18<1:19:26,  7.03s/it]                                                         90%| | 5822/6500 [11:00:18<1:19:26,  7.03s/it] 90%| | 5823/6500 [11:00:25<1:17:45,  6.89s/it]                                                         90%| | 5823/6500 [11:00:25<1:17:45,  6.89s/it] 90%| | 5824/6500 [11:00:31<1:16:28,  6.79s/it]                                                         90%| | 5824/6500 [11:00:31<1:16:28,  6.79s/it] 90%| | 5825/6500 [11:00:38<1:15:33,  6.72s/it]                                                         90%| | 5825/6500 [11:00:38<1:1{'loss': 0.3981, 'learning_rate': 2.631199576300236e-06, 'epoch': 0.9}
{'loss': 0.3622, 'learning_rate': 2.623466609411052e-06, 'epoch': 0.9}
{'loss': 0.6366, 'learning_rate': 2.615744716498492e-06, 'epoch': 0.9}
{'loss': 0.375, 'learning_rate': 2.608033899367507e-06, 'epoch': 0.9}
{'loss': 0.3615, 'learning_rate': 2.60033415982045e-06, 'epoch': 0.9}
5:33,  6.72s/it] 90%| | 5826/6500 [11:00:45<1:15:33,  6.73s/it]                                                         90%| | 5826/6500 [11:00:45<1:15:33,  6.73s/it] 90%| | 5827/6500 [11:00:51<1:14:55,  6.68s/it]                                                         90%| | 5827/6500 [11:00:51<1:14:55,  6.68s/it] 90%| | 5828/6500 [11:00:58<1:14:26,  6.65s/it]                                                         90%| | 5828/6500 [11:00:58<1:14:26,  6.65s/it] 90%| | 5829/6500 [11:01:04<1:14:05,  6.62s/it]                                                         90%| | 5829/6500 [11:01:04<1:14:05,  6.62s/it] 90%| | 5830/6500 [11:01:11<1:13:48,  6.61s/it]                                                         90%| | 5830/6500 [11:01:11<1:13:48,  6.61s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8792836666107178, 'eval_runtime': 2.0026, 'eval_samples_per_second': 5.992, 'eval_steps_per_second': 1.498, 'epoch': 0.9}
                                                         90%| | 5830/6500 [11:01:13<1:13:48,  6.61s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-5830
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5830
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5830
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5830
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5830
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5830/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5830/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5830/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5830/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3685, 'learning_rate': 2.592645499657087e-06, 'epoch': 0.9}
{'loss': 0.3537, 'learning_rate': 2.584967920674597e-06, 'epoch': 0.9}
{'loss': 0.3575, 'learning_rate': 2.5773014246675665e-06, 'epoch': 0.9}
{'loss': 0.3726, 'learning_rate': 2.5696460134279955e-06, 'epoch': 0.9}
{'loss': 0.3692, 'learning_rate': 2.562001688745291e-06, 'epoch': 0.9}
 90%| | 5831/6500 [11:01:20<1:21:36,  7.32s/it]                                                         90%| | 5831/6500 [11:01:20<1:21:36,  7.32s/it] 90%| | 5832/6500 [11:01:26<1:19:01,  7.10s/it]                                                         90%| | 5832/6500 [11:01:26<1:19:01,  7.10s/it] 90%| | 5833/6500 [11:01:33<1:17:11,  6.94s/it]                                                         90%| | 5833/6500 [11:01:33<1:17:11,  6.94s/it] 90%| | 5834/6500 [11:01:40<1:17:56,  7.02s/it]                                                         90%| | 5834/6500 [11:01:40<1:17:56,  7.02s/it] 90%| | 5835/6500 [11:01:47<1:16:20,  6.89s/it]                                                         90%| | 5835/6500 [11:01:47<1:1{'loss': 0.3599, 'learning_rate': 2.554368452406258e-06, 'epoch': 0.9}
{'loss': 0.3669, 'learning_rate': 2.5467463061951303e-06, 'epoch': 0.9}
{'loss': 0.358, 'learning_rate': 2.539135251893526e-06, 'epoch': 0.9}
{'loss': 0.3593, 'learning_rate': 2.531535291280496e-06, 'epoch': 0.9}
{'loss': 0.346, 'learning_rate': 2.523946426132473e-06, 'epoch': 0.9}
6:20,  6.89s/it] 90%| | 5836/6500 [11:01:53<1:15:13,  6.80s/it]                                                         90%| | 5836/6500 [11:01:53<1:15:13,  6.80s/it] 90%| | 5837/6500 [11:02:00<1:14:25,  6.73s/it]                                                         90%| | 5837/6500 [11:02:00<1:14:25,  6.73s/it] 90%| | 5838/6500 [11:02:07<1:13:51,  6.69s/it]                                                         90%| | 5838/6500 [11:02:07<1:13:51,  6.69s/it] 90%| | 5839/6500 [11:02:13<1:13:19,  6.66s/it]                                                         90%| | 5839/6500 [11:02:13<1:13:19,  6.66s/it] 90%| | 5840/6500 [11:02:20<1:12:56,  6.63s/it]                                                         90%| | 5840/6500 [11:02:20<1:12:56,  6.63s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8798468112945557, 'eval_runtime': 1.492, 'eval_samples_per_second': 8.043, 'eval_steps_per_second': 2.011, 'epoch': 0.9}
                                                         90%| | 5840/6500 [11:02:21<1:12:56,  6.63s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-5840
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5840
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5840
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5840
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5840
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5840/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5840/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5840/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5840/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3971, 'learning_rate': 2.516368658223317e-06, 'epoch': 0.9}
{'loss': 0.3665, 'learning_rate': 2.508801989324283e-06, 'epoch': 0.9}
{'loss': 0.6311, 'learning_rate': 2.5012464212040287e-06, 'epoch': 0.9}
{'loss': 0.374, 'learning_rate': 2.49370195562863e-06, 'epoch': 0.9}
{'loss': 0.3584, 'learning_rate': 2.486168594361554e-06, 'epoch': 0.9}
 90%| | 5841/6500 [11:02:28<1:18:53,  7.18s/it]                                                         90%| | 5841/6500 [11:02:28<1:18:53,  7.18s/it] 90%| | 5842/6500 [11:02:35<1:16:45,  7.00s/it]                                                         90%| | 5842/6500 [11:02:35<1:16:45,  7.00s/it] 90%| | 5843/6500 [11:02:41<1:15:14,  6.87s/it]                                                         90%| | 5843/6500 [11:02:41<1:15:14,  6.87s/it] 90%| | 5844/6500 [11:02:48<1:14:09,  6.78s/it]                                                         90%| | 5844/6500 [11:02:48<1:14:09,  6.78s/it] 90%| | 5845/6500 [11:02:54<1:13:22,  6.72s/it]                                                         90%| | 5845/6500 [11:02:54<1:1{'loss': 0.3577, 'learning_rate': 2.4786463391636874e-06, 'epoch': 0.9}
{'loss': 0.3505, 'learning_rate': 2.4711351917933e-06, 'epoch': 0.9}
{'loss': 0.366, 'learning_rate': 2.4636351540060777e-06, 'epoch': 0.9}
{'loss': 0.3531, 'learning_rate': 2.456146227555117e-06, 'epoch': 0.9}
{'loss': 0.3659, 'learning_rate': 2.4486684141908966e-06, 'epoch': 0.9}
3:22,  6.72s/it] 90%| | 5846/6500 [11:03:01<1:12:47,  6.68s/it]                                                         90%| | 5846/6500 [11:03:01<1:12:47,  6.68s/it] 90%| | 5847/6500 [11:03:08<1:12:21,  6.65s/it]                                                         90%| | 5847/6500 [11:03:08<1:12:21,  6.65s/it] 90%| | 5848/6500 [11:03:14<1:12:00,  6.63s/it]                                                         90%| | 5848/6500 [11:03:14<1:12:00,  6.63s/it] 90%| | 5849/6500 [11:03:21<1:11:42,  6.61s/it]                                                         90%| | 5849/6500 [11:03:21<1:11:42,  6.61s/it] 90%| | 5850/6500 [11:03:27<1:11:29,  6.60s/it]                                                         90%| | 5850/6500 [11:03:27<1:11:29,  6.60s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8816515803337097, 'eval_runtime': 1.4783, 'eval_samples_per_second': 8.117, 'eval_steps_per_second': 2.029, 'epoch': 0.9}
                                                         90%| | 5850/6500 [11:03:29<1:11:29,  6.60s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-5850
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5850
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5850
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5850
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5850
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5850/pytorch_model.bin
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5850/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5850/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5850/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3648, 'learning_rate': 2.441201715661323e-06, 'epoch': 0.9}
{'loss': 0.3686, 'learning_rate': 2.4337461337116894e-06, 'epoch': 0.9}
{'loss': 0.3716, 'learning_rate': 2.4263016700846854e-06, 'epoch': 0.9}
{'loss': 0.3585, 'learning_rate': 2.4188683265204127e-06, 'epoch': 0.9}
{'loss': 0.3518, 'learning_rate': 2.4114461047563706e-06, 'epoch': 0.9}
 90%| | 5851/6500 [11:03:37<1:20:39,  7.46s/it]                                                         90%| | 5851/6500 [11:03:37<1:20:39,  7.46s/it] 90%| | 5852/6500 [11:03:43<1:17:43,  7.20s/it]                                                         90%| | 5852/6500 [11:03:43<1:17:43,  7.20s/it] 90%| | 5853/6500 [11:03:50<1:15:36,  7.01s/it]                                                         90%| | 5853/6500 [11:03:50<1:15:36,  7.01s/it] 90%| | 5854/6500 [11:03:57<1:14:07,  6.88s/it]                                                         90%| | 5854/6500 [11:03:57<1:14:07,  6.88s/it] 90%| | 5855/6500 [11:04:03<1:13:02,  6.79s/it]                                                         90%| | 5855/6500 [11:04:03<1:1{'loss': 0.3996, 'learning_rate': 2.4040350065274554e-06, 'epoch': 0.9}
{'loss': 0.3682, 'learning_rate': 2.39663503356598e-06, 'epoch': 0.9}
{'loss': 0.6417, 'learning_rate': 2.389246187601618e-06, 'epoch': 0.9}
{'loss': 0.3754, 'learning_rate': 2.3818684703614923e-06, 'epoch': 0.9}
{'loss': 0.3699, 'learning_rate': 2.374501883570085e-06, 'epoch': 0.9}
3:02,  6.79s/it] 90%| | 5856/6500 [11:04:10<1:12:14,  6.73s/it]                                                         90%| | 5856/6500 [11:04:10<1:12:14,  6.73s/it] 90%| | 5857/6500 [11:04:16<1:11:41,  6.69s/it]                                                         90%| | 5857/6500 [11:04:16<1:11:41,  6.69s/it] 90%| | 5858/6500 [11:04:23<1:11:14,  6.66s/it]                                                         90%| | 5858/6500 [11:04:23<1:11:14,  6.66s/it] 90%| | 5859/6500 [11:04:30<1:10:53,  6.64s/it]                                                         90%| | 5859/6500 [11:04:30<1:10:53,  6.64s/it] 90%| | 5860/6500 [11:04:36<1:10:36,  6.62s/it]                                                         90%| | 5860/6500 [11:04:36<1:10:36,  6.62s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8805097341537476, 'eval_runtime': 1.4803, 'eval_samples_per_second': 8.106, 'eval_steps_per_second': 2.027, 'epoch': 0.9}
                                                         90%| | 5860/6500 [11:04:38<1:10:36,  6.62s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-5860
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5860
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5860
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5860
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5860
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5860/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5860/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5860/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5860/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3591, 'learning_rate': 2.3671464289492906e-06, 'epoch': 0.9}
{'loss': 0.355, 'learning_rate': 2.359802108218412e-06, 'epoch': 0.9}
{'loss': 0.3624, 'learning_rate': 2.352468923094131e-06, 'epoch': 0.9}
{'loss': 0.362, 'learning_rate': 2.345146875290538e-06, 'epoch': 0.9}
{'loss': 0.3745, 'learning_rate': 2.337835966519114e-06, 'epoch': 0.9}
 90%| | 5861/6500 [11:04:45<1:16:17,  7.16s/it]                                                         90%| | 5861/6500 [11:04:45<1:16:17,  7.16s/it] 90%| | 5862/6500 [11:04:51<1:14:11,  6.98s/it]                                                         90%| | 5862/6500 [11:04:51<1:14:11,  6.98s/it] 90%| | 5863/6500 [11:04:58<1:12:41,  6.85s/it]                                                         90%| | 5863/6500 [11:04:58<1:12:41,  6.85s/it] 90%| | 5864/6500 [11:05:04<1:11:35,  6.75s/it]                                                         90%| | 5864/6500 [11:05:04<1:11:35,  6.75s/it] 90%| | 5865/6500 [11:05:11<1:10:50,  6.69s/it]                                                         90%| | 5865/6500 [11:05:11<1:1{'loss': 0.3667, 'learning_rate': 2.330536198488753e-06, 'epoch': 0.9}
{'loss': 0.3651, 'learning_rate': 2.3232475729057122e-06, 'epoch': 0.9}
{'loss': 0.3639, 'learning_rate': 2.315970091473668e-06, 'epoch': 0.9}
{'loss': 0.3501, 'learning_rate': 2.3087037558936987e-06, 'epoch': 0.9}
{'loss': 0.3583, 'learning_rate': 2.3014485678642563e-06, 'epoch': 0.9}
0:50,  6.69s/it] 90%| | 5866/6500 [11:05:17<1:10:15,  6.65s/it]                                                         90%| | 5866/6500 [11:05:17<1:10:15,  6.65s/it] 90%| | 5867/6500 [11:05:25<1:13:07,  6.93s/it]                                                         90%| | 5867/6500 [11:05:25<1:13:07,  6.93s/it] 90%| | 5868/6500 [11:05:31<1:11:47,  6.82s/it]                                                         90%| | 5868/6500 [11:05:31<1:11:47,  6.82s/it] 90%| | 5869/6500 [11:05:38<1:10:48,  6.73s/it]                                                         90%| | 5869/6500 [11:05:38<1:10:48,  6.73s/it] 90%| | 5870/6500 [11:05:44<1:10:05,  6.67s/it]                                                         90%| | 5870/6500 [11:05:44<1:10:05,  6.67s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8803302049636841, 'eval_runtime': 1.4767, 'eval_samples_per_second': 8.126, 'eval_steps_per_second': 2.032, 'epoch': 0.9}
                                                         90%| | 5870/6500 [11:05:46<1:10:05,  6.67s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-5870
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5870
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5870
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5870
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5870
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5870/pytorch_model.bin
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5870/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5870/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5870/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3877, 'learning_rate': 2.2942045290811955e-06, 'epoch': 0.9}
{'loss': 0.3625, 'learning_rate': 2.286971641237773e-06, 'epoch': 0.9}
{'loss': 0.6419, 'learning_rate': 2.2797499060246253e-06, 'epoch': 0.9}
{'loss': 0.359, 'learning_rate': 2.2725393251297964e-06, 'epoch': 0.9}
{'loss': 0.3727, 'learning_rate': 2.2653399002387164e-06, 'epoch': 0.9}
 90%| | 5871/6500 [11:05:53<1:15:24,  7.19s/it]                                                         90%| | 5871/6500 [11:05:53<1:15:24,  7.19s/it] 90%| | 5872/6500 [11:05:59<1:13:15,  7.00s/it]                                                         90%| | 5872/6500 [11:05:59<1:13:15,  7.00s/it] 90%| | 5873/6500 [11:06:06<1:11:43,  6.86s/it]                                                         90%| | 5873/6500 [11:06:06<1:11:43,  6.86s/it] 90%| | 5874/6500 [11:06:12<1:10:35,  6.77s/it]                                                         90%| | 5874/6500 [11:06:12<1:10:35,  6.77s/it] 90%| | 5875/6500 [11:06:19<1:09:45,  6.70s/it]                                                         90%| | 5875/6500 [11:06:19<1:0{'loss': 0.3634, 'learning_rate': 2.2581516330342003e-06, 'epoch': 0.9}
{'loss': 0.36, 'learning_rate': 2.2509745251964697e-06, 'epoch': 0.9}
{'loss': 0.362, 'learning_rate': 2.243808578403117e-06, 'epoch': 0.9}
{'loss': 0.3677, 'learning_rate': 2.236653794329152e-06, 'epoch': 0.9}
{'loss': 0.3733, 'learning_rate': 2.229510174646954e-06, 'epoch': 0.9}
9:45,  6.70s/it] 90%| | 5876/6500 [11:06:26<1:09:10,  6.65s/it]                                                         90%| | 5876/6500 [11:06:26<1:09:10,  6.65s/it] 90%| | 5877/6500 [11:06:33<1:11:46,  6.91s/it]                                                         90%| | 5877/6500 [11:06:33<1:11:46,  6.91s/it] 90%| | 5878/6500 [11:06:40<1:10:29,  6.80s/it]                                                         90%| | 5878/6500 [11:06:40<1:10:29,  6.80s/it] 90%| | 5879/6500 [11:06:46<1:09:33,  6.72s/it]                                                         90%| | 5879/6500 [11:06:46<1:09:33,  6.72s/it] 90%| | 5880/6500 [11:06:53<1:08:53,  6.67s/it]                                                         90%| | 5880/6500 [11:06:53<1:08:53,  6.67s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8816896677017212, 'eval_runtime': 1.6308, 'eval_samples_per_second': 7.358, 'eval_steps_per_second': 1.84, 'epoch': 0.9}
                                                         90%| | 5880/6500 [11:06:54<1:08:53,  6.67s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-5880
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5880
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5880
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5880
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5880
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5880/pytorch_model.bin
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5880/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5880/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5880/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.37, 'learning_rate': 2.2223777210262975e-06, 'epoch': 0.9}
{'loss': 0.3641, 'learning_rate': 2.2152564351343664e-06, 'epoch': 0.9}
{'loss': 0.3698, 'learning_rate': 2.208146318635701e-06, 'epoch': 0.91}
{'loss': 0.3477, 'learning_rate': 2.2010473731922553e-06, 'epoch': 0.91}
{'loss': 0.3659, 'learning_rate': 2.1939596004633635e-06, 'epoch': 0.91}
 90%| | 5881/6500 [11:07:01<1:14:48,  7.25s/it]                                                         90%| | 5881/6500 [11:07:01<1:14:48,  7.25s/it] 90%| | 5882/6500 [11:07:08<1:12:29,  7.04s/it]                                                         90%| | 5882/6500 [11:07:08<1:12:29,  7.04s/it] 91%| | 5883/6500 [11:07:15<1:13:56,  7.19s/it]                                                         91%| | 5883/6500 [11:07:15<1:13:56,  7.19s/it] 91%| | 5884/6500 [11:07:22<1:11:49,  7.00s/it]                                                         91%| | 5884/6500 [11:07:22<1:11:49,  7.00s/it] 91%| | 5885/6500 [11:07:28<1:10:16,  6.86s/it]                                                         91%| | 5885/6500 [11:07:28<1:1{'loss': 0.3812, 'learning_rate': 2.1868830021057497e-06, 'epoch': 0.91}
{'loss': 0.3735, 'learning_rate': 2.1798175797735298e-06, 'epoch': 0.91}
{'loss': 0.6401, 'learning_rate': 2.1727633351182e-06, 'epoch': 0.91}
{'loss': 0.3594, 'learning_rate': 2.16572026978864e-06, 'epoch': 0.91}
{'loss': 0.3786, 'learning_rate': 2.1586883854311346e-06, 'epoch': 0.91}
0:16,  6.86s/it] 91%| | 5886/6500 [11:07:35<1:09:12,  6.76s/it]                                                         91%| | 5886/6500 [11:07:35<1:09:12,  6.76s/it] 91%| | 5887/6500 [11:07:42<1:08:23,  6.69s/it]                                                         91%| | 5887/6500 [11:07:42<1:08:23,  6.69s/it] 91%| | 5888/6500 [11:07:48<1:07:51,  6.65s/it]                                                         91%| | 5888/6500 [11:07:48<1:07:51,  6.65s/it] 91%| | 5889/6500 [11:07:55<1:07:25,  6.62s/it]                                                         91%| | 5889/6500 [11:07:55<1:07:25,  6.62s/it] 91%| | 5890/6500 [11:08:01<1:07:00,  6.59s/it]                                                         91%| | 5890/6500 [11:08:01<1:07:00,  6.59s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8825749158859253, 'eval_runtime': 1.4709, 'eval_samples_per_second': 8.158, 'eval_steps_per_second': 2.04, 'epoch': 0.91}
                                                         91%| | 5890/6500 [11:08:03<1:07:00,  6.59s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-5890
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5890
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5890
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5890
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5890
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5890/pytorch_model.bin
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5890/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5890/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5890/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3472, 'learning_rate': 2.1516676836893347e-06, 'epoch': 0.91}
{'loss': 0.3542, 'learning_rate': 2.1446581662042942e-06, 'epoch': 0.91}
{'loss': 0.3684, 'learning_rate': 2.1376598346144416e-06, 'epoch': 0.91}
{'loss': 0.3501, 'learning_rate': 2.1306726905555905e-06, 'epoch': 0.91}
{'loss': 0.3682, 'learning_rate': 2.1236967356609515e-06, 'epoch': 0.91}
 91%| | 5891/6500 [11:08:10<1:12:32,  7.15s/it]                                                         91%| | 5891/6500 [11:08:10<1:12:32,  7.15s/it] 91%| | 5892/6500 [11:08:16<1:10:41,  6.98s/it]                                                         91%| | 5892/6500 [11:08:16<1:10:41,  6.98s/it] 91%| | 5893/6500 [11:08:23<1:09:21,  6.86s/it]                                                         91%| | 5893/6500 [11:08:23<1:09:21,  6.86s/it] 91%| | 5894/6500 [11:08:29<1:08:23,  6.77s/it]                                                         91%| | 5894/6500 [11:08:29<1:08:23,  6.77s/it] 91%| | 5895/6500 [11:08:36<1:07:39,  6.71s/it]                                                         91%| | 5895/6500 [11:08:36<1:0{'loss': 0.3754, 'learning_rate': 2.116731971561109e-06, 'epoch': 0.91}
{'loss': 0.3553, 'learning_rate': 2.1097783998840324e-06, 'epoch': 0.91}
{'loss': 0.3706, 'learning_rate': 2.102836022255078e-06, 'epoch': 0.91}
{'loss': 0.3531, 'learning_rate': 2.0959048402969807e-06, 'epoch': 0.91}
{'loss': 0.4009, 'learning_rate': 2.088984855629872e-06, 'epoch': 0.91}
7:39,  6.71s/it] 91%| | 5896/6500 [11:08:42<1:07:07,  6.67s/it]                                                         91%| | 5896/6500 [11:08:42<1:07:07,  6.67s/it] 91%| | 5897/6500 [11:08:49<1:06:45,  6.64s/it]                                                         91%| | 5897/6500 [11:08:49<1:06:45,  6.64s/it] 91%| | 5898/6500 [11:08:56<1:06:26,  6.62s/it]                                                         91%| | 5898/6500 [11:08:56<1:06:26,  6.62s/it] 91%| | 5899/6500 [11:09:03<1:08:52,  6.88s/it]                                                         91%| | 5899/6500 [11:09:03<1:08:52,  6.88s/it] 91%| | 5900/6500 [11:09:10<1:07:50,  6.78s/it]                                                         91%| | 5900/6500 [11:09:10<1:07:50,  6.78s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8807547092437744, 'eval_runtime': 1.4716, 'eval_samples_per_second': 8.155, 'eval_steps_per_second': 2.039, 'epoch': 0.91}
                                                         91%| | 5900/6500 [11:09:11<1:07:50,  6.78s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-5900
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5900
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5900
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5900
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5900
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5900/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5900/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5900/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5900/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3528, 'learning_rate': 2.0820760698712473e-06, 'epoch': 0.91}
{'loss': 0.6398, 'learning_rate': 2.0751784846359925e-06, 'epoch': 0.91}
{'loss': 0.3743, 'learning_rate': 2.0682921015363787e-06, 'epoch': 0.91}
{'loss': 0.3614, 'learning_rate': 2.061416922182058e-06, 'epoch': 0.91}
{'loss': 0.3718, 'learning_rate': 2.0545529481800608e-06, 'epoch': 0.91}
 91%| | 5901/6500 [11:09:18<1:12:42,  7.28s/it]                                                         91%| | 5901/6500 [11:09:18<1:12:42,  7.28s/it] 91%| | 5902/6500 [11:09:25<1:10:27,  7.07s/it]                                                         91%| | 5902/6500 [11:09:25<1:10:27,  7.07s/it] 91%| | 5903/6500 [11:09:31<1:08:49,  6.92s/it]                                                         91%| | 5903/6500 [11:09:31<1:08:49,  6.92s/it] 91%| | 5904/6500 [11:09:38<1:07:39,  6.81s/it]                                                         91%| | 5904/6500 [11:09:38<1:07:39,  6.81s/it] 91%| | 5905/6500 [11:09:44<1:06:48,  6.74s/it]                                                         91%| | 5905/6500 [11:09:44<1:0{'loss': 0.3559, 'learning_rate': 2.0477001811347985e-06, 'epoch': 0.91}
{'loss': 0.3633, 'learning_rate': 2.0408586226480618e-06, 'epoch': 0.91}
{'loss': 0.3707, 'learning_rate': 2.0340282743190275e-06, 'epoch': 0.91}
{'loss': 0.3731, 'learning_rate': 2.0272091377442458e-06, 'epoch': 0.91}
{'loss': 0.3649, 'learning_rate': 2.020401214517648e-06, 'epoch': 0.91}
6:48,  6.74s/it] 91%| | 5906/6500 [11:09:51<1:06:12,  6.69s/it]                                                         91%| | 5906/6500 [11:09:51<1:06:12,  6.69s/it] 91%| | 5907/6500 [11:09:58<1:05:44,  6.65s/it]                                                         91%| | 5907/6500 [11:09:58<1:05:44,  6.65s/it] 91%| | 5908/6500 [11:10:04<1:05:24,  6.63s/it]                                                         91%| | 5908/6500 [11:10:04<1:05:24,  6.63s/it] 91%| | 5909/6500 [11:10:11<1:05:05,  6.61s/it]                                                         91%| | 5909/6500 [11:10:11<1:05:05,  6.61s/it] 91%| | 5910/6500 [11:10:17<1:04:51,  6.60s/it]                                                         91%| | 5910/6500 [11:10:17<1:04:51,  6.60s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8808593153953552, 'eval_runtime': 1.4697, 'eval_samples_per_second': 8.165, 'eval_steps_per_second': 2.041, 'epoch': 0.91}
                                                         91%| | 5910/6500 [11:10:19<1:04:51,  6.60s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-5910
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5910
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5910
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5910
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5910
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5910/pytorch_model.bin
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5910/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5910/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5910/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3727, 'learning_rate': 2.013604506230554e-06, 'epoch': 0.91}
{'loss': 0.3655, 'learning_rate': 2.006819014471639e-06, 'epoch': 0.91}
{'loss': 0.3622, 'learning_rate': 2.000044740826973e-06, 'epoch': 0.91}
{'loss': 0.3459, 'learning_rate': 1.9932816868800053e-06, 'epoch': 0.91}
{'loss': 0.3943, 'learning_rate': 1.986529854211555e-06, 'epoch': 0.91}
 91%| | 5911/6500 [11:10:26<1:10:05,  7.14s/it]                                                         91%| | 5911/6500 [11:10:26<1:10:05,  7.14s/it] 91%| | 5912/6500 [11:10:32<1:08:19,  6.97s/it]                                                         91%| | 5912/6500 [11:10:32<1:08:19,  6.97s/it] 91%| | 5913/6500 [11:10:39<1:07:00,  6.85s/it]                                                         91%| | 5913/6500 [11:10:39<1:07:00,  6.85s/it] 91%| | 5914/6500 [11:10:45<1:06:02,  6.76s/it]                                                         91%| | 5914/6500 [11:10:45<1:06:02,  6.76s/it] 91%| | 5915/6500 [11:10:53<1:07:09,  6.89s/it]                                                         91%| | 5915/6500 [11:10:53<1:0{'loss': 0.3671, 'learning_rate': 1.979789244399832e-06, 'epoch': 0.91}
{'loss': 0.6284, 'learning_rate': 1.9730598590203986e-06, 'epoch': 0.91}
{'loss': 0.3802, 'learning_rate': 1.9663416996462183e-06, 'epoch': 0.91}
{'loss': 0.3497, 'learning_rate': 1.959634767847612e-06, 'epoch': 0.91}
{'loss': 0.3647, 'learning_rate': 1.952939065192294e-06, 'epoch': 0.91}
7:09,  6.89s/it] 91%| | 5916/6500 [11:10:59<1:06:06,  6.79s/it]                                                         91%| | 5916/6500 [11:10:59<1:06:06,  6.79s/it] 91%| | 5917/6500 [11:11:06<1:05:20,  6.73s/it]                                                         91%| | 5917/6500 [11:11:06<1:05:20,  6.73s/it] 91%| | 5918/6500 [11:11:12<1:04:46,  6.68s/it]                                                         91%| | 5918/6500 [11:11:12<1:04:46,  6.68s/it] 91%| | 5919/6500 [11:11:19<1:04:21,  6.65s/it]                                                         91%| | 5919/6500 [11:11:19<1:04:21,  6.65s/it] 91%| | 5920/6500 [11:11:25<1:03:59,  6.62s/it]                                                         91%| | 5920/6500 [11:11:25<1:03:59,  6.62s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8811541199684143, 'eval_runtime': 1.4892, 'eval_samples_per_second': 8.058, 'eval_steps_per_second': 2.015, 'epoch': 0.91}
                                                         91%| | 5920/6500 [11:11:27<1:03:59,  6.62s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-5920
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5920
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5920
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5920
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5920
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5920/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5920/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5920/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5920/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3543, 'learning_rate': 1.9462545932453336e-06, 'epoch': 0.91}
{'loss': 0.3562, 'learning_rate': 1.9395813535691977e-06, 'epoch': 0.91}
{'loss': 0.363, 'learning_rate': 1.932919347723705e-06, 'epoch': 0.91}
{'loss': 0.3693, 'learning_rate': 1.9262685772660606e-06, 'epoch': 0.91}
{'loss': 0.365, 'learning_rate': 1.9196290437508424e-06, 'epoch': 0.91}
 91%| | 5921/6500 [11:11:34<1:09:13,  7.17s/it]                                                         91%| | 5921/6500 [11:11:34<1:09:13,  7.17s/it] 91%| | 5922/6500 [11:11:40<1:07:20,  6.99s/it]                                                         91%| | 5922/6500 [11:11:40<1:07:20,  6.99s/it] 91%| | 5923/6500 [11:11:47<1:06:01,  6.87s/it]                                                         91%| | 5923/6500 [11:11:47<1:06:01,  6.87s/it] 91%| | 5924/6500 [11:11:54<1:05:04,  6.78s/it]                                                         91%| | 5924/6500 [11:11:54<1:05:04,  6.78s/it] 91%| | 5925/6500 [11:12:00<1:04:21,  6.72s/it]                                                         91%| | 5925/6500 [11:12:00<1:0{'loss': 0.364, 'learning_rate': 1.913000748730004e-06, 'epoch': 0.91}
{'loss': 0.361, 'learning_rate': 1.9063836937528667e-06, 'epoch': 0.91}
{'loss': 0.3573, 'learning_rate': 1.899777880366127e-06, 'epoch': 0.91}
{'loss': 0.3493, 'learning_rate': 1.8931833101138497e-06, 'epoch': 0.91}
{'loss': 0.396, 'learning_rate': 1.8865999845374793e-06, 'epoch': 0.91}
4:21,  6.72s/it] 91%| | 5926/6500 [11:12:07<1:03:49,  6.67s/it]                                                         91%| | 5926/6500 [11:12:07<1:03:49,  6.67s/it] 91%| | 5927/6500 [11:12:13<1:03:26,  6.64s/it]                                                         91%| | 5927/6500 [11:12:13<1:03:26,  6.64s/it] 91%| | 5928/6500 [11:12:20<1:03:09,  6.63s/it]                                                         91%| | 5928/6500 [11:12:20<1:03:09,  6.63s/it] 91%| | 5929/6500 [11:12:26<1:02:52,  6.61s/it]                                                         91%| | 5929/6500 [11:12:26<1:02:52,  6.61s/it] 91%| | 5930/6500 [11:12:33<1:02:39,  6.60s/it]                                                         91%| | 5930/6500 [11:12:33<1:02:39,  6.60s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8802956342697144, 'eval_runtime': 1.7213, 'eval_samples_per_second': 6.971, 'eval_steps_per_second': 1.743, 'epoch': 0.91}
                                                         91%| | 5930/6500 [11:12:35<1:02:39,  6.60s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-5930
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  I AM HERE AND I AM SAVING THE MODEL/projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5930
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5930

I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5930
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5930
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5930/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5930/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5930/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5930/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3629, 'learning_rate': 1.8800279051758353e-06, 'epoch': 0.91}
{'loss': 0.6375, 'learning_rate': 1.8734670735650883e-06, 'epoch': 0.91}
{'loss': 0.377, 'learning_rate': 1.8669174912388066e-06, 'epoch': 0.91}
{'loss': 0.3576, 'learning_rate': 1.860379159727893e-06, 'epoch': 0.91}
{'loss': 0.3529, 'learning_rate': 1.853852080560664e-06, 'epoch': 0.91}
 91%| | 5931/6500 [11:12:42<1:10:12,  7.40s/it]                                                         91%| | 5931/6500 [11:12:42<1:10:12,  7.40s/it] 91%|| 5932/6500 [11:12:49<1:07:44,  7.16s/it]                                                         91%|| 5932/6500 [11:12:49<1:07:44,  7.16s/it] 91%|| 5933/6500 [11:12:55<1:05:57,  6.98s/it]                                                         91%|| 5933/6500 [11:12:55<1:05:57,  6.98s/it] 91%|| 5934/6500 [11:13:02<1:04:39,  6.85s/it]                                                         91%|| 5934/6500 [11:13:02<1:04:39,  6.85s/it] 91%|| 5935/6500 [11:13:09<1:03:43,  6.77s/it]                                                         91%|| 5935/65{'loss': 0.3504, 'learning_rate': 1.8473362552627826e-06, 'epoch': 0.91}
{'loss': 0.3594, 'learning_rate': 1.840831685357275e-06, 'epoch': 0.91}
{'loss': 0.3577, 'learning_rate': 1.834338372364547e-06, 'epoch': 0.91}
{'loss': 0.3706, 'learning_rate': 1.8278563178023733e-06, 'epoch': 0.91}
{'loss': 0.371, 'learning_rate': 1.8213855231858923e-06, 'epoch': 0.91}
00 [11:13:09<1:03:43,  6.77s/it] 91%|| 5936/6500 [11:13:15<1:03:03,  6.71s/it]                                                         91%|| 5936/6500 [11:13:15<1:03:03,  6.71s/it] 91%|| 5937/6500 [11:13:22<1:02:32,  6.67s/it]                                                         91%|| 5937/6500 [11:13:22<1:02:32,  6.67s/it] 91%|| 5938/6500 [11:13:28<1:02:11,  6.64s/it]                                                         91%|| 5938/6500 [11:13:28<1:02:11,  6.64s/it] 91%|| 5939/6500 [11:13:35<1:01:54,  6.62s/it]                                                         91%|| 5939/6500 [11:13:35<1:01:54,  6.62s/it] 91%|| 5940/6500 [11:13:41<1:01:39,  6.61s/it]                                                         91%|| 5940/6500 [11:13:41<1:01:39,  6.61s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8813567161560059, 'eval_runtime': 1.488, 'eval_samples_per_second': 8.065, 'eval_steps_per_second': 2.016, 'epoch': 0.91}
                                                         91%|| 5940/6500 [11:13:43<1:01:39,  6.61s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-5940
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5940I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5940

I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5940
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5940
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5940/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5940/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5940/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5940/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3587, 'learning_rate': 1.8149259900276216e-06, 'epoch': 0.91}
{'loss': 0.3637, 'learning_rate': 1.8084777198374315e-06, 'epoch': 0.91}
{'loss': 0.3558, 'learning_rate': 1.8020407141225548e-06, 'epoch': 0.91}
{'loss': 0.3565, 'learning_rate': 1.7956149743876217e-06, 'epoch': 0.91}
{'loss': 0.3875, 'learning_rate': 1.7892005021345915e-06, 'epoch': 0.91}
 91%|| 5941/6500 [11:13:50<1:06:43,  7.16s/it]                                                         91%|| 5941/6500 [11:13:50<1:06:43,  7.16s/it] 91%|| 5942/6500 [11:13:56<1:04:56,  6.98s/it]                                                         91%|| 5942/6500 [11:13:56<1:04:56,  6.98s/it] 91%|| 5943/6500 [11:14:03<1:03:39,  6.86s/it]                                                         91%|| 5943/6500 [11:14:03<1:03:39,  6.86s/it] 91%|| 5944/6500 [11:14:10<1:02:42,  6.77s/it]                                                         91%|| 5944/6500 [11:14:10<1:02:42,  6.77s/it] 91%|| 5945/6500 [11:14:16<1:02:03,  6.71s/it]                                                         91%|| 594{'loss': 0.3631, 'learning_rate': 1.7827972988628261e-06, 'epoch': 0.91}
{'loss': 0.6385, 'learning_rate': 1.7764053660690228e-06, 'epoch': 0.91}
{'loss': 0.3583, 'learning_rate': 1.7700247052472586e-06, 'epoch': 0.92}
{'loss': 0.3721, 'learning_rate': 1.7636553178889792e-06, 'epoch': 0.92}
{'loss': 0.3578, 'learning_rate': 1.7572972054829884e-06, 'epoch': 0.92}
5/6500 [11:14:16<1:02:03,  6.71s/it] 91%|| 5946/6500 [11:14:23<1:01:32,  6.67s/it]                                                         91%|| 5946/6500 [11:14:23<1:01:32,  6.67s/it] 91%|| 5947/6500 [11:14:29<1:01:09,  6.64s/it]                                                         91%|| 5947/6500 [11:14:29<1:01:09,  6.64s/it] 92%|| 5948/6500 [11:14:37<1:03:31,  6.91s/it]                                                         92%|| 5948/6500 [11:14:37<1:03:31,  6.91s/it] 92%|| 5949/6500 [11:14:43<1:02:29,  6.81s/it]                                                         92%|| 5949/6500 [11:14:43<1:02:29,  6.81s/it] 92%|| 5950/6500 [11:14:50<1:01:46,  6.74s/it]                                                         92%|| 5950/6500 [11:14:50<1:01:46,  6.74s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8818813562393188, 'eval_runtime': 1.4717, 'eval_samples_per_second': 8.154, 'eval_steps_per_second': 2.038, 'epoch': 0.92}
                                                         92%|| 5950/6500 [11:14:51<1:01:46,  6.74s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-5950
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5950 /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5950

I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5950
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5950
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5950/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5950/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5950/pytorch_model.binthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5950/pytorch_model.bin

/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.354, 'learning_rate': 1.7509503695154527e-06, 'epoch': 0.92}
{'loss': 0.3618, 'learning_rate': 1.7446148114699134e-06, 'epoch': 0.92}
{'loss': 0.375, 'learning_rate': 1.7382905328272635e-06, 'epoch': 0.92}
{'loss': 0.3708, 'learning_rate': 1.7319775350657652e-06, 'epoch': 0.92}
{'loss': 0.3651, 'learning_rate': 1.7256758196610435e-06, 'epoch': 0.92}
 92%|| 5951/6500 [11:14:58<1:06:21,  7.25s/it]                                                         92%|| 5951/6500 [11:14:58<1:06:21,  7.25s/it] 92%|| 5952/6500 [11:15:05<1:04:21,  7.05s/it]                                                         92%|| 5952/6500 [11:15:05<1:04:21,  7.05s/it] 92%|| 5953/6500 [11:15:12<1:02:55,  6.90s/it]                                                         92%|| 5953/6500 [11:15:12<1:02:55,  6.90s/it] 92%|| 5954/6500 [11:15:18<1:01:54,  6.80s/it]                                                         92%|| 5954/6500 [11:15:18<1:01:54,  6.80s/it] 92%|| 5955/6500 [11:15:25<1:01:08,  6.73s/it]                                                         92%|| 595{'loss': 0.3578, 'learning_rate': 1.7193853880860811e-06, 'epoch': 0.92}
{'loss': 0.3734, 'learning_rate': 1.713106241811241e-06, 'epoch': 0.92}
{'loss': 0.3477, 'learning_rate': 1.7068383823042212e-06, 'epoch': 0.92}
{'loss': 0.3626, 'learning_rate': 1.7005818110301053e-06, 'epoch': 0.92}
{'loss': 0.3814, 'learning_rate': 1.6943365294513236e-06, 'epoch': 0.92}
5/6500 [11:15:25<1:01:08,  6.73s/it] 92%|| 5956/6500 [11:15:31<1:00:39,  6.69s/it]                                                         92%|| 5956/6500 [11:15:31<1:00:39,  6.69s/it] 92%|| 5957/6500 [11:15:38<1:00:14,  6.66s/it]                                                         92%|| 5957/6500 [11:15:38<1:00:14,  6.66s/it] 92%|| 5958/6500 [11:15:44<59:52,  6.63s/it]                                                         92%|| 5958/6500 [11:15:44<59:52,  6.63s/it] 92%|| 5959/6500 [11:15:51<59:35,  6.61s/it]                                                       92%|| 5959/6500 [11:15:51<59:35,  6.61s/it] 92%|| 5960/6500 [11:15:58<59:23,  6.60s/it]                                                       92%|| 5960/6500 [11:15:58<59:23,  6.60s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.88080233335495, 'eval_runtime': 1.474, 'eval_samples_per_second': 8.141, 'eval_steps_per_second': 2.035, 'epoch': 0.92}
                                                       92%|| 5960/6500 [11:15:59<59:23,  6.60s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-5960
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5960I AM HERE AND I AM SAVING THE MODEL

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5960
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5960
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5960
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5960/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5960/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5960/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5960/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3656, 'learning_rate': 1.688102539027675e-06, 'epoch': 0.92}
{'loss': 0.6373, 'learning_rate': 1.6818798412163216e-06, 'epoch': 0.92}
{'loss': 0.3571, 'learning_rate': 1.6756684374717724e-06, 'epoch': 0.92}
{'loss': 0.3785, 'learning_rate': 1.6694683292459157e-06, 'epoch': 0.92}
{'loss': 0.3409, 'learning_rate': 1.6632795179879756e-06, 'epoch': 0.92}
 92%|| 5961/6500 [11:16:06<1:04:15,  7.15s/it]                                                         92%|| 5961/6500 [11:16:06<1:04:15,  7.15s/it] 92%|| 5962/6500 [11:16:13<1:02:33,  6.98s/it]                                                         92%|| 5962/6500 [11:16:13<1:02:33,  6.98s/it] 92%|| 5963/6500 [11:16:19<1:01:21,  6.86s/it]                                                         92%|| 5963/6500 [11:16:19<1:01:21,  6.86s/it] 92%|| 5964/6500 [11:16:27<1:02:53,  7.04s/it]                                                         92%|| 5964/6500 [11:16:27<1:02:53,  7.04s/it] 92%|| 5965/6500 [11:16:33<1:01:31,  6.90s/it]                                                         92%|| 596{'loss': 0.3569, 'learning_rate': 1.6571020051445563e-06, 'epoch': 0.92}
{'loss': 0.3612, 'learning_rate': 1.6509357921596136e-06, 'epoch': 0.92}
{'loss': 0.3566, 'learning_rate': 1.6447808804744668e-06, 'epoch': 0.92}
{'loss': 0.3709, 'learning_rate': 1.638637271527782e-06, 'epoch': 0.92}
{'loss': 0.38, 'learning_rate': 1.632504966755588e-06, 'epoch': 0.92}
5/6500 [11:16:33<1:01:31,  6.90s/it] 92%|| 5966/6500 [11:16:40<1:00:32,  6.80s/it]                                                         92%|| 5966/6500 [11:16:40<1:00:32,  6.80s/it] 92%|| 5967/6500 [11:16:46<59:46,  6.73s/it]                                                         92%|| 5967/6500 [11:16:46<59:46,  6.73s/it] 92%|| 5968/6500 [11:16:53<59:14,  6.68s/it]                                                       92%|| 5968/6500 [11:16:53<59:14,  6.68s/it] 92%|| 5969/6500 [11:16:59<58:48,  6.65s/it]                                                       92%|| 5969/6500 [11:16:59<58:48,  6.65s/it] 92%|| 5970/6500 [11:17:06<58:29,  6.62s/it]                                                       92%|| 5970/6500 [11:17:06<58:29,  6.62s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8821069598197937, 'eval_runtime': 1.4716, 'eval_samples_per_second': 8.154, 'eval_steps_per_second': 2.039, 'epoch': 0.92}
                                                       92%|| 5970/6500 [11:17:07<58:29,  6.62s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-5970
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5970
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5970
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5970
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5970
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5970/pytorch_model.binthe pytorch model path is
 /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5970/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5970/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5970/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.354, 'learning_rate': 1.626383967591283e-06, 'epoch': 0.92}
{'loss': 0.3693, 'learning_rate': 1.6202742754656108e-06, 'epoch': 0.92}
{'loss': 0.3513, 'learning_rate': 1.614175891806674e-06, 'epoch': 0.92}
{'loss': 0.3929, 'learning_rate': 1.6080888180399268e-06, 'epoch': 0.92}
{'loss': 0.3538, 'learning_rate': 1.6020130555881974e-06, 'epoch': 0.92}
 92%|| 5971/6500 [11:17:14<1:03:13,  7.17s/it]                                                         92%|| 5971/6500 [11:17:14<1:03:13,  7.17s/it] 92%|| 5972/6500 [11:17:21<1:01:30,  6.99s/it]                                                         92%|| 5972/6500 [11:17:21<1:01:30,  6.99s/it] 92%|| 5973/6500 [11:17:28<1:00:17,  6.86s/it]                                                         92%|| 5973/6500 [11:17:28<1:00:17,  6.86s/it] 92%|| 5974/6500 [11:17:34<59:25,  6.78s/it]                                                         92%|| 5974/6500 [11:17:34<59:25,  6.78s/it] 92%|| 5975/6500 [11:17:41<58:46,  6.72s/it]                                                       92%|| 5975/6500 [{'loss': 0.6404, 'learning_rate': 1.59594860587165e-06, 'epoch': 0.92}
{'loss': 0.3718, 'learning_rate': 1.5898954703078117e-06, 'epoch': 0.92}
{'loss': 0.3602, 'learning_rate': 1.5838536503115675e-06, 'epoch': 0.92}
{'loss': 0.3703, 'learning_rate': 1.5778231472951598e-06, 'epoch': 0.92}
{'loss': 0.351, 'learning_rate': 1.571803962668178e-06, 'epoch': 0.92}
11:17:41<58:46,  6.72s/it] 92%|| 5976/6500 [11:17:47<58:15,  6.67s/it]                                                       92%|| 5976/6500 [11:17:47<58:15,  6.67s/it] 92%|| 5977/6500 [11:17:54<57:51,  6.64s/it]                                                       92%|| 5977/6500 [11:17:54<57:51,  6.64s/it] 92%|| 5978/6500 [11:18:00<57:34,  6.62s/it]                                                       92%|| 5978/6500 [11:18:00<57:34,  6.62s/it] 92%|| 5979/6500 [11:18:07<57:20,  6.60s/it]                                                       92%|| 5979/6500 [11:18:07<57:20,  6.60s/it] 92%|| 5980/6500 [11:18:14<59:28,  6.86s/it]                                                       92%|| 5980/6500 [11:18:14<59:28,  6.86s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8807121515274048, 'eval_runtime': 1.4794, 'eval_samples_per_second': 8.112, 'eval_steps_per_second': 2.028, 'epoch': 0.92}
                                                       92%|| 5980/6500 [11:18:16<59:28,  6.86s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-5980
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5980
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5980
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5980
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5980
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5980/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5980/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5980/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5980/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.362, 'learning_rate': 1.5657960978375742e-06, 'epoch': 0.92}
{'loss': 0.3646, 'learning_rate': 1.5597995542076471e-06, 'epoch': 0.92}
{'loss': 0.3664, 'learning_rate': 1.5538143331800536e-06, 'epoch': 0.92}
{'loss': 0.3694, 'learning_rate': 1.5478404361538023e-06, 'epoch': 0.92}
{'loss': 0.3694, 'learning_rate': 1.541877864525254e-06, 'epoch': 0.92}
 92%|| 5981/6500 [11:18:23<1:03:34,  7.35s/it]                                                         92%|| 5981/6500 [11:18:23<1:03:34,  7.35s/it] 92%|| 5982/6500 [11:18:30<1:01:28,  7.12s/it]                                                         92%|| 5982/6500 [11:18:30<1:01:28,  7.12s/it] 92%|| 5983/6500 [11:18:36<59:58,  6.96s/it]                                                         92%|| 5983/6500 [11:18:36<59:58,  6.96s/it] 92%|| 5984/6500 [11:18:43<58:51,  6.84s/it]                                                       92%|| 5984/6500 [11:18:43<58:51,  6.84s/it] 92%|| 5985/6500 [11:18:49<58:01,  6.76s/it]                                                       92%|| 5985/6500 [11:18:{'loss': 0.363, 'learning_rate': 1.5359266196881216e-06, 'epoch': 0.92}
{'loss': 0.3712, 'learning_rate': 1.5299867030334814e-06, 'epoch': 0.92}
{'loss': 0.353, 'learning_rate': 1.5240581159497447e-06, 'epoch': 0.92}
{'loss': 0.4023, 'learning_rate': 1.5181408598226865e-06, 'epoch': 0.92}
{'loss': 0.358, 'learning_rate': 1.5122349360354227e-06, 'epoch': 0.92}
49<58:01,  6.76s/it] 92%|| 5986/6500 [11:18:56<57:27,  6.71s/it]                                                       92%|| 5986/6500 [11:18:56<57:27,  6.71s/it] 92%|| 5987/6500 [11:19:02<56:58,  6.66s/it]                                                       92%|| 5987/6500 [11:19:02<56:58,  6.66s/it] 92%|| 5988/6500 [11:19:09<56:34,  6.63s/it]                                                       92%|| 5988/6500 [11:19:09<56:34,  6.63s/it] 92%|| 5989/6500 [11:19:16<56:18,  6.61s/it]                                                       92%|| 5989/6500 [11:19:16<56:18,  6.61s/it] 92%|| 5990/6500 [11:19:22<56:10,  6.61s/it]                                                       92%|| 5990/6500 [11:19:22<56:10,  6.61s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8803409337997437, 'eval_runtime': 1.4776, 'eval_samples_per_second': 8.121, 'eval_steps_per_second': 2.03, 'epoch': 0.92}
                                                       92%|| 5990/6500 [11:19:24<56:10,  6.61s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-5990
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5990
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5990
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5990
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5990
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5990/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5990/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5990/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-5990/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.6347, 'learning_rate': 1.5063403459684378e-06, 'epoch': 0.92}
{'loss': 0.3766, 'learning_rate': 1.5004570909995464e-06, 'epoch': 0.92}
{'loss': 0.3561, 'learning_rate': 1.4945851725039262e-06, 'epoch': 0.92}
{'loss': 0.3765, 'learning_rate': 1.4887245918541071e-06, 'epoch': 0.92}
{'loss': 0.3586, 'learning_rate': 1.48287535041996e-06, 'epoch': 0.92}
 92%|| 5991/6500 [11:19:31<1:00:43,  7.16s/it]                                                         92%|| 5991/6500 [11:19:31<1:00:43,  7.16s/it] 92%|| 5992/6500 [11:19:37<59:06,  6.98s/it]                                                         92%|| 5992/6500 [11:19:37<59:06,  6.98s/it] 92%|| 5993/6500 [11:19:44<57:57,  6.86s/it]                                                       92%|| 5993/6500 [11:19:44<57:57,  6.86s/it] 92%|| 5994/6500 [11:19:50<57:04,  6.77s/it]                                                       92%|| 5994/6500 [11:19:50<57:04,  6.77s/it] 92%|| 5995/6500 [11:19:57<56:27,  6.71s/it]                                                       92%|| 5995/6500 [11:19:57<56:{'loss': 0.3609, 'learning_rate': 1.4770374495687134e-06, 'epoch': 0.92}
{'loss': 0.3674, 'learning_rate': 1.4712108906649369e-06, 'epoch': 0.92}
{'loss': 0.3716, 'learning_rate': 1.4653956750705577e-06, 'epoch': 0.92}
{'loss': 0.3751, 'learning_rate': 1.4595918041448442e-06, 'epoch': 0.92}
{'loss': 0.3696, 'learning_rate': 1.4537992792444111e-06, 'epoch': 0.92}
27,  6.71s/it] 92%|| 5996/6500 [11:20:04<57:33,  6.85s/it]                                                       92%|| 5996/6500 [11:20:04<57:33,  6.85s/it] 92%|| 5997/6500 [11:20:11<56:43,  6.77s/it]                                                       92%|| 5997/6500 [11:20:11<56:43,  6.77s/it] 92%|| 5998/6500 [11:20:17<56:06,  6.71s/it]                                                       92%|| 5998/6500 [11:20:17<56:06,  6.71s/it] 92%|| 5999/6500 [11:20:24<55:38,  6.66s/it]                                                       92%|| 5999/6500 [11:20:24<55:38,  6.66s/it] 92%|| 6000/6500 [11:20:30<55:18,  6.64s/it]                                                       92%|| 6000/6500 [11:20:30<55:18,  6.64s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8797299861907959, 'eval_runtime': 1.9141, 'eval_samples_per_second': 6.269, 'eval_steps_per_second': 1.567, 'epoch': 0.92}
                                                       92%|| 6000/6500 [11:20:32<55:18,  6.64s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-6000
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-6000
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-6000
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-6000
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-6000
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-6000/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-6000/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-6000/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-6000/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3649, 'learning_rate': 1.4480181017232364e-06, 'epoch': 0.92}
{'loss': 0.3583, 'learning_rate': 1.4422482729326226e-06, 'epoch': 0.92}
{'loss': 0.356, 'learning_rate': 1.4364897942212462e-06, 'epoch': 0.92}
{'loss': 0.3986, 'learning_rate': 1.430742666935103e-06, 'epoch': 0.92}
{'loss': 0.3675, 'learning_rate': 1.4250068924175575e-06, 'epoch': 0.92}
 92%|| 6001/6500 [11:20:39<1:00:53,  7.32s/it]                                                         92%|| 6001/6500 [11:20:39<1:00:53,  7.32s/it] 92%|| 6002/6500 [11:20:46<58:54,  7.10s/it]                                                         92%|| 6002/6500 [11:20:46<58:54,  7.10s/it] 92%|| 6003/6500 [11:20:52<57:25,  6.93s/it]                                                       92%|| 6003/6500 [11:20:52<57:25,  6.93s/it] 92%|| 6004/6500 [11:20:59<56:25,  6.83s/it]                                                       92%|| 6004/6500 [11:20:59<56:25,  6.83s/it] 92%|| 6005/6500 [11:21:06<55:42,  6.75s/it]                                                       92%|| 6005/6500 [11:21:06<55:{'loss': 0.6365, 'learning_rate': 1.419282472009309e-06, 'epoch': 0.92}
{'loss': 0.3735, 'learning_rate': 1.4135694070484096e-06, 'epoch': 0.92}
{'loss': 0.3632, 'learning_rate': 1.407867698870252e-06, 'epoch': 0.92}
{'loss': 0.3521, 'learning_rate': 1.4021773488075706e-06, 'epoch': 0.92}
{'loss': 0.3563, 'learning_rate': 1.3964983581904567e-06, 'epoch': 0.92}
42,  6.75s/it] 92%|| 6006/6500 [11:21:12<55:08,  6.70s/it]                                                       92%|| 6006/6500 [11:21:12<55:08,  6.70s/it] 92%|| 6007/6500 [11:21:19<54:44,  6.66s/it]                                                       92%|| 6007/6500 [11:21:19<54:44,  6.66s/it] 92%|| 6008/6500 [11:21:25<54:25,  6.64s/it]                                                       92%|| 6008/6500 [11:21:25<54:25,  6.64s/it] 92%|| 6009/6500 [11:21:32<54:07,  6.61s/it]                                                       92%|| 6009/6500 [11:21:32<54:07,  6.61s/it] 92%|| 6010/6500 [11:21:38<53:55,  6.60s/it]                                                       92%|| 6010/6500 [11:21:38<53:55,  6.60s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8800877928733826, 'eval_runtime': 1.4814, 'eval_samples_per_second': 8.101, 'eval_steps_per_second': 2.025, 'epoch': 0.92}
                                                       92%|| 6010/6500 [11:21:40<53:55,  6.60s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-6010
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-6010
/projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-6010
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-6010
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-6010
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-6010/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-6010/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-6010/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-6010/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3659, 'learning_rate': 1.3908307283463373e-06, 'epoch': 0.92}
{'loss': 0.3633, 'learning_rate': 1.385174460599986e-06, 'epoch': 0.92}
{'loss': 0.3737, 'learning_rate': 1.3795295562735234e-06, 'epoch': 0.93}
{'loss': 0.3635, 'learning_rate': 1.3738960166864101e-06, 'epoch': 0.93}
{'loss': 0.3609, 'learning_rate': 1.3682738431554487e-06, 'epoch': 0.93}
 92%|| 6011/6500 [11:21:47<58:17,  7.15s/it]                                                       92%|| 6011/6500 [11:21:47<58:17,  7.15s/it] 92%|| 6012/6500 [11:21:54<58:12,  7.16s/it]                                                       92%|| 6012/6500 [11:21:54<58:12,  7.16s/it] 93%|| 6013/6500 [11:22:01<56:36,  6.98s/it]                                                       93%|| 6013/6500 [11:22:01<56:36,  6.98s/it] 93%|| 6014/6500 [11:22:07<55:30,  6.85s/it]                                                       93%|| 6014/6500 [11:22:07<55:30,  6.85s/it] 93%|| 6015/6500 [11:22:14<54:39,  6.76s/it]                                                       93%|| 6015/6500 [11:22:14<54:39,  6.7{'loss': 0.3662, 'learning_rate': 1.3626630369947935e-06, 'epoch': 0.93}
{'loss': 0.3524, 'learning_rate': 1.3570635995159287e-06, 'epoch': 0.93}
{'loss': 0.3553, 'learning_rate': 1.3514755320277017e-06, 'epoch': 0.93}
{'loss': 0.3862, 'learning_rate': 1.345898835836279e-06, 'epoch': 0.93}
{'loss': 0.3704, 'learning_rate': 1.3403335122451787e-06, 'epoch': 0.93}
6s/it] 93%|| 6016/6500 [11:22:20<54:04,  6.70s/it]                                                       93%|| 6016/6500 [11:22:20<54:04,  6.70s/it] 93%|| 6017/6500 [11:22:27<53:31,  6.65s/it]                                                       93%|| 6017/6500 [11:22:27<53:31,  6.65s/it] 93%|| 6018/6500 [11:22:33<53:07,  6.61s/it]                                                       93%|| 6018/6500 [11:22:33<53:07,  6.61s/it] 93%|| 6019/6500 [11:22:40<52:51,  6.59s/it]                                                       93%|| 6019/6500 [11:22:40<52:51,  6.59s/it] 93%|| 6020/6500 [11:22:46<52:36,  6.58s/it]                                                       93%|| 6020/6500 [11:22:46<52:36,  6.58s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8809927105903625, 'eval_runtime': 1.4897, 'eval_samples_per_second': 8.055, 'eval_steps_per_second': 2.014, 'epoch': 0.93}
                                                       93%|| 6020/6500 [11:22:48<52:36,  6.58s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-6020
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-6020
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-6020
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-6020
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-6020
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-6020/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-6020/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-6020/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-6020/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.633, 'learning_rate': 1.3347795625552607e-06, 'epoch': 0.93}
{'loss': 0.3701, 'learning_rate': 1.329236988064736e-06, 'epoch': 0.93}
{'loss': 0.362, 'learning_rate': 1.3237057900691407e-06, 'epoch': 0.93}
{'loss': 0.3568, 'learning_rate': 1.3181859698613575e-06, 'epoch': 0.93}
{'loss': 0.352, 'learning_rate': 1.3126775287316151e-06, 'epoch': 0.93}
 93%|| 6021/6500 [11:22:55<56:54,  7.13s/it]                                                       93%|| 6021/6500 [11:22:55<56:54,  7.13s/it] 93%|| 6022/6500 [11:23:01<55:22,  6.95s/it]                                                       93%|| 6022/6500 [11:23:01<55:22,  6.95s/it] 93%|| 6023/6500 [11:23:08<54:22,  6.84s/it]                                                       93%|| 6023/6500 [11:23:08<54:22,  6.84s/it] 93%|| 6024/6500 [11:23:14<53:31,  6.75s/it]                                                       93%|| 6024/6500 [11:23:14<53:31,  6.75s/it] 93%|| 6025/6500 [11:23:21<52:54,  6.68s/it]                                                       93%|| 6025/6500 [11:23:21<52:54,  6.6{'loss': 0.3625, 'learning_rate': 1.3071804679674782e-06, 'epoch': 0.93}
{'loss': 0.3574, 'learning_rate': 1.3016947888538523e-06, 'epoch': 0.93}
{'loss': 0.3674, 'learning_rate': 1.2962204926729725e-06, 'epoch': 0.93}
{'loss': 0.372, 'learning_rate': 1.2907575807044382e-06, 'epoch': 0.93}
{'loss': 0.365, 'learning_rate': 1.2853060542251548e-06, 'epoch': 0.93}
8s/it] 93%|| 6026/6500 [11:23:27<52:27,  6.64s/it]                                                       93%|| 6026/6500 [11:23:27<52:27,  6.64s/it] 93%|| 6027/6500 [11:23:34<52:06,  6.61s/it]                                                       93%|| 6027/6500 [11:23:34<52:06,  6.61s/it] 93%|| 6028/6500 [11:23:42<54:11,  6.89s/it]                                                       93%|| 6028/6500 [11:23:42<54:11,  6.89s/it] 93%|| 6029/6500 [11:23:48<53:15,  6.78s/it]                                                       93%|| 6029/6500 [11:23:48<53:15,  6.78s/it] 93%|| 6030/6500 [11:23:55<52:33,  6.71s/it]                                                       93%|| 6030/6500 [11:23:55<52:33,  6.71s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.882265031337738, 'eval_runtime': 1.4781, 'eval_samples_per_second': 8.118, 'eval_steps_per_second': 2.03, 'epoch': 0.93}
                                                       93%|| 6030/6500 [11:23:56<52:33,  6.71s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-6030
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-6030
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-6030
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-6030
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-6030
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-6030/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-6030/pytorch_model.bin
the pytorch model path is the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-6030/pytorch_model.bin
/projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-6030/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3738, 'learning_rate': 1.2798659145093927e-06, 'epoch': 0.93}
{'loss': 0.3533, 'learning_rate': 1.274437162828751e-06, 'epoch': 0.93}
{'loss': 0.3635, 'learning_rate': 1.2690198004521647e-06, 'epoch': 0.93}
{'loss': 0.3908, 'learning_rate': 1.2636138286459099e-06, 'epoch': 0.93}
{'loss': 0.3661, 'learning_rate': 1.2582192486735977e-06, 'epoch': 0.93}
 93%|| 6031/6500 [11:24:03<56:21,  7.21s/it]                                                       93%|| 6031/6500 [11:24:03<56:21,  7.21s/it] 93%|| 6032/6500 [11:24:10<54:41,  7.01s/it]                                                       93%|| 6032/6500 [11:24:10<54:41,  7.01s/it] 93%|| 6033/6500 [11:24:16<53:27,  6.87s/it]                                                       93%|| 6033/6500 [11:24:16<53:27,  6.87s/it] 93%|| 6034/6500 [11:24:23<52:35,  6.77s/it]                                                       93%|| 6034/6500 [11:24:23<52:35,  6.77s/it] 93%|| 6035/6500 [11:24:29<51:56,  6.70s/it]                                                       93%|| 6035/6500 [11:24:29<51:56,  6.7{'loss': 0.6444, 'learning_rate': 1.2528360617961866e-06, 'epoch': 0.93}
{'loss': 0.3607, 'learning_rate': 1.2474642692719586e-06, 'epoch': 0.93}
{'loss': 0.3822, 'learning_rate': 1.242103872356537e-06, 'epoch': 0.93}
{'loss': 0.3511, 'learning_rate': 1.2367548723028754e-06, 'epoch': 0.93}
{'loss': 0.3588, 'learning_rate': 1.2314172703612902e-06, 'epoch': 0.93}
0s/it] 93%|| 6036/6500 [11:24:36<51:25,  6.65s/it]                                                       93%|| 6036/6500 [11:24:36<51:25,  6.65s/it] 93%|| 6037/6500 [11:24:42<51:02,  6.61s/it]                                                       93%|| 6037/6500 [11:24:42<51:02,  6.61s/it] 93%|| 6038/6500 [11:24:49<50:45,  6.59s/it]                                                       93%|| 6038/6500 [11:24:49<50:45,  6.59s/it] 93%|| 6039/6500 [11:24:55<50:29,  6.57s/it]                                                       93%|| 6039/6500 [11:24:55<50:29,  6.57s/it] 93%|| 6040/6500 [11:25:02<50:20,  6.57s/it]                                                       93%|| 6040/6500 [11:25:02<50:20,  6.57s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8807423710823059, 'eval_runtime': 1.4782, 'eval_samples_per_second': 8.118, 'eval_steps_per_second': 2.029, 'epoch': 0.93}
                                                       93%|| 6040/6500 [11:25:03<50:20,  6.57s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-6040
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-6040
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-6040
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-6040
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-6040
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-6040/pytorch_model.bin
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-6040/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-6040/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-6040/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.358, 'learning_rate': 1.2260910677793947e-06, 'epoch': 0.93}
{'loss': 0.3656, 'learning_rate': 1.2207762658021593e-06, 'epoch': 0.93}
{'loss': 0.3754, 'learning_rate': 1.215472865671885e-06, 'epoch': 0.93}
{'loss': 0.3705, 'learning_rate': 1.210180868628219e-06, 'epoch': 0.93}
{'loss': 0.3598, 'learning_rate': 1.2049002759081275e-06, 'epoch': 0.93}
 93%|| 6041/6500 [11:25:10<54:28,  7.12s/it]                                                       93%|| 6041/6500 [11:25:10<54:28,  7.12s/it] 93%|| 6042/6500 [11:25:17<53:00,  6.94s/it]                                                       93%|| 6042/6500 [11:25:17<53:00,  6.94s/it] 93%|| 6043/6500 [11:25:23<51:57,  6.82s/it]                                                       93%|| 6043/6500 [11:25:23<51:57,  6.82s/it] 93%|| 6044/6500 [11:25:31<53:11,  7.00s/it]                                                       93%|| 6044/6500 [11:25:31<53:11,  7.00s/it] 93%|| 6045/6500 [11:25:37<52:04,  6.87s/it]                                                       93%|| 6045/6500 [11:25:37<52:04,  6.8{'loss': 0.3647, 'learning_rate': 1.1996310887459172e-06, 'epoch': 0.93}
{'loss': 0.3515, 'learning_rate': 1.1943733083732312e-06, 'epoch': 0.93}
{'loss': 0.3864, 'learning_rate': 1.189126936019036e-06, 'epoch': 0.93}
{'loss': 0.3538, 'learning_rate': 1.1838919729096453e-06, 'epoch': 0.93}
{'loss': 0.5324, 'learning_rate': 1.1786684202687026e-06, 'epoch': 0.93}
7s/it] 93%|| 6046/6500 [11:25:44<51:12,  6.77s/it]                                                       93%|| 6046/6500 [11:25:44<51:12,  6.77s/it] 93%|| 6047/6500 [11:25:50<50:34,  6.70s/it]                                                       93%|| 6047/6500 [11:25:50<50:34,  6.70s/it] 93%|| 6048/6500 [11:25:57<50:04,  6.65s/it]                                                       93%|| 6048/6500 [11:25:57<50:04,  6.65s/it] 93%|| 6049/6500 [11:26:03<49:44,  6.62s/it]                                                       93%|| 6049/6500 [11:26:03<49:44,  6.62s/it] 93%|| 6050/6500 [11:26:10<49:26,  6.59s/it]                                                       93%|| 6050/6500 [11:26:10<49:26,  6.59s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8814019560813904, 'eval_runtime': 1.4753, 'eval_samples_per_second': 8.134, 'eval_steps_per_second': 2.033, 'epoch': 0.93}
                                                       93%|| 6050/6500 [11:26:11<49:26,  6.59s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-6050
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-6050
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-6050
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-6050
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-6050
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-6050/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-6050/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-6050/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-6050/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.4793, 'learning_rate': 1.173456279317181e-06, 'epoch': 0.93}
{'loss': 0.3651, 'learning_rate': 1.1682555512733783e-06, 'epoch': 0.93}
{'loss': 0.3762, 'learning_rate': 1.1630662373529444e-06, 'epoch': 0.93}
{'loss': 0.3562, 'learning_rate': 1.1578883387688366e-06, 'epoch': 0.93}
{'loss': 0.3572, 'learning_rate': 1.1527218567313703e-06, 'epoch': 0.93}
 93%|| 6051/6500 [11:26:18<53:26,  7.14s/it]                                                       93%|| 6051/6500 [11:26:18<53:26,  7.14s/it] 93%|| 6052/6500 [11:26:25<51:56,  6.96s/it]                                                       93%|| 6052/6500 [11:26:25<51:56,  6.96s/it] 93%|| 6053/6500 [11:26:31<50:52,  6.83s/it]                                                       93%|| 6053/6500 [11:26:31<50:52,  6.83s/it] 93%|| 6054/6500 [11:26:38<50:07,  6.74s/it]                                                       93%|| 6054/6500 [11:26:38<50:07,  6.74s/it] 93%|| 6055/6500 [11:26:45<49:34,  6.68s/it]                                                       93%|| 6055/6500 [11:26:45<49:34,  6.6{'loss': 0.366, 'learning_rate': 1.1475667924481682e-06, 'epoch': 0.93}
{'loss': 0.3633, 'learning_rate': 1.1424231471242054e-06, 'epoch': 0.93}
{'loss': 0.3773, 'learning_rate': 1.13729092196177e-06, 'epoch': 0.93}
{'loss': 0.374, 'learning_rate': 1.1321701181604915e-06, 'epoch': 0.93}
{'loss': 0.3619, 'learning_rate': 1.1270607369173291e-06, 'epoch': 0.93}
8s/it] 93%|| 6056/6500 [11:26:51<49:07,  6.64s/it]                                                       93%|| 6056/6500 [11:26:51<49:07,  6.64s/it] 93%|| 6057/6500 [11:26:58<48:46,  6.61s/it]                                                       93%|| 6057/6500 [11:26:58<48:46,  6.61s/it] 93%|| 6058/6500 [11:27:04<48:31,  6.59s/it]                                                       93%|| 6058/6500 [11:27:04<48:31,  6.59s/it] 93%|| 6059/6500 [11:27:11<48:22,  6.58s/it]                                                       93%|| 6059/6500 [11:27:11<48:22,  6.58s/it] 93%|| 6060/6500 [11:27:17<48:09,  6.57s/it]                                                       93%|| 6060/6500 [11:27:17<48:09,  6.57s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8802021741867065, 'eval_runtime': 1.4797, 'eval_samples_per_second': 8.11, 'eval_steps_per_second': 2.027, 'epoch': 0.93}
                                                       93%|| 6060/6500 [11:27:19<48:09,  6.57s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-6060
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-6060
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-6060
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-6060
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-6060
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-6060/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-6060/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-6060/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-6060/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.363, 'learning_rate': 1.1219627794265664e-06, 'epoch': 0.93}
{'loss': 0.3477, 'learning_rate': 1.116876246879822e-06, 'epoch': 0.93}
{'loss': 0.3943, 'learning_rate': 1.11180114046604e-06, 'epoch': 0.93}
{'loss': 0.3516, 'learning_rate': 1.1067374613714932e-06, 'epoch': 0.93}
{'loss': 0.6405, 'learning_rate': 1.1016852107797903e-06, 'epoch': 0.93}
 93%|| 6061/6500 [11:27:27<54:02,  7.39s/it]                                                       93%|| 6061/6500 [11:27:27<54:02,  7.39s/it] 93%|| 6062/6500 [11:27:33<52:05,  7.14s/it]                                                       93%|| 6062/6500 [11:27:33<52:05,  7.14s/it] 93%|| 6063/6500 [11:27:40<50:42,  6.96s/it]                                                       93%|| 6063/6500 [11:27:40<50:42,  6.96s/it] 93%|| 6064/6500 [11:27:46<49:42,  6.84s/it]                                                       93%|| 6064/6500 [11:27:46<49:42,  6.84s/it] 93%|| 6065/6500 [11:27:53<48:56,  6.75s/it]                                                       93%|| 6065/6500 [11:27:53<48:56,  6.7{'loss': 0.371, 'learning_rate': 1.0966443898718648e-06, 'epoch': 0.93}
{'loss': 0.3582, 'learning_rate': 1.091614999825974e-06, 'epoch': 0.93}
{'loss': 0.3728, 'learning_rate': 1.0865970418177051e-06, 'epoch': 0.93}
{'loss': 0.3499, 'learning_rate': 1.081590517019987e-06, 'epoch': 0.93}
{'loss': 0.3597, 'learning_rate': 1.07659542660305e-06, 'epoch': 0.93}
5s/it] 93%|| 6066/6500 [11:27:59<48:26,  6.70s/it]                                                       93%|| 6066/6500 [11:27:59<48:26,  6.70s/it] 93%|| 6067/6500 [11:28:06<48:02,  6.66s/it]                                                       93%|| 6067/6500 [11:28:06<48:02,  6.66s/it] 93%|| 6068/6500 [11:28:12<47:41,  6.62s/it]                                                       93%|| 6068/6500 [11:28:12<47:41,  6.62s/it] 93%|| 6069/6500 [11:28:19<47:26,  6.60s/it]                                                       93%|| 6069/6500 [11:28:19<47:26,  6.60s/it] 93%|| 6070/6500 [11:28:26<47:12,  6.59s/it]                                                       93%|| 6070/6500 [11:28:26<47:12,  6.59s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8807045221328735, 'eval_runtime': 1.4765, 'eval_samples_per_second': 8.127, 'eval_steps_per_second': 2.032, 'epoch': 0.93}
                                                       93%|| 6070/6500 [11:28:27<47:12,  6.59s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-6070
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-6070
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-6070
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-6070
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-6070
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-6070/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-6070/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-6070/pytorch_model.binthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-6070/pytorch_model.bin

/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3636, 'learning_rate': 1.0716117717344765e-06, 'epoch': 0.93}
{'loss': 0.3634, 'learning_rate': 1.066639553579163e-06, 'epoch': 0.93}
{'loss': 0.3621, 'learning_rate': 1.0616787732993295e-06, 'epoch': 0.93}
{'loss': 0.3637, 'learning_rate': 1.0567294320545428e-06, 'epoch': 0.93}
{'loss': 0.3593, 'learning_rate': 1.0517915310016614e-06, 'epoch': 0.93}
 93%|| 6071/6500 [11:28:34<51:07,  7.15s/it]                                                       93%|| 6071/6500 [11:28:34<51:07,  7.15s/it] 93%|| 6072/6500 [11:28:41<49:42,  6.97s/it]                                                       93%|| 6072/6500 [11:28:41<49:42,  6.97s/it] 93%|| 6073/6500 [11:28:47<48:41,  6.84s/it]                                                       93%|| 6073/6500 [11:28:47<48:41,  6.84s/it] 93%|| 6074/6500 [11:28:54<47:55,  6.75s/it]                                                       93%|| 6074/6500 [11:28:54<47:55,  6.75s/it] 93%|| 6075/6500 [11:29:00<47:21,  6.69s/it]                                                       93%|| 6075/6500 [11:29:00<47:21,  6.6{'loss': 0.3641, 'learning_rate': 1.0468650712949057e-06, 'epoch': 0.93}
{'loss': 0.3565, 'learning_rate': 1.041950054085794e-06, 'epoch': 0.93}
{'loss': 0.3989, 'learning_rate': 1.0370464805231905e-06, 'epoch': 0.94}
{'loss': 0.3647, 'learning_rate': 1.0321543517532727e-06, 'epoch': 0.94}
{'loss': 0.6272, 'learning_rate': 1.0272736689195429e-06, 'epoch': 0.94}
9s/it] 93%|| 6076/6500 [11:29:07<46:57,  6.64s/it]                                                       93%|| 6076/6500 [11:29:07<46:57,  6.64s/it] 93%|| 6077/6500 [11:29:14<48:34,  6.89s/it]                                                       93%|| 6077/6500 [11:29:14<48:34,  6.89s/it] 94%|| 6078/6500 [11:29:21<47:43,  6.79s/it]                                                       94%|| 6078/6500 [11:29:21<47:43,  6.79s/it] 94%|| 6079/6500 [11:29:27<47:07,  6.72s/it]                                                       94%|| 6079/6500 [11:29:27<47:07,  6.72s/it] 94%|| 6080/6500 [11:29:34<46:39,  6.66s/it]                                                       94%|| 6080/6500 [11:29:34<46:39,  6.66s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8816491961479187, 'eval_runtime': 1.4769, 'eval_samples_per_second': 8.125, 'eval_steps_per_second': 2.031, 'epoch': 0.94}
                                                       94%|| 6080/6500 [11:29:35<46:39,  6.66s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-6080
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-6080
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-6080
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-6080
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-6080
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-6080/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-6080/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-6080/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-6080/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3811, 'learning_rate': 1.0224044331628379e-06, 'epoch': 0.94}
{'loss': 0.3599, 'learning_rate': 1.0175466456213034e-06, 'epoch': 0.94}
{'loss': 0.3577, 'learning_rate': 1.0127003074304253e-06, 'epoch': 0.94}
{'loss': 0.3612, 'learning_rate': 1.0078654197230031e-06, 'epoch': 0.94}
{'loss': 0.3664, 'learning_rate': 1.0030419836291605e-06, 'epoch': 0.94}
 94%|| 6081/6500 [11:29:42<50:13,  7.19s/it]                                                       94%|| 6081/6500 [11:29:42<50:13,  7.19s/it] 94%|| 6082/6500 [11:29:49<48:45,  7.00s/it]                                                       94%|| 6082/6500 [11:29:49<48:45,  7.00s/it] 94%|| 6083/6500 [11:29:55<47:39,  6.86s/it]                                                       94%|| 6083/6500 [11:29:55<47:39,  6.86s/it] 94%|| 6084/6500 [11:30:02<46:56,  6.77s/it]                                                       94%|| 6084/6500 [11:30:02<46:56,  6.77s/it] 94%|| 6085/6500 [11:30:08<46:25,  6.71s/it]                                                       94%|| 6085/6500 [11:30:08<46:25,  6.7{'loss': 0.3661, 'learning_rate': 9.98230000276351e-07, 'epoch': 0.94}
{'loss': 0.3748, 'learning_rate': 9.93429470789342e-07, 'epoch': 0.94}
{'loss': 0.3645, 'learning_rate': 9.886403962902246e-07, 'epoch': 0.94}
{'loss': 0.3664, 'learning_rate': 9.838627778984256e-07, 'epoch': 0.94}
{'loss': 0.369, 'learning_rate': 9.790966167306793e-07, 'epoch': 0.94}
1s/it] 94%|| 6086/6500 [11:30:15<45:58,  6.66s/it]                                                       94%|| 6086/6500 [11:30:15<45:58,  6.66s/it] 94%|| 6087/6500 [11:30:22<45:38,  6.63s/it]                                                       94%|| 6087/6500 [11:30:22<45:38,  6.63s/it] 94%|| 6088/6500 [11:30:28<45:21,  6.61s/it]                                                       94%|| 6088/6500 [11:30:28<45:21,  6.61s/it] 94%|| 6089/6500 [11:30:35<45:07,  6.59s/it]                                                       94%|| 6089/6500 [11:30:35<45:07,  6.59s/it] 94%|| 6090/6500 [11:30:41<44:54,  6.57s/it]                                                       94%|| 6090/6500 [11:30:41<44:54,  6.57s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8806161284446716, 'eval_runtime': 1.4737, 'eval_samples_per_second': 8.143, 'eval_steps_per_second': 2.036, 'epoch': 0.94}
                                                       94%|| 6090/6500 [11:30:43<44:54,  6.57s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-6090
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-6090I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-6090

I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-6090
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-6090
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-6090/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-6090/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-6090/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-6090/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3568, 'learning_rate': 9.743419139010447e-07, 'epoch': 0.94}
{'loss': 0.3533, 'learning_rate': 9.695986705209048e-07, 'epoch': 0.94}
{'loss': 0.3936, 'learning_rate': 9.64866887698973e-07, 'epoch': 0.94}
{'loss': 0.3667, 'learning_rate': 9.601465665412645e-07, 'epoch': 0.94}
{'loss': 0.6383, 'learning_rate': 9.554377081511301e-07, 'epoch': 0.94}
 94%|| 6091/6500 [11:30:50<48:34,  7.13s/it]                                                       94%|| 6091/6500 [11:30:50<48:34,  7.13s/it] 94%|| 6092/6500 [11:30:56<47:15,  6.95s/it]                                                       94%|| 6092/6500 [11:30:56<47:15,  6.95s/it] 94%|| 6093/6500 [11:31:03<47:36,  7.02s/it]                                                       94%|| 6093/6500 [11:31:03<47:36,  7.02s/it] 94%|| 6094/6500 [11:31:10<46:33,  6.88s/it]                                                       94%|| 6094/6500 [11:31:10<46:33,  6.88s/it] 94%|| 6095/6500 [11:31:16<45:47,  6.78s/it]                                                       94%|| 6095/6500 [11:31:16<45:47,  6.7{'loss': 0.3773, 'learning_rate': 9.507403136292336e-07, 'epoch': 0.94}
{'loss': 0.3568, 'learning_rate': 9.460543840735636e-07, 'epoch': 0.94}
{'loss': 0.354, 'learning_rate': 9.413799205794272e-07, 'epoch': 0.94}
{'loss': 0.3534, 'learning_rate': 9.367169242394557e-07, 'epoch': 0.94}
{'loss': 0.3581, 'learning_rate': 9.320653961435943e-07, 'epoch': 0.94}
8s/it] 94%|| 6096/6500 [11:31:23<45:12,  6.71s/it]                                                       94%|| 6096/6500 [11:31:23<45:12,  6.71s/it] 94%|| 6097/6500 [11:31:30<44:47,  6.67s/it]                                                       94%|| 6097/6500 [11:31:30<44:47,  6.67s/it] 94%|| 6098/6500 [11:31:36<44:24,  6.63s/it]                                                       94%|| 6098/6500 [11:31:36<44:24,  6.63s/it] 94%|| 6099/6500 [11:31:43<44:09,  6.61s/it]                                                       94%|| 6099/6500 [11:31:43<44:09,  6.61s/it] 94%|| 6100/6500 [11:31:49<43:57,  6.59s/it]                                                       94%|| 6100/6500 [11:31:49<43:57,  6.59s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8815483450889587, 'eval_runtime': 1.7331, 'eval_samples_per_second': 6.924, 'eval_steps_per_second': 1.731, 'epoch': 0.94}
                                                       94%|| 6100/6500 [11:31:51<43:57,  6.59s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-6100
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-6100
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-6100
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-6100
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-6100
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-6100/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-6100/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-6100/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-6100/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3586, 'learning_rate': 9.274253373791064e-07, 'epoch': 0.94}
{'loss': 0.3727, 'learning_rate': 9.2279674903058e-07, 'epoch': 0.94}
{'loss': 0.3701, 'learning_rate': 9.181796321799163e-07, 'epoch': 0.94}
{'loss': 0.3657, 'learning_rate': 9.135739879063465e-07, 'epoch': 0.94}
{'loss': 0.3638, 'learning_rate': 9.089798172864094e-07, 'epoch': 0.94}
 94%|| 6101/6500 [11:31:58<48:06,  7.23s/it]                                                       94%|| 6101/6500 [11:31:58<48:06,  7.23s/it] 94%|| 6102/6500 [11:32:05<46:37,  7.03s/it]                                                       94%|| 6102/6500 [11:32:05<46:37,  7.03s/it] 94%|| 6103/6500 [11:32:11<45:33,  6.89s/it]                                                       94%|| 6103/6500 [11:32:11<45:33,  6.89s/it] 94%|| 6104/6500 [11:32:18<44:47,  6.79s/it]                                                       94%|| 6104/6500 [11:32:18<44:47,  6.79s/it] 94%|| 6105/6500 [11:32:24<44:13,  6.72s/it]                                                       94%|| 6105/6500 [11:32:24<44:13,  6.7{'loss': 0.347, 'learning_rate': 9.043971213939573e-07, 'epoch': 0.94}
{'loss': 0.358, 'learning_rate': 8.998259013001719e-07, 'epoch': 0.94}
{'loss': 0.3848, 'learning_rate': 8.952661580735433e-07, 'epoch': 0.94}
{'loss': 0.3685, 'learning_rate': 8.907178927798965e-07, 'epoch': 0.94}
{'loss': 0.6372, 'learning_rate': 8.861811064823477e-07, 'epoch': 0.94}
2s/it] 94%|| 6106/6500 [11:32:31<43:47,  6.67s/it]                                                       94%|| 6106/6500 [11:32:31<43:47,  6.67s/it] 94%|| 6107/6500 [11:32:37<43:26,  6.63s/it]                                                       94%|| 6107/6500 [11:32:37<43:26,  6.63s/it] 94%|| 6108/6500 [11:32:44<43:08,  6.60s/it]                                                       94%|| 6108/6500 [11:32:44<43:08,  6.60s/it] 94%|| 6109/6500 [11:32:51<44:08,  6.77s/it]                                                       94%|| 6109/6500 [11:32:51<44:08,  6.77s/it] 94%|| 6110/6500 [11:32:58<43:36,  6.71s/it]                                                       94%|| 6110/6500 [11:32:58<43:36,  6.71s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8815563321113586, 'eval_runtime': 1.5025, 'eval_samples_per_second': 7.987, 'eval_steps_per_second': 1.997, 'epoch': 0.94}
                                                       94%|| 6110/6500 [11:32:59<43:36,  6.71s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-6110
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-6110
/projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-6110
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-6110
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-6110
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-6110/pytorch_model.bin
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-6110/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-6110/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-6110/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.362, 'learning_rate': 8.816558002413488e-07, 'epoch': 0.94}
{'loss': 0.3636, 'learning_rate': 8.771419751146648e-07, 'epoch': 0.94}
{'loss': 0.3563, 'learning_rate': 8.726396321573682e-07, 'epoch': 0.94}
{'loss': 0.351, 'learning_rate': 8.681487724218618e-07, 'epoch': 0.94}
{'loss': 0.3576, 'learning_rate': 8.636693969578558e-07, 'epoch': 0.94}
 94%|| 6111/6500 [11:33:06<46:48,  7.22s/it]                                                       94%|| 6111/6500 [11:33:06<46:48,  7.22s/it] 94%|| 6112/6500 [11:33:13<45:23,  7.02s/it]                                                       94%|| 6112/6500 [11:33:13<45:23,  7.02s/it] 94%|| 6113/6500 [11:33:19<44:21,  6.88s/it]                                                       94%|| 6113/6500 [11:33:19<44:21,  6.88s/it] 94%|| 6114/6500 [11:33:26<43:36,  6.78s/it]                                                       94%|| 6114/6500 [11:33:26<43:36,  6.78s/it] 94%|| 6115/6500 [11:33:32<43:03,  6.71s/it]                                                       94%|| 6115/6500 [11:33:32<43:03,  6.7{'loss': 0.3628, 'learning_rate': 8.592015068123738e-07, 'epoch': 0.94}
{'loss': 0.3691, 'learning_rate': 8.547451030297526e-07, 'epoch': 0.94}
{'loss': 0.3672, 'learning_rate': 8.503001866516592e-07, 'epoch': 0.94}
{'loss': 0.3612, 'learning_rate': 8.458667587170621e-07, 'epoch': 0.94}
{'loss': 0.3695, 'learning_rate': 8.414448202622494e-07, 'epoch': 0.94}
1s/it] 94%|| 6116/6500 [11:33:39<42:37,  6.66s/it]                                                       94%|| 6116/6500 [11:33:39<42:37,  6.66s/it] 94%|| 6117/6500 [11:33:45<42:17,  6.62s/it]                                                       94%|| 6117/6500 [11:33:45<42:17,  6.62s/it] 94%|| 6118/6500 [11:33:52<42:02,  6.60s/it]                                                       94%|| 6118/6500 [11:33:52<42:02,  6.60s/it] 94%|| 6119/6500 [11:33:58<41:49,  6.59s/it]                                                       94%|| 6119/6500 [11:33:58<41:49,  6.59s/it] 94%|| 6120/6500 [11:34:05<41:38,  6.57s/it]                                                       94%|| 6120/6500 [11:34:05<41:38,  6.57s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8812384605407715, 'eval_runtime': 1.4766, 'eval_samples_per_second': 8.127, 'eval_steps_per_second': 2.032, 'epoch': 0.94}
                                                       94%|| 6120/6500 [11:34:06<41:38,  6.57s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-6120
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-6120
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-6120
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-6120
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-6120
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-6120/pytorch_model.bin
the pytorch model path is the pytorch model path is/projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-6120/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-6120/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-6120/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.349, 'learning_rate': 8.370343723208163e-07, 'epoch': 0.94}
{'loss': 0.3633, 'learning_rate': 8.326354159236882e-07, 'epoch': 0.94}
{'loss': 0.3765, 'learning_rate': 8.282479520990871e-07, 'epoch': 0.94}
{'loss': 0.3668, 'learning_rate': 8.238719818725593e-07, 'epoch': 0.94}
{'loss': 0.6402, 'learning_rate': 8.195075062669588e-07, 'epoch': 0.94}
 94%|| 6121/6500 [11:34:13<45:00,  7.12s/it]                                                       94%|| 6121/6500 [11:34:13<45:00,  7.12s/it] 94%|| 6122/6500 [11:34:20<43:46,  6.95s/it]                                                       94%|| 6122/6500 [11:34:20<43:46,  6.95s/it] 94%|| 6123/6500 [11:34:26<42:54,  6.83s/it]                                                       94%|| 6123/6500 [11:34:26<42:54,  6.83s/it] 94%|| 6124/6500 [11:34:33<42:16,  6.75s/it]                                                       94%|| 6124/6500 [11:34:33<42:16,  6.75s/it] 94%|| 6125/6500 [11:34:40<43:25,  6.95s/it]                                                       94%|| 6125/6500 [11:34:40<43:25,  6.9{'loss': 0.3583, 'learning_rate': 8.15154526302453e-07, 'epoch': 0.94}
{'loss': 0.3787, 'learning_rate': 8.108130429965388e-07, 'epoch': 0.94}
{'loss': 0.3501, 'learning_rate': 8.064830573639881e-07, 'epoch': 0.94}
{'loss': 0.3595, 'learning_rate': 8.021645704169301e-07, 'epoch': 0.94}
{'loss': 0.3659, 'learning_rate': 7.978575831647683e-07, 'epoch': 0.94}
5s/it] 94%|| 6126/6500 [11:34:47<42:33,  6.83s/it]                                                       94%|| 6126/6500 [11:34:47<42:33,  6.83s/it] 94%|| 6127/6500 [11:34:53<41:55,  6.74s/it]                                                       94%|| 6127/6500 [11:34:53<41:55,  6.74s/it] 94%|| 6128/6500 [11:35:00<41:26,  6.68s/it]                                                       94%|| 6128/6500 [11:35:00<41:26,  6.68s/it] 94%|| 6129/6500 [11:35:07<41:05,  6.64s/it]                                                       94%|| 6129/6500 [11:35:07<41:05,  6.64s/it] 94%|| 6130/6500 [11:35:13<40:47,  6.62s/it]                                                       94%|| 6130/6500 [11:35:13<40:47,  6.62s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8802365660667419, 'eval_runtime': 1.474, 'eval_samples_per_second': 8.141, 'eval_steps_per_second': 2.035, 'epoch': 0.94}
                                                       94%|| 6130/6500 [11:35:15<40:47,  6.62s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-6130
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-6130
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-6130
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-6130
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-6130
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-6130/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-6130/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-6130/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-6130/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3629, 'learning_rate': 7.935620966142476e-07, 'epoch': 0.94}
{'loss': 0.3723, 'learning_rate': 7.892781117694037e-07, 'epoch': 0.94}
{'loss': 0.3767, 'learning_rate': 7.850056296315967e-07, 'epoch': 0.94}
{'loss': 0.3574, 'learning_rate': 7.807446511994942e-07, 'epoch': 0.94}
{'loss': 0.3636, 'learning_rate': 7.764951774690665e-07, 'epoch': 0.94}
 94%|| 6131/6500 [11:35:22<43:59,  7.15s/it]                                                       94%|| 6131/6500 [11:35:22<43:59,  7.15s/it] 94%|| 6132/6500 [11:35:28<42:47,  6.98s/it]                                                       94%|| 6132/6500 [11:35:28<42:47,  6.98s/it] 94%|| 6133/6500 [11:35:35<41:53,  6.85s/it]                                                       94%|| 6133/6500 [11:35:35<41:53,  6.85s/it] 94%|| 6134/6500 [11:35:41<41:13,  6.76s/it]                                                       94%|| 6134/6500 [11:35:41<41:13,  6.76s/it] 94%|| 6135/6500 [11:35:48<40:43,  6.69s/it]                                                       94%|| 6135/6500 [11:35:48<40:43,  6.6{'loss': 0.3488, 'learning_rate': 7.722572094336133e-07, 'epoch': 0.94}
{'loss': 0.3949, 'learning_rate': 7.680307480837201e-07, 'epoch': 0.94}
{'loss': 0.3513, 'learning_rate': 7.638157944073132e-07, 'epoch': 0.94}
{'loss': 0.6381, 'learning_rate': 7.596123493895991e-07, 'epoch': 0.94}
{'loss': 0.3707, 'learning_rate': 7.554204140131138e-07, 'epoch': 0.94}
9s/it] 94%|| 6136/6500 [11:35:54<40:20,  6.65s/it]                                                       94%|| 6136/6500 [11:35:54<40:20,  6.65s/it] 94%|| 6137/6500 [11:36:01<40:03,  6.62s/it]                                                       94%|| 6137/6500 [11:36:01<40:03,  6.62s/it] 94%|| 6138/6500 [11:36:07<39:49,  6.60s/it]                                                       94%|| 6138/6500 [11:36:07<39:49,  6.60s/it] 94%|| 6139/6500 [11:36:14<39:36,  6.58s/it]                                                       94%|| 6139/6500 [11:36:14<39:36,  6.58s/it] 94%|| 6140/6500 [11:36:20<39:27,  6.58s/it]                                                       94%|| 6140/6500 [11:36:20<39:27,  6.58s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8818042874336243, 'eval_runtime': 1.4785, 'eval_samples_per_second': 8.116, 'eval_steps_per_second': 2.029, 'epoch': 0.94}
                                                       94%|| 6140/6500 [11:36:22<39:27,  6.58s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-6140
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  I AM HERE AND I AM SAVING THE MODEL
/projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-6140the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-6140

I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-6140
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-6140
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-6140/pytorch_model.bin
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-6140/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-6140/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-6140/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3576, 'learning_rate': 7.512399892576904e-07, 'epoch': 0.94}
{'loss': 0.3697, 'learning_rate': 7.470710761004862e-07, 'epoch': 0.94}
{'loss': 0.3484, 'learning_rate': 7.429136755159493e-07, 'epoch': 0.95}
{'loss': 0.3581, 'learning_rate': 7.387677884758582e-07, 'epoch': 0.95}
{'loss': 0.3564, 'learning_rate': 7.346334159492818e-07, 'epoch': 0.95}
 94%|| 6141/6500 [11:36:30<44:17,  7.40s/it]                                                       94%|| 6141/6500 [11:36:30<44:17,  7.40s/it] 94%|| 6142/6500 [11:36:36<42:37,  7.14s/it]                                                       94%|| 6142/6500 [11:36:36<42:37,  7.14s/it] 95%|| 6143/6500 [11:36:43<41:25,  6.96s/it]                                                       95%|| 6143/6500 [11:36:43<41:25,  6.96s/it] 95%|| 6144/6500 [11:36:49<40:35,  6.84s/it]                                                       95%|| 6144/6500 [11:36:49<40:35,  6.84s/it] 95%|| 6145/6500 [11:36:56<39:58,  6.76s/it]                                                       95%|| 6145/6500 [11:36:56<39:58,  6.7{'loss': 0.3643, 'learning_rate': 7.305105589026085e-07, 'epoch': 0.95}
{'loss': 0.3639, 'learning_rate': 7.263992182995228e-07, 'epoch': 0.95}
{'loss': 0.37, 'learning_rate': 7.222993951010338e-07, 'epoch': 0.95}
{'loss': 0.3625, 'learning_rate': 7.182110902654527e-07, 'epoch': 0.95}
{'loss': 0.3651, 'learning_rate': 7.141343047483873e-07, 'epoch': 0.95}
6s/it] 95%|| 6146/6500 [11:37:03<39:30,  6.70s/it]                                                       95%|| 6146/6500 [11:37:03<39:30,  6.70s/it] 95%|| 6147/6500 [11:37:09<39:09,  6.66s/it]                                                       95%|| 6147/6500 [11:37:09<39:09,  6.66s/it] 95%|| 6148/6500 [11:37:16<38:51,  6.62s/it]                                                       95%|| 6148/6500 [11:37:16<38:51,  6.62s/it] 95%|| 6149/6500 [11:37:22<38:38,  6.61s/it]                                                       95%|| 6149/6500 [11:37:22<38:38,  6.61s/it] 95%|| 6150/6500 [11:37:29<38:27,  6.59s/it]                                                       95%|| 6150/6500 [11:37:29<38:27,  6.59s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8810174465179443, 'eval_runtime': 1.4813, 'eval_samples_per_second': 8.101, 'eval_steps_per_second': 2.025, 'epoch': 0.95}
                                                       95%|| 6150/6500 [11:37:30<38:27,  6.59s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-6150
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-6150I AM HERE AND I AM SAVING THE MODEL

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-6150
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-6150
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-6150
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-6150/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-6150/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-6150/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-6150/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3473, 'learning_rate': 7.100690395027699e-07, 'epoch': 0.95}
{'loss': 0.3982, 'learning_rate': 7.06015295478818e-07, 'epoch': 0.95}
{'loss': 0.3531, 'learning_rate': 7.019730736240848e-07, 'epoch': 0.95}
{'loss': 0.6351, 'learning_rate': 6.979423748834035e-07, 'epoch': 0.95}
{'loss': 0.3805, 'learning_rate': 6.93923200198937e-07, 'epoch': 0.95}
 95%|| 6151/6500 [11:37:37<41:32,  7.14s/it]                                                       95%|| 6151/6500 [11:37:37<41:32,  7.14s/it] 95%|| 6152/6500 [11:37:44<40:23,  6.96s/it]                                                       95%|| 6152/6500 [11:37:44<40:23,  6.96s/it] 95%|| 6153/6500 [11:37:50<39:32,  6.84s/it]                                                       95%|| 6153/6500 [11:37:50<39:32,  6.84s/it] 95%|| 6154/6500 [11:37:57<38:54,  6.75s/it]                                                       95%|| 6154/6500 [11:37:57<38:54,  6.75s/it] 95%|| 6155/6500 [11:38:03<38:26,  6.69s/it]                                                       95%|| 6155/6500 [11:38:03<38:26,  6.6{'loss': 0.3504, 'learning_rate': 6.89915550510134e-07, 'epoch': 0.95}
{'loss': 0.3675, 'learning_rate': 6.859194267537561e-07, 'epoch': 0.95}
{'loss': 0.3526, 'learning_rate': 6.819348298638839e-07, 'epoch': 0.95}
{'loss': 0.357, 'learning_rate': 6.779617607718835e-07, 'epoch': 0.95}
{'loss': 0.3649, 'learning_rate': 6.7400022040644e-07, 'epoch': 0.95}
9s/it] 95%|| 6156/6500 [11:38:10<38:03,  6.64s/it]                                                       95%|| 6156/6500 [11:38:10<38:03,  6.64s/it] 95%|| 6157/6500 [11:38:16<37:46,  6.61s/it]                                                       95%|| 6157/6500 [11:38:16<37:46,  6.61s/it] 95%|| 6158/6500 [11:38:24<39:05,  6.86s/it]                                                       95%|| 6158/6500 [11:38:24<39:05,  6.86s/it] 95%|| 6159/6500 [11:38:30<38:27,  6.77s/it]                                                       95%|| 6159/6500 [11:38:30<38:27,  6.77s/it] 95%|| 6160/6500 [11:38:37<37:58,  6.70s/it]                                                       95%|| 6160/6500 [11:38:37<37:58,  6.70s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8807548880577087, 'eval_runtime': 1.4842, 'eval_samples_per_second': 8.085, 'eval_steps_per_second': 2.021, 'epoch': 0.95}
                                                       95%|| 6160/6500 [11:38:38<37:58,  6.70s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-6160
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-6160I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-6160

I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-6160
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-6160
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-6160/pytorch_model.bin
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-6160/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-6160/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-6160/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.373, 'learning_rate': 6.700502096935347e-07, 'epoch': 0.95}
{'loss': 0.3636, 'learning_rate': 6.661117295564623e-07, 'epoch': 0.95}
{'loss': 0.366, 'learning_rate': 6.621847809158255e-07, 'epoch': 0.95}
{'loss': 0.3698, 'learning_rate': 6.582693646895066e-07, 'epoch': 0.95}
{'loss': 0.3618, 'learning_rate': 6.543654817927292e-07, 'epoch': 0.95}
 95%|| 6161/6500 [11:38:45<40:44,  7.21s/it]                                                       95%|| 6161/6500 [11:38:45<40:44,  7.21s/it] 95%|| 6162/6500 [11:38:52<39:28,  7.01s/it]                                                       95%|| 6162/6500 [11:38:52<39:28,  7.01s/it] 95%|| 6163/6500 [11:38:58<38:34,  6.87s/it]                                                       95%|| 6163/6500 [11:38:58<38:34,  6.87s/it] 95%|| 6164/6500 [11:39:05<37:55,  6.77s/it]                                                       95%|| 6164/6500 [11:39:05<37:55,  6.77s/it] 95%|| 6165/6500 [11:39:12<37:24,  6.70s/it]                                                       95%|| 6165/6500 [11:39:12<37:24,  6.7{'loss': 0.3513, 'learning_rate': 6.50473133137991e-07, 'epoch': 0.95}
{'loss': 0.3994, 'learning_rate': 6.465923196351087e-07, 'epoch': 0.95}
{'loss': 0.3648, 'learning_rate': 6.427230421911956e-07, 'epoch': 0.95}
{'loss': 0.6366, 'learning_rate': 6.388653017106838e-07, 'epoch': 0.95}
{'loss': 0.3767, 'learning_rate': 6.350190990952743e-07, 'epoch': 0.95}
0s/it] 95%|| 6166/6500 [11:39:18<37:01,  6.65s/it]                                                       95%|| 6166/6500 [11:39:18<37:01,  6.65s/it] 95%|| 6167/6500 [11:39:25<36:44,  6.62s/it]                                                       95%|| 6167/6500 [11:39:25<36:44,  6.62s/it] 95%|| 6168/6500 [11:39:31<36:31,  6.60s/it]                                                       95%|| 6168/6500 [11:39:31<36:31,  6.60s/it] 95%|| 6169/6500 [11:39:38<36:16,  6.58s/it]                                                       95%|| 6169/6500 [11:39:38<36:16,  6.58s/it] 95%|| 6170/6500 [11:39:44<36:07,  6.57s/it]                                                       95%|| 6170/6500 [11:39:44<36:07,  6.57s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.881314754486084, 'eval_runtime': 1.4787, 'eval_samples_per_second': 8.115, 'eval_steps_per_second': 2.029, 'epoch': 0.95}
                                                       95%|| 6170/6500 [11:39:46<36:07,  6.57s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-6170
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-6170
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-6170
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-6170
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-6170
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-6170/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-6170/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-6170/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-6170/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3634, 'learning_rate': 6.311844352440144e-07, 'epoch': 0.95}
{'loss': 0.3548, 'learning_rate': 6.273613110532206e-07, 'epoch': 0.95}
{'loss': 0.3597, 'learning_rate': 6.235497274165337e-07, 'epoch': 0.95}
{'loss': 0.3614, 'learning_rate': 6.197496852248796e-07, 'epoch': 0.95}
{'loss': 0.3626, 'learning_rate': 6.159611853665037e-07, 'epoch': 0.95}
 95%|| 6171/6500 [11:39:53<39:00,  7.11s/it]                                                       95%|| 6171/6500 [11:39:53<39:00,  7.11s/it] 95%|| 6172/6500 [11:39:59<37:57,  6.94s/it]                                                       95%|| 6172/6500 [11:39:59<37:57,  6.94s/it] 95%|| 6173/6500 [11:40:06<37:10,  6.82s/it]                                                       95%|| 6173/6500 [11:40:06<37:10,  6.82s/it] 95%|| 6174/6500 [11:40:13<37:37,  6.92s/it]                                                       95%|| 6174/6500 [11:40:13<37:37,  6.92s/it] 95%|| 6175/6500 [11:40:19<36:53,  6.81s/it]                                                       95%|| 6175/6500 [11:40:19<36:53,  6.8{'loss': 0.3773, 'learning_rate': 6.121842287269419e-07, 'epoch': 0.95}
{'loss': 0.3765, 'learning_rate': 6.084188161890325e-07, 'epoch': 0.95}
{'loss': 0.3616, 'learning_rate': 6.046649486329158e-07, 'epoch': 0.95}
{'loss': 0.3698, 'learning_rate': 6.009226269360402e-07, 'epoch': 0.95}
{'loss': 0.3566, 'learning_rate': 5.971918519731557e-07, 'epoch': 0.95}
1s/it] 95%|| 6176/6500 [11:40:26<36:20,  6.73s/it]                                                       95%|| 6176/6500 [11:40:26<36:20,  6.73s/it] 95%|| 6177/6500 [11:40:33<36:18,  6.74s/it]                                                       95%|| 6177/6500 [11:40:33<36:18,  6.74s/it] 95%|| 6178/6500 [11:40:39<35:52,  6.68s/it]                                                       95%|| 6178/6500 [11:40:39<35:52,  6.68s/it] 95%|| 6179/6500 [11:40:46<35:31,  6.64s/it]                                                       95%|| 6179/6500 [11:40:46<35:31,  6.64s/it] 95%|| 6180/6500 [11:40:52<35:14,  6.61s/it]                                                       95%|| 6180/6500 [11:40:52<35:14,  6.61s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8810672163963318, 'eval_runtime': 1.484, 'eval_samples_per_second': 8.086, 'eval_steps_per_second': 2.022, 'epoch': 0.95}
                                                       95%|| 6180/6500 [11:40:54<35:14,  6.61s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-6180
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  I AM HERE AND I AM SAVING THE MODEL/projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-6180
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-6180

I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-6180
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-6180
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-6180/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-6180/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-6180/pytorch_model.binthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-6180/pytorch_model.bin

/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3632, 'learning_rate': 5.934726246162925e-07, 'epoch': 0.95}
{'loss': 0.3899, 'learning_rate': 5.897649457348109e-07, 'epoch': 0.95}
{'loss': 0.3691, 'learning_rate': 5.860688161953564e-07, 'epoch': 0.95}
{'loss': 0.6402, 'learning_rate': 5.823842368618715e-07, 'epoch': 0.95}
{'loss': 0.3667, 'learning_rate': 5.787112085956059e-07, 'epoch': 0.95}
 95%|| 6181/6500 [11:41:01<37:55,  7.13s/it]                                                       95%|| 6181/6500 [11:41:01<37:55,  7.13s/it] 95%|| 6182/6500 [11:41:07<36:52,  6.96s/it]                                                       95%|| 6182/6500 [11:41:07<36:52,  6.96s/it] 95%|| 6183/6500 [11:41:14<36:05,  6.83s/it]                                                       95%|| 6183/6500 [11:41:14<36:05,  6.83s/it] 95%|| 6184/6500 [11:41:20<35:30,  6.74s/it]                                                       95%|| 6184/6500 [11:41:20<35:30,  6.74s/it] 95%|| 6185/6500 [11:41:27<35:04,  6.68s/it]                                                       95%|| 6185/6500 [11:41:27<35:04,  6.6{'loss': 0.3688, 'learning_rate': 5.750497322551118e-07, 'epoch': 0.95}
{'loss': 0.3498, 'learning_rate': 5.713998086962325e-07, 'epoch': 0.95}
{'loss': 0.3539, 'learning_rate': 5.677614387721131e-07, 'epoch': 0.95}
{'loss': 0.3627, 'learning_rate': 5.641346233332123e-07, 'epoch': 0.95}
{'loss': 0.372, 'learning_rate': 5.605193632272632e-07, 'epoch': 0.95}
8s/it] 95%|| 6186/6500 [11:41:33<34:43,  6.63s/it]                                                       95%|| 6186/6500 [11:41:33<34:43,  6.63s/it] 95%|| 6187/6500 [11:41:40<34:29,  6.61s/it]                                                       95%|| 6187/6500 [11:41:40<34:29,  6.61s/it] 95%|| 6188/6500 [11:41:46<34:16,  6.59s/it]                                                       95%|| 6188/6500 [11:41:47<34:16,  6.59s/it] 95%|| 6189/6500 [11:41:53<34:04,  6.57s/it]                                                       95%|| 6189/6500 [11:41:53<34:04,  6.57s/it] 95%|| 6190/6500 [11:42:00<34:51,  6.75s/it]                                                       95%|| 6190/6500 [11:42:00<34:51,  6.75s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8809212446212769, 'eval_runtime': 1.5064, 'eval_samples_per_second': 7.966, 'eval_steps_per_second': 1.992, 'epoch': 0.95}
                                                       95%|| 6190/6500 [11:42:02<34:51,  6.75s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-6190
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-6190I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-6190

I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-6190
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-6190
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-6190/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-6190/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-6190/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-6190/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.372, 'learning_rate': 5.569156592993175e-07, 'epoch': 0.95}
{'loss': 0.3604, 'learning_rate': 5.533235123917235e-07, 'epoch': 0.95}
{'loss': 0.3606, 'learning_rate': 5.497429233441098e-07, 'epoch': 0.95}
{'loss': 0.3698, 'learning_rate': 5.46173892993429e-07, 'epoch': 0.95}
{'loss': 0.3511, 'learning_rate': 5.426164221739138e-07, 'epoch': 0.95}
 95%|| 6191/6500 [11:42:09<37:20,  7.25s/it]                                                       95%|| 6191/6500 [11:42:09<37:20,  7.25s/it] 95%|| 6192/6500 [11:42:15<36:08,  7.04s/it]                                                       95%|| 6192/6500 [11:42:15<36:08,  7.04s/it] 95%|| 6193/6500 [11:42:22<35:16,  6.89s/it]                                                       95%|| 6193/6500 [11:42:22<35:16,  6.89s/it] 95%|| 6194/6500 [11:42:28<34:36,  6.79s/it]                                                       95%|| 6194/6500 [11:42:28<34:36,  6.79s/it] 95%|| 6195/6500 [11:42:35<34:07,  6.71s/it]                                                       95%|| 6195/6500 [11:42:35<34:07,  6.7{'loss': 0.3629, 'learning_rate': 5.390705117171047e-07, 'epoch': 0.95}
{'loss': 0.3839, 'learning_rate': 5.355361624518329e-07, 'epoch': 0.95}
{'loss': 0.3626, 'learning_rate': 5.320133752042378e-07, 'epoch': 0.95}
{'loss': 0.6353, 'learning_rate': 5.285021507977495e-07, 'epoch': 0.95}
{'loss': 0.3576, 'learning_rate': 5.250024900530893e-07, 'epoch': 0.95}
1s/it] 95%|| 6196/6500 [11:42:41<33:45,  6.66s/it]                                                       95%|| 6196/6500 [11:42:41<33:45,  6.66s/it] 95%|| 6197/6500 [11:42:48<33:27,  6.62s/it]                                                       95%|| 6197/6500 [11:42:48<33:27,  6.62s/it] 95%|| 6198/6500 [11:42:54<33:12,  6.60s/it]                                                       95%|| 6198/6500 [11:42:54<33:12,  6.60s/it] 95%|| 6199/6500 [11:43:01<32:58,  6.57s/it]                                                       95%|| 6199/6500 [11:43:01<32:58,  6.57s/it] 95%|| 6200/6500 [11:43:07<32:49,  6.56s/it]                                                       95%|| 6200/6500 [11:43:07<32:49,  6.56s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8813624978065491, 'eval_runtime': 1.481, 'eval_samples_per_second': 8.103, 'eval_steps_per_second': 2.026, 'epoch': 0.95}
                                                       95%|| 6200/6500 [11:43:09<32:49,  6.56s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-6200
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-6200
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-6200
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-6200
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-6200
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-6200/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-6200/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-6200/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-6200/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3747, 'learning_rate': 5.215143937882805e-07, 'epoch': 0.95}
{'loss': 0.3462, 'learning_rate': 5.180378628186488e-07, 'epoch': 0.95}
{'loss': 0.3511, 'learning_rate': 5.145728979568165e-07, 'epoch': 0.95}
{'loss': 0.3637, 'learning_rate': 5.111195000126912e-07, 'epoch': 0.95}
{'loss': 0.3518, 'learning_rate': 5.076776697934826e-07, 'epoch': 0.95}
 95%|| 6201/6500 [11:43:16<35:27,  7.12s/it]                                                       95%|| 6201/6500 [11:43:16<35:27,  7.12s/it] 95%|| 6202/6500 [11:43:22<34:28,  6.94s/it]                                                       95%|| 6202/6500 [11:43:22<34:28,  6.94s/it] 95%|| 6203/6500 [11:43:29<33:44,  6.82s/it]                                                       95%|| 6203/6500 [11:43:29<33:44,  6.82s/it] 95%|| 6204/6500 [11:43:35<33:12,  6.73s/it]                                                       95%|| 6204/6500 [11:43:35<33:12,  6.73s/it] 95%|| 6205/6500 [11:43:42<32:49,  6.68s/it]                                                       95%|| 6205/6500 [11:43:42<32:49,  6.6{'loss': 0.3733, 'learning_rate': 5.04247408103703e-07, 'epoch': 0.95}
{'loss': 0.38, 'learning_rate': 5.008287157451496e-07, 'epoch': 0.95}
{'loss': 0.3613, 'learning_rate': 4.97421593516928e-07, 'epoch': 0.96}
{'loss': 0.3647, 'learning_rate': 4.94026042215423e-07, 'epoch': 0.96}
{'loss': 0.3548, 'learning_rate': 4.906420626343334e-07, 'epoch': 0.96}
8s/it] 95%|| 6206/6500 [11:43:50<34:01,  6.94s/it]                                                       95%|| 6206/6500 [11:43:50<34:01,  6.94s/it] 95%|| 6207/6500 [11:43:56<33:19,  6.82s/it]                                                       95%|| 6207/6500 [11:43:56<33:19,  6.82s/it] 96%|| 6208/6500 [11:44:03<32:47,  6.74s/it]                                                       96%|| 6208/6500 [11:44:03<32:47,  6.74s/it] 96%|| 6209/6500 [11:44:09<32:23,  6.68s/it]                                                       96%|| 6209/6500 [11:44:09<32:23,  6.68s/it] 96%|| 6210/6500 [11:44:16<32:03,  6.63s/it]                                                       96%|| 6210/6500 [11:44:16<32:03,  6.63s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.880839467048645, 'eval_runtime': 1.4789, 'eval_samples_per_second': 8.114, 'eval_steps_per_second': 2.029, 'epoch': 0.96}
                                                       96%|| 6210/6500 [11:44:17<32:03,  6.63s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-6210
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-6210
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-6210
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-6210
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-6210
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-6210/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-6210/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-6210/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-6210/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3903, 'learning_rate': 4.872696555646373e-07, 'epoch': 0.96}
{'loss': 0.3583, 'learning_rate': 4.839088217946208e-07, 'epoch': 0.96}
{'loss': 0.6299, 'learning_rate': 4.805595621098502e-07, 'epoch': 0.96}
{'loss': 0.3843, 'learning_rate': 4.772218772932047e-07, 'epoch': 0.96}
{'loss': 0.3627, 'learning_rate': 4.738957681248379e-07, 'epoch': 0.96}
 96%|| 6211/6500 [11:44:24<34:26,  7.15s/it]                                                       96%|| 6211/6500 [11:44:24<34:26,  7.15s/it] 96%|| 6212/6500 [11:44:31<33:25,  6.97s/it]                                                       96%|| 6212/6500 [11:44:31<33:25,  6.97s/it] 96%|| 6213/6500 [11:44:37<32:40,  6.83s/it]                                                       96%|| 6213/6500 [11:44:37<32:40,  6.83s/it] 96%|| 6214/6500 [11:44:44<32:07,  6.74s/it]                                                       96%|| 6214/6500 [11:44:44<32:07,  6.74s/it] 96%|| 6215/6500 [11:44:50<31:42,  6.67s/it]                                                       96%|| 6215/6500 [11:44:50<31:42,  6.6{'loss': 0.3772, 'learning_rate': 4.7058123538221144e-07, 'epoch': 0.96}
{'loss': 0.3531, 'learning_rate': 4.672782798400777e-07, 'epoch': 0.96}
{'loss': 0.3589, 'learning_rate': 4.639869022704801e-07, 'epoch': 0.96}
{'loss': 0.3635, 'learning_rate': 4.607071034427646e-07, 'epoch': 0.96}
{'loss': 0.3674, 'learning_rate': 4.574388841235566e-07, 'epoch': 0.96}
7s/it] 96%|| 6216/6500 [11:44:57<31:22,  6.63s/it]                                                       96%|| 6216/6500 [11:44:57<31:22,  6.63s/it] 96%|| 6217/6500 [11:45:03<31:07,  6.60s/it]                                                       96%|| 6217/6500 [11:45:03<31:07,  6.60s/it] 96%|| 6218/6500 [11:45:10<30:55,  6.58s/it]                                                       96%|| 6218/6500 [11:45:10<30:55,  6.58s/it] 96%|| 6219/6500 [11:45:16<30:44,  6.56s/it]                                                       96%|| 6219/6500 [11:45:16<30:44,  6.56s/it] 96%|| 6220/6500 [11:45:23<30:35,  6.56s/it]                                                       96%|| 6220/6500 [11:45:23<30:35,  6.56s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8809477686882019, 'eval_runtime': 1.4776, 'eval_samples_per_second': 8.121, 'eval_steps_per_second': 2.03, 'epoch': 0.96}
                                                       96%|| 6220/6500 [11:45:24<30:35,  6.56s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-6220
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-6220I AM HERE AND I AM SAVING THE MODEL

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-6220
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-6220
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-6220
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-6220/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-6220/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-6220/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-6220/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.363, 'learning_rate': 4.541822450767896e-07, 'epoch': 0.96}
{'loss': 0.3679, 'learning_rate': 4.5093718706367136e-07, 'epoch': 0.96}
{'loss': 0.351, 'learning_rate': 4.4770371084272846e-07, 'epoch': 0.96}
{'loss': 0.3706, 'learning_rate': 4.444818171697618e-07, 'epoch': 0.96}
{'loss': 0.3516, 'learning_rate': 4.41271506797869e-07, 'epoch': 0.96}
 96%|| 6221/6500 [11:45:31<32:59,  7.10s/it]                                                       96%|| 6221/6500 [11:45:31<32:59,  7.10s/it] 96%|| 6222/6500 [11:45:39<33:19,  7.19s/it]                                                       96%|| 6222/6500 [11:45:39<33:19,  7.19s/it] 96%|| 6223/6500 [11:45:45<32:16,  6.99s/it]                                                       96%|| 6223/6500 [11:45:45<32:16,  6.99s/it] 96%|| 6224/6500 [11:45:52<31:32,  6.86s/it]                                                       96%|| 6224/6500 [11:45:52<31:32,  6.86s/it] 96%|| 6225/6500 [11:45:58<30:57,  6.75s/it]                                                       96%|| 6225/6500 [11:45:58<30:57,  6.7{'loss': 0.3924, 'learning_rate': 4.3807278047743317e-07, 'epoch': 0.96}
{'loss': 0.3515, 'learning_rate': 4.348856389561451e-07, 'epoch': 0.96}
{'loss': 0.633, 'learning_rate': 4.3171008297898107e-07, 'epoch': 0.96}
{'loss': 0.379, 'learning_rate': 4.2854611328820295e-07, 'epoch': 0.96}
{'loss': 0.3572, 'learning_rate': 4.253937306233691e-07, 'epoch': 0.96}
5s/it] 96%|| 6226/6500 [11:46:05<30:33,  6.69s/it]                                                       96%|| 6226/6500 [11:46:05<30:33,  6.69s/it] 96%|| 6227/6500 [11:46:11<30:12,  6.64s/it]                                                       96%|| 6227/6500 [11:46:11<30:12,  6.64s/it] 96%|| 6228/6500 [11:46:18<29:55,  6.60s/it]                                                       96%|| 6228/6500 [11:46:18<29:55,  6.60s/it] 96%|| 6229/6500 [11:46:24<29:43,  6.58s/it]                                                       96%|| 6229/6500 [11:46:24<29:43,  6.58s/it] 96%|| 6230/6500 [11:46:31<29:32,  6.57s/it]                                                       96%|| 6230/6500 [11:46:31<29:32,  6.57s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.881489634513855, 'eval_runtime': 1.4785, 'eval_samples_per_second': 8.116, 'eval_steps_per_second': 2.029, 'epoch': 0.96}
                                                       96%|| 6230/6500 [11:46:32<29:32,  6.57s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-6230
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-6230
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-6230
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-6230
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-6230
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-6230/pytorch_model.binthe pytorch model path is
 /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-6230/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-6230/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-6230/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3695, 'learning_rate': 4.222529357213345e-07, 'epoch': 0.96}
{'loss': 0.3625, 'learning_rate': 4.191237293162398e-07, 'epoch': 0.96}
{'loss': 0.3632, 'learning_rate': 4.1600611213951645e-07, 'epoch': 0.96}
{'loss': 0.3682, 'learning_rate': 4.129000849198872e-07, 'epoch': 0.96}
{'loss': 0.3685, 'learning_rate': 4.0980564838336566e-07, 'epoch': 0.96}
 96%|| 6231/6500 [11:46:39<31:52,  7.11s/it]                                                       96%|| 6231/6500 [11:46:39<31:52,  7.11s/it] 96%|| 6232/6500 [11:46:46<30:58,  6.94s/it]                                                       96%|| 6232/6500 [11:46:46<30:58,  6.94s/it] 96%|| 6233/6500 [11:46:52<30:19,  6.82s/it]                                                       96%|| 6233/6500 [11:46:52<30:19,  6.82s/it] 96%|| 6234/6500 [11:46:59<29:50,  6.73s/it]                                                       96%|| 6234/6500 [11:46:59<29:50,  6.73s/it] 96%|| 6235/6500 [11:47:05<29:28,  6.67s/it]                                                       96%|| 6235/6500 [11:47:05<29:28,  6.6{'loss': 0.3656, 'learning_rate': 4.067228032532677e-07, 'epoch': 0.96}
{'loss': 0.3685, 'learning_rate': 4.0365155025017807e-07, 'epoch': 0.96}
{'loss': 0.3647, 'learning_rate': 4.0059189009198364e-07, 'epoch': 0.96}
{'loss': 0.3592, 'learning_rate': 3.975438234938733e-07, 'epoch': 0.96}
{'loss': 0.3467, 'learning_rate': 3.9450735116830505e-07, 'epoch': 0.96}
7s/it] 96%|| 6236/6500 [11:47:12<29:11,  6.63s/it]                                                       96%|| 6236/6500 [11:47:12<29:11,  6.63s/it] 96%|| 6237/6500 [11:47:18<28:57,  6.61s/it]                                                       96%|| 6237/6500 [11:47:18<28:57,  6.61s/it] 96%|| 6238/6500 [11:47:26<29:55,  6.85s/it]                                                       96%|| 6238/6500 [11:47:26<29:55,  6.85s/it] 96%|| 6239/6500 [11:47:32<29:26,  6.77s/it]                                                       96%|| 6239/6500 [11:47:32<29:26,  6.77s/it] 96%|| 6240/6500 [11:47:39<29:02,  6.70s/it]                                                       96%|| 6240/6500 [11:47:39<29:02,  6.70s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8814338445663452, 'eval_runtime': 1.4775, 'eval_samples_per_second': 8.122, 'eval_steps_per_second': 2.031, 'epoch': 0.96}
                                                       96%|| 6240/6500 [11:47:40<29:02,  6.70s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-6240
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-6240
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-6240
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-6240
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-6240
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-6240/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-6240/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-6240/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-6240/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3948, 'learning_rate': 3.914824738250333e-07, 'epoch': 0.96}
{'loss': 0.3683, 'learning_rate': 3.884691921711092e-07, 'epoch': 0.96}
{'loss': 0.637, 'learning_rate': 3.854675069108693e-07, 'epoch': 0.96}
{'loss': 0.3692, 'learning_rate': 3.8247741874594123e-07, 'epoch': 0.96}
{'loss': 0.3623, 'learning_rate': 3.7949892837523814e-07, 'epoch': 0.96}
 96%|| 6241/6500 [11:47:47<31:04,  7.20s/it]                                                       96%|| 6241/6500 [11:47:47<31:04,  7.20s/it] 96%|| 6242/6500 [11:47:54<30:06,  7.00s/it]                                                       96%|| 6242/6500 [11:47:54<30:06,  7.00s/it] 96%|| 6243/6500 [11:48:00<29:24,  6.86s/it]                                                       96%|| 6243/6500 [11:48:00<29:24,  6.86s/it] 96%|| 6244/6500 [11:48:07<28:53,  6.77s/it]                                                       96%|| 6244/6500 [11:48:07<28:53,  6.77s/it] 96%|| 6245/6500 [11:48:14<28:29,  6.70s/it]                                                       96%|| 6245/6500 [11:48:14<28:29,  6.7{'loss': 0.3606, 'learning_rate': 3.765320364949587e-07, 'epoch': 0.96}
{'loss': 0.3547, 'learning_rate': 3.735767437985982e-07, 'epoch': 0.96}
{'loss': 0.3661, 'learning_rate': 3.706330509769429e-07, 'epoch': 0.96}
{'loss': 0.3587, 'learning_rate': 3.677009587180591e-07, 'epoch': 0.96}
{'loss': 0.3712, 'learning_rate': 3.647804677073097e-07, 'epoch': 0.96}
0s/it] 96%|| 6246/6500 [11:48:20<28:10,  6.65s/it]                                                       96%|| 6246/6500 [11:48:20<28:10,  6.65s/it] 96%|| 6247/6500 [11:48:27<27:55,  6.62s/it]                                                       96%|| 6247/6500 [11:48:27<27:55,  6.62s/it] 96%|| 6248/6500 [11:48:33<27:43,  6.60s/it]                                                       96%|| 6248/6500 [11:48:33<27:43,  6.60s/it] 96%|| 6249/6500 [11:48:40<27:32,  6.58s/it]                                                       96%|| 6249/6500 [11:48:40<27:32,  6.58s/it] 96%|| 6250/6500 [11:48:46<27:23,  6.58s/it]                                                       96%|| 6250/6500 [11:48:46<27:23,  6.58s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.880318284034729, 'eval_runtime': 1.4792, 'eval_samples_per_second': 8.112, 'eval_steps_per_second': 2.028, 'epoch': 0.96}
                                                       96%|| 6250/6500 [11:48:48<27:23,  6.58s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-6250
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-6250I AM HERE AND I AM SAVING THE MODEL

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-6250
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-6250
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-6250
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-6250/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-6250/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-6250/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-6250/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3593, 'learning_rate': 3.6187157862733743e-07, 'epoch': 0.96}
{'loss': 0.3617, 'learning_rate': 3.589742921580763e-07, 'epoch': 0.96}
{'loss': 0.372, 'learning_rate': 3.5608860897675677e-07, 'epoch': 0.96}
{'loss': 0.3587, 'learning_rate': 3.532145297578837e-07, 'epoch': 0.96}
{'loss': 0.3557, 'learning_rate': 3.5035205517325307e-07, 'epoch': 0.96}
 96%|| 6251/6500 [11:48:55<29:30,  7.11s/it]                                                       96%|| 6251/6500 [11:48:55<29:30,  7.11s/it] 96%|| 6252/6500 [11:49:01<28:41,  6.94s/it]                                                       96%|| 6252/6500 [11:49:01<28:41,  6.94s/it] 96%|| 6253/6500 [11:49:08<28:05,  6.82s/it]                                                       96%|| 6253/6500 [11:49:08<28:05,  6.82s/it] 96%|| 6254/6500 [11:49:14<27:38,  6.74s/it]                                                       96%|| 6254/6500 [11:49:14<27:38,  6.74s/it] 96%|| 6255/6500 [11:49:22<28:24,  6.96s/it]                                                       96%|| 6255/6500 [11:49:22<28:24,  6.9{'loss': 0.399, 'learning_rate': 3.4750118589196303e-07, 'epoch': 0.96}
{'loss': 0.3686, 'learning_rate': 3.4466192258036935e-07, 'epoch': 0.96}
{'loss': 0.6376, 'learning_rate': 3.4183426590214673e-07, 'epoch': 0.96}
{'loss': 0.3682, 'learning_rate': 3.3901821651823875e-07, 'epoch': 0.96}
{'loss': 0.3627, 'learning_rate': 3.3621377508687435e-07, 'epoch': 0.96}
6s/it] 96%|| 6256/6500 [11:49:28<27:48,  6.84s/it]                                                       96%|| 6256/6500 [11:49:28<27:48,  6.84s/it] 96%|| 6257/6500 [11:49:35<27:20,  6.75s/it]                                                       96%|| 6257/6500 [11:49:35<27:20,  6.75s/it] 96%|| 6258/6500 [11:49:41<26:58,  6.69s/it]                                                       96%|| 6258/6500 [11:49:41<26:58,  6.69s/it] 96%|| 6259/6500 [11:49:48<26:41,  6.65s/it]                                                       96%|| 6259/6500 [11:49:48<26:41,  6.65s/it] 96%|| 6260/6500 [11:49:54<26:26,  6.61s/it]                                                       96%|| 6260/6500 [11:49:54<26:26,  6.61s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8811809420585632, 'eval_runtime': 1.4741, 'eval_samples_per_second': 8.141, 'eval_steps_per_second': 2.035, 'epoch': 0.96}
                                                       96%|| 6260/6500 [11:49:56<26:26,  6.61s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-6260
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-6260I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-6260

I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-6260
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-6260
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-6260/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-6260/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-6260/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-6260/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.357, 'learning_rate': 3.334209422635848e-07, 'epoch': 0.96}
{'loss': 0.3565, 'learning_rate': 3.3063971870117005e-07, 'epoch': 0.96}
{'loss': 0.3648, 'learning_rate': 3.2787010504972125e-07, 'epoch': 0.96}
{'loss': 0.3743, 'learning_rate': 3.251121019566317e-07, 'epoch': 0.96}
{'loss': 0.3728, 'learning_rate': 3.223657100665578e-07, 'epoch': 0.96}
 96%|| 6261/6500 [11:50:03<28:29,  7.15s/it]                                                       96%|| 6261/6500 [11:50:03<28:29,  7.15s/it] 96%|| 6262/6500 [11:50:09<27:39,  6.97s/it]                                                       96%|| 6262/6500 [11:50:09<27:39,  6.97s/it] 96%|| 6263/6500 [11:50:16<27:01,  6.84s/it]                                                       96%|| 6263/6500 [11:50:16<27:01,  6.84s/it] 96%|| 6264/6500 [11:50:22<26:32,  6.75s/it]                                                       96%|| 6264/6500 [11:50:22<26:32,  6.75s/it] 96%|| 6265/6500 [11:50:29<26:11,  6.69s/it]                                                       96%|| 6265/6500 [11:50:29<26:11,  6.6{'loss': 0.3657, 'learning_rate': 3.1963093002145285e-07, 'epoch': 0.96}
{'loss': 0.3676, 'learning_rate': 3.169077624605554e-07, 'epoch': 0.96}
{'loss': 0.3689, 'learning_rate': 3.1419620802038973e-07, 'epoch': 0.96}
{'loss': 0.3497, 'learning_rate': 3.11496267334771e-07, 'epoch': 0.96}
{'loss': 0.362, 'learning_rate': 3.0880794103478327e-07, 'epoch': 0.96}
9s/it] 96%|| 6266/6500 [11:50:36<25:54,  6.64s/it]                                                       96%|| 6266/6500 [11:50:36<25:54,  6.64s/it] 96%|| 6267/6500 [11:50:42<25:39,  6.61s/it]                                                       96%|| 6267/6500 [11:50:42<25:39,  6.61s/it] 96%|| 6268/6500 [11:50:49<25:27,  6.59s/it]                                                       96%|| 6268/6500 [11:50:49<25:27,  6.59s/it] 96%|| 6269/6500 [11:50:55<25:17,  6.57s/it]                                                       96%|| 6269/6500 [11:50:55<25:17,  6.57s/it] 96%|| 6270/6500 [11:51:02<25:07,  6.56s/it]                                                       96%|| 6270/6500 [11:51:02<25:07,  6.56s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8809992671012878, 'eval_runtime': 1.4716, 'eval_samples_per_second': 8.154, 'eval_steps_per_second': 2.039, 'epoch': 0.96}
                                                       96%|| 6270/6500 [11:51:03<25:07,  6.56s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-6270
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-6270I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-6270

I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-6270
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-6270
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-6270/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-6270/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-6270/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-6270/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3842, 'learning_rate': 3.061312297488128e-07, 'epoch': 0.96}
{'loss': 0.3689, 'learning_rate': 3.034661341025258e-07, 'epoch': 0.96}
{'loss': 0.6391, 'learning_rate': 3.0081265471886834e-07, 'epoch': 0.97}
{'loss': 0.3596, 'learning_rate': 2.981707922180776e-07, 'epoch': 0.97}
{'loss': 0.3715, 'learning_rate': 2.955405472176709e-07, 'epoch': 0.97}
 96%|| 6271/6500 [11:51:11<27:52,  7.30s/it]                                                       96%|| 6271/6500 [11:51:11<27:52,  7.30s/it] 96%|| 6272/6500 [11:51:17<26:53,  7.08s/it]                                                       96%|| 6272/6500 [11:51:17<26:53,  7.08s/it] 97%|| 6273/6500 [11:51:24<26:09,  6.91s/it]                                                       97%|| 6273/6500 [11:51:24<26:09,  6.91s/it] 97%|| 6274/6500 [11:51:30<25:37,  6.80s/it]                                                       97%|| 6274/6500 [11:51:30<25:37,  6.80s/it] 97%|| 6275/6500 [11:51:37<25:14,  6.73s/it]                                                       97%|| 6275/6500 [11:51:37<25:14,  6.7{'loss': 0.3532, 'learning_rate': 2.9292192033245623e-07, 'epoch': 0.97}
{'loss': 0.3532, 'learning_rate': 2.9031491217451636e-07, 'epoch': 0.97}
{'loss': 0.3585, 'learning_rate': 2.877195233532248e-07, 'epoch': 0.97}
{'loss': 0.3633, 'learning_rate': 2.8513575447524644e-07, 'epoch': 0.97}
{'loss': 0.3717, 'learning_rate': 2.825636061445092e-07, 'epoch': 0.97}
3s/it] 97%|| 6276/6500 [11:51:43<24:55,  6.68s/it]                                                       97%|| 6276/6500 [11:51:43<24:55,  6.68s/it] 97%|| 6277/6500 [11:51:50<24:39,  6.63s/it]                                                       97%|| 6277/6500 [11:51:50<24:39,  6.63s/it] 97%|| 6278/6500 [11:51:57<24:25,  6.60s/it]                                                       97%|| 6278/6500 [11:51:57<24:25,  6.60s/it] 97%|| 6279/6500 [11:52:03<24:13,  6.58s/it]                                                       97%|| 6279/6500 [11:52:03<24:13,  6.58s/it] 97%|| 6280/6500 [11:52:10<24:04,  6.57s/it]                                                       97%|| 6280/6500 [11:52:10<24:04,  6.57s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8815439343452454, 'eval_runtime': 1.4869, 'eval_samples_per_second': 8.07, 'eval_steps_per_second': 2.018, 'epoch': 0.97}
                                                       97%|| 6280/6500 [11:52:11<24:04,  6.57s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-6280
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-6280I AM HERE AND I AM SAVING THE MODEL

the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-6280
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-6280
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-6280
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-6280/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-6280/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-6280/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-6280/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3718, 'learning_rate': 2.8000307896223765e-07, 'epoch': 0.97}
{'loss': 0.3569, 'learning_rate': 2.774541735269476e-07, 'epoch': 0.97}
{'loss': 0.3662, 'learning_rate': 2.7491689043442903e-07, 'epoch': 0.97}
{'loss': 0.3503, 'learning_rate': 2.7239123027775204e-07, 'epoch': 0.97}
{'loss': 0.3668, 'learning_rate': 2.6987719364727215e-07, 'epoch': 0.97}
 97%|| 6281/6500 [11:52:18<25:59,  7.12s/it]                                                       97%|| 6281/6500 [11:52:18<25:59,  7.12s/it] 97%|| 6282/6500 [11:52:25<25:13,  6.94s/it]                                                       97%|| 6282/6500 [11:52:25<25:13,  6.94s/it] 97%|| 6283/6500 [11:52:31<24:41,  6.83s/it]                                                       97%|| 6283/6500 [11:52:31<24:41,  6.83s/it] 97%|| 6284/6500 [11:52:38<24:15,  6.74s/it]                                                       97%|| 6284/6500 [11:52:38<24:15,  6.74s/it] 97%|| 6285/6500 [11:52:44<23:57,  6.69s/it]                                                       97%|| 6285/6500 [11:52:44<23:57,  6.6{'loss': 0.3732, 'learning_rate': 2.6737478113063596e-07, 'epoch': 0.97}
{'loss': 0.4921, 'learning_rate': 2.6488399331277004e-07, 'epoch': 0.97}
{'loss': 0.5224, 'learning_rate': 2.6240483077586976e-07, 'epoch': 0.97}
{'loss': 0.3576, 'learning_rate': 2.599372940994327e-07, 'epoch': 0.97}
{'loss': 0.3703, 'learning_rate': 2.574813838602308e-07, 'epoch': 0.97}
9s/it] 97%|| 6286/6500 [11:52:51<23:41,  6.64s/it]                                                       97%|| 6286/6500 [11:52:51<23:41,  6.64s/it] 97%|| 6287/6500 [11:52:58<24:25,  6.88s/it]                                                       97%|| 6287/6500 [11:52:58<24:25,  6.88s/it] 97%|| 6288/6500 [11:53:05<23:57,  6.78s/it]                                                       97%|| 6288/6500 [11:53:05<23:57,  6.78s/it] 97%|| 6289/6500 [11:53:11<23:35,  6.71s/it]                                                       97%|| 6289/6500 [11:53:11<23:35,  6.71s/it] 97%|| 6290/6500 [11:53:18<23:18,  6.66s/it]                                                       97%|| 6290/6500 [11:53:18<23:18,  6.66s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.880657434463501, 'eval_runtime': 1.4769, 'eval_samples_per_second': 8.125, 'eval_steps_per_second': 2.031, 'epoch': 0.97}
                                                       97%|| 6290/6500 [11:53:19<23:18,  6.66s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-6290
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-6290
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-6290
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-6290
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-6290
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-6290/pytorch_model.bin
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-6290/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-6290/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-6290/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3464, 'learning_rate': 2.55037100632316e-07, 'epoch': 0.97}
{'loss': 0.3494, 'learning_rate': 2.5260444498702573e-07, 'epoch': 0.97}
{'loss': 0.3639, 'learning_rate': 2.501834174929718e-07, 'epoch': 0.97}
{'loss': 0.3538, 'learning_rate': 2.4777401871606267e-07, 'epoch': 0.97}
{'loss': 0.3768, 'learning_rate': 2.453762492194811e-07, 'epoch': 0.97}
 97%|| 6291/6500 [11:53:26<24:58,  7.17s/it]                                                       97%|| 6291/6500 [11:53:26<24:58,  7.17s/it] 97%|| 6292/6500 [11:53:33<24:11,  6.98s/it]                                                       97%|| 6292/6500 [11:53:33<24:11,  6.98s/it] 97%|| 6293/6500 [11:53:39<23:37,  6.85s/it]                                                       97%|| 6293/6500 [11:53:39<23:37,  6.85s/it] 97%|| 6294/6500 [11:53:46<23:12,  6.76s/it]                                                       97%|| 6294/6500 [11:53:46<23:12,  6.76s/it] 97%|| 6295/6500 [11:53:52<22:52,  6.70s/it]                                                       97%|| 6295/6500 [11:53:52<22:52,  6.7{'loss': 0.3711, 'learning_rate': 2.429901095636844e-07, 'epoch': 0.97}
{'loss': 0.3551, 'learning_rate': 2.4061560030642636e-07, 'epoch': 0.97}
{'loss': 0.3644, 'learning_rate': 2.382527220027242e-07, 'epoch': 0.97}
{'loss': 0.3471, 'learning_rate': 2.3590147520489713e-07, 'epoch': 0.97}
{'loss': 0.3942, 'learning_rate': 2.3356186046252783e-07, 'epoch': 0.97}
0s/it] 97%|| 6296/6500 [11:53:59<22:36,  6.65s/it]                                                       97%|| 6296/6500 [11:53:59<22:36,  6.65s/it] 97%|| 6297/6500 [11:54:05<22:23,  6.62s/it]                                                       97%|| 6297/6500 [11:54:05<22:23,  6.62s/it] 97%|| 6298/6500 [11:54:12<22:12,  6.60s/it]                                                       97%|| 6298/6500 [11:54:12<22:12,  6.60s/it] 97%|| 6299/6500 [11:54:19<22:03,  6.58s/it]                                                       97%|| 6299/6500 [11:54:19<22:03,  6.58s/it] 97%|| 6300/6500 [11:54:25<21:55,  6.58s/it]                                                       97%|| 6300/6500 [11:54:25<21:55,  6.58s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8811283111572266, 'eval_runtime': 1.4756, 'eval_samples_per_second': 8.132, 'eval_steps_per_second': 2.033, 'epoch': 0.97}
                                                       97%|| 6300/6500 [11:54:27<21:55,  6.58s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-6300
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-6300
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-6300
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-6300
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-6300
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-6300/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-6300/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-6300/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-6300/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3498, 'learning_rate': 2.3123387832248987e-07, 'epoch': 0.97}
{'loss': 0.6346, 'learning_rate': 2.289175293289314e-07, 'epoch': 0.97}
{'loss': 0.3728, 'learning_rate': 2.2661281402328595e-07, 'epoch': 0.97}
{'loss': 0.3605, 'learning_rate': 2.24319732944267e-07, 'epoch': 0.97}
{'loss': 0.3723, 'learning_rate': 2.2203828662787363e-07, 'epoch': 0.97}
 97%|| 6301/6500 [11:54:33<23:34,  7.11s/it]                                                       97%|| 6301/6500 [11:54:33<23:34,  7.11s/it] 97%|| 6302/6500 [11:54:40<22:54,  6.94s/it]                                                       97%|| 6302/6500 [11:54:40<22:54,  6.94s/it] 97%|| 6303/6500 [11:54:47<23:15,  7.09s/it]                                                       97%|| 6303/6500 [11:54:47<23:15,  7.09s/it] 97%|| 6304/6500 [11:54:54<22:37,  6.92s/it]                                                       97%|| 6304/6500 [11:54:54<22:37,  6.92s/it] 97%|| 6305/6500 [11:55:01<22:08,  6.81s/it]                                                       97%|| 6305/6500 [11:55:01<22:08,  6.8{'loss': 0.3528, 'learning_rate': 2.1976847560737367e-07, 'epoch': 0.97}
{'loss': 0.365, 'learning_rate': 2.175103004133261e-07, 'epoch': 0.97}
{'loss': 0.3656, 'learning_rate': 2.152637615735642e-07, 'epoch': 0.97}
{'loss': 0.3707, 'learning_rate': 2.1302885961319575e-07, 'epoch': 0.97}
{'loss': 0.3613, 'learning_rate': 2.1080559505462505e-07, 'epoch': 0.97}
1s/it] 97%|| 6306/6500 [11:55:07<21:45,  6.73s/it]                                                       97%|| 6306/6500 [11:55:07<21:45,  6.73s/it] 97%|| 6307/6500 [11:55:14<21:28,  6.68s/it]                                                       97%|| 6307/6500 [11:55:14<21:28,  6.68s/it] 97%|| 6308/6500 [11:55:20<21:14,  6.64s/it]                                                       97%|| 6308/6500 [11:55:20<21:14,  6.64s/it] 97%|| 6309/6500 [11:55:27<21:02,  6.61s/it]                                                       97%|| 6309/6500 [11:55:27<21:02,  6.61s/it] 97%|| 6310/6500 [11:55:33<20:52,  6.59s/it]                                                       97%|| 6310/6500 [11:55:33<20:52,  6.59s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8810054063796997, 'eval_runtime': 1.4779, 'eval_samples_per_second': 8.12, 'eval_steps_per_second': 2.03, 'epoch': 0.97}
                                                       97%|| 6310/6500 [11:55:35<20:52,  6.59s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-6310
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-6310
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-6310
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-6310
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-6310
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-6310/pytorch_model.bin
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-6310/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-6310/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-6310/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3666, 'learning_rate': 2.0859396841752531e-07, 'epoch': 0.97}
{'loss': 0.3601, 'learning_rate': 2.0639398021884414e-07, 'epoch': 0.97}
{'loss': 0.3609, 'learning_rate': 2.0420563097282573e-07, 'epoch': 0.97}
{'loss': 0.3497, 'learning_rate': 2.0202892119097204e-07, 'epoch': 0.97}
{'loss': 0.3938, 'learning_rate': 1.9986385138208718e-07, 'epoch': 0.97}
 97%|| 6311/6500 [11:55:42<22:25,  7.12s/it]                                                       97%|| 6311/6500 [11:55:42<22:25,  7.12s/it] 97%|| 6312/6500 [11:55:49<22:08,  7.07s/it]                                                       97%|| 6312/6500 [11:55:49<22:08,  7.07s/it] 97%|| 6313/6500 [11:55:55<21:32,  6.91s/it]                                                       97%|| 6313/6500 [11:55:55<21:32,  6.91s/it] 97%|| 6314/6500 [11:56:02<21:05,  6.80s/it]                                                       97%|| 6314/6500 [11:56:02<21:05,  6.80s/it] 97%|| 6315/6500 [11:56:08<20:44,  6.73s/it]                                                       97%|| 6315/6500 [11:56:08<20:44,  6.7{'loss': 0.3675, 'learning_rate': 1.97710422052233e-07, 'epoch': 0.97}
{'loss': 0.6239, 'learning_rate': 1.9556863370476242e-07, 'epoch': 0.97}
{'loss': 0.374, 'learning_rate': 1.9343848684031384e-07, 'epoch': 0.97}
{'loss': 0.3575, 'learning_rate': 1.9131998195678902e-07, 'epoch': 0.97}
{'loss': 0.3598, 'learning_rate': 1.8921311954937516e-07, 'epoch': 0.97}
3s/it] 97%|| 6316/6500 [11:56:15<20:27,  6.67s/it]                                                       97%|| 6316/6500 [11:56:15<20:27,  6.67s/it] 97%|| 6317/6500 [11:56:21<20:13,  6.63s/it]                                                       97%|| 6317/6500 [11:56:21<20:13,  6.63s/it] 97%|| 6318/6500 [11:56:28<20:01,  6.60s/it]                                                       97%|| 6318/6500 [11:56:28<20:01,  6.60s/it] 97%|| 6319/6500 [11:56:35<20:51,  6.91s/it]                                                       97%|| 6319/6500 [11:56:35<20:51,  6.91s/it] 97%|| 6320/6500 [11:56:42<20:25,  6.81s/it]                                                       97%|| 6320/6500 [11:56:42<20:25,  6.81s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8803585171699524, 'eval_runtime': 1.477, 'eval_samples_per_second': 8.124, 'eval_steps_per_second': 2.031, 'epoch': 0.97}
                                                       97%|| 6320/6500 [11:56:43<20:25,  6.81s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-6320
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-6320I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-6320

I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-6320
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-6320
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-6320/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-6320/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-6320/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-6320/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3507, 'learning_rate': 1.8711790011053942e-07, 'epoch': 0.97}
{'loss': 0.3613, 'learning_rate': 1.8503432413002898e-07, 'epoch': 0.97}
{'loss': 0.3579, 'learning_rate': 1.8296239209486532e-07, 'epoch': 0.97}
{'loss': 0.3695, 'learning_rate': 1.8090210448934443e-07, 'epoch': 0.97}
{'loss': 0.3635, 'learning_rate': 1.788534617950588e-07, 'epoch': 0.97}
 97%|| 6321/6500 [11:56:50<21:45,  7.29s/it]                                                       97%|| 6321/6500 [11:56:50<21:45,  7.29s/it] 97%|| 6322/6500 [11:56:57<20:58,  7.07s/it]                                                       97%|| 6322/6500 [11:56:57<20:58,  7.07s/it] 97%|| 6323/6500 [11:57:04<20:23,  6.91s/it]                                                       97%|| 6323/6500 [11:57:04<20:23,  6.91s/it] 97%|| 6324/6500 [11:57:10<19:57,  6.80s/it]                                                       97%|| 6324/6500 [11:57:10<19:57,  6.80s/it] 97%|| 6325/6500 [11:57:17<19:36,  6.73s/it]                                                       97%|| 6325/6500 [11:57:17<19:36,  6.7{'loss': 0.3659, 'learning_rate': 1.768164644908532e-07, 'epoch': 0.97}
{'loss': 0.3673, 'learning_rate': 1.7479111305287456e-07, 'epoch': 0.97}
{'loss': 0.3612, 'learning_rate': 1.7277740795452747e-07, 'epoch': 0.97}
{'loss': 0.3448, 'learning_rate': 1.7077534966650766e-07, 'epoch': 0.97}
{'loss': 0.4009, 'learning_rate': 1.6878493865678524e-07, 'epoch': 0.97}
3s/it] 97%|| 6326/6500 [11:57:23<19:21,  6.67s/it]                                                       97%|| 6326/6500 [11:57:23<19:21,  6.67s/it] 97%|| 6327/6500 [11:57:30<19:07,  6.63s/it]                                                       97%|| 6327/6500 [11:57:30<19:07,  6.63s/it] 97%|| 6328/6500 [11:57:36<18:56,  6.61s/it]                                                       97%|| 6328/6500 [11:57:36<18:56,  6.61s/it] 97%|| 6329/6500 [11:57:43<18:47,  6.59s/it]                                                       97%|| 6329/6500 [11:57:43<18:47,  6.59s/it] 97%|| 6330/6500 [11:57:49<18:38,  6.58s/it]                                                       97%|| 6330/6500 [11:57:49<18:38,  6.58s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8805974125862122, 'eval_runtime': 1.4775, 'eval_samples_per_second': 8.122, 'eval_steps_per_second': 2.03, 'epoch': 0.97}
                                                       97%|| 6330/6500 [11:57:51<18:38,  6.58s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-6330
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-6330
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-6330
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-6330
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-6330
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-6330/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-6330/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-6330/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-6330/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3623, 'learning_rate': 1.6680617539059918e-07, 'epoch': 0.97}
{'loss': 0.6413, 'learning_rate': 1.64839060330485e-07, 'epoch': 0.97}
{'loss': 0.3706, 'learning_rate': 1.628835939362361e-07, 'epoch': 0.97}
{'loss': 0.3651, 'learning_rate': 1.609397766649312e-07, 'epoch': 0.97}
{'loss': 0.353, 'learning_rate': 1.5900760897092358e-07, 'epoch': 0.97}
 97%|| 6331/6500 [11:57:58<20:02,  7.11s/it]                                                       97%|| 6331/6500 [11:57:58<20:02,  7.11s/it] 97%|| 6332/6500 [11:58:04<19:26,  6.94s/it]                                                       97%|| 6332/6500 [11:58:04<19:26,  6.94s/it] 97%|| 6333/6500 [11:58:11<18:59,  6.82s/it]                                                       97%|| 6333/6500 [11:58:11<18:59,  6.82s/it] 97%|| 6334/6500 [11:58:17<18:38,  6.74s/it]                                                       97%|| 6334/6500 [11:58:17<18:38,  6.74s/it] 97%|| 6335/6500 [11:58:25<19:07,  6.95s/it]                                                       97%|| 6335/6500 [11:58:25<19:07,  6.9{'loss': 0.3561, 'learning_rate': 1.5708709130585753e-07, 'epoch': 0.97}
{'loss': 0.3628, 'learning_rate': 1.5517822411862949e-07, 'epoch': 0.97}
{'loss': 0.3603, 'learning_rate': 1.5328100785542697e-07, 'epoch': 0.98}
{'loss': 0.3737, 'learning_rate': 1.5139544295971753e-07, 'epoch': 0.98}
{'loss': 0.3696, 'learning_rate': 1.49521529872243e-07, 'epoch': 0.98}
5s/it] 97%|| 6336/6500 [11:58:31<18:39,  6.83s/it]                                                       97%|| 6336/6500 [11:58:31<18:39,  6.83s/it] 97%|| 6337/6500 [11:58:38<18:19,  6.74s/it]                                                       97%|| 6337/6500 [11:58:38<18:19,  6.74s/it] 98%|| 6338/6500 [11:58:44<18:02,  6.68s/it]                                                       98%|| 6338/6500 [11:58:44<18:02,  6.68s/it] 98%|| 6339/6500 [11:58:51<17:49,  6.64s/it]                                                       98%|| 6339/6500 [11:58:51<17:49,  6.64s/it] 98%|| 6340/6500 [11:58:58<17:38,  6.61s/it]                                                       98%|| 6340/6500 [11:58:58<17:38,  6.61s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.881288468837738, 'eval_runtime': 1.4799, 'eval_samples_per_second': 8.108, 'eval_steps_per_second': 2.027, 'epoch': 0.98}
                                                       98%|| 6340/6500 [11:58:59<17:38,  6.61s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-6340
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-6340
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-6340
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-6340
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-6340
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-6340/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-6340/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-6340/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-6340/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3666, 'learning_rate': 1.4765926903100856e-07, 'epoch': 0.98}
{'loss': 0.3737, 'learning_rate': 1.4580866087131605e-07, 'epoch': 0.98}
{'loss': 0.359, 'learning_rate': 1.439697058257361e-07, 'epoch': 0.98}
{'loss': 0.3576, 'learning_rate': 1.4214240432410264e-07, 'epoch': 0.98}
{'loss': 0.3904, 'learning_rate': 1.4032675679354067e-07, 'epoch': 0.98}
 98%|| 6341/6500 [11:59:06<18:54,  7.14s/it]                                                       98%|| 6341/6500 [11:59:06<18:54,  7.14s/it] 98%|| 6342/6500 [11:59:12<18:19,  6.96s/it]                                                       98%|| 6342/6500 [11:59:12<18:19,  6.96s/it] 98%|| 6343/6500 [11:59:19<17:52,  6.83s/it]                                                       98%|| 6343/6500 [11:59:19<17:52,  6.83s/it] 98%|| 6344/6500 [11:59:25<17:31,  6.74s/it]                                                       98%|| 6344/6500 [11:59:25<17:31,  6.74s/it] 98%|| 6345/6500 [11:59:32<17:15,  6.68s/it]                                                       98%|| 6345/6500 [11:59:32<17:15,  6.6{'loss': 0.3666, 'learning_rate': 1.3852276365844407e-07, 'epoch': 0.98}
{'loss': 0.6417, 'learning_rate': 1.367304253404922e-07, 'epoch': 0.98}
{'loss': 0.3587, 'learning_rate': 1.3494974225863322e-07, 'epoch': 0.98}
{'loss': 0.3732, 'learning_rate': 1.3318071482908424e-07, 'epoch': 0.98}
{'loss': 0.3601, 'learning_rate': 1.314233434653478e-07, 'epoch': 0.98}
8s/it] 98%|| 6346/6500 [11:59:39<17:02,  6.64s/it]                                                       98%|| 6346/6500 [11:59:39<17:02,  6.64s/it] 98%|| 6347/6500 [11:59:45<16:50,  6.60s/it]                                                       98%|| 6347/6500 [11:59:45<16:50,  6.60s/it] 98%|| 6348/6500 [11:59:52<16:40,  6.58s/it]                                                       98%|| 6348/6500 [11:59:52<16:40,  6.58s/it] 98%|| 6349/6500 [11:59:58<16:31,  6.57s/it]                                                       98%|| 6349/6500 [11:59:58<16:31,  6.57s/it] 98%|| 6350/6500 [12:00:05<16:23,  6.56s/it]                                                       98%|| 6350/6500 [12:00:05<16:23,  6.56s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8807415962219238, 'eval_runtime': 1.503, 'eval_samples_per_second': 7.984, 'eval_steps_per_second': 1.996, 'epoch': 0.98}
                                                       98%|| 6350/6500 [12:00:06<16:23,  6.56s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-6350
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in I AM HERE AND I AM SAVING THE MODEL
 the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-6350
/projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-6350
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-6350
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-6350
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-6350/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-6350/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-6350/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-6350/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3627, 'learning_rate': 1.2967762857820087e-07, 'epoch': 0.98}
{'loss': 0.3541, 'learning_rate': 1.2794357057568928e-07, 'epoch': 0.98}
{'loss': 0.3719, 'learning_rate': 1.262211698631388e-07, 'epoch': 0.98}
{'loss': 0.3832, 'learning_rate': 1.2451042684316071e-07, 'epoch': 0.98}
{'loss': 0.3694, 'learning_rate': 1.228113419156185e-07, 'epoch': 0.98}
 98%|| 6351/6500 [12:00:13<17:40,  7.12s/it]                                                       98%|| 6351/6500 [12:00:13<17:40,  7.12s/it] 98%|| 6352/6500 [12:00:20<17:35,  7.13s/it]                                                       98%|| 6352/6500 [12:00:20<17:35,  7.13s/it] 98%|| 6353/6500 [12:00:27<17:02,  6.96s/it]                                                       98%|| 6353/6500 [12:00:27<17:02,  6.96s/it] 98%|| 6354/6500 [12:00:33<16:37,  6.83s/it]                                                       98%|| 6354/6500 [12:00:33<16:37,  6.83s/it] 98%|| 6355/6500 [12:00:40<16:19,  6.75s/it]                                                       98%|| 6355/6500 [12:00:40<16:19,  6.7{'loss': 0.3633, 'learning_rate': 1.211239154776611e-07, 'epoch': 0.98}
{'loss': 0.3763, 'learning_rate': 1.1944814792372305e-07, 'epoch': 0.98}
{'loss': 0.3495, 'learning_rate': 1.1778403964550766e-07, 'epoch': 0.98}
{'loss': 0.3695, 'learning_rate': 1.1613159103197602e-07, 'epoch': 0.98}
{'loss': 0.382, 'learning_rate': 1.1449080246939137e-07, 'epoch': 0.98}
5s/it] 98%|| 6356/6500 [12:00:46<16:03,  6.69s/it]                                                       98%|| 6356/6500 [12:00:46<16:03,  6.69s/it] 98%|| 6357/6500 [12:00:53<15:50,  6.65s/it]                                                       98%|| 6357/6500 [12:00:53<15:50,  6.65s/it] 98%|| 6358/6500 [12:01:00<15:40,  6.62s/it]                                                       98%|| 6358/6500 [12:01:00<15:40,  6.62s/it] 98%|| 6359/6500 [12:01:06<15:31,  6.61s/it]                                                       98%|| 6359/6500 [12:01:06<15:31,  6.61s/it] 98%|| 6360/6500 [12:01:13<15:22,  6.59s/it]                                                       98%|| 6360/6500 [12:01:13<15:22,  6.59s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8816549777984619, 'eval_runtime': 1.4807, 'eval_samples_per_second': 8.104, 'eval_steps_per_second': 2.026, 'epoch': 0.98}
                                                       98%|| 6360/6500 [12:01:14<15:22,  6.59s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-6360
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-6360
/projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-6360
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-6360
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-6360
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-6360/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-6360/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-6360/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-6360/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3714, 'learning_rate': 1.1286167434126915e-07, 'epoch': 0.98}
{'loss': 0.6366, 'learning_rate': 1.1124420702841587e-07, 'epoch': 0.98}
{'loss': 0.3613, 'learning_rate': 1.0963840090889576e-07, 'epoch': 0.98}
{'loss': 0.3762, 'learning_rate': 1.0804425635806415e-07, 'epoch': 0.98}
{'loss': 0.3472, 'learning_rate': 1.064617737485396e-07, 'epoch': 0.98}
 98%|| 6361/6500 [12:01:21<16:29,  7.12s/it]                                                       98%|| 6361/6500 [12:01:21<16:29,  7.12s/it] 98%|| 6362/6500 [12:01:28<15:58,  6.95s/it]                                                       98%|| 6362/6500 [12:01:28<15:58,  6.95s/it] 98%|| 6363/6500 [12:01:34<15:35,  6.83s/it]                                                       98%|| 6363/6500 [12:01:34<15:35,  6.83s/it] 98%|| 6364/6500 [12:01:41<15:16,  6.74s/it]                                                       98%|| 6364/6500 [12:01:41<15:16,  6.74s/it] 98%|| 6365/6500 [12:01:47<15:02,  6.69s/it]                                                       98%|| 6365/6500 [12:01:47<15:02,  6.6{'loss': 0.3517, 'learning_rate': 1.0489095345021516e-07, 'epoch': 0.98}
{'loss': 0.3643, 'learning_rate': 1.033317958302693e-07, 'epoch': 0.98}
{'loss': 0.3637, 'learning_rate': 1.0178430125313277e-07, 'epoch': 0.98}
{'loss': 0.3711, 'learning_rate': 1.0024847008053284e-07, 'epoch': 0.98}
{'loss': 0.3729, 'learning_rate': 9.87243026714546e-08, 'epoch': 0.98}
9s/it] 98%|| 6366/6500 [12:01:54<14:50,  6.65s/it]                                                       98%|| 6366/6500 [12:01:54<14:50,  6.65s/it] 98%|| 6367/6500 [12:02:00<14:39,  6.61s/it]                                                       98%|| 6367/6500 [12:02:00<14:39,  6.61s/it] 98%|| 6368/6500 [12:02:08<15:05,  6.86s/it]                                                       98%|| 6368/6500 [12:02:08<15:05,  6.86s/it] 98%|| 6369/6500 [12:02:14<14:46,  6.77s/it]                                                       98%|| 6369/6500 [12:02:14<14:46,  6.77s/it] 98%|| 6370/6500 [12:02:21<14:32,  6.71s/it]                                                       98%|| 6370/6500 [12:02:21<14:32,  6.71s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8807868361473083, 'eval_runtime': 1.4794, 'eval_samples_per_second': 8.111, 'eval_steps_per_second': 2.028, 'epoch': 0.98}
                                                       98%|| 6370/6500 [12:02:22<14:32,  6.71s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-6370
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-6370
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-6370
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-6370
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-6370
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-6370/pytorch_model.bin
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-6370/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-6370/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-6370/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.354, 'learning_rate': 9.721179938216862e-08, 'epoch': 0.98}
{'loss': 0.3667, 'learning_rate': 9.571096056620876e-08, 'epoch': 0.98}
{'loss': 0.3466, 'learning_rate': 9.42217865743944e-08, 'epoch': 0.98}
{'loss': 0.3969, 'learning_rate': 9.27442777547971e-08, 'epoch': 0.98}
{'loss': 0.3514, 'learning_rate': 9.127843445279061e-08, 'epoch': 0.98}
 98%|| 6371/6500 [12:02:29<15:28,  7.20s/it]                                                       98%|| 6371/6500 [12:02:29<15:28,  7.20s/it] 98%|| 6372/6500 [12:02:36<14:56,  7.01s/it]                                                       98%|| 6372/6500 [12:02:36<14:56,  7.01s/it] 98%|| 6373/6500 [12:02:42<14:32,  6.87s/it]                                                       98%|| 6373/6500 [12:02:42<14:32,  6.87s/it] 98%|| 6374/6500 [12:02:49<14:13,  6.77s/it]                                                       98%|| 6374/6500 [12:02:49<14:13,  6.77s/it] 98%|| 6375/6500 [12:02:55<13:57,  6.70s/it]                                                       98%|| 6375/6500 [12:02:55<13:57,  6.7{'loss': 0.6362, 'learning_rate': 8.982425701099529e-08, 'epoch': 0.98}
{'loss': 0.3711, 'learning_rate': 8.838174576932256e-08, 'epoch': 0.98}
{'loss': 0.3585, 'learning_rate': 8.695090106495274e-08, 'epoch': 0.98}
{'loss': 0.3697, 'learning_rate': 8.553172323232939e-08, 'epoch': 0.98}
{'loss': 0.3509, 'learning_rate': 8.412421260318159e-08, 'epoch': 0.98}
0s/it] 98%|| 6376/6500 [12:03:02<13:45,  6.65s/it]                                                       98%|| 6376/6500 [12:03:02<13:45,  6.65s/it] 98%|| 6377/6500 [12:03:09<13:34,  6.63s/it]                                                       98%|| 6377/6500 [12:03:09<13:34,  6.63s/it] 98%|| 6378/6500 [12:03:15<13:25,  6.60s/it]                                                       98%|| 6378/6500 [12:03:15<13:25,  6.60s/it] 98%|| 6379/6500 [12:03:22<13:16,  6.59s/it]                                                       98%|| 6379/6500 [12:03:22<13:16,  6.59s/it] 98%|| 6380/6500 [12:03:28<13:09,  6.58s/it]                                                       98%|| 6380/6500 [12:03:28<13:09,  6.58s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.880379855632782, 'eval_runtime': 1.4805, 'eval_samples_per_second': 8.105, 'eval_steps_per_second': 2.026, 'epoch': 0.98}
                                                       98%|| 6380/6500 [12:03:30<13:09,  6.58s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-6380
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-6380
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-6380
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-6380
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-6380
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-6380/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-6380/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-6380/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-6380/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3583, 'learning_rate': 8.27283695065073e-08, 'epoch': 0.98}
{'loss': 0.3592, 'learning_rate': 8.134419426856776e-08, 'epoch': 0.98}
{'loss': 0.3605, 'learning_rate': 7.997168721292082e-08, 'epoch': 0.98}
{'loss': 0.368, 'learning_rate': 7.861084866037094e-08, 'epoch': 0.98}
{'loss': 0.3734, 'learning_rate': 7.726167892900815e-08, 'epoch': 0.98}
 98%|| 6381/6500 [12:03:37<14:06,  7.12s/it]                                                       98%|| 6381/6500 [12:03:37<14:06,  7.12s/it] 98%|| 6382/6500 [12:03:43<13:39,  6.95s/it]                                                       98%|| 6382/6500 [12:03:43<13:39,  6.95s/it] 98%|| 6383/6500 [12:03:50<13:19,  6.83s/it]                                                       98%|| 6383/6500 [12:03:50<13:19,  6.83s/it] 98%|| 6384/6500 [12:03:57<13:40,  7.07s/it]                                                       98%|| 6384/6500 [12:03:57<13:40,  7.07s/it] 98%|| 6385/6500 [12:04:04<13:16,  6.92s/it]                                                       98%|| 6385/6500 [12:04:04<13:16,  6.9{'loss': 0.3563, 'learning_rate': 7.59241783341913e-08, 'epoch': 0.98}
{'loss': 0.3667, 'learning_rate': 7.459834718855363e-08, 'epoch': 0.98}
{'loss': 0.3476, 'learning_rate': 7.32841858020028e-08, 'epoch': 0.98}
{'loss': 0.4017, 'learning_rate': 7.198169448170977e-08, 'epoch': 0.98}
{'loss': 0.3563, 'learning_rate': 7.0690873532131e-08, 'epoch': 0.98}
2s/it] 98%|| 6386/6500 [12:04:12<13:42,  7.21s/it]                                                       98%|| 6386/6500 [12:04:12<13:42,  7.21s/it] 98%|| 6387/6500 [12:04:18<13:13,  7.02s/it]                                                       98%|| 6387/6500 [12:04:18<13:13,  7.02s/it] 98%|| 6388/6500 [12:04:25<12:51,  6.89s/it]                                                       98%|| 6388/6500 [12:04:25<12:51,  6.89s/it] 98%|| 6389/6500 [12:04:31<12:33,  6.79s/it]                                                       98%|| 6389/6500 [12:04:31<12:33,  6.79s/it] 98%|| 6390/6500 [12:04:38<12:18,  6.72s/it]                                                       98%|| 6390/6500 [12:04:38<12:18,  6.72s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8822057843208313, 'eval_runtime': 1.493, 'eval_samples_per_second': 8.037, 'eval_steps_per_second': 2.009, 'epoch': 0.98}
                                                       98%|| 6390/6500 [12:04:40<12:18,  6.72s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-6390
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-6390
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-6390
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-6390
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-6390
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-6390/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-6390/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-6390/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-6390/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.6367, 'learning_rate': 6.941172325499179e-08, 'epoch': 0.98}
{'loss': 0.3748, 'learning_rate': 6.814424394926966e-08, 'epoch': 0.98}
{'loss': 0.3586, 'learning_rate': 6.688843591124428e-08, 'epoch': 0.98}
{'loss': 0.3714, 'learning_rate': 6.56442994344475e-08, 'epoch': 0.98}
{'loss': 0.3534, 'learning_rate': 6.441183480969116e-08, 'epoch': 0.98}
 98%|| 6391/6500 [12:04:46<13:08,  7.23s/it]                                                       98%|| 6391/6500 [12:04:46<13:08,  7.23s/it] 98%|| 6392/6500 [12:04:53<12:39,  7.03s/it]                                                       98%|| 6392/6500 [12:04:53<12:39,  7.03s/it] 98%|| 6393/6500 [12:05:00<12:16,  6.88s/it]                                                       98%|| 6393/6500 [12:05:00<12:16,  6.88s/it] 98%|| 6394/6500 [12:05:06<11:59,  6.79s/it]                                                       98%|| 6394/6500 [12:05:06<11:59,  6.79s/it] 98%|| 6395/6500 [12:05:13<11:45,  6.71s/it]                                                       98%|| 6395/6500 [12:05:13<11:45,  6.7{'loss': 0.3649, 'learning_rate': 6.31910423250559e-08, 'epoch': 0.98}
{'loss': 0.3594, 'learning_rate': 6.198192226589682e-08, 'epoch': 0.98}
{'loss': 0.368, 'learning_rate': 6.078447491482675e-08, 'epoch': 0.98}
{'loss': 0.3653, 'learning_rate': 5.959870055175509e-08, 'epoch': 0.98}
{'loss': 0.3662, 'learning_rate': 5.8424599453843486e-08, 'epoch': 0.98}
1s/it] 98%|| 6396/6500 [12:05:19<11:33,  6.67s/it]                                                       98%|| 6396/6500 [12:05:19<11:33,  6.67s/it] 98%|| 6397/6500 [12:05:26<11:23,  6.63s/it]                                                       98%|| 6397/6500 [12:05:26<11:23,  6.63s/it] 98%|| 6398/6500 [12:05:32<11:14,  6.61s/it]                                                       98%|| 6398/6500 [12:05:32<11:14,  6.61s/it] 98%|| 6399/6500 [12:05:39<11:05,  6.59s/it]                                                       98%|| 6399/6500 [12:05:39<11:05,  6.59s/it] 98%|| 6400/6500 [12:05:46<11:26,  6.87s/it]                                                       98%|| 6400/6500 [12:05:46<11:26,  6.87s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8816479444503784, 'eval_runtime': 1.4891, 'eval_samples_per_second': 8.058, 'eval_steps_per_second': 2.015, 'epoch': 0.98}
                                                       98%|| 6400/6500 [12:05:48<11:26,  6.87s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-6400
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-6400
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-6400
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-6400
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-6400
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-6400/pytorch_model.bin 
/projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-6400/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-6400/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-6400/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3584, 'learning_rate': 5.726217189553351e-08, 'epoch': 0.98}
{'loss': 0.3576, 'learning_rate': 5.61114181485356e-08, 'epoch': 0.98}
{'loss': 0.3491, 'learning_rate': 5.497233848182348e-08, 'epoch': 0.99}
{'loss': 0.3976, 'learning_rate': 5.384493316166195e-08, 'epoch': 0.99}
{'loss': 0.3616, 'learning_rate': 5.272920245156798e-08, 'epoch': 0.99}
 98%|| 6401/6500 [12:05:55<12:04,  7.32s/it]                                                       98%|| 6401/6500 [12:05:55<12:04,  7.32s/it] 98%|| 6402/6500 [12:06:01<11:34,  7.09s/it]                                                       98%|| 6402/6500 [12:06:01<11:34,  7.09s/it] 99%|| 6403/6500 [12:06:08<11:11,  6.92s/it]                                                       99%|| 6403/6500 [12:06:08<11:11,  6.92s/it] 99%|| 6404/6500 [12:06:14<10:53,  6.81s/it]                                                       99%|| 6404/6500 [12:06:14<10:53,  6.81s/it] 99%|| 6405/6500 [12:06:21<10:39,  6.73s/it]                                                       99%|| 6405/6500 [12:06:21<10:39,  6.7{'loss': 0.6316, 'learning_rate': 5.162514661233853e-08, 'epoch': 0.99}
{'loss': 0.3768, 'learning_rate': 5.053276590203937e-08, 'epoch': 0.99}
{'loss': 0.3597, 'learning_rate': 4.9452060576010705e-08, 'epoch': 0.99}
{'loss': 0.3596, 'learning_rate': 4.838303088685603e-08, 'epoch': 0.99}
{'loss': 0.366, 'learning_rate': 4.732567708445878e-08, 'epoch': 0.99}
3s/it] 99%|| 6406/6500 [12:06:27<10:26,  6.67s/it]                                                       99%|| 6406/6500 [12:06:27<10:26,  6.67s/it] 99%|| 6407/6500 [12:06:34<10:16,  6.63s/it]                                                       99%|| 6407/6500 [12:06:34<10:16,  6.63s/it] 99%|| 6408/6500 [12:06:41<10:08,  6.61s/it]                                                       99%|| 6408/6500 [12:06:41<10:08,  6.61s/it] 99%|| 6409/6500 [12:06:47<09:59,  6.59s/it]                                                       99%|| 6409/6500 [12:06:47<09:59,  6.59s/it] 99%|| 6410/6500 [12:06:54<09:51,  6.57s/it]                                                       99%|| 6410/6500 [12:06:54<09:51,  6.57s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.880715012550354, 'eval_runtime': 1.5396, 'eval_samples_per_second': 7.794, 'eval_steps_per_second': 1.949, 'epoch': 0.99}
                                                       99%|| 6410/6500 [12:06:55<09:51,  6.57s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-6410
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  I AM HERE AND I AM SAVING THE MODEL/projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-6410
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-6410

I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-6410
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-6410
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-6410/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-6410/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-6410/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-6410/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3615, 'learning_rate': 4.627999941596572e-08, 'epoch': 0.99}
{'loss': 0.363, 'learning_rate': 4.5245998125798e-08, 'epoch': 0.99}
{'loss': 0.3749, 'learning_rate': 4.422367345565115e-08, 'epoch': 0.99}
{'loss': 0.3739, 'learning_rate': 4.321302564448404e-08, 'epoch': 0.99}
{'loss': 0.3615, 'learning_rate': 4.2214054928529925e-08, 'epoch': 0.99}
 99%|| 6411/6500 [12:07:02<10:35,  7.14s/it]                                                       99%|| 6411/6500 [12:07:02<10:35,  7.14s/it] 99%|| 6412/6500 [12:07:09<10:11,  6.95s/it]                                                       99%|| 6412/6500 [12:07:09<10:11,  6.95s/it] 99%|| 6413/6500 [12:07:15<09:54,  6.83s/it]                                                       99%|| 6413/6500 [12:07:15<09:54,  6.83s/it] 99%|| 6414/6500 [12:07:22<09:39,  6.74s/it]                                                       99%|| 6414/6500 [12:07:22<09:39,  6.74s/it] 99%|| 6415/6500 [12:07:28<09:27,  6.68s/it]                                                       99%|| 6415/6500 [12:07:28<09:27,  6.6{'loss': 0.3696, 'learning_rate': 4.12267615412909e-08, 'epoch': 0.99}
{'loss': 0.3536, 'learning_rate': 4.025114571354349e-08, 'epoch': 0.99}
{'loss': 0.3512, 'learning_rate': 3.928720767333305e-08, 'epoch': 0.99}
{'loss': 0.3918, 'learning_rate': 3.8334947645968235e-08, 'epoch': 0.99}
{'loss': 0.3667, 'learning_rate': 3.739436585403766e-08, 'epoch': 0.99}
8s/it] 99%|| 6416/6500 [12:07:35<09:33,  6.83s/it]                                                       99%|| 6416/6500 [12:07:35<09:33,  6.83s/it] 99%|| 6417/6500 [12:07:42<09:19,  6.74s/it]                                                       99%|| 6417/6500 [12:07:42<09:19,  6.74s/it] 99%|| 6418/6500 [12:07:48<09:07,  6.68s/it]                                                       99%|| 6418/6500 [12:07:48<09:07,  6.68s/it] 99%|| 6419/6500 [12:07:55<08:57,  6.64s/it]                                                       99%|| 6419/6500 [12:07:55<08:57,  6.64s/it] 99%|| 6420/6500 [12:08:02<08:49,  6.62s/it]                                                       99%|| 6420/6500 [12:08:02<08:49,  6.62s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8809805512428284, 'eval_runtime': 1.7787, 'eval_samples_per_second': 6.747, 'eval_steps_per_second': 1.687, 'epoch': 0.99}
                                                       99%|| 6420/6500 [12:08:03<08:49,  6.62s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-6420
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-6420
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-6420
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-6420
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-6420
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-6420/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-6420/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-6420/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-6420/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.6345, 'learning_rate': 3.64654625173988e-08, 'epoch': 0.99}
{'loss': 0.3679, 'learning_rate': 3.554823785317241e-08, 'epoch': 0.99}
{'loss': 0.3657, 'learning_rate': 3.464269207575366e-08, 'epoch': 0.99}
{'loss': 0.3523, 'learning_rate': 3.374882539681767e-08, 'epoch': 0.99}
{'loss': 0.3531, 'learning_rate': 3.2866638025286226e-08, 'epoch': 0.99}
 99%|| 6421/6500 [12:08:10<09:31,  7.24s/it]                                                       99%|| 6421/6500 [12:08:10<09:31,  7.24s/it] 99%|| 6422/6500 [12:08:17<09:08,  7.03s/it]                                                       99%|| 6422/6500 [12:08:17<09:08,  7.03s/it] 99%|| 6423/6500 [12:08:23<08:50,  6.89s/it]                                                       99%|| 6423/6500 [12:08:23<08:50,  6.89s/it] 99%|| 6424/6500 [12:08:30<08:35,  6.78s/it]                                                       99%|| 6424/6500 [12:08:30<08:35,  6.78s/it] 99%|| 6425/6500 [12:08:36<08:23,  6.71s/it]                                                       99%|| 6425/6500 [12:08:36<08:23,  6.7{'loss': 0.3591, 'learning_rate': 3.199613016737768e-08, 'epoch': 0.99}
{'loss': 0.3597, 'learning_rate': 3.113730202656262e-08, 'epoch': 0.99}
{'loss': 0.3692, 'learning_rate': 3.029015380359157e-08, 'epoch': 0.99}
{'loss': 0.3647, 'learning_rate': 2.9454685696472805e-08, 'epoch': 0.99}
{'loss': 0.3577, 'learning_rate': 2.8630897900494557e-08, 'epoch': 0.99}
1s/it] 99%|| 6426/6500 [12:08:43<08:13,  6.66s/it]                                                       99%|| 6426/6500 [12:08:43<08:13,  6.66s/it] 99%|| 6427/6500 [12:08:50<08:03,  6.63s/it]                                                       99%|| 6427/6500 [12:08:50<08:03,  6.63s/it] 99%|| 6428/6500 [12:08:56<07:55,  6.60s/it]                                                       99%|| 6428/6500 [12:08:56<07:55,  6.60s/it] 99%|| 6429/6500 [12:09:03<07:47,  6.58s/it]                                                       99%|| 6429/6500 [12:09:03<07:47,  6.58s/it] 99%|| 6430/6500 [12:09:09<07:39,  6.57s/it]                                                       99%|| 6430/6500 [12:09:09<07:39,  6.57s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8808714151382446, 'eval_runtime': 1.4761, 'eval_samples_per_second': 8.129, 'eval_steps_per_second': 2.032, 'epoch': 0.99}
                                                       99%|| 6430/6500 [12:09:11<07:39,  6.57s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-6430
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-6430
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-6430
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-6430
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-6430
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-6430/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-6430/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-6430/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-6430/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.377, 'learning_rate': 2.781879060821391e-08, 'epoch': 0.99}
{'loss': 0.3573, 'learning_rate': 2.7018364009451234e-08, 'epoch': 0.99}
{'loss': 0.3653, 'learning_rate': 2.622961829131243e-08, 'epoch': 0.99}
{'loss': 0.3865, 'learning_rate': 2.5452553638150023e-08, 'epoch': 0.99}
{'loss': 0.3659, 'learning_rate': 2.4687170231602054e-08, 'epoch': 0.99}
 99%|| 6431/6500 [12:09:18<08:11,  7.12s/it]                                                       99%|| 6431/6500 [12:09:18<08:11,  7.12s/it] 99%|| 6432/6500 [12:09:25<08:05,  7.14s/it]                                                       99%|| 6432/6500 [12:09:25<08:05,  7.14s/it] 99%|| 6433/6500 [12:09:31<07:46,  6.96s/it]                                                       99%|| 6433/6500 [12:09:31<07:46,  6.96s/it] 99%|| 6434/6500 [12:09:38<07:31,  6.84s/it]                                                       99%|| 6434/6500 [12:09:38<07:31,  6.84s/it] 99%|| 6435/6500 [12:09:44<07:18,  6.75s/it]                                                       99%|| 6435/6500 [12:09:44<07:18,  6.7{'loss': 0.6409, 'learning_rate': 2.3933468250575408e-08, 'epoch': 0.99}
{'loss': 0.3578, 'learning_rate': 2.319144787124583e-08, 'epoch': 0.99}
{'loss': 0.3724, 'learning_rate': 2.246110926704681e-08, 'epoch': 0.99}
{'loss': 0.3521, 'learning_rate': 2.1742452608691788e-08, 'epoch': 0.99}
{'loss': 0.3604, 'learning_rate': 2.1035478064168612e-08, 'epoch': 0.99}
5s/it] 99%|| 6436/6500 [12:09:51<07:08,  6.69s/it]                                                       99%|| 6436/6500 [12:09:51<07:08,  6.69s/it] 99%|| 6437/6500 [12:09:57<06:58,  6.65s/it]                                                       99%|| 6437/6500 [12:09:57<06:58,  6.65s/it] 99%|| 6438/6500 [12:10:04<06:50,  6.61s/it]                                                       99%|| 6438/6500 [12:10:04<06:50,  6.61s/it] 99%|| 6439/6500 [12:10:11<06:42,  6.59s/it]                                                       99%|| 6439/6500 [12:10:11<06:42,  6.59s/it] 99%|| 6440/6500 [12:10:17<06:35,  6.59s/it]                                                       99%|| 6440/6500 [12:10:17<06:35,  6.59s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8811412453651428, 'eval_runtime': 1.4751, 'eval_samples_per_second': 8.135, 'eval_steps_per_second': 2.034, 'epoch': 0.99}
                                                       99%|| 6440/6500 [12:10:19<06:35,  6.59s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-6440
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-6440
 /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-6440
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-6440
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-6440
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-6440/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-6440/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-6440/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-6440/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.363, 'learning_rate': 2.0340185798728427e-08, 'epoch': 0.99}
{'loss': 0.3687, 'learning_rate': 1.9656575974885684e-08, 'epoch': 0.99}
{'loss': 0.3733, 'learning_rate': 1.8984648752429225e-08, 'epoch': 0.99}
{'loss': 0.3754, 'learning_rate': 1.8324404288427853e-08, 'epoch': 0.99}
{'loss': 0.3635, 'learning_rate': 1.7675842737197025e-08, 'epoch': 0.99}
 99%|| 6441/6500 [12:10:26<07:01,  7.14s/it]                                                       99%|| 6441/6500 [12:10:26<07:01,  7.14s/it] 99%|| 6442/6500 [12:10:32<06:43,  6.96s/it]                                                       99%|| 6442/6500 [12:10:32<06:43,  6.96s/it] 99%|| 6443/6500 [12:10:39<06:29,  6.84s/it]                                                       99%|| 6443/6500 [12:10:39<06:29,  6.84s/it] 99%|| 6444/6500 [12:10:45<06:17,  6.75s/it]                                                       99%|| 6444/6500 [12:10:45<06:17,  6.75s/it] 99%|| 6445/6500 [12:10:52<06:07,  6.69s/it]                                                       99%|| 6445/6500 [12:10:52<06:07,  6.6{'loss': 0.3679, 'learning_rate': 1.7038964250343238e-08, 'epoch': 0.99}
{'loss': 0.3515, 'learning_rate': 1.6413768976730747e-08, 'epoch': 0.99}
{'loss': 0.3879, 'learning_rate': 1.5800257062498215e-08, 'epoch': 0.99}
{'loss': 0.3557, 'learning_rate': 1.519842865104204e-08, 'epoch': 0.99}
{'loss': 0.6241, 'learning_rate': 1.4608283883044138e-08, 'epoch': 0.99}
9s/it] 99%|| 6446/6500 [12:10:58<05:58,  6.64s/it]                                                       99%|| 6446/6500 [12:10:58<05:58,  6.64s/it] 99%|| 6447/6500 [12:11:05<05:50,  6.61s/it]                                                       99%|| 6447/6500 [12:11:05<05:50,  6.61s/it] 99%|| 6448/6500 [12:11:12<05:58,  6.90s/it]                                                       99%|| 6448/6500 [12:11:12<05:58,  6.90s/it] 99%|| 6449/6500 [12:11:19<05:46,  6.80s/it]                                                       99%|| 6449/6500 [12:11:19<05:46,  6.80s/it] 99%|| 6450/6500 [12:11:25<05:35,  6.72s/it]                                                       99%|| 6450/6500 [12:11:25<05:35,  6.72s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8812179565429688, 'eval_runtime': 1.4697, 'eval_samples_per_second': 8.165, 'eval_steps_per_second': 2.041, 'epoch': 0.99}
                                                       99%|| 6450/6500 [12:11:27<05:35,  6.72s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-6450
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-6450I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-6450

I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-6450
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-6450
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-6450/pytorch_model.bin
the pytorch model path isthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-6450/pytorch_model.bin
 /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-6450/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-6450/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3915, 'learning_rate': 1.4029822896438616e-08, 'epoch': 0.99}
{'loss': 0.3571, 'learning_rate': 1.3463045826445086e-08, 'epoch': 0.99}
{'loss': 0.375, 'learning_rate': 1.2907952805540913e-08, 'epoch': 0.99}
{'loss': 0.3487, 'learning_rate': 1.2364543963477859e-08, 'epoch': 0.99}
{'loss': 0.3552, 'learning_rate': 1.1832819427270991e-08, 'epoch': 0.99}
 99%|| 6451/6500 [12:11:34<05:53,  7.21s/it]                                                       99%|| 6451/6500 [12:11:34<05:53,  7.21s/it] 99%|| 6452/6500 [12:11:40<05:36,  7.01s/it]                                                       99%|| 6452/6500 [12:11:40<05:36,  7.01s/it] 99%|| 6453/6500 [12:11:47<05:22,  6.87s/it]                                                       99%|| 6453/6500 [12:11:47<05:22,  6.87s/it] 99%|| 6454/6500 [12:11:53<05:11,  6.77s/it]                                                       99%|| 6454/6500 [12:11:53<05:11,  6.77s/it] 99%|| 6455/6500 [12:12:00<05:01,  6.70s/it]                                                       99%|| 6455/6500 [12:12:00<05:01,  6.7{'loss': 0.3585, 'learning_rate': 1.1312779321209777e-08, 'epoch': 0.99}
{'loss': 0.3637, 'learning_rate': 1.0804423766852533e-08, 'epoch': 0.99}
{'loss': 0.3714, 'learning_rate': 1.0307752883020883e-08, 'epoch': 0.99}
{'loss': 0.3688, 'learning_rate': 9.822766785805293e-09, 'epoch': 0.99}
{'loss': 0.3537, 'learning_rate': 9.34946558857619e-09, 'epoch': 0.99}
0s/it] 99%|| 6456/6500 [12:12:07<04:52,  6.65s/it]                                                       99%|| 6456/6500 [12:12:07<04:52,  6.65s/it] 99%|| 6457/6500 [12:12:13<04:44,  6.62s/it]                                                       99%|| 6457/6500 [12:12:13<04:44,  6.62s/it] 99%|| 6458/6500 [12:12:20<04:36,  6.59s/it]                                                       99%|| 6458/6500 [12:12:20<04:36,  6.59s/it] 99%|| 6459/6500 [12:12:26<04:29,  6.58s/it]                                                       99%|| 6459/6500 [12:12:26<04:29,  6.58s/it] 99%|| 6460/6500 [12:12:33<04:22,  6.57s/it]                                                       99%|| 6460/6500 [12:12:33<04:22,  6.57s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8803697824478149, 'eval_runtime': 1.7226, 'eval_samples_per_second': 6.966, 'eval_steps_per_second': 1.742, 'epoch': 0.99}
                                                       99%|| 6460/6500 [12:12:34<04:22,  6.57s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-6460
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-6460
/projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-6460
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-6460
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-6460
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-6460/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-6460/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-6460/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-6460/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.366, 'learning_rate': 8.887849401961745e-09, 'epoch': 0.99}
{'loss': 0.3463, 'learning_rate': 8.437918333864536e-09, 'epoch': 0.99}
{'loss': 0.3947, 'learning_rate': 7.999672489444888e-09, 'epoch': 0.99}
{'loss': 0.3555, 'learning_rate': 7.573111971148627e-09, 'epoch': 0.99}
{'loss': 0.6345, 'learning_rate': 7.1582368786737855e-09, 'epoch': 0.99}
 99%|| 6461/6500 [12:12:41<04:40,  7.18s/it]                                                       99%|| 6461/6500 [12:12:41<04:40,  7.18s/it] 99%|| 6462/6500 [12:12:48<04:25,  6.99s/it]                                                       99%|| 6462/6500 [12:12:48<04:25,  6.99s/it] 99%|| 6463/6500 [12:12:54<04:13,  6.85s/it]                                                       99%|| 6463/6500 [12:12:54<04:13,  6.85s/it] 99%|| 6464/6500 [12:13:01<04:03,  6.76s/it]                                                       99%|| 6464/6500 [12:13:01<04:03,  6.76s/it] 99%|| 6465/6500 [12:13:08<04:04,  6.97s/it]                                                       99%|| 6465/6500 [12:13:08<04:04,  6.9{'loss': 0.3788, 'learning_rate': 6.7550473090038925e-09, 'epoch': 0.99}
{'loss': 0.3536, 'learning_rate': 6.36354335638023e-09, 'epoch': 0.99}
{'loss': 0.3654, 'learning_rate': 5.983725112307381e-09, 'epoch': 1.0}
{'loss': 0.3501, 'learning_rate': 5.61559266556988e-09, 'epoch': 1.0}
{'loss': 0.3547, 'learning_rate': 5.259146102221113e-09, 'epoch': 1.0}
7s/it] 99%|| 6466/6500 [12:13:15<03:52,  6.85s/it]                                                       99%|| 6466/6500 [12:13:15<03:52,  6.85s/it] 99%|| 6467/6500 [12:13:21<03:42,  6.75s/it]                                                       99%|| 6467/6500 [12:13:21<03:42,  6.75s/it]100%|| 6468/6500 [12:13:28<03:33,  6.68s/it]                                                      100%|| 6468/6500 [12:13:28<03:33,  6.68s/it]100%|| 6469/6500 [12:13:35<03:26,  6.65s/it]                                                      100%|| 6469/6500 [12:13:35<03:26,  6.65s/it]100%|| 6470/6500 [12:13:41<03:18,  6.62s/it]                                                      100%|| 6470/6500 [12:13:41<03:18,  6.62s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.881497859954834, 'eval_runtime': 1.4794, 'eval_samples_per_second': 8.112, 'eval_steps_per_second': 2.028, 'epoch': 1.0}
                                                      100%|| 6470/6500 [12:13:43<03:18,  6.62s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-6470
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-6470
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-6470
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-6470
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-6470
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-6470/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-6470/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-6470/pytorch_model.binthe pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-6470/pytorch_model.bin

/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3661, 'learning_rate': 4.914385505572217e-09, 'epoch': 1.0}
{'loss': 0.3603, 'learning_rate': 4.581310956208729e-09, 'epoch': 1.0}
{'loss': 0.3639, 'learning_rate': 4.2599225319850385e-09, 'epoch': 1.0}
{'loss': 0.3651, 'learning_rate': 3.950220308029939e-09, 'epoch': 1.0}
{'loss': 0.3585, 'learning_rate': 3.65220435672442e-09, 'epoch': 1.0}
100%|| 6471/6500 [12:13:50<03:27,  7.16s/it]                                                      100%|| 6471/6500 [12:13:50<03:27,  7.16s/it]100%|| 6472/6500 [12:13:56<03:15,  6.98s/it]                                                      100%|| 6472/6500 [12:13:56<03:15,  6.98s/it]100%|| 6473/6500 [12:14:03<03:05,  6.85s/it]                                                      100%|| 6473/6500 [12:14:03<03:05,  6.85s/it]100%|| 6474/6500 [12:14:09<02:55,  6.77s/it]                                                      100%|| 6474/6500 [12:14:09<02:55,  6.77s/it]100%|| 6475/6500 [12:14:16<02:47,  6.70s/it]                                                      100%|| 6475/6500 [12:14:16<02:47,  6.7{'loss': 0.3581, 'learning_rate': 3.365874747740527e-09, 'epoch': 1.0}
{'loss': 0.3511, 'learning_rate': 3.0912315479914024e-09, 'epoch': 1.0}
{'loss': 0.3893, 'learning_rate': 2.828274821686794e-09, 'epoch': 1.0}
{'loss': 0.3653, 'learning_rate': 2.5770046302830974e-09, 'epoch': 1.0}
{'loss': 0.6313, 'learning_rate': 2.3374210325166625e-09, 'epoch': 1.0}
0s/it]100%|| 6476/6500 [12:14:22<02:39,  6.65s/it]                                                      100%|| 6476/6500 [12:14:22<02:39,  6.65s/it]100%|| 6477/6500 [12:14:29<02:32,  6.62s/it]                                                      100%|| 6477/6500 [12:14:29<02:32,  6.62s/it]100%|| 6478/6500 [12:14:35<02:25,  6.60s/it]                                                      100%|| 6478/6500 [12:14:35<02:25,  6.60s/it]100%|| 6479/6500 [12:14:42<02:18,  6.58s/it]                                                      100%|| 6479/6500 [12:14:42<02:18,  6.58s/it]100%|| 6480/6500 [12:14:49<02:11,  6.57s/it]                                                      100%|| 6480/6500 [12:14:49<02:11,  6.57s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8807358145713806, 'eval_runtime': 1.4765, 'eval_samples_per_second': 8.127, 'eval_steps_per_second': 2.032, 'epoch': 1.0}
                                                      100%|| 6480/6500 [12:14:50<02:11,  6.57s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-6480
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-6480
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-6480
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-6480
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-6480
the pytorch model path isthe pytorch model path isthe pytorch model path is  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-6480/pytorch_model.bin/projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-6480/pytorch_model.bin

 /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-6480/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-6480/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3735, 'learning_rate': 2.109524084381587e-09, 'epoch': 1.0}
{'loss': 0.3609, 'learning_rate': 1.8933138391574732e-09, 'epoch': 1.0}
{'loss': 0.3601, 'learning_rate': 1.6887903473816746e-09, 'epoch': 1.0}
{'loss': 0.3567, 'learning_rate': 1.4959536568492915e-09, 'epoch': 1.0}
{'loss': 0.3688, 'learning_rate': 1.3148038126464813e-09, 'epoch': 1.0}
100%|| 6481/6500 [12:14:58<02:20,  7.42s/it]                                                      100%|| 6481/6500 [12:14:58<02:20,  7.42s/it]100%|| 6482/6500 [12:15:04<02:08,  7.16s/it]                                                      100%|| 6482/6500 [12:15:04<02:08,  7.16s/it]100%|| 6483/6500 [12:15:11<01:58,  6.98s/it]                                                      100%|| 6483/6500 [12:15:11<01:58,  6.98s/it]100%|| 6484/6500 [12:15:18<01:49,  6.85s/it]                                                      100%|| 6484/6500 [12:15:18<01:49,  6.85s/it]100%|| 6485/6500 [12:15:24<01:41,  6.76s/it]                                                      100%|| 6485/6500 [12:15:24<01:41,  6.7{'loss': 0.3648, 'learning_rate': 1.1453408571060476e-09, 'epoch': 1.0}
{'loss': 0.3701, 'learning_rate': 9.875648298518504e-10, 'epoch': 1.0}
{'loss': 0.3649, 'learning_rate': 8.41475767748845e-10, 'epoch': 1.0}
{'loss': 0.3638, 'learning_rate': 7.070737049530429e-10, 'epoch': 1.0}
{'loss': 0.3686, 'learning_rate': 5.843586728782046e-10, 'epoch': 1.0}
6s/it]100%|| 6486/6500 [12:15:31<01:33,  6.70s/it]                                                      100%|| 6486/6500 [12:15:31<01:33,  6.70s/it]100%|| 6487/6500 [12:15:37<01:26,  6.65s/it]                                                      100%|| 6487/6500 [12:15:37<01:26,  6.65s/it]100%|| 6488/6500 [12:15:44<01:19,  6.63s/it]                                                      100%|| 6488/6500 [12:15:44<01:19,  6.63s/it]100%|| 6489/6500 [12:15:50<01:12,  6.61s/it]                                                      100%|| 6489/6500 [12:15:50<01:12,  6.61s/it]100%|| 6490/6500 [12:15:57<01:05,  6.60s/it]                                                      100%|| 6490/6500 [12:15:57<01:05,  6.60s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8824615478515625, 'eval_runtime': 1.4783, 'eval_samples_per_second': 8.118, 'eval_steps_per_second': 2.029, 'epoch': 1.0}
                                                      100%|| 6490/6500 [12:15:58<01:05,  6.60s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-6490
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-6490
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-6490
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-6490
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-6490
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-6490/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-6490/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-6490/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-6490/pytorch_model.bin
/sw/external/python/anaconda3_gpu/lib/python3.9/site-packages/torch/utils/checkpoint.py:434: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/u/bzd2/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.3497, 'learning_rate': 4.733307002069421e-10, 'epoch': 1.0}
{'loss': 0.3479, 'learning_rate': 3.739898128962693e-10, 'epoch': 1.0}
{'loss': 0.3947, 'learning_rate': 2.8633603416095e-10, 'epoch': 1.0}
{'loss': 0.3641, 'learning_rate': 2.1036938449014998e-10, 'epoch': 1.0}
{'loss': 0.6316, 'learning_rate': 1.4608988164188653e-10, 'epoch': 1.0}
100%|| 6491/6500 [12:16:05<01:04,  7.15s/it]                                                      100%|| 6491/6500 [12:16:05<01:04,  7.15s/it]100%|| 6492/6500 [12:16:12<00:55,  6.97s/it]                                                      100%|| 6492/6500 [12:16:12<00:55,  6.97s/it]100%|| 6493/6500 [12:16:18<00:47,  6.84s/it]                                                      100%|| 6493/6500 [12:16:18<00:47,  6.84s/it]100%|| 6494/6500 [12:16:25<00:40,  6.76s/it]                                                      100%|| 6494/6500 [12:16:25<00:40,  6.76s/it]100%|| 6495/6500 [12:16:32<00:33,  6.69s/it]                                                      100%|| 6495/6500 [12:16:32<00:33,  6.6{'loss': 0.3695, 'learning_rate': 9.349754064302829e-11, 'epoch': 1.0}
{'loss': 0.3642, 'learning_rate': 5.259237378374415e-11, 'epoch': 1.0}
{'loss': 0.3503, 'learning_rate': 2.3374390623054355e-11, 'epoch': 1.0}
{'loss': 0.3516, 'learning_rate': 5.8435979999327265e-12, 'epoch': 1.0}
{'loss': 0.3587, 'learning_rate': 0.0, 'epoch': 1.0}
9s/it]100%|| 6496/6500 [12:16:38<00:26,  6.65s/it]                                                      100%|| 6496/6500 [12:16:38<00:26,  6.65s/it]100%|| 6497/6500 [12:16:46<00:20,  6.89s/it]                                                      100%|| 6497/6500 [12:16:46<00:20,  6.89s/it]100%|| 6498/6500 [12:16:52<00:13,  6.79s/it]                                                      100%|| 6498/6500 [12:16:52<00:13,  6.79s/it]100%|| 6499/6500 [12:16:59<00:06,  6.74s/it]                                                      100%|| 6499/6500 [12:16:59<00:06,  6.74s/it]100%|| 6500/6500 [12:17:05<00:00,  6.74s/it]                                                      100%|| 6500/6500 [12:17:05<00:00,  6.74s/it]***** Running Evaluation *****
  Num examples: Unknown
  Batch size = 1
{'eval_loss': 0.8815258741378784, 'eval_runtime': 1.5167, 'eval_samples_per_second': 7.912, 'eval_steps_per_second': 1.978, 'epoch': 1.0}
                                                      100%|| 6500/6500 [12:17:07<00:00,  6.74s/it]Saving model checkpoint to /projects/bbvz/bzd2/checkpoints_experiment_6/tmp-checkpoint-6500
0: main local process completed Renaming model checkpoint folder to true location, releasing all replicas
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-6500
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-6500
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-6500
I AM HERE AND I AM SAVING THE MODEL
the checkpoint model will be saved in  /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-6500
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-6500/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-6500/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-6500/pytorch_model.bin
the pytorch model path is /projects/bbvz/bzd2/checkpoints_experiment_6/checkpoint-6500/pytorch_model.bin


Training completed. Do not forget to share your model on huggingface.co/models =)


{'train_runtime': 44249.6328, 'train_samples_per_second': 9.401, 'train_steps_per_second': 0.147, 'train_loss': 0.5314519335902654, 'epoch': 1.0}
Saving last checkpoint of the model
Saving last checkpoint of the model
Saving last checkpoint of the model
                                                      100%|| 6500/6500 [12:17:07<00:00,  6.74s/it]100%|| 6500/6500 [12:17:08<00:00,  6.80s/it]
Saving last checkpoint of the model
wandb: 
wandb: Run history:
wandb:                      eval/loss 
wandb:                   eval/runtime 
wandb:        eval/samples_per_second 
wandb:          eval/steps_per_second 
wandb:                    train/epoch 
wandb:              train/global_step 
wandb:            train/learning_rate 
wandb:                     train/loss 
wandb:               train/total_flos 
wandb:               train/train_loss 
wandb:            train/train_runtime 
wandb: train/train_samples_per_second 
wandb:   train/train_steps_per_second 
wandb: 
wandb: Run summary:
wandb:                      eval/loss 0.88153
wandb:                   eval/runtime 1.5167
wandb:        eval/samples_per_second 7.912
wandb:          eval/steps_per_second 1.978
wandb:                    train/epoch 1.0
wandb:              train/global_step 6500
wandb:            train/learning_rate 0.0
wandb:                     train/loss 0.3587
wandb:               train/total_flos 2.624767660130304e+18
wandb:               train/train_loss 0.53145
wandb:            train/train_runtime 44249.6328
wandb: train/train_samples_per_second 9.401
wandb:   train/train_steps_per_second 0.147
wandb: 
wandb:  View run FinalRuns-/projects/bbvz/bzd2/checkpoints_experiment_6 at: https://wandb.ai/complex_dnn/huggingface/runs/yjccl13z
wandb:  View job at https://wandb.ai/complex_dnn/huggingface/jobs/QXJ0aWZhY3RDb2xsZWN0aW9uOjExOTYzMDY1Mg==/version_details/v9
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: /projects/bbvz/bzd2/wandb/run-20231213_171835-yjccl13z/logs
/var/spool/slurmd/job2748383/slurm_script: line 167: --save_freq=50: command not found
